{
  "data": [
    {
      "abstract": "Data is mostly stored in digital format rather than hard copy because the former is safer, more secure, smaller in size, and faster to retrieve than the latter. With the increasing number of electronic documents to be organized for users to obtain knowledge and integrate information, document clustering has been applied by grouping textual documents based on their similarities. Many attempts have been made to perform textual document clustering with highly accurate results (i.e., close to nature classes) and high processing performance. However, such proposed techniques work in batch (or static) mode in which performance tend to be sacrificed with the use of all the terms in the document, at times resulting in overlapping or scalability issues. Few studies that focus on dynamic clustering also reported on performance issues. This paper contributes in the investigation of textual document clustering approaches and highlights the importance of using dynamic clustering in mining frequent terms with included named entity. This method is used to achieve high efficiency and high-quality data clustering. The method is also beneficial to be used in textual document clustering algorithms for many text domain applications.  2013 IEEE.",
      "title": "15081 Dynamic semantic textual document clustering using frequent terms and named entity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891139264&partnerID=40&md5=5ea08e54519298cd0851b6045fd8a10f"
    },
    {
      "abstract": "The explosive growth of information stored in unstructured texts created a great demand for new and powerful tools to acquire useful information, such as text mining. Document clustering is one of its the powerful methods and by which document retrieval, organization and summarization can be achieved. However, it represents a challenge when dealing with a big number of data due to high dimensionality of the feature space and to the semantic correlation between features. In this paper, we propose a new sequential document clustering algorithm that uses a statistical and semantic feature selection methods. The semantic process was proposed to improve the frequency mechanism with the semantic relations of the text documents. The proposed algorithm selects iteratively relevant features and performs clustering until convergence. To evaluate its performance, experiments on two corpora have been conducted. The obtained results show that the performance of our algorithm is superior to that obtained by the existing algorithms.  2013 IEEE.",
      "title": "15082 Text clustering using statistical and semantic data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887879774&partnerID=40&md5=c4ecf2cb94ba14d32fab5d1a3c93af2b"
    },
    {
      "abstract": "This paper proposes a text clustering method using the reweighted term based on semantic features for utilizing NFC content. The proposed method uses text document samples of cluster by user to reduce the semantic gap between the user s requirement and clustering results by machine for utilizing NFC access information. The method can enhance the text clustering because it uses the reweighted term which can well represent an inherent structure of text document set relevant to a users requirement regarding NFC tags.",
      "title": "15088 Text clustering using semantic features for utilizing NFC access information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883225824&partnerID=40&md5=9b7ea7775624be8a722f6a14499f29c2"
    },
    {
      "abstract": "The use of naive Bayesian classifier (NB) and the classifier by the k nearest neighbours (kNN) in a classification semantic analysis of authors texts of English fiction has been analyzed. The authors works are considered in the vector space the basis of which is formed by the frequency characteristics of semantic fields of nouns and verbs. Highly precise classification of authors texts in the vector space of semantic fields indicates the presence of particular spheres of the authors idiolect in this space which characterizes the individual authors style.  2013 Taylor & Francis Group, LLC.",
      "title": "15090 Classification analysis of authorship fiction texts in the space of semantic fields",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880262814&partnerID=40&md5=467a42f45d4d30b5375028c8add1a8a6"
    },
    {
      "abstract": "The aim of document clustering is to produce coherent clusters of similar documents. Clustering algorithms rely on text normalisation techniques to represent and cluster documents. Although most document clustering algorithms perform well in specific knowledge domains, processing cross-domain document repositories is still a challenge. This paper attempts to address this challenge. It investigates the performance of the sk-means clustering algorithm across domains, by comparing the cluster coherence produced with semantic-based and traditional (TF-IDF-based) document representations. The evaluation is conducted on 20 different generic sub-domains of a thousand documents, each randomly selected from the Reuters21578 corpus. The experimental results obtained from the evaluation demonstrate improved coherence of clusters produced by using a semantically enhanced text stemmer (SETS), when compared to the text normalisation obtained with the Porter stemmer. In addition, semantic-based text normalisation is shown to be resistant to noise, which is often introduced in the index aggregation stage, a stage that acquires features to represent documents.  2013-IOS Press and the authors. All rights reserved.",
      "title": "15092 Enhanced cross-domain document clustering with a semantically enhanced text stemmer (SETS)",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880246980&partnerID=40&md5=aeb8feaf50b3d7e91cc3037acabf63bd"
    },
    {
      "abstract": "In view of problems of high dimension, the sparsity of information and inconsideration of semantic relation between words of TF-IDF space vector, a method that uses semantic word set as features to reduce dimension and strengthen information density is proposed. This study uses the latent semantic analysis algorithm to obtain the semantic relations between words, and establishes the semantic dictionary by ESD, then we use the word set as features to express text features, and form TCSD combining with the clustering algorithm to cluster the corpus. The experimental results show that the precision rate is 94.29% and the recall rate is 94.28%, which indicate that TCSD performs better than the algorithms that use words as features. Copyright  2013 Binary Information Press.",
      "title": "15097 Uyghur text clustering based on semantic word set",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875767775&partnerID=40&md5=1c057bae5cc58816126673f293d4da4d"
    },
    {
      "abstract": "Despite the availability of large corpora of unannotated biomedical scientific texts, domain machine learning-based systems tend to draw only on comparatively small manually annotated corpora. In this work, we explore opportunities to support supervised machine learning through the use of word representations induced from large unannotated corpora. We evaluate a number of established methods extrinsically, by studying the capacity of induced representations to support machine learning-based natural language processing tasks, specifically named entity recognition on three different corpora and semantic category disambiguation on a large automatically acquired corpus. Experiments demonstrate both a clear benefit of many semantic representations on both tasks and all corpora as well as a strong domain dependence, indicating that semantic representations should be induced on documents drawn from the domain relevant to the supervised learning tasks they aim to support.",
      "title": "15101 Size (and domain) matters: Evaluating semantic word space representations for biomedical text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874260421&partnerID=40&md5=cda894b211b74023874586d09c464942"
    },
    {
      "abstract": "In this article, we explore an application in an area of research called wellbeing informatics. More specifically, we consider how to build a system that could be used for searching stories that relate to the interest of the user (content relevance), and help the user in his or her developmental process by providing encouragement, useful experiences, or otherwise supportive content (emotive relevance). The first objective is covered through topic modeling applying independent component analysis and the second by using sentiment analysis. We also use style analysis to exclude stories that are inappropriate in style. We discuss linguistic theories and methodological aspects of this area, outline a hybrid methodology that can be used in selecting stories that match both the content and emotive criteria, and present the results of experiments that have been used to validate the approach.  2012 Springer-Verlag.",
      "title": "15104 Text mining for wellbeing: Selecting stories using semantic and pragmatic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867686529&partnerID=40&md5=16444b6ca2537a1cdb649887a944cd0c"
    },
    {
      "abstract": "Kernel-based learning algorithms have been proven successful and powerful in many different tasks. However, directly applying them to text classification (TC) field will suffer inherent semantic ambiguity and prohibitive computational costs, which obstruct their practical use in large scale and real time applications with fast testing requirement. To solve this problem, this paper proposes a refined semantic kernel matching pursuit (KMP) approach for fast TC. This approach firstly introduces latent semantic kernel as a new type of dictionary function to KMP, which can deal with high dimensionality, sparsity and ambiguity suffered in text. Moreover, taking the practical issue of data distribution drift with time changing into account, we further propose a method for constructing refined semantic dictionary via concept factorization, which can maintain the updated representative samples in the limited storage to be utilized for various model updating schemes in future. The experimental results demonstrate the proposed method can significantly improve the computational efficiency in predicating phase while preserving considerable performance.  2012 Binary Information Press.",
      "title": "15105 Refined semantic kernel matching pursuit for fast text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868373481&partnerID=40&md5=1f779cb29d30ccd919244b878b26cd62"
    },
    {
      "abstract": "Majority of the existing text classification algorithms are based on the bag of words (BOW) approach, in which the documents are represented as weighted occurrence frequencies of individual terms. However, semantic relations between terms are ignored in this representation. There are several studies which address this problem by integrating background knowledge such as WordNet, ODP or Wikipedia as a semantic source. However, vast majority of these studies are applied to English texts and to the date there are no similar studies on classification of Turkish documents. We empirically analyze the effect of using Turkish Wikipedia (Vikipedi) as a semantic resource in classification of Turkish documents. Our results demonstrate that performance of classification algorithms can be improved by exploiting Vikipedi concepts. Additionally, we show that Vikipedi concepts have surprisingly large coverage in our datasets which mostly consist of Turkish newspaper articles.  2012 IEEE.",
      "title": "15106 Exploiting Turkish Wikipedia as a semantic resource for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866613078&partnerID=40&md5=5a1ab4c2ff88b8daf635d94a797de905"
    },
    {
      "abstract": "Natural Language Generation (NLG) focuses on the generation of written texts in natural language from some underlying semantic representation of information. A new semantic representation called Rich Semantic Graph (RSG) has been proposed to be used as an intermediate representation during recent research for Natural Language processing applications. In this paper, a new model to generate an English text from RSG is proposed. The proposed model can be exploited in Text Summarization, Machine Translation and Information Retrieval applications. In this model, WordNet ontology is used to generate multiple texts according to the word synonyms. Also, the model enables users to determine the output text style by selecting one of two writing styles (Cause-Effect and Description-Narration). Finally, the model evaluates the generated texts to rank them based on two criteria: most frequently used words and discourse sentence relations.  2012 Cairo University.",
      "title": "15107 Rich semantic representation based approach for text generation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864848502&partnerID=40&md5=d5e7b891d4914239413263a7b2b16e81"
    },
    {
      "abstract": "The effect of spam filtering method based on statistics is not good in filtering the new-type spam with synonymous substitution and camouflage. So a new text clustering method based on Semantic Body for filtering Chinese spam is proposed. In this paper, the word sense disambiguation, lexical chain based on HowNet and statistic-based TFIDF are adopted to extract features of mails. The Semantic Body is obtained from the process. The text clustering based on semantic distance is utilized to dealing with Semantic Body. The experimental results under CCERT Chinese-rules.cf show that the proposed approach has a good performance for new type Chinese text spam in filtering.",
      "title": "15111 A novel method of text clustering for Chinese spam based on Semantic Body",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866347583&partnerID=40&md5=d564809bcfa637601ac95f2c14264b94"
    },
    {
      "abstract": "More and more Chinese users express their views through Micro-blogging service during the rapid growth of the Internet today. Clustering these short Chinese texts to get knowledge automatically is an important field for many companies and governments. Because of 140 characters limit, users present any information with only a few words. Considering the view of the user is outstanding and the key words frequency is low, a Chinese short text incremental clustering algorithm based on weighted semantics and Naive Bayes is proposed in this paper. It employs novel weighted semantics and Naive Bayes to compute the similarity between the texts, which effectively addresses the problem of sparse feature vectors. Moreover, clustering the sample incrementally can also reduce the running time. Experimental results show that the algorithm is more accurate and efficient.  2011 by Binary Information Press.",
      "title": "15113 A short Chinese text incremental clustering algorithm based on weighted semantics and Naive Bayes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862670097&partnerID=40&md5=c0de49178fcdabe1196aa7480aecea24"
    },
    {
      "abstract": "Background: Biofuels produced from biomass are considered to be promising sustainable alternatives to fossil fuels. The conversion of lignocellulose into fermentable sugars for biofuels production requires the use of enzyme cocktails that can efficiently and economically hydrolyze lignocellulosic biomass. As many fungi naturally break down lignocellulose, the identification and characterization of the enzymes involved is a key challenge in the research and development of biomass-derived products and fuels. One approach to meeting this challenge is to mine the rapidly-expanding repertoire of microbial genomes for enzymes with the appropriate catalytic properties. Results: Semantic technologies, including natural language processing, ontologies, semantic Web services and Web-based collaboration tools, promise to support users in handling complex data, thereby facilitating knowledge-intensive tasks. An ongoing challenge is to select the appropriate technologies and combine them in a coherent system that brings measurable improvements to the users. We present our ongoing development of a semantic infrastructure in support of genomics-based lignocellulose research. Part of this effort is the automated curation of knowledge from information on fungal enzymes that is available in the literature and genome resources. Conclusions: Working closely with fungal biology researchers who manually curate the existing literature, we developed ontological natural language processing pipelines integrated in a Web-based interface to assist them in two main tasks: mining the literature for relevant knowledge, and at the same time providing rich and semantically linked information.  2012 Meurs et al.",
      "title": "15114 Semantic text mining support for lignocellulose research",
      "url": "Outro artigo de outra conferÃªncia foi considerado duplicado deste."
    },
    {
      "abstract": "This paper discusses an algorithm for identifying semantic arguments of a verb, word senses of a polysemous word, noun phrases in a sentence. The heart of the algorithm is a probabilistic graphical model. In contrast with other existed graphical models, such as Naive Bayes models, CRFs, HMMs, and MEMMs, this model determines a sequence of optimal class assignments among M choices for a sequence of N input symbols without using dynamic programming, running fast-O(MN), and taking less memory space-O(M). Experiments conducted on standard data sets show encourage results.  2012 Springer-Verlag.",
      "title": "15116 Developing an algorithm for mining semantics in texts",
      "url": " New Delhi"
    },
    {
      "abstract": "Text mining and ontology learning can be effectively employed to acquire the Chinese semantic information. This paper explores a framework of semantic text mining based on ontology learning to find the potential semantic knowledge from the immensity text information on the Internet. This framework consists of four parts: Data Acquisition, Feature Extraction, Ontology Construction, and Text Knowledge Pattern Discovery. Then the framework is applied into an actual case to try to find out the valuable information, and even to assist the consumers with selecting proper products. The results show that this framework is reasonable and effective.  2012 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).",
      "title": "15118 A framework of Chinese semantic text mining based on ontology learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863165529&partnerID=40&md5=df188a732a63abab085f6900adecc097"
    },
    {
      "abstract": "Sparse representation originating from signal compressed sensing theory has attracted increasing interest in computer vision research community. In this paper, we present a novel non-parametric feature selection method based on sparse representation in text classification. In order to solve the problem of polysems and synonyms in VSM, we construct semantic structure to represent document with PLSA. Motivated by the fact that kernel trick can capture the nonlinear similarity of features, which may reduce the feature quantization error, we propose Empirical Kernel Sparse Representation (EKSR). We apply EKSR to reconstruct weight vector between samples, then design evaluating mechanism CKernel Sparsity Score (KSS) to select excellent feature subset. As the natural discriminative power of EKSR, KSS can find Agood@ feature which preserves the original structure with less information loss. The results of experiment both on English and Chinese dataset demonstrate the effectiveness of the proposed method.  2012 Asian Network for Scientific Information.",
      "title": "15119 Kernel sparse feature selection based on semantics in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863132144&partnerID=40&md5=c1f2b9adfd92d5ce036ab8596765b87a"
    },
    {
      "abstract": "Data clustering is a powerful technique for discovering knowledge from textual documents. In this field, K-means family algorithms have many applications because of simplicity and high speed in clustering of large scale data. In these algorithms, the criterion of cosine similarity only measures the pairwise similarity of documents that it doesnt have fine operation whenever the clusters are not properly separated. On the contrary, the concepts of Neighbors and Link with the spot of general information in calculating of closeness rate of two documents, in addition to pairwise similarity between them, have better operation. In this model, semantic relations between words have been ignored and only documents with the same terms have been clustered together. This study uses WordNet Ontology for making new model of documents representation that semantic relations between words for reweighing words frequency in documents vector space model, have been used and then Neighbors and Link concepts applied to this model. Results of using the proposed method (Semantic Neighbors) on real-world text data show better operation than previous methods and more efficient in text document clustering.  2011 Academic Journals Inc.",
      "title": "15121 Text document clustering using semantic neighbors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84857811780&partnerID=40&md5=505c3363facaf9305fa18f85f197e753"
    },
    {
      "abstract": "Since the semantic relationship between words is neglected, the results of the text clustering algorithms that only use word frequency are not precision. In this paper, a semantic tree based text clustering algorithm which is based on WordNet is proposed. In order to reduce the time complexity, we adopt parallel algorithm in multi-processes model. This parallel algorithm starts some processes at the same time. The master process undertakes the task of data partitioning, sending information, collecting information and clustering the result. The slave processes basically are in charge of statistics of word frequency, calculating the weights and getting hypernyms of some words according to the semantic tree. The results of experiment show that this algorithm is not only higher in precision, but also with lower time complexity.  2011 AICIT.",
      "title": "15123 Research on the parallel text clustering algorithm based on the semantic tree",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869453273&partnerID=40&md5=b84b612baced185d08bf6bd88567c1d2"
    },
    {
      "abstract": "In this paper, we report our attempt to evaluate the degree of online collaboration with quantitative analysis. Since most online collaborations are carried out through natural language, such quantitative analysis must rely on natural language processing. Here we discuss our work in analyzing an online social visualization web site using the semantic similarity measures of collaborative textual snippets. We use word-based textual similarity measures to study the relationships among user comments. We also detect and visualize the patterns of user collaborations.  2011 IEEE.",
      "title": "15125 Mining collaboration through textual semantic interpretation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863040158&partnerID=40&md5=54a73eb3f85af91281dd7d204e36b8c4"
    },
    {
      "abstract": "Internet is a huge repository of disparate information growing at an exponential rate. Efficient and effective document retrieval and classification systems are required to turn the massive amount of data into useful information, and eventually into knowledge. A traditional approach to document classification requires labelled data in order to construct reliable and accurate classifiers. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. The Semantic based cross-domain classification by providing the algorithm in the extended vector space model of in-domain and out-of-domain documents. Semantic information was embedded within the document representation, and proved via experimentation that improved classification accuracy can be achieved. The concepts form individual features, with undergoing stemming, or splitting of multi-word expressions.",
      "title": "15127 Cross-domain text classification using semantic based approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856165936&partnerID=40&md5=d82eaf86f957639b00928bf437a870d3"
    },
    {
      "abstract": "A number of powerful kernel-based learning machines, such as support vector machines (SVMs), kernel Fisher discriminant analysis (KFDA), have been proposed with competitive performance. However, directly applying existing attractive kernel approaches to text classification (TC) task will suffer semantic related information deficiency and incur huge computation costs hindering their practical use in numerous large scale and real-time applications with fast testing requirement. To tackle this problem, this paper proposes a novel semantic kernel-based framework for efficient TC which offers a sparse representation of the final optimal prediction function while preserving the semantic related information in kernel approximate subspace. Experiments on 20-Newsgroup dataset demonstrate the proposed method compared with SVM and KNN (K-nearest neighbor) can significantly reduce the computation costs in predicating phase while maintaining considerable classification accuracy.  2011 Springer-Verlag.",
      "title": "15128 Efficient semantic kernel-based text classification using matching pursuit KFDA",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-81855177430&partnerID=40&md5=06af05a8907dd5d3f9e195cc0048ced6"
    },
    {
      "abstract": "Typically, in textual document classification the documents are represented in the vector space using the Bag of Words (BOW) approach. Despite its ease of use, BOW representation cannot handle word synonymy and polysemy problems and does not consider semantic relatedness between words. In this paper, we overcome the shortages of the BOW approach by embedding a known WordNet-based semantic relatedness measure for pairs of words, namely Omiotis, into a semantic kernel. The suggested measure incorporates the TF-IDF weighting scheme, thus creating a semantic kernel which combines both semantic and statistical information from text. Empirical evaluation with real data sets demonstrates that our approach successfully achieves improved classification accuracy with respect to the standard BOW representation, when Omiotis is embedded in four different classifiers.  2011 Springer-Verlag.",
      "title": "15131 A knowledge-based semantic kernel for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053966832&partnerID=40&md5=269747c5a74d6e6c00838e6dec93ded4"
    },
    {
      "abstract": "This paper presents CORE (COnnecting REpositories), a system that aims to facilitate the access and navigation across scientific papers stored in Open Access repositories. This is being achieved by harvesting metadata and full-text content from Open Access repositories, by applying text mining techniques to discover semanticly related articles and by representing and exposing these relations as Linked Data. The information about associations between articles expressed in an interoperable format will enable the emergence of a wide range of applications. The potential of CORE can be demonstrated on two use-cases: (1) Improving the the navigation capabilities of digital libraries by the means of a CORE pluging, (2) Providing access to digital content from smart phones and tablet devices by the means of the CORE Mobile application.  2011 Springer-Verlag.",
      "title": "15132 Connecting repositories in the open access domain using text mining and semantic data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053012031&partnerID=40&md5=3bc859f030df2a1e4bbd647d62d2091b"
    },
    {
      "abstract": "Popular Chinese text classification algorithms are mostly based on word frequency statistics features, ignoring the characteristics of Chinese text between the semantic relevance. To further improve the Chinese text classification results, the paper presents a new semantic-based kernel of SVM algorithm for Chinese text classification, through simple idea and smaller implementation costs. Experiments show that compared with traditional SVM algorithm, the algorithm in the Chinese text classification efficiency and accuracy has significantly improved, with good classification results.  2011 IEEE.",
      "title": "15133 A new SVM Chinese text of classification algorithm based on the semantic kernel",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052947558&partnerID=40&md5=bcc412e40b9dbc192c7084beaa115fcf"
    },
    {
      "abstract": "The text clustering based on Vector Space Model has problems, such as high-dimensional and sparse, unable to solve synonym and polyseme etc. And meanwhile, k-means clustering algorithm has shortcomings, which depends on the initial clustering center and needs to fix the number of clusters in advance. Aiming at these problems, in this paper, a text clustering algorithm based on Latent Semantic Analysis and Optimization is proposed. This algorithm can not only overcome the problems of Vector Space Model, but also can avoid the shortcomings of k-means algorithm. And compared with the text clustering algorithm based on Latent Semantic Analysis and the text clustering algorithm based on Vector Space Model and optimization, our algorithm is proved which can preferably improve the effect of text clustering, and upgrade the precision ratio and recall ration of text.  2011 IEEE.",
      "title": "15134 Research on the text clustering algorithm based on latent semantic analysis and optimization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051912415&partnerID=40&md5=6b04c66af44677e048b0053fbd156e95"
    },
    {
      "abstract": "In this paper we introduce and analyze two improvements to GDClust [1], a system for document clustering based on the co-occurrence of frequent subgraphs. GDClust (Graph-Based Document Clustering) works with frequent senses derived from the constraints provided by the natural language rather than working with the co-occurrences of frequent keywords commonly used in the vector space model (VSM) of document clustering. Text documents are transformed to hierarchical document-graphs, and an efficient graph-mining technique is used to find frequent subgraphs. Discovered frequent subgraphs are then utilized to generate accurate sense-based document clusters. In this paper, we introduce two novel mechanisms called the Subgraph-Extension Generator (SEG) and the Maximum Subgraph-Extension Generator (MaxSEG) which directly utilize constraints from the natural language to reduce the number of candidates and the overhead imposed by our first implementation of GDClust.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "15135 Semantically-guided clustering of text documents via frequent subgraphs discovery",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960130454&partnerID=40&md5=b1f5bb137bde48b61a764a89aa5ca886"
    },
    {
      "abstract": "The aim of computational semantics is to capture the meaning of natural language expressions in representations suitable for performing inferences, in the service of understanding human language in written or spoken form. First-order logic is a good starting point, both from the representation and inference point of view. But even if one makes the choice of first-order logic as representation language, this is not enough: the computational semanticist needs to make further decisions on how to model events, tense, modal contexts, anaphora and plural entities. Semantic representations are usually built on top of a syntactic analysis, using unification, techniques from the lambda-calculus or linear logic, to do the book-keeping of variable naming. Inference has many potential applications in computational semantics. One way to implement inference is using algorithms from automated deduction dedicated to first-order logic, such as theorem proving and model building. Theorem proving can help in finding contradictions or checking for new information. Finite model building can be seen as a complementary inference task to theorem proving, and it often makes sense to use both procedures in parallel. The models produced by model generators for texts not only show that the text is contradiction-free",
      "title": "15139 A Survey of Computational Semantics: Representation, Inference and Knowledge in Wide-Coverage Text Understanding",
      "url": ""
    },
    {
      "abstract": "In this paper, we present models for mining text relations between named entities, which can deal with data highly affected by linguistic noise. Our models are made robust by: (a) the exploitation of state-of-the-art statistical algorithms such as support vector machines (SVMs) along with effective and versatile pattern mining methods, e. g. word sequence kernels",
      "title": "15140 Supervised semantic relation mining from Linguistically noisy text documents",
      "url": "Article"
    },
    {
      "abstract": "To represent the textual knowledge more expressively, a kind of semantic-based graph structure is proposed, in which more semantic and ordering information among terms as well as the structural information of the text are incorporated. Such model can be constructed by extracting representative terms from texts and their mutually semantic relationships. Afterward, it is represented as a graph, whose nodes are the selected terms and whose edges are the corresponding relationships respectively. Moreover, the weight is assigned to each edge so that the strength of relationship between two terms can be measured. Furthermore, for this weighted directed graph structure, a novel graph similarity algorithm is developed by extracting the maximum common subgraph between two concerned graphs, which can therefore be used to measure the distance between two graph structures, i.e., two texts, and further be applied to classification tasks. Finally, some experiments have been conducted with the Chinese benchmark corpus for validation. The experimental results have proved the better performance of the proposed textual knowledge representation model in terms of its precision and recall.  2011.",
      "title": "15142 Enhancing text representation for classification tasks with semantic graph structures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79956126342&partnerID=40&md5=30bc4497b392b2787a31c7f2c8d3cc24"
    },
    {
      "abstract": "Dealing with the large-scale text knowledge on the Web has become increasingly important with the development of the Web, yet it confronts with several challenges, one of which is to find out as much semantics as possible to represent text knowledge. As the text semantic mining process is also the knowledge representation process of text, this paper proposes a text knowledge representation model called text semantic mining model (TSMM) based on the algebra of human concept learning, which both carries rich semantics and is constructed automatically with a lower complexity. Herein, the algebra of human concept learning is introduced, which enables TSMM containing rich semantics. Then the formalization and the construction process of TSMM are discussed. Moreover, three types of reasoning rules based on TSMM are proposed. Lastly, experiments and the comparison with current text representation models show that the given model performs better than others. Copyright  2011, IGI Global.",
      "title": "15143 Text semantic mining model based on the algebra of human concept learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054912044&partnerID=40&md5=ad32e912e9a9a4d7021e8eb51aebaeed"
    },
    {
      "abstract": "In this paper, a novel Chinese text feature selection algorithm based on Probability latent semantic analysis (PLSA) was presented for text classification. The algorithm first employs the Expectation-maximization method (EM) to calculate the correlations between words and the latent topics for every category documents. It then selects feature words for each latent topics and merge those words to describe the corresponding category documents. At last, it merges all feature words of every category into classification feature words. An empirical comparison with other four effective feature selection methods on a benchmark data is presented in this paper. The results show that this method could get the best classification performance.",
      "title": "15144 A novel feature selection method based on probability latent semantic analysis for chinese text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79958141957&partnerID=40&md5=d7ba73a11c75f1e0f1b7692143e28ce0"
    },
    {
      "abstract": "The paper presents a graph-based, shallow semantic analysis-driven approach for modeling document contents. This allows to extract additional information about meaning of text and effects in improved document classification. Its performance is compared against the legacy bag-of-words and Schenker et al. approaches with k - NN classification based on Polish and English news articles.  2010 IEEE.",
      "title": "15146 Is shallow semantic analysis really that shallow? A study on improving text classification performance",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79551553881&partnerID=40&md5=f442d08d07778385534329ddd05bcd8a"
    },
    {
      "abstract": "This paper presents a system that performs automatic semantic-based text categorization. Using Princeton WordNet, a series of induced methods were implemented that extract semantic features from text and utilize them to decide how similar a document is to different topics. In addition, a bag-of-words method incorporating no knowledge from WordNet is implemented in the system as a basis to compare different WordNet-based approaches. This paper describes the system and reports on a simple analysis performed to evaluate the different implemented methods. At the end, a discussion on the limitations of this study and the future work to optimize the system is presented.  2010 IEEE.",
      "title": "15147 A semantic-based text classification system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960356638&partnerID=40&md5=b19c0b925bd57989d8c17604c5d8c517"
    },
    {
      "abstract": "The Semantic Text Analysis Tool (STAT) helps analysts at the NASA Johnson Space Center review discrepancy reports by turning unstructured technical text into useful structured data.  2010 IEEE.",
      "title": "15149 Semantic annotation of aerospace problem reports to support text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649412822&partnerID=40&md5=06f37af731c2b999f43251e1de1f8203"
    },
    {
      "abstract": "Growing Self Organizing Map (GSOM) has proven benefits in text clustering. Latent Semantic Analysis (LSA) also has been used in text clustering to capture the latent concepts from text. This paper presents a novel combination of GSOM and LSA to improve text clustering results compared to using GSOM on its own. LSA is an inherently global algorithm that looks at trends and patterns globally and GSOM is a nearest neighborhood based algorithm which looks at local patterns. Combination of these two can be used to discover both the global and local patterns. In the proposed model, initial text corpus is converted into its vector space representation using the traditional Term Frequency - Inverse Document Frequency (TF-IDF) technique. Then the Singular Value Decomposition (SVD) followed by Frobenius norm is applied on the resulting high dimensional vector to come up with a new vector with an optimal number of dimensions. Experiments using the proposed model were conducted and compared with the original GSOM under the same conditions. Experiment results demonstrate that the new combination of these well known techniques enhances the accuracy of clustering results and the computational time than the GSOM alone.  2010 IEEE.",
      "title": "15150 Enhancing GSOM text clustering with Latent Semantic Analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952759560&partnerID=40&md5=3a987f6570f97dcd1ce98ed073661338"
    },
    {
      "abstract": "In this paper, we present a knowledge based approach to capture semantic representations from text for intelligent systems that know the representations of interest in advance. Our approach performs this task by generating phrases from these representations and then matching these phrases against text using a set of syntactic and semantic transformations. The representation that best matches a piece of text is selected as its meaning. We evaluate our approach in the context of a real-world intelligent system that tracks the maturity of wireless technologies, and show how our approach performs well on capturing semantic representations from text. Copyright  2010 Inderscience Enterprises Ltd.",
      "title": "15152 A knowledge based approach for capturing rich semantic representations from text for intelligent systems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-82155176270&partnerID=40&md5=7989fda1cd90c958d466cfa11773f176"
    },
    {
      "abstract": "Text classification is a widely studied topic in the area of machine learning. A number of techniques have been developed to represent and classify text documents. Most of the techniques try to achieve good classification performance while taking a document only by its words (e.g. statistical analysis on word frequency and distribution patterns). One of the recent trends in text classification research is to incorporate more semantic interpretation in text classification, especially by using Wikipedia. This paper introduces a technique for incorporating the vast amount of human knowledge accumulated in Wikipedia into text representation and classification. The aim is to improve classification performance by transforming general terms into a set of related concepts grouped around semantic themes. In order to achieve this goal, this paper proposes a unique method for breaking the enormous amount of extracted Wikipedia knowledge (concepts) into smaller pieces (subsets of concepts). The subsets of concepts are separately used to represent the same set of documents in a number of different ways, from which an ensemble of classifiers is built. Experimental results show that an ensemble of classifiers individually trained on a different representation of the document set performs better with increased accuracy and stability than that of a classifier trained only on the original document set. 2010 IEEE.",
      "title": "15153 Semantic enrichment of text representation with wikipedia for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78751492151&partnerID=40&md5=a5235230c39f34d15e1ec4072b7bdc76"
    },
    {
      "abstract": "Text categorization is one of the most common themes in data mining and machine learning fields. Unlike structured data, unstructured text data is more complicated to be analyzed because it contains too much information, e.g., syntactic and semantic. In this paper, we propose a semantics-based model to represent text data in two levels. One level is for syntactic information and the other is for semantic information. Syntactic level represents each document as a term vector, and the component records tf-idf value of each term. The semantic level represents document with Wikipedia concepts related to terms in syntactic level. The syntactic and semantic information are efficiently combined by our proposed multi-layer classification framework. Experimental results on benchmark dataset (Reuters-21578) have shown that the proposed representation model plus proposed classification framework improves the performance of text classification by comparing with the flat text representation models (term VSM, concept VSM, term+concept VSM) plus existing classification methods.  2010 Springer-Verlag.",
      "title": "15154 Semantics-based representation model for multi-layer text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78449243096&partnerID=40&md5=86d76d45363bbe6aa62e6c03a048384b"
    },
    {
      "abstract": "The mining methods for comment text polarity are usually used to adopted supervised learning algorithms, but supervised learning algorithms require significant manual labor marked the training set, and its text set in dealing with will be also faced with dimension disaster, sparse vector, high spatial and temporal complexity, low recall and precision rates that cannot be used for a flood of text polarity classification task. In response to these circumstances, this article will introduce a new polarity clustering algorithm for text of the Chinese commentary, constructed specifically for the Chinese comment on the polarity of the text polarities dictionary meaning of words, a criterion function based on semantic means clustering K-means algorithm. The study is the use of semantic clustering method based on Chinese texts deal with a subjective exploration. The methodologies of experiment, statistics, and analysis are used to do this research. The results of experiment showed that average recall rate of 81.22%, average accuracy rate of 67.76%, indicating that the algorithm is feasible and effective.  2010 IEEE.",
      "title": "15155 A new polarity clustering algorithm based on semantic criterion function for text of the Chinese commentary",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149297910&partnerID=40&md5=bc8fefb3c89bca731a3bfd553646e5e0"
    },
    {
      "abstract": "Feature selection and weighting is of vital concern in text classification process which improves the efficiency and accuracy of text classifier. Vector Space Model is used to represent the documents using Bag of Word BOW model with term weighting phenomena. Documents representation through this model has some limitations that are, ignoring term dependencies, structure and ordering of the terms in documents. To overcome this problem, Semantics Base Feature Vector using Part of Speech (POS), is proposed, which is used to extract the concept of terms using WordNet, co-occurring and associated terms. The proposed method is applied on small documents dataset which shows that this method outperforms then term frequency/ inverse document frequency (TF-IDF) with BOW feature selection method for text classification.  2010 IEEE.",
      "title": "15156 Semantic based features selection and weighting method for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78049381625&partnerID=40&md5=37b66caac42d47d4c134d3d42bc7e928"
    },
    {
      "abstract": "The last two decades of invigorating research in the area of human genome sequencing marked the beginning of large-scale data collection. Much of the valuable knowledge gained is found in published articles, and thus in un-structured textual form. To aid in searching and extracting knowledge from textual sources, we present BioEve, a fully automated system to extract bio-molecular events from Medline abstracts. BioEve first semantically classifies each sentence to the class type of the event mentioned in the sentence, and then using high coverage, class-specific, hand-crafted rules, it extracts the participants of that event. Copyright  2010 ACM.",
      "title": "15159 Semantic classification and dependency parsing enabled automated bio-molecular event extraction from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958066309&partnerID=40&md5=1fb3ce12d30e02198fd233577025fc9e"
    },
    {
      "abstract": "The problem of semantic clustering of subject-oriented natural language texts is discussed. Based on the theory of Formal Concept Analysis, an approach is proposed to determine the measure of semantic affinity of texts.  2010 Pleiades Publishing, Ltd.",
      "title": "15160 Semantic Clustering and Affinity Measure of Subject-Oriented Language Texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956591909&partnerID=40&md5=dd1067e8bd59d6675058be91b01a69b8"
    },
    {
      "abstract": "The document clustering is an important technique of Natural Language Processing (NLP). The paper presents performance of partitional and agglomerative algorithms applied to clustering large number of Polish newspaper articles. We investigate different representations of the documents. The focus of the paper is on the applicability of the Latent Semantic Analysis to such clustering for Polish.  2010 Springer-Verlag.",
      "title": "15161 Clustering Polish texts with latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955463386&partnerID=40&md5=e6a69efe2b34bde9733fe35109ed8944"
    },
    {
      "abstract": "On large scale dataset, the effect of automatic text classification is now still far from perfect. Its a common agreement that more sufficient text semantic meaning be adopted in text representation to deal with the challenge. This paper introduces semantic meaning of coreference in and to improve traditional BOW representation. The result of text classification experiment shows that, contrasted with traditional BOW representation, the improved model increases the discernment to positive instances. And that the classification performance of the new BOW representation model is no less good than that of stemmed BOW representation model.  2010 IEEE.",
      "title": "15163 Use semantic meaning of coreference to improve classification text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954735695&partnerID=40&md5=c6f6a6770ce30993e6db96664df22c7c"
    },
    {
      "abstract": "Since a decade, text categorization has become an active field of research in the machine learning community. Most of the approaches are based on the term occurrence frequency. The performance of such surface-based methods can decrease when the texts are too complex, i.e., ambiguous. One alternative is to use the semantic-based approaches to process textual documents according to their meaning. In this paper, we propose a Concept-based Vector Space Model which reflects the more abstract version of the semantic information instead of the Vector Space Model for the text. This model adjusts the weight of the Vector Space by importing the hypernymy-hyponymy relation between synonymy sets and the Concept Chain in the WordNet. Experimental results on several data sets show that the proposed approach, conception built from Wordnet, can achieve significant improvements with respect to the baseline algorithm.  2010 Springer-Verlag.",
      "title": "15164 Extract semantic information from WordNet to improve text classification performance",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954595538&partnerID=40&md5=88434dbbb833750d6b49c1456178d7f9"
    },
    {
      "abstract": "Text mining is one promising way of extracting information automatically from the vast biological literature. To maximize its potential, the knowledge encoded in the text should be translated to some semantic representation such as entities and relations, which could be analyzed by machines. But large-scale practical systems for this purpose are rare. We present BeeSpace question/answering (BSQA) system that performs integrated text mining for insect biology, covering diverse aspects from molecular interactions of genes to insect behavior. BSQA recognizes a number of entities and relations in Medline documents about the model insect, Drosophila melanogaster. For any text query, BSQA exploits entity annotation of retrieved documents to identify important concepts in different categories. By utilizing the extracted relations, BSQA is also able to answer many biologically motivated questions, from simple ones such as, which anatomical part is a gene expressed in, to more complex ones involving multiple types of relations. BSQA is freely available at http://www .beespace.uiuc.edu/QuestionAnswer.  The Author(s) 2010. Published by Oxford University Press.",
      "title": "15166 BSQA: Integrated text mining using entity relation semantics extracted from biological literature of insects",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954265330&partnerID=40&md5=6b0f67da7eb30090bb8bd9eb37536978"
    },
    {
      "abstract": "In this study, we examine and validate the use of existing text mining techniques (based on the vector space model and latent semantic indexing) to detect similarities between patent documents and scientific publications. Clearly, experts involved in domain studies would benefit from techniques that allow similarity to be detected-and hence facilitate mapping, categorization and classification efforts. In addition, given current debates on the relevance and appropriateness of academic patenting, the ability to assess content-relatedness between sets of documents-in this case, patents and publications-might become relevant and useful. We list several options available to arrive at content based similarity measures. Different options of a vector space model and latent semantic indexing approach have been selected and applied to the publications and patents of a sample of academic inventors (n = 6). We also validated the outcomes by using independently obtained validation scores of human raters. While we conclude that text mining techniques can be valuable for detecting similarities between patents and publications, our findings also indicate that the various options available to arrive at similarity measures vary considerably in terms of accuracy: some generally accepted text mining options, like dimensionality reduction and LSA, do not yield the best results when working with smaller document sets. Implications and directions for further research are discussed.  2009 Budapest, Hungary.",
      "title": "15168 Exploring the feasibility and accuracy of Latent Semantic Analysis based text mining techniques to detect similarity between patent documents and scientific publications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952881605&partnerID=40&md5=be8e571de03c4c7a88364dcb0ea455e5"
    },
    {
      "abstract": "In view of ignoring semantic relationship between words, high dimensionality of data and computational complexity when current text clustering algorithms deal with Chinese texts. This paper presents a new method to cluster Chinese texts based on semantics in a specific field-TCBS (Text Clustering Based on Semantics) algorithm. The algorithm is based on the agglomerative hierarchical clustering algorithm, it expresses Chinese texts with the characteristic words and sets relative threshold in order to improve the efficiency of clustering. Compared with the traditional algorithms, the experimental results show that TCBS has effectively enhanced the quality of the clustering.  2009 IEEE.",
      "title": "15171 Chinese text clustering method based on semantics and special domain",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77749295721&partnerID=40&md5=96854b9df8d6c3a29a07b4aa500acdb8"
    },
    {
      "abstract": "In Biomedical research, the ability to retrieve the adequate information from the ever growing literature is an extremely important asset. This work provides an enhanced and general purpose approach to the process of document retrieval that enables the filtering of PubMed query results. The system is based on semantic indexing providing, for each set of retrieved documents, a network that links documents and relevant terms obtained by the annotation of biological entities (e.g. genes or proteins). This network provides distinct user perspectives and allows navigation over documents with similar terms and is also used to assess document relevance. A network learning procedure, based on previous work from e-mail spam filtering, is proposed, receiving as input a training set of manually classified documents.  2009 Springer Berlin Heidelberg.",
      "title": "15174 Biomedical text mining applied to document retrieval and semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952577426&partnerID=40&md5=ccea2eb46230883894b00e7cadefdf86"
    },
    {
      "abstract": "The paper presents an approach to text representation for search tasks. Heterogeneous semantic networks are defined and their construction from natural language is described. The success of application of semantic networks in intelligent search engine is shown.",
      "title": "15177 Heterogeneous semantic networks for text representation in intelligent search engine EXACTUS",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887248829&partnerID=40&md5=043924678e75da25d42278e55326d3a8"
    },
    {
      "abstract": "Clustering of short texts, such as snippets, presents great challenges in existing aggregated search techniques due to the problem of data sparseness and the complex semantics of natural language. As short texts do not provide sufficient term occurring information, traditional text representation methods, such as bag of words model, have several limitations when directly applied to short texts tasks. In this paper, we propose a novel framework to improve the performance of short texts clustering by exploiting the internal semantics from original text and external concepts from world knowledge. The proposed method employs a hierarchical three-level structure to tackle the data sparsity problem of original short texts and reconstruct the corresponding feature space with the integration of multiple semantic knowledge bases - Wikipedia and WordNet. Empirical evaluation with Reuters and real web dataset demonstrates that our approach is able to achieve significant improvement as compared to the state-of-the-art methods. Copyright 2009 ACM.",
      "title": "15180 Exploiting internal and external semantics for the clustering of short texts using world knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74549114844&partnerID=40&md5=4f901461f8aae9edb0f4d406fd96fc80"
    },
    {
      "abstract": "We present a generic approach for semantic based classification of text documents to pre-defined categories. The proposed technique is applied to the domain of patent analytics for the purpose of classifying a collection of patent documents to one or many nodes in a user-defined taxonomy. The proposed approach is a multi-step process consisting of noun extraction, word sense disambiguation, semantic relatedness computation between pair of words using WordNet and confidence score computation. The proposed algorithm resulted in good accuracy on experimental dataset and can be easily adapted and customized to other domains other the patent landscape analysis domain discussed in this paper.  2009 Springer.",
      "title": "15182 Semantic based text classification of patent documents to a user-defined taxonomy",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350337347&partnerID=40&md5=225443fb272e7d0e4f8ec40d66dec8e8"
    },
    {
      "abstract": "Most text clustering techniques are based on words and/or phrases weights in the text. Such representation is often unsatisfactory because it ignores the relationships between terms, and considers them as independent features. In this paper, a new semantic similarity based model (SSBM) is proposed. The semantic similarity based model computes semantic similarities by utilizing WordNet as an ontology. The proposed model captures the semantic similarities between documents that contain semantically similar terms but unnecessarily syntactically identical. The semantic similarity based model assigns a new weight to document terms reflecting the semantic relationships between terms that co-occur literally in the document. Our model in conjunction with the extended gloss overlaps measure and the adapted Lesk algorithm solves ambiguity, synonymy problems that are not detected using traditional term frequency based text mining techniques. The proposed model is evaluated on the Reuters-21578 and the 20-Newsgroups text collections datasets. The performance is assessed in terms of the Fmeasure, Purity and Entropy quality measures. The obtained results show promising performance improvements compared to the traditional term based vector space model (VSM) as well as other existing methods that include semantic similarity measures in text clustering.  2009 Springer Berlin Heidelberg.",
      "title": "15183 New semantic similarity based model for text clustering using extended gloss overlaps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350225972&partnerID=40&md5=3b9bd10fca83c4533408c1b6bd2175ca"
    },
    {
      "abstract": "Most text classification systems use bag-of-words representation of documents to find the classification target function. Linguistic structures such as morphology, syntax and semantic are completely neglected in the learning process. This paper proposes a new document representation that, while including its context independent sentence meaning, is able to be used by a structured kernel function, namely the direct product kernel. The proposal is evaluated using a dataset of articles from a Portuguese daily newspaper and classifiers are built using the SVM algorithm. The results show that this structured representation, while only partially describing documents significance has the same discriminative power over classes as the traditional bag-of-words approach.  2009 Springer Berlin Heidelberg.",
      "title": "15184 Using graph-kernels to represent semantic information in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350228914&partnerID=40&md5=a136688befbb07ecd1cda864ebacd97e"
    },
    {
      "abstract": "In text classification, one key problem is its inherent dichotomy of polysemy and synonym",
      "title": "15185 Sprinkled latent semantic indexing for text classification with background knowledge",
      "url": "Conference Paper"
    },
    {
      "abstract": "In many contexts today, documents are available in a number of versions. In addition to explicit knowledge that can be queried/searched in documents, these documents also contain implicit knowledge that can be found by text mining. In this paper we will study association rule mining of temporal document collections, and extend previous work within the area by 1) performing mining based on semantics as well as 2) studying the impact of appropriate techniques for ranking of rules.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "15189 Semantic-Based temporal text-rule mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650555929&partnerID=40&md5=dda6ee2ad6baa2cbd61a611b23004ab2"
    },
    {
      "abstract": "We describe a semantic clustering method designed to address shortcomings in the common bag-of-words document representation for functional semantic classification tasks. The method uses WordNet-based distance metrics to construct a similarity matrix, and expectation maximization to find and represent clusters of semantically related terms. Using these clusters as features for machine learning helps maintain performance across distinct, domain-specific vocabularies while reducing the size of the document representation. We present promising results along these lines, and evaluate several algorithms and parameters that influence machine learning performance. We discuss limitations of the study and future work for optimizing and evaluating the method.  2009 American Institute of Physics.",
      "title": "15190 Semantic clustering for a functional text classification task",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650541343&partnerID=40&md5=dcdcabfe599b221199b96d19dfe613cd"
    },
    {
      "abstract": "A novel approach is proposed in this paper to aid real-time enterprise ontology evolution in a continuous fashion. Automatic semantic aliasing (ASA) and text mining (TM) are the two collaborating mechanisms (together known as ASA&TM) that support this approach. The text miner finds new knowledge items from open sources (e.g. the web or given repertoires), and the ASA mechanism associates all the canonical knowledge items in the ontology and those found by text mining via their degrees of similarity. Real-time enterprise ontology evolution makes the host system increasingly smarter because it keeps the host systems ontological knowledge abreast of the contemporary advances. The ASA&TM approach was verified in the Nongs mobile clinics based pervasive TCM (Traditional Chinese Medicine) clinical telemedicine environment. All the experimental results unanimously indicate that the proposed approach is definitively effective for the designated purpose.  2008 Springer Berlin Heidelberg.",
      "title": "15195 Real-time enterprise ontology evolution to aid effective clinical telemedicine with text mining and automatic semantic aliasing support",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049100481&partnerID=40&md5=29c5dce82922bd1dcc5ceaa1d62ed1cf"
    },
    {
      "abstract": "In this paper, we present a knowledge based approach to capture semantic representations from natural language for a class of applications where the representations of interest are known in advance. Our approach performs this task by generating phrases from these representations and matching these phrases against text using a set of syntactic and semantic transformations. The representation that best matches a piece of text is selected as its meaning. We evaluate our approach on a corpus of news articles collected from over 150 online news sources, and show how our approach performs well on capturing semantic representations from text.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "15196 A knowledge based approach for capturing rich semantic representations from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57849169036&partnerID=40&md5=ed8883655e8647f6c2ab5567a866f8e1"
    },
    {
      "abstract": "Most traditional text clustering methods are based on bag of words (BOW) representation based on frequency statistics in a set of documents. BOW, however, ignores the important information on the semantic relationships between key terms. To overcome this problem, several methods have been proposed to enrich text representation with external resource in the past, such as WordNet. However, many of these approaches suffer from some limitations: 1) WordNet has limited coverage and has a lack of effective word-sense disambiguation ability",
      "title": "15197 Enhancing text clustering by leveraging wikipedia semantics",
      "url": ""
    },
    {
      "abstract": "Document classification presents difficult challenges due to the sparsity and the high dimensionality of text data, and to the complex semantics of the natural language. The traditional document representation is a word-based vector (Bag of Words, or BOW), where each dimension is associated with a term of the dictionary containing all the words that appear in the corpus. Although simple and commonly used, this representation has several limitations. It is essential to embed semantic information and conceptual patterns in order to enhance the prediction capabilities of classification algorithms. In this paper, we overcome the shortages of the BOW approach by embedding background knowledge derived from Wikipedia into a semantic kernel, which is then used to enrich the representation of documents. Our empirical evaluation with real data sets demonstrates that our approach successfully achieves improved classification accuracy with respect to the BOW technique, and to other recently developed methods. Copyright 2008 ACM.",
      "title": "15199 Building semantic kernels for text classification using wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-65449130494&partnerID=40&md5=92a75b1d53e35296a148ed23cdd7c4e9"
    },
    {
      "abstract": "Image captions represent manual semantic annotation of images. These act as essential cues to represent the semantics of an image. This paper describes the process of representing, discovering, storing the semantics in a knowledge base, and then applying the semantics to aid the retrieval of visual information. We exploit a Natural Language Processing (NLP) framework in order to extract the knowledge from image captions and to transform those unstructured data into a semantic model. The novelty of the proposed framework is to use a semantic model to find implicit relationships among the concepts of photographs which are not mentioned directly in text captions. Latent Semantic Indexing (LSI) is deployed to handle ontology imperfections. Experiments tested and validated the major hypotheses of this approach.",
      "title": "15203 Semantic representation of text captions to aid sport image retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-66749190718&partnerID=40&md5=a9c2b48d1acc7e32b280e0aaa9af2abc"
    },
    {
      "abstract": "Wikipedia, a collaborative Wiki-based encyclopedia, has become a huge phenomenon among Internet users. It covers huge number of concepts of various fields such as Arts, Geography, History, Science, Sports and Games. Since it is becoming a database storing all human knowledge, Wikipedia mining is a promising approach that bridges the Semantic Web and the Social Web (a. k. a. Web 2.0). In fact, in the previous researches on Wikipedia mining, it is strongly proved that Wikipedia has a remarkable capability as a corpus for knowledge extraction, especially for relatedness measurement among concepts. However, semantic relatedness is just a numerical strength of a relation but does not have an explicit relation type. To extract inferable semantic relations with explicit relation types, we need to analyze not only the link structure but also texts in Wikipedia. In this paper, we propose a consistent approach of semantic relation extraction from Wikipedia. The method consists of three sub-processes highly optimized for Wikipedia mining",
      "title": "15204 Wikipedia link structure and text mining for semantic relation extraction towards a huge scale global web ontology",
      "url": ""
    },
    {
      "abstract": "We present a novel fine-grained semantic representation of text and an approach to constructing it. This representation is largely extractable by todays technologies and facilitates more detailed semantic analysis. We discuss the requirements driving the representation, suggest how it might be of value in the automated tutoring domain, and provide evidence of its validity.  2012, American College of Rheumatology.",
      "title": "15208 Extracting a representation from text for semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859893014&partnerID=40&md5=bda783056d23879ed798aecbf698453b"
    },
    {
      "abstract": "As the common clustering algorithms use vector space model (VSM) to represent document, the conceptual relationships between related terms which do not co-occur literally are ignored. A genetic algorithm-based clustering technique, named GA clustering, in conjunction with ontology is proposed in this article to overcome this problem. In general, the ontology measures can be partitioned into two categories: thesaurus-based methods and corpus-based methods. We take advantage of the hierarchical structure and the broad coverage taxonomy of Wordnet as the thesaurus-based ontology. However, the corpus-based method is rather complicated to handle in practical application. We propose a transformed latent semantic analysis (LSA) model as the corpus-based method in this paper. Moreover, two hybrid strategies, the combinations of the various similarity measures, are implemented in the clustering experiments. The results show that our GA clustering algorithm, in conjunction with the thesaurus based and the LSA-based method, apparently outperforms that with other similarity measures. Moreover, the superiority of the GA clustering algorithm proposed over the commonly used k-means algorithm and the standard GA is demonstrated by the improvements of the clustering performance.",
      "title": "15209 Self-adaptive GA, quantitative semantic similarity measures and ontology-based text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650351509&partnerID=40&md5=54c3249c6310200532d838e9a2afd3c2"
    },
    {
      "abstract": "Document representation is one of the crucial components that determine the effectiveness of text classification tasks. Traditional document representation approaches typically adopt a popular bag-of-word method as the underlying document representation. Although its a simple and efficient method, the major shortcoming of bag-of-word representation is in the independent of word feature assumption. Many researchers have attempted to address this issue by incorporating semantic information into document representation. In this paper, we study the effect of semantic representation on the effectiveness of text classification systems. We employed a novel semantic smoothing technique to derive semantic information in a form of mapping probability between topic signatures and single-word features. Two classifiers, Naive Bayes and Support Vector Machine, were selected to carry out the classification experiments. Overall, our topic-signature semantic representation approaches significantly outperformed traditional bag-of-word representation in most datasets.  2008 IEEE.",
      "title": "15212 Semantic representation in text classification using topic signature mapping",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56349141680&partnerID=40&md5=6e3ca4cb5f2accc2123bfd9dca98d8c7"
    },
    {
      "abstract": "Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "15213 Extracting semantic networks from text via relational clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56049102891&partnerID=40&md5=b7a509f5c67c654e8aeb60eeeb3f6f51"
    },
    {
      "abstract": "Spectral Graph Transducer(SGT) is one of the superior graph-based transductive learning methods for classification. As for the Spectral Graph Transducer algorithm, a good graph representation for data to be processed is very important. In this paper, we try to incorporate Latent Semantic Indexing(LSI) into SGT for text classification. Firstly, we exploit LSI to represent documents as vectors in a latent semantic space since we propose that the documents and their semantic relationships can be reflected more pertinently in this latent semantic space. Then, a graph needed by SGT is constructed. In the graph, a node corresponds to a vector from LSI. Finally, we apply the graph to Spectral Graph Transducer for text classification. The experiments gave us excellent results on both English and Chinese text classification datasets and demonstrated the validation of our assumption. Copyright  2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "15214 Incorporating latent semantic indexing into spectral graph transducer for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-55849114387&partnerID=40&md5=aa4c48d3da9d039756d45401a551452f"
    },
    {
      "abstract": "The aim of this work is to present a new semantic structure of knowledge representation for textual fields. The purpose of this structure is to allow us to handle this kind of textual fields as the rest of the fields in the database in processes such as OLAP, datawarehousing, semantic querying, etc. The architecture of the system is described in the work as well as a detailed description of the mathematical formalism of the structure. The mechanism to carry out the transformation is given together with an experimental example with a medical database.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "15215 A new semantic representation for short texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-53049098881&partnerID=40&md5=d29d70593a2e6c1f2cb7083c77a7f5c7"
    },
    {
      "abstract": "Bayesian text classifiers face a common issue which is referred to as data sparsity problem, especially when the size of training data is very small. The frequently used Laplacian smoothing and corpus-based background smoothing are not effective in handling it. Instead, we propose a novel semantic smoothing method to address the sparse problem. Our method extracts explicit topic signatures (e.g. words, multiword phrases, and ontology-based concepts) from a document and then statistically maps them into single-word features. We conduct comprehensive experiments on three testing collections (OHSUMED, LATimes, and 20NG) to compare semantic smoothing with other approaches. When the size of training documents is small, the bayesian classifier with semantic smoothing not only outperforms the classifiers with background smoothing and Laplacian smoothing, but also beats the state-of-the-art active learning classifiers and SVM classifiers. In this paper, we also compare three types of topic signatures with respect to their effectiveness and efficiency for semantic smoothing. Copyright  by SIAM.",
      "title": "15216 Semantic smoothing for bayesian text classification with small training data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52649083748&partnerID=40&md5=9b7e461d1fee7e4a860a97b490c6d705"
    },
    {
      "abstract": "Semantic Indexing based on Partial Least Squares (SIPLS) is an effective feature extraction method for text classification. SIPLS integrates the global category information Y with the document-class matrix X to create the latent semantic spaces. However, the global latent space may not be the optimal one for each class. To solve this problem, the Local SIPLS (LSIPLS) method is proposed which creates one SIPLS space for each class. Without the influence of global information, the local discriminative components are convenient to be extracted in LSIPLS. Compared with global SIPLS, LSIPLS obtains similar performance with rather compact dimensionality. Empirical results on Reuter corpus prove that LSIPLS is a powerful tool for text classification.",
      "title": "15219 Local semantic indexing based on partial least squares for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-48549098754&partnerID=40&md5=4958000f2457bb559a3c7f87df39046b"
    },
    {
      "abstract": "Current representation schemes for automatic text classification treat documents as syntactically unstructured collections of words or concepts. Past attempts to encode syntactic structure have treated part-of-speech information as another word-like feature, but have been shown to be less effective than non-structural approaches. Here, we investigate three methods to augment semantic modelling with syntactic structure, which encode the structure across all features of the document vector while preserving text semantics. We present classification results for these methods versus the Bag-of-Concepts semantic modelling representation to determine which method best improves classification scores.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "15221 Methods for augmenting semantic models with structural information for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-41849144472&partnerID=40&md5=9bfeb3e595cdd19a5c2f01d79deed6e3"
    },
    {
      "abstract": "We consider the problem of automatic classification of text documents, in particular, scientific abstracts and use two types of classifiers: ordinal and numerical. For the first type we use a fuzzy extension of the Borda voting method while for the second type we use a fuzzy Borda method in combination with the semantic grading.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15222 Multi-attribute text classification using the fuzzy Borda method and semantic grades",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37249020409&partnerID=40&md5=59864101f682dbf4d0ccb692112581c0"
    },
    {
      "abstract": "The exploitation of syntactic structures and semantic background knowledge has always been an appealing subject in the context of text retrieval and information management. The usefulness of this kind of information has been shown most prominently in highly specialized tasks, such as classification in Question Answering (QA) scenarios. So far, however, additional syntactic or semantic information has been used only individually. In this paper, we propose a principled approach for jointly exploiting both types of information. We propose a new type of kernel, the Semantic Syntactic Tree Kernel (SSTK), which incorporates linguistic structures, e.g. syntactic dependencies, and semantic background knowledge, e.g. term similarity based on WordNet, to automatically learn question categories in QA. We show the power of this approach in a series of experiments with a well known Question Classification dataset.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15223 Combined syntactic and semantic kernels for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37149055744&partnerID=40&md5=b998fc66a5f96bfe2cb05ea653bf65af"
    },
    {
      "abstract": "Traditional text classification studied in the information retrieval and machine learning literature is mainly based on topics. That is, each class represents a particular topic, e.g., sports and politics. However, many real-world problems require more refined classification based on some semantic perspectives. For example, in a set of sentences about a disease, some may report outbreaks of the disease, some may describe how to cure the disease, and yet some may discuss how to prevent the disease. To classify sentences at this semantic level, the traditional bag-of-words model is no longer sufficient. In this paper, we study semantic sentence classification of disease reporting. We show that both keywords and sentence semantic features are useful. Our results demonstrated that this integrated approach is highly effective.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15224 Semantic text classification of emergent disease reports",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049166106&partnerID=40&md5=f7858f37117b2fb1d8dd2d83eba5e93a"
    },
    {
      "abstract": "This paper deals with the problem of development and implementation of semantic navigation through Web-content. Multi-agent architecture of a solution for Semantic Web and innovative services are presented. In the context of the proposed solution Web mining is carried out by special OntosMiner agents, which provide the ontology-driven processing of multilingual text collections on the basis of the special kind of content extraction technologies. First evaluation results of the presented solution are discussed as well.  Spnnger-Verlag Berlin Heidelberg 2007.",
      "title": "15225 Ontos solutions for semantic web: Text mining, navigation and analytics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049181026&partnerID=40&md5=c6f6691690206d431f171939024b3a71"
    },
    {
      "abstract": "Semantic frameworks can be used to improve the accuracy and expressiveness of natural language processing for the purpose of extracting meaning from text documents. Such a framework represents knowledge using semantic networks and can be generated using information mined from text documents. The key issue however is to identify relevant concepts and their inter-relationships. In this paper, we have presented a scheme for semantic integration of information extracted from text documents. The extraction principle is based on linguistic and semantic analysis of text. Entities and relations are extracted using Natural Language Processing techniques. A method for collating information extracted from multiple sources to generate the semantic net is also presented. The efficacy of the proposed semantic framework is established through experiments carried out for visualizing information embedded in biomedical texts extracted from PubMed database.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15228 Semantic integration of information through relation mining - Application to bio-medical text processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38149072368&partnerID=40&md5=36b59d5b175f3083bd82412c99d87fc0"
    },
    {
      "abstract": "This chapter introduces a variant of the principle of compositionality in quantitative text semantics as an alternative to the bag-of-features approach. The variant includes effects of context-sensitive interpretation as well as processes of meaning constitution and change in the sense of usage-based semantics. Its starting point is a combination of semantic space modeling and text structure analysis. The principle is implemented by means of a hierarchical constraint satisfaction process which utilizes the notion of hierarchical text structure superimposed by graph-inducing coherence relations. The major contribution of the chapter is a conceptualization and formalization of the principle of compositionality in terms of semantic spaces which tackles some well known deficits of existing approaches. In particular this relates to the missing linguistic interpretability of statistical meaning representations.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15229 Compositionality in quantitative semantics. A theoretical perspective on text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-60449115048&partnerID=40&md5=c04098d3fee4ee2b2d6770ab61b4f72f"
    },
    {
      "abstract": "Clustering text data streams is an important issue in data mining community and has a number of applications such as news group filtering, text crawling, document organization and topic detection and tracing etc. However, most methods are similarity-based approaches and use the TF*IDF scheme to represent the semantics of text data and often lead to poor clustering quality. In this paper, we firstly give an improved semantic smoothing model for text data stream environment. Then we use the improved semantic model to improve the clustering quality and present an online clustering algorithm for clustering massive text data streams. In our algorithm, a new cluster statistics structure, cluster profile, is presented in which the semantics of text data streams are captured. We also present the experimental results illustrating the effectiveness of our technique.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "15231 Clustering massive text data streams by semantic smoothing model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049026484&partnerID=40&md5=ac9ae6387a897b7a52e3ea4cc62798dd"
    },
    {
      "abstract": "In this paper we describe some approaches to text mining, which are supported by an original software system developed in Java for support of information retrieval and text mining (JBowl), as well as its possible use in a distributed environment. The system JBowl1 is being developed as an open source software with the intention to provide an easily extensible, modular framework for pre-processing, indexing and further exploration of large text collections. The overall architecture of the system is described, followed by some typical use case scenarios, which have been used in some previous projects. Then, basic principles and technologies used for service-oriented computing, web services and semantic web services are presented. We further discuss how the JBowl system can be adopted into a distributed environment via technologies available already and what benefits can bring such an adaptation. This is in particular important in the context of a new integrated EU-funded project KP-Lab2 (Knowledge Practices Laboratory) that is briefly presented as well as the role of the proposed text mining services, which are currently being designed and developed there.",
      "title": "15232 Some approaches to text mining and their potential for semantic web applications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51749099398&partnerID=40&md5=c8921cd96f1d346cfb1b708f8b6561a7"
    },
    {
      "abstract": "Temporal expressions - references to points in time or periods of time - are widespread in text, and their proper interpretation is essential for any natural language processing task that requires the extraction of temporal information. Work on the interpretation of temporal expressions in text has generally been pursued in one of two paradigms: the formal semantics approach, where an attempt is made to provide a well-grounded theoretical basis for the interpretation of these expressions, and the more pragmatically-focused approach represented by the development of the TIMEX2 standard, with its origins in work in information extraction. The former emphasises formal elegance and consistency",
      "title": "15233 The semantic representation of temporal expressions in text",
      "url": "20th Australian Joint Conference on Artificial Intelligence, AI 2007, Gold Coast, Australia"
    },
    {
      "abstract": "An important problem with mining textual information is that in this unstructured form is not readily accessible to be used by computers. This has been written for human readers and requires, when feasible, some natural language interpretation. Although full processing is still out of reach with current technology, there are tools using basic pattern recognition techniques and heuristics that are capable of extracting valuable information from free text based on the elements contained in it (i.e., keywords). This technology is usually referred to as Text Mining, and aims at discovering unseen and interesting patterns in textual databases [8], [19].  2007 Springer-Verlag London Limited.",
      "title": "15235 Evolving explanatory novel patterns for semantically-based text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957663649&partnerID=40&md5=f1817d0a672ed1144620efce62443ac5"
    },
    {
      "abstract": "This paper presents a new paradigm for mining documents by exploiting the semantic information of their texts. A formal semantic representation of linguistic inputs is introduced and utilized to build a semantic representation for documents. The representation is constructed through accumulation of syntactic and semantic analysis outputs. A new distance measure is developed to determine the similarities between contents of documents. The measure is based on inexact matching of attributed trees. It involves the computation of all distinct similarity common sub-trees, and can be computed efficiently. It is believed that the proposed representation along with the proposed similarity measure will enable more effective document mining processes. The proposed techniques to mine documents were implemented as components in a mining system. A case study of semantic document clustering is presented to demonstrate the working and the efficacy of the framework. Experimental work is reported, and its results are presented and analyzed.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "15243 Document mining based on semantic understanding of text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845226671&partnerID=40&md5=73a3db9ce801b54812b0160746667859"
    },
    {
      "abstract": "Current text classification systems typically use term stems for representing document content. Semantic Web technologies allow the usage of features on a higher semantic level than single words for text classification purposes. In this paper we propose such an enhancement of the classical document representation through concepts extracted from background knowledge. Boosting, a successful machine learning technique is used for classification. Comparative experimental evaluations in three different settings support our approach through consistent improvement of the results. An analysis of the results shows that this improvement is due to two separate effects.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "15244 Boosting for text classification with semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845221513&partnerID=40&md5=d12bc9237b49ed8ff50648e87a1255a4"
    },
    {
      "abstract": "Ant-based text clustering is a promising technique that has attracted great research attention. This paper attempts to improve the standard ant-based text-clustering algorithm in two dimensions. On one hand, the ontology-based semantic similarity measure is used in conjunction with the traditional vector-space-model-based measure to provide more accurate assessment of the similarity between documents. On the other, the ant behavior model is modified to pursue better algorithmic performance. Especially, the ant movement rule is adjusted so as to direct a laden ant toward a dense area of the same type of items as the ants carrying item, and to direct an unladen ant toward an area that contains an item dissimilar with the surrounding items within its Moore neighborhood. Using WordNet as the base ontology for assessing the semantic similarity between documents, the proposed algorithm is tested with a sample set of documents excerpted from the Reuters-21578 corpus and the experiment results partly indicate that the proposed algorithm perform better than the standard ant-based text-clustering algorithm and the k-means algorithm.  Systems Engineering Society of China and Springer 2006.",
      "title": "15245 A modified ant-based text clustering algorithm with semantic similarity measure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845629466&partnerID=40&md5=b9532b3a4634ad58cd84ea690d0a0f70"
    },
    {
      "abstract": "In this paper, we present an approach for classifying documents based on the notion of a semantic similarity and the effective representation of the content of the documents. The content of a document is annotated and the resulting annotation is represented by a labeled tree whose nodes and edges are represented by concepts lying within a domain ontology. A reasoning process may be carried out on annotation trees, allowing the comparison of documents between each others, for classification or information retrieval purposes. An algorithm for classifying documents with respect to semantic similarity and a discussion conclude the paper.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "15247 A proposal for annotation, semantic similarity and classification of textual documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750250279&partnerID=40&md5=fe5eeb7c28629bdd386c4b4129026652"
    },
    {
      "abstract": "This paper proposes an effective scoring scheme for feature selection in Text Mining, using characteristics of Small-World Phenomenon on the semantic networks of documents. Our focus is on the reservation of both syntactic and statistical information of words, rather than solely simple frequency summarization in prevailing scoring schemes, such as TFIDF. Experimental results on TREC dataset show that our scoring scheme outperforms the prevailing schemes.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "15249 Semantic scoring based on small-world phenomenon for feature selection in text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749392702&partnerID=40&md5=5389deb91548208dfdfafdeda556b408"
    },
    {
      "abstract": "Objective: Acquiring and representing biomedical knowledge is an increasingly important component of contemporary bioinformatics. A critical step of the process is to identify and retrieve relevant documents among the vast volume of modern biomedical literature efficiently. In the real world, many information retrieval tasks are difficult because of high data dimensionality and the lack of annotated examples to train a retrieval algorithm. Under such a scenario, the performance of information retrieval algorithms is often unsatisfactory, therefore improvements are needed. Design: We studied two approaches that enhance the text categorization performance on sparse and high data dimensionality: (1) semantic-preserving dimension reduction by representing text with semantic-enriched features",
      "title": "15250 Enhancing Text Categorization with Semantic-enriched Representation and Training Data Augmentation",
      "url": ""
    },
    {
      "abstract": "This paper proposed an incremental text clustering algorithm based on semantic sequence. Using similarity relation of semantic sequences and calculating the cover of similarity semantic sequences set, the candidate cluster with minimum entropy overlap value was selected as a result cluster every time in this algorithm. The comparison of experimental results shows that the precision of the algorithm is higher than other algorithms under same conditions and this is obvious especially on long documents set.",
      "title": "15251 An incremental algorithm of text clustering based on semantic sequences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845884604&partnerID=40&md5=ac80ed0a3ac299563ebeb9886c62eac6"
    },
    {
      "abstract": "We report on a set of experiments in text mining, specifically, finding semantic patterns given only a few keywords. The experiments employ the Counter-training framework for discovery of semantic knowledge from raw text in a weakly supervised fashion. The experiments indicate that the framework is suitable for efficient acquisition of semantic word classes and collocation patterns, which may be used for Information Extraction.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "15254 Mining the semantics of text via Counter-training",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33744824631&partnerID=40&md5=2d577c8a3adf5b917f878ce0b1bf8700"
    },
    {
      "abstract": "This paper presents a new model integrating a recurrent neural network (RNN) and a least squares support vector machine (LS-SVM) for classification of document titles according to different predetermined categories. The new model proposed in this paper is abbreviated as Neuro-SVM. Based on the Neuro-SVM model, a system is implemented, using latent semantic indexing (LSI) to generate probabilistic coefficients from document titles, which are used as the input to the system. The systems performance is demonstrated with a corpus of 96956 words, from University of Denvers Penrose Library catalogue and the accuracy rate of the proposed system is found to be 99.66%.  2005 IEEE.",
      "title": "15255 A neuro-SVM model for text classification using latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745958002&partnerID=40&md5=7c57600ff67d03a6b5ecadae33a25c50"
    },
    {
      "abstract": "Text documents have sparse data spaces, and nearest neighbors may belong to different classes when using current existing proximity measures to describe the correlation of documents. In this paper, we propose an asymmetric similarity measure to strengthen the discriminative feature of document objects. We construct a semantic correlation network by asymmetric similarity between documents and conjecture the power law feature of the connections distributions. Hub points which exist in semantic correlation network are classified by an agglomerative hierarchical clustering approach named SCN. Both objects similarity and neighbors similarity are considered in the definition of hub points proximity. Finally, we assign the rest text objects to their nearest hub points. The experimental evaluation on textual data sets demonstrates the validity and efficiency of SCN. The comparison with other clustering algorithms shows the superiority of our approach.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "15256 Semantic correlation network based text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745633099&partnerID=40&md5=ec47c5eb2a5027fa69551fcbd98fd892"
    },
    {
      "abstract": "To overcome the limitations of traditional text classification approaches based on bag-of-words representation and to effectively incorporate linguistic knowledge and conceptual index into text vector space representation, based on WordNet thesaurus and Latent Semantic Indexing (LSI) model, combinative method of them is presented to realize Naive Bayes text classification and simple vector distance text classification, and five groups of contrastive experiments are made respectively. The results show that the accuracy rates of the two text classification methods are both gradually advanced along with more and more in-depth semantic analysis, which indicates that semantic mining is very important and necessary to text classification. The comparative analysis of the related work is also given.",
      "title": "Naive Research of English text classification methods based on semantic meaning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33847731539&partnerID=40&md5=a48a5809fcf1a99ac0dccd8369ff4280"
    },
    {
      "abstract": "The quantification of evaluating semantic relatedness among texts has been a challenging issue that pervades much of machine learning and natural language processing. This paper presents a hybrid approach of a text-mining technique for measuring semantic relatedness among texts. In this work we develop several text classifiers using Support Vector Machines (SVM) method to supporting acquisition of relatedness among texts. First, we utilized our developed text mining algorithms, including text mining techniques based on classification of texts in several text collections. After that, we employ various SVM classifiers to deal with evaluation of relatedness of the target documents. The results indicate that this approach can also be fitted to other research work, such as information filtering, and re-categorizing resulting documents of search engine queries.  2005 IEEE.",
      "title": "15259 A classifier-based text mining approach for evaluating semantic relatedness using support vector machines",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-24744454160&partnerID=40&md5=13b5f1f4d685239b73d78fe02267cf1a"
    },
    {
      "abstract": "This paper presents work that uses Transductive Latent Semantic Indexing (LSI) for text classification. In addition to relying on labeled training data, we improve classification accuracy by incorporating the set of test examples in the classification process. Rather than performing LSIs singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available test examples. We report the performance of LSI on data sets both with and without the inclusion of the test examples, and we show that tailoring the SVD process to the test examples can be even more useful than adding additional training data. This method can be especially useful to combat possible inclusion of unrelated data in the original corpus, and to compensate for limited amounts of data. Additionally, we evaluate the vocabulary of the training and test sets and present the results of a series of experiments to illustrate how the test set is used in an advantageous way.  World Scientific Publishing Company.",
      "title": "15261 Transductive learning for short-text classification problems using latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-17044410110&partnerID=40&md5=0612dd9c0543cfebea2fffe16ed9da6a"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) has been shown to be extremely useful in information retrieval, but it is not an optimal representation for text classification. It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation. Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information. However, their performance improvements over original term vectors are still very limited. In this paper, we propose a new local LSI method called Local Relevancy Weighted LSI to improve text classification by performing a separate Single Value Decomposition (SVD) on the transformed local region of each class. Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension.  2004 IEEE.",
      "title": "15262 Improving text classification using local Latent Semantic Indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-19544372770&partnerID=40&md5=55282de6b3ba5ecf36dd078112596536"
    },
    {
      "abstract": "In this paper, a new explanatory and high-level approach to knowledge discovery from texts is described which uses natural language techniques and and evolutionary computation based optimization to find novel patterns in textual information. In addition, some results showing the promise of the approach towards effective text mining when compared to human performance are briefly discussed.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "15263 Semantically-driven explanatory text mining: Beyond keywords",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944453072&partnerID=40&md5=23aa6aa2496f1415c8ae299a0edf6238"
    },
    {
      "abstract": "Aiming at the status that various electronic text materials are increasing rapidly, this paper brings forward a model of automatic classification of electronic text information in order to manage and use these text information effectively: the algorithm of segmentation of word based on word dictionary and statistics, preprocessing of text, design of weight function of feature words and collecting them, expression of text vector space, latent semantic indexing and clustering algorithm of text, etc. The experiment has proved that the model had satisfactory classification effect as well as high calculation and storage efficiency.",
      "title": "15264 A Chinese text classification model based on vector space and semantic meaning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-6344231568&partnerID=40&md5=094106df554603b17a0d2d7fb64c8891"
    },
    {
      "abstract": "Recent work in knowledge representation undertaken as part of the Semantic Web initiative has enabled a common infrastructure (Resource Description Framework (RDF) and RDF Schema) for sharing knowledge of ontologies and instances. In this paper we present a framework for combining the shallow levels of semantic description commonly used in MUC-style information extraction with the deeper semantic structures available in such ontologies. The framework is implemented within the PIA project software called Ontology Forge. Ontology Forge offers a server-based hosting environment for ontologies, a server-side information extraction system for reducing the effort of writing annotations and a many-featured ontology/annotation editor. We discuss the knowledge framework, some features of the system and summarize results from extended named entity experiments designed to capture instances in texts using support vector machine software.",
      "title": "15267 A framework for integrating deep and shallow semantic structures in text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-8344253597&partnerID=40&md5=9e13c55358200706eb152b9587dc860d"
    },
    {
      "abstract": "Common text clustering techniques offer rather poor capabilities for explaining to their users why a particular result has been achieved. They have the disadvantage that they do not relate semantically nearby terms and that they cannot explain how resulting clusters are related to each other. In this paper, we discuss a way of integrating a large thesaurus and the computation of lattices of resulting clusters into common text clustering in order to overcome these two problems. As its major result, our approach achieves an explanation using an appropriate level of granularity at the concept level as well as an appropriate size and complexity of the explaining lattice of resulting clusters.",
      "title": "15268 Explaining text clustering results using semantic structures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-9444279065&partnerID=40&md5=51bfe9c956c0bc58454a51dde768077c"
    },
    {
      "abstract": "The term-based vector space model is a prominent technique to retrieve textual information. In this paper we examine the usefulness of phrases as terms in vector-based document classification. We focus on statistical techniques to extract both adjacent and window phrases from documents. We discover that the positive effect of adding phrase terms is very limited, if we have already achieved good performance using single-word terms, even when SVD/LSI is used as dimensionality reduction method.  2002 IEEE.",
      "title": "15270 Evaluating the utility of statistical phrases and latent semantic indexing for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750718642&partnerID=40&md5=6519446a7e68ef745a9b816fdd2fc153"
    },
    {
      "abstract": "This paper describes a new application of a text-mining algorithm to the text sources of bilingual corpora. In the past, the majority of the approaches applied to measuring semantic relatedness was based on edge counting methods through a semantic network, such as WordNet It is not well suited for applications in specific domains in which the standard lexical knowledge bases are not available. In this work, we propose an alternative solution for acquisition of semantic relatedness from text corpora by means of a machine learning technique, namely the self-organizing maps. This paper presents a hybrid approach to discovering a concept-based feature map containing word clusters and document clusters from multilingual text collections. Using SOM-based automatic clustering techniques, we have conducted several experiments to uncover associated documents based on Chinese-English bilingual parallel corpora, and a hybrid Chinese-English corpus. In essence, this work provides a method for automatic text clustering, which resolves some of the language difficulties in concept discovery and categorization from multilingual text corpora.",
      "title": "15273 Text mining of multilingual corpora via computing semantic relatedness",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0037731520&partnerID=40&md5=fb18f3859258a7462eae6cd8abcbe4e3"
    },
    {
      "abstract": "The research project AgNeT develops Agents for Neural Text routing in the internet. Unrestricted potentially faulty text messages arrive at a certain delivery point (e.g. email address or world wide web address). These text messages are scanned and then distributed to one of several expert agents according to a certain task criterium. Possible specific scenarios within this framework include the learning of the routing of publication titles or news titles. In this paper we describe extensive experiments for semantic text routing based on classified library titles and newswire titles. This task is challenging since incoming messages may contain constructions which have not been anticipated. Therefore, the contributions of this research are in learning and generalizing neural architectures for the robust interpretation of potentially noisy unrestricted messages. Neural networks were developed and examined for this topic since they support robustness and learning in noisy unrestricted real-world texts. We describe and compare different sets of experiments. The first set of experiments tests a recurrent neural network for the task of library title classification. Then we describe a larger more difficult newswire classification task from information retrieval. The comparison of the examined models demonstrates that techniques from information retrieval integrated into recurrent plausibility networks performed well even under noise and for different corpora.  2000 Kluwer Academic Publishers.",
      "title": "15275 Neural network agents for learning semantic text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0012460006&partnerID=40&md5=4b222706acb5a8dfcb7651ae03d51081"
    },
    {
      "abstract": "Adapting to users requirements is a key factor for enterprise success. Despite the existence of several approaches that point in this direction, simplifying integration and interoperability among users, suppliers and the enterprise during product lifecycle, is still an open issue. Ontologies have been used in some manufacturing applications and they promise to be a valid approach to model manufacturing resources of enterprises (e.g. machinery and raw material). Nevertheless, in this domain, most of the ontologies have been developed following methodologies based on development from scratch, thus ontologies previously developed have been discarded. Such ontological methodologies tend to hold the interoperability issues in some level. In this paper, a method that integrates ontology reuse with ontology validation and learning is presented. An upper (top-level) ontology for manufacturing was used as a reference to evaluate and to improve specific domain ontology. The evaluation procedure was based on the systemic methodology for ontology learning (SMOL). As a result of the application of SMOL, an ontology entitled Machine of a Process (MOP) was developed. The terminology included in MOP was validated by means of a text mining procedure called Term Frequency-Inverse Document Frequency (TF-IDF) which was carried out on documents from the domain in this study. Competency questions were performed on preexisting domain ontologies and MOP, proving that this new ontology has a performance better than the domain ontologies used as seed.  2013 Elsevier B.V.",
      "title": "15282 Towards a machine of a process (MOP) ontology to facilitate e-commerce of industrial machinery",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888006032&partnerID=40&md5=50c750edf389da7710292ff9c523b2d6"
    },
    {
      "abstract": "The huge amount of free-form unstructured text in the blogosphere, its increasing rate of production, and its shrinking window of relevance, present serious challenges to the public policy analyst who seeks to take public opinion into account. Most of the tools which address this problem use XML tagging and other Web 3.0 approaches, which do not address the actual content of blog posts and the associated commentary. We give a tutorial review of latent semantic analysis and the self-organizing maps, as considered in this context, and show how to apply the self-organizing map over a probabilistic latent semantic space to the problem of completely unsupervised clustering of unstructured text in such a way as to be entirely independent of spelling, grammar, and even source language. This provides an algorithm suitable for clustering free-form commentary with a well-structured test environment. The algorithm is applied to academic paper abstracts instead, treated as unstructured text as though they were blog posts, because this set of documents has a known ground truth. The algorithm constructs a word category map and a document map in which words with similar meaning and documents with similar content are clustered together.  2013 John Wiley & Sons, Ltd.",
      "title": "15286 Self-organizing maps for latent semantic analysis of free-form text in support of public policy analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890865274&partnerID=40&md5=7d19fd17eb0418027807716fef7bb2b2"
    },
    {
      "abstract": "We present a study on the semantic representation of the knowledge embedded in the notes to Convivio, a philosophical essay composed by Dante Alighieri. The notes were produced by an Italian scholar in structured text format. First, we analyzed the content of the document annotated by the scholar and on this basis we created an underlying high-level UML model that plays the role of an initial conceptualization. In the second step, we tried to identify terms belonging to vocabularies used in the Digital Libraries domain, which could be used for specifying the UML conceptualization. To this end, we investigated several existing ontologies, and we chose the terms that we considered useful to represent our knowledge. In order to describe all the knowledge embedded in the notes, we added classes and properties. Finally, we built a RDF graph that represents the semantics of the notes.  2013 ACM.",
      "title": "15291 A preliminary study on the semantic representation of the notes to Dante Alighieris Convivio",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891070884&partnerID=40&md5=423f48eda7b8f997f009574a5a7004e5"
    },
    {
      "abstract": "Phrase identification using frequencies is employed in text mining studies. In this work, we propose that all phrases occur in a document are not equal as the semantic relevance between the phrases and text varies. We proposed to identify the relevance of phrases in natural language texts using phrase weights. The pilot study results are worth to investigate further.  2013 IEEE.",
      "title": "15293 Mining and computing phrase weight in texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891092118&partnerID=40&md5=bf74f0e82710f122d3d406156594408e"
    },
    {
      "abstract": "The semantic comparison of short sections of text is an emerging aspect of Natural Language Processing (NLP). In this paper we present a novel Short Text Semantic Similarity (STSS) method, Lightweight Semantic Similarity (LSS), to address the issues that arise with sparse text representation. The proposed approach captures the semantic information contained when comparing text to process the similarity. The methodology combines semantic term similarities with a vector similarity method used within statistical analysis. A modification of the term vectors using synset similarity values addresses issues that are encountered with sparse text. LSS is shown to be comparable to current semantic similarity approaches, LSA and STASIS, whilst having a lower computational footprint.  2013 IEEE.",
      "title": "15294 A fast and efficient semantic short text similarity metric",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891049480&partnerID=40&md5=5fdb0c9ca634e63cd2437d4359710a38"
    },
    {
      "abstract": "Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.",
      "title": "15296 Headerless, quoteless, but not hopeless? Using pairwise email classification to disentangle email threads",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890537287&partnerID=40&md5=c0e76eb5290676b533f81c6bf57331f8"
    },
    {
      "abstract": "We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs.",
      "title": "15297 The Unit Graphs framework: Foundational concepts and semantic consequence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890535737&partnerID=40&md5=f14e247413854d24165bb2501aad03a7"
    },
    {
      "abstract": "The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics.",
      "title": "15298 Rationale, concepts, and current outcome of the unit graphs framework",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890448302&partnerID=40&md5=831a8bb612532f6367afcc4e0bd8bbd4"
    },
    {
      "abstract": "With 19%-28% of Internet users participating in online health discussions, it became imperative to be able to detect and analyze posted personal health information (PHI). In this work we introduce two semantic-based methods for mining PHI on social networks which will warn the users about potential privacy breaches. One method uses WordNet as a source of health-related knowledge, another - an ontology of personal relations. We use Twitter data to empirically evaluate our methods. We also apply Machine Learning to demonstrate advantages of our extraction procedure when tweets containing PHI have to be automatically identified among other tweets.",
      "title": "15299 How joe and jane tweet about their health: Mining for personal health information on twitter",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890453483&partnerID=40&md5=49a648ad4a063fdfa7b197166365fe50"
    },
    {
      "abstract": "Chatter bots are computer programs that can simulate a conversation through text chat. Current chatter bots perform well in artificial conversations consisting of pairs of utterance exchanges like a questionanswer session where the context switches with every pair. But they perform poorly in longer conversations where the context is maintained across several utterance exchanges. Existing approaches to artificial conversation generation focus on linguistic and grammatical modeling to generate individual sentence-level utterances. We present a framework that enables longer and more meaningful conversations by combining concepts of content representation and conversation semantics. We also present a metric for evaluating the conversations based on Grices maxims, that form the central idea in the theory of pragmatics. Copyright  2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",
      "title": "15300 A framework for simulating and evaluating artificial chatter bot conversations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889830645&partnerID=40&md5=3e2b0118a3bff8fe409f613e666141ea"
    },
    {
      "abstract": "This paper presents a logic prover approach to predicting textual similarity. Sentences are represented using three logic forms capturing different levels of knowledge, from only content words to semantic representations extracted with an existing semantic parser. A logic prover is used to find proofs and derive semantic features that are combined in a machine learning framework. Experimental results show that incorporating the semantic structure of sentences yields better results than simpler pairwise word similarity measures. Copyright  2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",
      "title": "15301 A logic prover approach to predicting textual similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889813527&partnerID=40&md5=6210a0fde06c31ae873af7dfdc206208"
    },
    {
      "abstract": "This paper discusses a new plagiarism detection method for text documents called Tree-based Conceptual Matching. The proposed method not only represents the content of a text document as a tree, but it also captured the underlying semantic meaning in terms of the relationships among its concepts. The method was adopted to detect plagiarism in text documents. The tree-based played a very important role in this method. It looked at the amount of detecting plagiarized sentences from the original documents. Experiments have been carried out using the CS11 standard plagiarism detection corpus. The results were evaluated using information retrieval measurements, which are Recall, Precision and F-measure. The results were compared with other methods for plagiarism detection and we found our method outperforms the other methods for plagiarism detection.  2013 IEEE.",
      "title": "15302 A tree-based conceptual matching for plagiarism detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889651193&partnerID=40&md5=ed9eb53cf4c7a2fccb0c05df51f2b199"
    },
    {
      "abstract": "In this paper, we address the text classification problem that a period of time created test data is different from the training data, and present a method for text classification based on temporal adaptation. We first applied lexical chains for the training data to collect terms with semantic relatedness, and created sets (we call these Sem sets). Semantically related terms in the documents are replaced to their representative term. For the results, we identified short terms that are salient for a specific period of time. Finally, we trained SVM classifiers by applying a temporal weighting function to each selected short terms within the training data, and classified test data. Temporal weighting function is weighted each short term in the training data according to the temporal distance between training and test data. The results using MedLine data showed that the method was comparable to the current state-of-the-art biased-SVM method, especially the method is effective when testing on data far from the training data. Copyright 2013 ACM.",
      "title": "15305 Timeline adaptation for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889578819&partnerID=40&md5=01548c1f20393bf2f0eaf28fbfb2f55e"
    },
    {
      "abstract": "Two approaches have been commonly used for recognizing Drug Name Entities in biomedical texts: machine learning based and domain specific resources-based approaches. In this work we focus on the second one by combining (1) a dictionary-based approach that collects terms from different pharmacological data sources such as DrugBank, MeSH, RxNorm and ATC index",
      "title": "15307 Combining dictionaries and ontologies for drug name recognition in biomedical texts",
      "url": ""
    },
    {
      "abstract": "This paper describes a novel probabilistic method of measuring semantic similarity for real-world noisy short texts like microblog posts. Our method adds related Wikipedia entities to a short text as its semantic representation and uses the vector of entities for computing semantic similarity. Adding related entities to texts is generally a compound problem that involves the extraction of key terms, finding related entities for each key term, and the aggregation of related entities. Explicit Semantic Analysis (ESA), a popular Wikipedia-based method, solves these problems by summing the weighted vectors of related entities. However, this heuristic weighting highly depends on the rule of majority decision and is not suited to short texts that contain few key terms but many noisy terms. The proposed probabilistic method synthesizes these procedures by extending naive Bayes and achieves robust estimates of related Wikipedia entities for short texts. Experimental results on short text clustering using Twitter data indicated that our method outperformed ESA for short texts containing noisy terms. Copyright is held by the owner/author(s).",
      "title": "15308 Probabilistic semantic similarity measurements for noisy short texts using wikipedia entities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889593759&partnerID=40&md5=268cad5b2084cc60e40fc6feb484837f"
    },
    {
      "abstract": "Text representation is one of the most fundamental works in text comprehension, processing, and search. Various works have been proposed to mine the semantics in texts and then to represent them. However, most of them only focus on how to mine semantics from the text itself while the background knowledge, which is very important to text understanding, is not taken into consideration. In this paper, on the basis of human cognitive process, we propose a multi-level text representation model within background knowledge, called TRMBK. It is composed of three levels, which are machine surface code (MSC), machine text base (MTB) and machine situational model (MSM). All of the three are able to be automatically constructed to acquire semantics both inside and outside of the text. Simultaneously, we also propose a method to automatically establish background knowledge and offer supports for the current text comprehension. Finally, experiments and comparisons have been presented to show the better performance of TRMBK.  2013 IEEE.",
      "title": "15310 A multi-level text representation model within background knowledge based on human cognitive process",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889019189&partnerID=40&md5=a0573b399787f6c3569b2b6024285e4a"
    },
    {
      "abstract": "Social Network Services have become an important medium for people to communicate ideas and share interests in recent years. Blogs published and shared by users in this virtual world are one of the main sources of user-generated information. Classifying these freestyle blogs can help understand user interests and assist applications such as search and marketing. In this paper, we propose a new method of multi-label classification for Chinese blogs. By applying Dempster-Shafer theory on semantic word similarity algorithms, we achieve automatic classification without use of difficult-to-obtain training sets. Experiments were conducted on real world data from RENREN.com, the biggest SNS (Social Network Services) in China. Results show that the proposed method achieves satisfactory performance in multilabeling real world SNS blogs as well as corpus.  2013 IEEE.",
      "title": "15311 Chinese SNS blog classification using semantic similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889063596&partnerID=40&md5=18b69b5fc53d1946f8e621f70ecf39ce"
    },
    {
      "abstract": "Named entity recognition is a crucial component of biomedical natural language processing, enabling information extraction and ultimately reasoning over and knowledge discovery from text. Much progress has been made in the design of rule-based and supervised tools, but they are often genre and task dependent. As such, adapting them to different genres of text or identifying new types of entities requires major effort in re-annotation or rule development. In this paper, we propose an unsupervised approach to extracting named entities from biomedical text. We describe a stepwise solution to tackle the challenges of entity boundary detection and entity type classification without relying on any handcrafted rules, heuristics, or annotated data. A noun phrase chunker followed by a filter based on inverse document frequency extracts candidate entities from free text. Classification of candidate entities into categories of interest is carried out by leveraging principles from distributional semantics. Experiments show that our system, especially the entity classification step, yields competitive results on two popular biomedical datasets of clinical notes and biological literature, and outperforms a baseline dictionary match approach. Detailed error analysis provides a road map for future work.  2013 The Authors.",
      "title": "15313 Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888192341&partnerID=40&md5=252bb85237a801bc8bd44f080eea03c0"
    },
    {
      "abstract": "Text categorization, especially short text categorization, is a difficult and challenging task since the text data is sparse and multidimensional. In traditional text classification methods, document texts are represented with Bag of Words (BOW) text representation schema, which is based on word co-occurrence and has many limitations. In this paper, we mapped document texts to Wikipedia concepts and used the Wikipedia-concept-based document representation method to take the place of traditional BOW model for text classification. In order to overcome the weakness of ignoring the semantic relationships among terms in document representation model and utilize rich semantic knowledge in Wikipedia, we constructed a semantic matrix to enrich Wikipedia-concept-based document representation. Experimental evaluation on five real datasets of long and short text shows that our approach outperforms the traditional BOW method.  2013 The Institute of Electronics, Information and Communication Engineers.",
      "title": "15314 Improving text categorization with semantic knowledge in wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889032196&partnerID=40&md5=6748b37bb51fc9440d7eb752d554b284"
    },
    {
      "abstract": "This paper extracts facts using micro-reading of text in contrast to approaches that extract common-sense knowledge using macro-reading methods. Our goal is to extract detailed facts about events from natural language using a predicate-centered view of events (who did what to whom, when and how). We exploit semantic role labels in order to create a novel predicate-centric ontology for entities in our knowledge base. This allows users to find uncommon facts easily. To this end, we tightly couple our knowledge base and ontology to an information visualization system that can be used to explore and navigate events extracted from a large natural language text collection. We use our methodology to create a web-based visual browser of history events in Wikipedia.  2013 ACM.",
      "title": "15316 Knowledge base population and visualization using an ontology based on semantic roles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888161441&partnerID=40&md5=5b003a9455e7955b3ce8ffce364110dc"
    },
    {
      "abstract": "Biomedical is a huge domain that combines a variety of research areas. MEDELINE is one of the largest biomedical databases. Thereby, the searching of pertinent information through Medline has become a difficult task. Thats why",
      "title": "15317 Bayesian information extraction network for Medline abstract",
      "url": ""
    },
    {
      "abstract": "The integration of a small fraction of the information present in the Web of Documents to the Linked Data Web can provide a significant shift on the amount of information available to data consumers. However, information extracted from text does not easily fit into the usually highly normalized structure of ontology-based datasets. While the representation of structured data assumes a high level of regularity, relatively simple and consistent conceptual models, the representation of information extracted from texts need to take into account large terminological variation, complex contextual/dependency patterns, and fuzzy or conflicting semantics. This work focuses on bridging the gap between structured and unstructured data, proposing the representation of text as structured discourse graphs (SDGs), targeting an RDF representation of unstructured data. The representation focuses on a semantic best-effort information extraction scenario, where information from text is extracted under a pay-as-you-go data quality perspective, trading terminological normalization for domain-independency, context capture, wider representation scope and maximization of textual information capture.  2013 IEEE.",
      "title": "15318 Representing texts as contextualized entity-centric linked data graphs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887954558&partnerID=40&md5=1aa98d71fbd1e4a67c8b1be05ba75d18"
    },
    {
      "abstract": "An effective knowledge extraction and quantification methodology from biomedical literature would allow the researcher to organize and analyze the results of high-throughput experiments on microarrays and next-generation sequencing technologies. Despite the large amount of raw information available on the web, a tool able to extract a measure of the correlation between a list of genes and biological processes is not yet available. In this paper, we present Gelsius, a workflow that incorporates biomedical literature to quantify the correlation between genes and terms describing biological processes. To achieve this target, we build different modules focusing on query expansion and document cononicalization. In this way, we reached to improve the measurement of correlation, performed using a latent semantic analysis approach. To the best of our knowledge, this is the first complete tool able to extract a measure of genes-biological processes correlation from literature. We demonstrate the effectiveness of the proposed workflow on six biological processes and a set of genes, by showing that correlation results for known relationships are in accordance with definitions of gene functions provided by NCI Thesaurus. On the other side, the tool is able to propose new candidate relationships for later experimental validation.  2004-2012 IEEE.",
      "title": "15321 Gelsius: A literature-based workflow for determining quantitative associations between genes and biological processes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887973256&partnerID=40&md5=1b0a469a15023d981326bd747c13f161"
    },
    {
      "abstract": "The paper presents a mining methodology, which is based on processing of spatial and temporal information in natural language text. It is based on methodology capable of recognizing the relationship between events and objects in the text, using spatial and temporal relationships between these events to enhance the semantic coherence of natural language text. Heterogeneous semantic network with extended family relationships is used to implement relational-situational analysis between the words in the sentence. For correlation lining between spatial and temporal categories and events in the text the mechanism of Markov-logic networks is used, allowing to operate in the peaks of Markov-logic networks with the normal logical formulas representing the combined spatial-temporal logic. The described system can be used in tasks of semantic search.  IDOSI Publications, 2013.",
      "title": "15324 Processing of spatial and temporal information in the text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887693941&partnerID=40&md5=67890dce0f23ac49ccd6f864ce1e5e55"
    },
    {
      "abstract": "This paper presents a new approach for automated compliance checking in the construction domain. The approach utilizes semantic modeling, semantic Natural Language Processing (NLP) techniques (including text classification and information extraction), and logic reasoning to facilitate automated textual regulatory document analysis and processing for extracting requirements from these documents and formalizing these requirements in a computer-processable format. The approach involves developing a set of algorithms and combining them into one computational platform: (1) semantic machine-learning-based algorithms for text classification (TC)",
      "title": "15328 Information transformation and automated reasoning for automated compliance checking in construction",
      "url": ""
    },
    {
      "abstract": "LDA (Latent Dirichlet Allocation) topic model has been applied into many applications in recent years. But LDA has a shortcoming that it cannot deal with various changes of data set well, which has become a limitation for its applications. Hierarchical Latent Dirichlet Allocation (hLDA) is a generalization of LDA and it can adapt itself to the growing data set automatically. hLDA can mine latent topics from a large amount of discrete data and organize these topics into a hierarchy, in which the topics of higher level are more abstractive while the topics of lower level are more specific. This hierarchy could achieve a deeper semantic model which is similar with human mind. Given a set of documents, hLDA generates a prior distribution of Bayesian nonparametrics using a nested Chinese restaurant process (nCRP)[1]. The documents sharing similar topics are organized into a cluster of path. hLDA learns the distribution of topics using a method of Bayesian posterior inference. This paper tries to study hLDA model in details and apply it into the application of Chinese text clustering. Experiments have shown that hLDA is a very promising model for text clustering.  2012 IEEE.",
      "title": "15331 HLDA based text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890395872&partnerID=40&md5=0277217320dbb3b106c2ae2233bc40d5"
    },
    {
      "abstract": "Instructions extraction extracts structured information from unstructured natural language instruction text, is an application of information extraction in the field of human-computer interaction. For a natural language instruction text, if we want to extract structural information which can able to describe the text semantic completely, it is critical to position these words or phrases and mark one description which belongs to their own semantic description. This paper first try to a solution which is semantic classification based on dictionary. Because of some shortcomings of the dictionary itself, the semantic classification results are poor. Through the analysis of dictionary-based semantic classification results, this paper proposes a semantic classification method which combining CRF, self-training and Dictionary. Use this method to conduct experiments in the field of vehicle. The experiment results show that our method can be effective in semantic classification for the natural language instruction text",
      "title": "15332 Research on automatic semantic classification of human-interaction instructions",
      "url": ""
    },
    {
      "abstract": "Graph-based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph-based model to represent the correlations between pairs of document terms. However, since the high-order correlations among multiple terms are disregarded during graph evaluation, the summarization performance could be limited unless integrating ad hoc language-dependent or semantics-based analysis. This paper presents a novel and general-purpose graph-based summarizer, namely GraphSum (Graph-based Summarizer). It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. The graph nodes, which represent combinations of two or more terms, are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations. Then, the produced node ranking is used to drive the sentence selection process. The experiments performed on benchmark and real-life documents demonstrate the effectiveness of the proposed approach compared to many state-of-the-art summarizers.  2013 Elsevier Inc. All rights reserved.",
      "title": "15335 GraphSum: Discovering correlations among multiple terms for graph-based summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883155552&partnerID=40&md5=ec6f18b9927c476dd631876f8e440d75"
    },
    {
      "abstract": "This paper presents the concept of hybrid semantic-document models to aid information management when using standards for complex technical domains. These standards are traditionally text based documents for human interpretation, but prose sections can often be ambiguous and can lead to discrepancies. Many organisations will produce semantic representations of the material. In developing these semantic representations, no relationship is maintained to the original prose. Maintaining the relationships has key benefits, including assessing conformance at a semantic level rather than prose, and enabling original content authors to explicitly define their intentions. This paper proposes a framework to help achieve these benefits.  2013 Elsevier B.V.",
      "title": "15338 Extending document models to incorporate semantic information for complex standards",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884502775&partnerID=40&md5=d3c3abace4ec95e7c368b5596a846b4e"
    },
    {
      "abstract": "Semantic relation extraction is a significant topic in semantic web and natural language processing with various important applications such as knowledge acquisition, web and text mining, information retrieval and search engine, text classification and summarization. Many approaches such rule base, machine learning and statistical methods have been applied, targeting different types of relation ranging from hyponymy, hypernymy, meronymy, holonymy to domain-specific relation. In this paper, we present a computational method for extraction of explicit and implicit semantic relation from text, by applying statistic and linear algebraic approaches besides syntactic and semantic processing of text.  2012 Springer-Verlag London Limited.",
      "title": "15341 SREC: Discourse-level semantic relation extraction from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885924121&partnerID=40&md5=7476430703fe16929f8a5e1b1e03f1e6"
    },
    {
      "abstract": "We propose a novel framework for overcoming the challenges in extracting causal relations from domain-specific texts. Our technique is minimally-supervised, alleviating the need for manually-annotated, expensive training data. As our main contribution, we show that open-domain corpora can be exploited as knowledge bases to overcome data sparsity issues posed by domain-specific relation extraction, and that they enable substantial performance gains. We also address longstanding challenges of extant minimally-supervised approaches. To suppress the negative impact of semantic drift, we propose a technique based on the Latent Relational Hypothesis. In addition, our approach discovers both explicit (e.g. to cause) and implicit (e.g. to destroy) causal patterns/relations. Unlike existing minimally-supervised techniques, we adopt a principled seed selection strategy, which enables us to discover a more diverse set of causal patterns/relations. Our experiments reveal that our approach outperforms a state-of-the-art baseline in discovering causal relations from a real-life, domain-specific corpus.  2013 Elsevier B.V.",
      "title": "15342 Minimally-supervised learning of domain-specific causal relations using an open-domain corpus as knowledge base",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889089199&partnerID=40&md5=2980e956bd76a1ff5ce3d4f8ecee9453"
    },
    {
      "abstract": "Determining the correlation between biomedical terms is a powerful instrument to help scientist research activity, both to understand experimental results and to design new ones. In particular, a great potential comes from the integration of the many heterogeneous information sources currently available on the Web. In this article we focus on the correlation between genes and biological processes. In this context, we present a methodology for integrating information from biomedical literature with other heterogeneous types of structured information. In particular, the information sources integrated in this work are PubMed abstracts, pathway databases, and NCI thesaurus definitions. The integration is performed at the semantic analysis level using a customized approach we developed to modulate the impact of the different sources on the correlation score. We report the results of a study concerning the impact of the information integration on the correlation score and of the user-level parameters we introduced to modulate the impact of pathway data or NCI definitions with respect to biomedical literature information, depending on the context of the search. To evaluate the methodology, we performed correlation measures on six biological processes and nine genes by comparing the results with and without the integration of pathways and NCI definitions. c 2013 ACM 1550-4832/2013/11-ART28 15.00.",
      "title": "15343 Integration of literature with heterogeneous information for genes correlation scoring",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888585062&partnerID=40&md5=72d1ced350477cba855c748ea0f49034"
    },
    {
      "abstract": "The rise of social media in couple of years has changed the general perspective of networking, socialization, and personalization. Use of data from social networks for different purposes, such as election prediction, sentimental analysis, marketing, communication, business, and education, is increasing day by day. Precise extraction of valuable information from short text messages posted on social media (Twitter) is a collaborative task. In this paper, we analyze tweets to classify data and sentiments from Twitter more precisely. The information from tweets are extracted using keyword based knowledge extraction. Moreover, the extracted knowledge is further enhanced using domain specific seed based enrichment technique. The proposed methodology facilitates the extraction of keywords, entities, synonyms, and parts of speech from tweets which are then used for tweets classification and sentimental analysis. The proposed system is tested on a collection of 40,000 tweets. The proposed methodology has performed better than the existing system in terms of tweets classification and sentiment analysis. By applying the Knowledge Enhancer and Synonym Binder module on the extracted information we have achieved increase in information gain in a range of 0.1% to 55%. The increase in information gain has enabled our proposed system to better summarize the twitter data for user sentiments regarding a keyword from a particular category.  2013 IEEE.",
      "title": "15347 Precise tweet classification and sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84886472491&partnerID=40&md5=2b18ad34afe717e5566ec162f3c101ba"
    },
    {
      "abstract": "Automated clustering of threads within and across web forums will greatly benefit both users and forum administrators in efficiently seeking, managing, and integrating the huge volume of content being generated. While clustering has been studied for other types of data, little work has been done on clustering forum threads",
      "title": "15349 Exploiting forum thread structures to improve thread clustering",
      "url": ""
    },
    {
      "abstract": "The growing need for text mining systems, such as opinion mining, requires a deep semantic understanding of the target language. In order to accomplish this, extracting the semantic information of functional expressions plays a crucial role, because functional expressions such as would like to and cant are key expressions to detecting customers needs and wants. However, in Japanese, functional expressions appear in the form of suffixes, and two different types of functional expressions are merged into one predicate: one influences the factual meaning of the predicate while the other is merely used for discourse purposes. This triggers an increase in surface forms, which hinders information extraction systems. In this article, we present a novel normalization technique that paraphrases complex functional expressions into simplified forms that retain only the crucial meaning of the predicate. We construct paraphrasing rules based on linguistic theories in syntax and semantics. The results of experiments indicate that our system achieves a high accuracy of 79.7%, while it reduces the differences in functional expressions by up to 66.7%. The results also show an improvement in the performance of predicate extraction, providing encouraging evidence of the usability of paraphrasing as a means of normalizing different language expressions.  2013 ACM.",
      "title": "15351 Normalizing complex functional expressions in Japanese predicates: Linguistically-directed rule-based paraphrasing and its application",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885648275&partnerID=40&md5=a93fe6ffb9cb609f20c06fb4a50577b2"
    },
    {
      "abstract": "Canonical Correlation Analysis (CCA) is used to infer a semantic space into which text documents, written in different languages, can be mapped to a language-independent representation, called latent topics. This highly reduces the complexity of dealing with different languages since we can train a document classifier using the labeled documents in one language, and then apply it to classify documents in another language. This topic modeling task is usually performed in a class-independent manner. The performance of CCA depends on the amount of documents used for inferring the semantic space. However, CCA has a high computational complexity with respect to the number of training documents. In this paper, we proposed a scalable variant of CCA, CD-CCA, to improve its scalability and complexity where the projection is performed in a class-dependent manner. It generates a semantic space for each category separately. Then a binary document classifier is trained for each category on its own semantic space. CD-CCA was applied on English-Chinese document classification. The experimental results showed that CD-CCA can deal with large training sets without hurting the performance of the underlying classifiers compared to traditional CCA. CD-CCA opens the door for distributed training of the semantic spaces of the different categories.  2013 IEEE.",
      "title": "15352 Class-dependent Canonical Correlation Analysis for scalable cross-lingual document categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885664695&partnerID=40&md5=00f9d279ff44a3dcbb55a8c86e0dd4ee"
    },
    {
      "abstract": "This paper describes experiments to adapt document summarization to the medical domain. Our summarizer combines linguistic features corresponding to text fragments (typically sentences) and applies a machine learning approach to extract the most important text fragments from a document to form a summary. The generic features comprise features used in previous research on summarization. We propose to adapt the summarizer to the medical domain by adding domain-specific features. We explore two types of additional features: medical domain features and semantic features. The evaluation of the summarizer is based on medical articles and targets different aspects: i) the classification of text fragments into ones which are important and ones which are unimportant for a summary",
      "title": "15358 Exploring domain-sensitive features for extractive summarization in the medical domain",
      "url": "Conference Paper"
    },
    {
      "abstract": "We present a new approach for analyzing topic models using visual analytics. We have developed TopicView, an application for visually comparing and exploring multiple models of text corpora, as a prototype for this type of analysis tool. TopicView uses multiple linked views to visually analyze conceptual and topical content, document relationships identified by models, and the impact of models on the results of document clustering. As case studies, we examine models created using two standard approaches: Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Conceptual content is compared through the combination of (i) a bipartite graph matching LSA concepts with LDA topics based on the cosine similarities of model factors and (ii) a table containing the terms for each LSA concept and LDA topic listed in decreasing order of importance. Document relationships are examined through the combination of (i) side-by-side document similarity graphs, (ii) a table listing the weights for each documents contribution to each concept/topic, and (iii) a full text reader for documents selected in either of the graphs or the table. The impact of LSA and LDA models on document clustering applications is explored through similar means, using proximities between documents and cluster exemplars for graph layout edge weighting and table entries. We demonstrate the utility of TopicViews visual approach to model assessment by comparing LSA and LDA models of several example corpora.  2013 World Scientific Publishing Company.",
      "title": "15364 Topicview: Visual analysis of topic models and their impact on document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887009381&partnerID=40&md5=8b34ed0d3e6f6c006e3f1d73c944d9c7"
    },
    {
      "abstract": "This paper presents assumptions for a system of automatic cataloging and semantic text documents searching. As an example, a document repository for metals processing technology was used. The system by using ontological model provides the user with a new approach to the exploration of database resources - easier and more intuitive information search. In the current document storage systems, searching is often based only on keywords and descriptions created manually by the system administrator. The use of text mining methods, especially latent semantic indexing, allows automatic clustering of documents with respect to their content. The result of this clustering is integrated with the ontological model, making navigation through documents resources intuitive and does not require the manual creation of directories. Such an approach seems to be particularly useful in a situation where we are dealing with large repositories of unstructured documents from such sources as the Internet. This situation is very typical for cases of searching information and knowledge in the area of metallurgy, for example with reeard to innovation and non-traditional suppliers of materials and equipment.",
      "title": "15368 System of semantic integration of non-structuralized documents in natural language in the domain of metallurgy",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884483602&partnerID=40&md5=1a0e039dbb8f68fda816fe5a28173589"
    },
    {
      "abstract": "Traditional text categorization methods only deal with the content of the documents and use some statistic based metrics to represent the documents. The representation is then used by a machine learning approach to determine the document class. In this picture, the meaning of the document is missing. In order to add meaning into the text categorization process, we start with using part-of-speech tagging (POS). As expected, in a document each part-of-speech tag does not contribute the same amount of information to the document meaning. In addition to the POS information, we make use of WordNet to add semantic features such as synonyms, hypernyms, hyponyms, meronyms and topics into classification process. Using WordNets semantic features introduces ambiguity and not all semantic features are really related to the document content. To overcome this problem, we introduce a new method to eliminate the ambiguity. Various combinations of POS, WordNet and word sense disambiguation are applied and the results show that using semantic features perform better than the traditional, context based methods.  2013 IEEE.",
      "title": "15370 A comprehensive analysis of using semantic information in text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883384735&partnerID=40&md5=ba5f8cf10871094fbcf57f49a27a9539"
    },
    {
      "abstract": "Sentiment classification is one of the important and popular application areas for text classification in which texts are labeled as positive and negative. Moreover, Naive Bayes (NB) is one of the mostly used algorithms in this area. NB having several advantages on lower complexity and simpler training procedure, it suffers from sparsity. Smoothing can be a solution for this problem, mostly Laplace Smoothing is used",
      "title": "15372 Wikipedia based semantic smoothing for twitter sentiment classification",
      "url": ""
    },
    {
      "abstract": "From a documentary point of view, an important aspect when we are conducting a rigorous labeling is to consider the geographic locations related to each document. Although there exist tools and geographic databases, it is not easy to find an automated labeling system for multilingual texts specialized in this type of recognition and further adapted to a particular context. This paper proposes a method that combines geographic location techniques with Natural Language Processing and statistical and semantic disambiguation tools to perform an appropriate labeling in a general way. The method can be configured and fine-tuned for a given context in order to optimize the results. The paper also details an experience of using the proposed method over a content management system in a real organization (a major Spanish newspaper). The experimental results obtained show an overall accuracy of around 80%, which shows the potential of the proposal.  2013 Springer-Verlag.",
      "title": "15373 GEO-NASS: A semantic tagging experience from geographical data on the media",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883275257&partnerID=40&md5=295da4dbb20499510f7d0a70a4a654a2"
    },
    {
      "abstract": "An approach of sentiment classification for online comments based on intuitionistic fuzzy reasoning is presented on the basis of the analysis of characteristics of sentiment classification. The approach employs membership function, non-membership function and hesitant function to depict uncertainties of features, quantitatively, by sample training, as well as sentiment expressions influenced by adverbs of degree, conjunctions and negative words are considered. Then the semantic orientation of a text is synthesized on the level of phrases, sentences and texts in sequence by means of aggregations of intuitionistic fuzzy information of features. The presented approach obtains high precision and recall when test using public corpus. (2013) Trans Tech Publications, Switzerland.  2013 Trans Tech Publications Ltd, Switzerland.",
      "title": "15375 Sentiment orientation classification of webpage online commentary based on intuitionistic fuzzy reasoning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883154895&partnerID=40&md5=17342cf84a9ad964ebca08f4b0e4fd9d"
    },
    {
      "abstract": "Automatic term extraction (ATE) aims at extracting domain-specific terms from a corpus of a certain domain. Termhood is one essential measure for judging whether a phrase is a term. Previous researches on termhood mainly depend on the word frequency information. In this paper, we propose to compute termhood based on semantic representation of words. A novel topic model, namely i-SWB, is developed to map the domain corpus into a latent semantic space, which is composed of some general topics, a background topic and a documents-specific topic. Experiments on four domains demonstrate that our approach outperforms the state-of-the-art ATE approaches. Copyright  2013 ACM.",
      "title": "15379 A novel topic model for automatic term extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883086326&partnerID=40&md5=84c5e2d64386b224e84186d9b180ad0d"
    },
    {
      "abstract": "This work presents results of the ongoing research in the area of natural language processing focusing on plagiarism detection, applying semantic networks and semantic compression. The results demonstrate that the semantic compression is a valuable addition to the existing methods used in plagiary detection. The application of the semantic compression boosts the efficiency of Sentence Hashing Algorithm for Plagiarism Detection 2 (SHAPD2) and w - shingling algorithm. Experiments were performed on Clough & Stephenson corpus as well as on an available PAN-PC plagiarism corpus used to evaluate plagiarism detection methods, so the results can be compared with other research teams.  2013 IEEE.",
      "title": "15382 Evaluation of the SHAPD2 algorithm efficiency in plagiarism detection tasks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84882245114&partnerID=40&md5=a6fabfc7be617ca96a7993d127a760d3"
    },
    {
      "abstract": "Verb is the most important word in a sentence as it asserts an action, events, feeling about the subject and object discussed in the sentence. For news articles, it is observable that there is always at least a verb attached to the person(s) mentioned in the news. As such, a hypothesis has been formed such that there must exist some verbs that specifically describe human being conducts within a news article. In this paper, we propose an approach which aims to identify named-entity (NE) that performs human activity automatically. More specifically, our approach attempts to identify person-related NE generally and person name predefined type specifically by studying the nature of verb that associated with human activity via TreeTagger, Stanford packages and WordNet. The experimental results show that it is viable to use verb in identifying person nameentity type. In addition, our empirical study proves that the approach is applicable to small text size articles. Another significant contribution of our approach is that it does not require training data set and anaphora resolution.  2013 Springer Science+Business Media New York.",
      "title": "15385 Automatic discovery of person-related named-entity in news articles based on verb analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84881306213&partnerID=40&md5=545de98a19b28e37063e5271b299988a"
    },
    {
      "abstract": "Representing the content of the text is really an important issue of knowledge representation. Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human languages. It processes the data through lexical analysis, Syntax analysis, Semantic analysis, Discourse processing, Pragmatic analysis. This paper compares various knowledge representation schemes. The algorithm in this paper splits the English sentences into phrases and then represents these in predicate logic by considering the types of sentences (Simple, Interrogative, Exclamatory, Passive etc.). The algorithm has been tested on real sentences of English. The algorithm has achieved an accuracy of 75%. This representation would be used in future for Semantic based Text summarization.  2013 IEEE.",
      "title": "15388 Knowledge representation: Predicate logic implementation using sentence-type for natural languages",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84881070631&partnerID=40&md5=d1c79ac9773268aad8c54d65de668bc0"
    },
    {
      "abstract": "With the development of computer science and information technology, the library is developing toward information and network. The library digital process converts the book into digital information. The high-quality preservation and management are achieved by computer technology as well as text classification techniques. It realizes knowledge appreciation. This paper introduces complex network theory in the text classification process and put forwards the ICA semantic clustering algorithm. It realizes the independent component analysis of complex network text classification. Through the ICA clustering algorithm of independent component, it realizes character words clustering extraction of text classification. The visualization of text retrieval is improved. Finally, we make a comparative analysis of collocation algorithm and ICA clustering algorithm through text classification and keyword search experiment. The paper gives the clustering degree of algorithm and accuracy figure. Through simulation analysis, we find that ICA clustering algorithm increases by 1.2% comparing with text classification clustering degree. Accuracy can be improved by 11.1% at most. It improves the efficiency and accuracy of text classification retrieval. It also provides a theoretical reference for text retrieval classification of eBook.  2013 ACADEMY PUBLISHER.",
      "title": "15390 Study on the application of text classification retrieval based on complex network and ICA algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880937281&partnerID=40&md5=1702641fbf62a5dbb12e207ae01b29d0"
    },
    {
      "abstract": "Text mining is becoming increasingly important in understanding customers and markets these days. This paper presents a method of mining texts about customer sentiments using a network analysis technique. A data set collected about two global mobile device manufactures were used for testing the method. The analysis results show that the method can be effectively used to extract key sentiments in the customers texts.",
      "title": "15391 Mining texts to understand customers image of brands",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880891839&partnerID=40&md5=184791818dfab6a73c352867be1e33ee"
    },
    {
      "abstract": "For clustering biomedical documents, we can consider three different types of information: the local-content (LC) information from documents, the global-content (GC) information from the whole MEDLINE collections, and the medical subject heading (MeSH)-semantic (MS) information. Previous methods for clustering biomedical documents are not necessarily effective for integrating different types of information, by which only one or two types of information have been used. Recently, the performance of MEDLINE document clustering has been enhanced by linearly combining both the LC and MS information. However, the simple linear combination could be ineffective because of the limitation of the representation space for combining different types of information (similarities) with different reliability. To overcome the limitation, we propose a new semisupervised spectral clustering method, i.e., SSNCut, for clustering over the LC similarities, with two types of constraints: must-link (ML) constraints on document pairs with high MS (or GC) similarities and cannot-link (CL) constraints on those with low similarities. We empirically demonstrate the performance of SSNCut on MEDLINE document clustering, by using 100 data sets of MEDLINE records. Experimental results show that SSNCut outperformed a linear combination method and several well-known semisupervised clustering methods, being statistically significant. Furthermore, the performance of SSNCut with constraints from both MS and GC similarities outperformed that from only one type of similarities. Another interesting finding was that ML constraints more effectively worked than CL constraints, since CL constraints include around 10% incorrect ones, whereas this number was only 1% for ML constraints.  2012 IEEE.",
      "title": "15392 Efficient semisupervised MEDLINE document clustering with MeSH-semantic and global-content constraints",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890347881&partnerID=40&md5=86e2eaafdd59c8729625a0f3c330447e"
    },
    {
      "abstract": "Text often expresses the writers emotional state or evokes emotions in the reader. The nature of emotional phenomena like reading and writing can be interpreted in different ways and represented with different computational models. Affective computing (AC) researchers often use a categorical model in which text data are associated with emotional labels. We introduce a new way of using normative databases as a way of processing text with a dimensional model and compare it with different categorical approaches. The approach is evaluated using four data sets of texts reflecting different emotional phenomena. An emotional thesaurus and a bag-of-words model are used to generate vectors for each pseudo-document, then for the categorical models three dimensionality reduction techniques are evaluated: Latent Semantic Analysis (LSA), Probabilistic Latent Semantic Analysis (PLSA), and Non-negative Matrix Factorization (NMF). For the dimensional model a normative database is used to produce three-dimensional vectors (valence, arousal, dominance) for each pseudo-document. This three-dimensional model can be used to generate psychologically driven visualizations. Both models can be used for affect detection based on distances amongst categories and pseudo-documents. Experiments show that the categorical model using NMF and the dimensional model tend to perform best.  2012 Wiley Periodicals, Inc.",
      "title": "15393 Emotions in text: Dimensional and categorical models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84881427306&partnerID=40&md5=2db20f792754b9aaea18a6e9dd368fa4"
    },
    {
      "abstract": "Submitted to Special Issue on Computational Approaches to Analysis of Emotion in Text Guest editors: Diana Inkpen and Carlo Strapparava Initial publication: NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 5, 2010, Los Angeles, CA Though data-driven in nature, emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge to isolate the emotional keywords or keysets necessary to the construction of affective categories. This makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse. This paper proposes a more general strategy which leverages two separate semantic levels: one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this approach is promising for automatic emotion analysis in text. This bodes well for its deployability in a variety of applications, such as sentiment prediction.  2012 Wiley Periodicals, Inc.",
      "title": "15396 Data-driven analysis of emotion in text using latent affective folding and embedding",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84881428960&partnerID=40&md5=b06a3aeee8abca8699b3b51bc9be0d39"
    },
    {
      "abstract": "The decision of organizations to invest (or not) into a semantic application is, currently, often based on vague considerations and personal feelings. What is lacking is a model that would help determine whether semantic approaches would be adequate, given the aspects of the particular business and concrete adopter. Such a model would however need to take into account the heterogeneity of different applications that exhibit semantic features. We present a thorough exercise, and a prototypical methodology abstracted from it, for proceeding in multiple steps, from loosely sorted and purely textual descriptions of semantic applications to structured and instructive adopter readiness models. The whole process relies on expert-level manual analysis of textual descriptions, automatic cluster analysis (leading to plausible categories of semantic applications), critical factor analysis, questionnaire survey addressing the developers of applications, and adaptation of principles known from building multi-layer Capability Maturity Models. Although the overall approach relies to a large degree on (potentially subjective) manual analysis, very lightweight quantitative evaluation was also made for relevant steps in the process.  2013 Elsevier B.V. All rights reserved.",
      "title": "15398 Towards savvy adoption of semantic technology: From published use cases to category-specific adopter readiness models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884283335&partnerID=40&md5=541fe92ef3c04052fbd4954d71f5acdc"
    },
    {
      "abstract": "Knowledge bases and structured summaries are playing a crucial role in many applications, such as text summarization, question answering, essay grading, and semantic search. Although, many systems (e.g., DBpedia and YaGo2) provide massive knowledge bases of such summaries, they all suffer from incompleteness, inconsistencies, and inaccuracies. These problems can be addressed and much improved by combining and integrating different knowledge bases, but their very large sizes and their reliance on different terminologies and ontologies make the task very difficult. In this demo, we will demonstrate a system that is achieving good success on this task by: i) employing available interlinks in the current knowledge bases (e.g. externalLink and redirect links in DBpedia) to combine information on individual entities, and ii) using widely available text corpora (e.g. Wikipedia) and our IBminer text-mining system, to generate and verify structured information, and reconcile terminologies across different knowledge bases. We will also demonstrate two tools designed to support the integration process in close collaboration with IBminer. The first is the InfoBox Knowledge-Base Browser (IBKB) which provides structured summaries and their provenance, and the second is the InfoBox Editor (IBE), which is designed to suggest relevant attributes for a user-specified subject, whereby the user can easily improve the knowledge base without requiring any knowledge about the internal terminology of individual systems.  2013 VLDB Endowment.",
      "title": "15399 IBminer: A text mining tool for constructing and populating infobox databases and knowledge bases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891097017&partnerID=40&md5=5ce3c1861c218ebe6d0c75160ee25c90"
    },
    {
      "abstract": "Sentence-based multi-document summarization is the task of generating a succinct summary of a document collection, which consists of the most salient document sentences. In recent years, the increasing availability of semantics-based models (e.g., ontologies and taxonomies) has prompted researchers to investigate their usefulness for improving summarizer performance. However, semantics-based document analysis is often applied as a preprocessing step, rather than integrating the discovered knowledge into the summarization process. This paper proposes a novel summarizer, namely Yago-based Summarizer, that relies on an ontology-based evaluation and selection of the document sentences. To capture the actual meaning and context of the document sentences and generate sound document summaries, an established entity recognition and disambiguation step based on the Yago ontology is integrated into the summarization process. The experimental results, which were achieved on the DUC04 benchmark collections, demonstrate the effectiveness of the proposed approach compared to a large number of competitors as well as the qualitative soundness of the generated summaries.  2013 Elsevier Ltd. All rights reserved.",
      "title": "15401 Multi-document summarization based on the Yago ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880537554&partnerID=40&md5=12b09817c85a5220c99a9b507dcdfa1a"
    },
    {
      "abstract": "In recent years, the Triple Helix model has identified feasible approaches to measuring relations among universities, industries, and governments. Results have been extended to different databases, regions, and perspectives. This paper explores how bibliometrics and text mining can inform Triple Helix analyses. It engages Competitive Technical Intelligence concepts and methods for studies of Newly Emerging Science & Technology (NEST) in support of technology management and policy. A semantic TRIZ approach is used to assess NEST innovation patterns by associating topics (using noun phrases to address subjects and objects) and actions (via verbs). We then classify these innovation patterns by the dominant categories of origination: Academy, Industry, or Government. We then use TRIZ tags and benchmarks to locate NEST progress using Technology Roadmapping. Triple Helix inferences can then be related to the visualized patterns. We demonstrate these analyses via a case study for dye-sensitized solar cells.  2013 Budapest, Hungary.",
      "title": "15404 Triple Helix innovation in Chinas dye-sensitized solar cell industry: hybrid methods with semantic TRIZ and technology roadmapping",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880183216&partnerID=40&md5=9bde3942ed466b844da06264bb7a6174"
    },
    {
      "abstract": "We present a platform that combines an approach to semantic extraction of medical information from clinical free-text documents with the processing of structured information from HIS records. The information extraction process uses a fine-grained linguistic analysis, and maps preprocessed terms to the concepts of domain-specific ontologies.",
      "title": "15406 A clinical information management platform for semantic exploitation of clinical data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880048454&partnerID=40&md5=d08f9ea6fb787b1eab7415b34daf8964"
    },
    {
      "abstract": "As the basic unit of human knowledge, the event reflects the movement, behavior and changes in the real world. A large number of narrative class texts contain various events, and events have essential inherent connections among each other in these texts. By studying some traditional text representation method, this paper considers the event as a basic semantic unit for narrative texts, and presents a new event co-occurrence network structure based Chinese text representation method, which combines the characteristics of the graph structure. This method uses event features and the co-occurrence relationship between event features to represent the text, and can retain the structure information and semantic information of the text to a greater extent. The experimental results show that automatic summary based on this method has better performance. 1553-9105/Copyright  2013 Binary Information Press.",
      "title": "15408 Research on event co-occurrence network structure based method for Chinese text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880387828&partnerID=40&md5=e0f07f28089f697b6cfeac3b60f787dc"
    },
    {
      "abstract": "The NSFC is the largest government funding agency in China, with the primary aim to fund and manage basic research. The agency is made up of seven scientific departments, four bureaus, one general office, and three associated units. The scientific departments are the decision-making units responsible for funding recommendations and management of funded projects. Selection of research projects is an important and recurring activity in many organizations such as government research funding agencies. Current method of grouping proposals are based on manual matching of similar research discipline areas but it fails to be accurate. Text clustering methods those are not having semantic approach provide less accuracy. A novel ontology based text mining approach to cluster proposals is proposed. Research project selection is an important task for government and private research funding agencies. When a large number of research proposals are received, it is common to group them according to their similarities in research disciplines. The grouped proposals are then assigned to the appropriate experts for peer review. The review results are collected, and the proposals are then ranked based on the aggregation of the experts review results.  2013 IEEE.",
      "title": "15409 An implementation of clustering project proposals on ontology based text mining approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84879870574&partnerID=40&md5=ec97cf40270d2cbc9e9f9a4fe2bbed24"
    },
    {
      "abstract": "The analysis of microblogging data related with stock markets can reveal relevant new signals of investor sentiment and attention. It may also provide sentiment and attention indicators in a more rapid and cost-effective manner than other sources. In this study, we created several indicators using Twitter data and investigated their value when modeling relevant stock market variables, namely returns, trading volume and volatility. We collected recent data from nine major technological companies. Several sentiment analysis methods were explored, by comparing 5 popular lexical resources and two novel lexicons (emoticon based and the merge of all 6 lexicons) and sentiment indicators produced using two strategies (based on daily words and individual tweet classifications). Also, we measured posting volume associated with tweets related to the analyzed companies. While a short time period is considered (32 days), we found scarce evidence that sentiment indicators can explain these stock returns. However, interesting results were obtained when measuring the value of using posting volume for fitting trading volume and, in particular, volatility. Copyright  2013 ACM.",
      "title": "15410 Some experiments on modeling stock market behavior using investor sentiment analysis and posting volume from twitter",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84879739902&partnerID=40&md5=c4eb63dfb3717ff51b60d4a9cbf8bb9f"
    },
    {
      "abstract": "Financial statements contain quantitative information and managers subjective evaluation of firms financial status. Using information released in U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax, and simple semantics. Our results show that the CRF models are reasonably effective to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. The contribution of our paper is to identify opinion patterns in multiword expressions (MWEs) forms rather than in single word forms. We find that the managers of corporations attempt to use more optimistic words to obfuscate negative financial performance and to accentuate the positive financial performance. Our results also show that decreasing earnings were often accompanied by ambiguous and mild statements in the reporting year and that increasing earnings were stated in assertive and positive way.",
      "title": "15415 Opinion mining for relating subjective expressions and annual earnings in US financial statements",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84878782428&partnerID=40&md5=2554cc8176cc705db37f24f8d7cccfed"
    },
    {
      "abstract": "In recent years, governmental and industrial espionage becomes an increased problem for governments and corporations. Especially information about current technology development and research activities are interesting targets for espionage. Thus, we introduce a new and automated methodology that investigates the information leakage risk of projects in research and technology (R&T) processed by an organization concerning governmental or industrial espionage. Latent semantic indexing is applied together with machine based learning and prediction modeling. This identifies semantic textual patterns representing technologies and their corresponding application fields that are of high relevance for the organizations strategy. These patterns are used to estimate organizations costs of an information leakage for each project. Further, a web mining approach is processed to identify worldwide knowledge distribution within the relevant technologies and corresponding application fields. This information is used to estimate the probability that an information leakage occur. A risk assessment methodology calculates the information leakage risk for each project. In a case study, the information leakage risk of defense based R&T projects is investigated. This is because defense based R&T is of particularly interest by espionage agents. Overall, it can be shown that the proposed methodology is successful in calculation the espionage information leakage risk of projects. This supports an organization by processing espionage risk management.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15416 Protecting research and technology from espionage",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874663135&partnerID=40&md5=0f574cce2542e6385d4037a3b4decd95"
    },
    {
      "abstract": "Web opinion feeds have become one of the most popular information sources users consult before buying products or contracting services. Negative opinions about a product can have a high impact in its sales figures. As a consequence, companies are more and more concerned about how to integrate opinion data in their business intelligence models so that they can predict sales figures or define new strategic goals. After analysing the requirements of this new application, this paper proposes a multidimensional data model to integrate sentiment data extracted from opinion posts in a traditional corporate data warehouse. Then, a new sentiment data extraction method that applies semantic annotation as a means to facilitate the integration of both types of data is presented. In this method, Wikipedia is used as the main knowledge resource, together with some well-known lexicons of opinion words and other corporate data and metadata stores describing the company products like, for example, technical specifications and user manuals. The resulting information system allows users to perform new analysis tasks by using the traditional OLAP-based data warehouse operators. We have developed a case study over a set of real opinions about digital devices which are offered by a wholesale dealer. Over this case study, the quality of the extracted sentiment data is evaluated, and some query examples that illustrate the potential uses of the integrated model are provided.  2013 Springer Science+Business Media New York.",
      "title": "15417 Storing and analysing voice of the market data in the corporate data warehouse",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84878989408&partnerID=40&md5=cc25340a2fb723d42bcd719e0856ed7b"
    },
    {
      "abstract": "After having recalled some well-known shortcomings linked with the Semantic Web approach to the creation of (application oriented) systems of rules-e.g.",
      "title": "15420 Advanced computational reasoning based on the NKRL conceptual model",
      "url": ""
    },
    {
      "abstract": "With the topic analysis models increasingly used in text categorization, many methods are developed to utilize topic analysis models to deal with the noises and sparseness of the text. It is very necessary to estimate the influence of the topic models to classification. And although exploiting external knowledge to enrich semantic of the text has achieved satisfactory results, choosing an appropriate universal corpus is still a knotty problem. In this study, we use topics extracted from texts by the LDA algorithm in two ways (using topics only and combining the topics and texts) to analyze the effect of topic models. And we also make use of different external corpus to value the importance of the external knowledge. The experimental results show that the topic models and the combination of different external datasets benefit categorization a lot. Copyright  2013 Binary Information Press.",
      "title": "15421 Analysis of influence of topic models and different external corpus to text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84879529907&partnerID=40&md5=c320905e7fea7b7e3401a9cadb012a70"
    },
    {
      "abstract": "Among the typical clustering methods, the K-means algorithm plays the most important role in clustering because of its simplicity and efficiency. However, it is sensitive to the initial points and easy to fall into local optimum. In order to avoid this kind of flaw, a patented text clustering algorithm Clustering by Genetic Algorithm Model (CGAM) is revealed in this paper. CGAM constructs the fitness function of genetic algorithm (GA) and convergence criterion for K-means algorithm because GA simulates the natural evolutionary process and deals with a larger search space. To tackle the rich semantics of Chinese texts, CGAM creates an innovative selection method of initial centers of GA and accommodates the contribution of characteristics of different parts of speech. Moreover, the impact of outliers is addressed and treated. Its performance is demonstrated by a series of experiments based on both Reuters-21578 and Chinese text corpus. Experimental results show that the CGAM achieves clustering results better than other GA based K-means algorithms and has been successfully applied to national program of business intelligence system in the context of huge set of contents in both Chinese and English.  2012 Springer Science+Business Media, LLC.",
      "title": "15423 High performance genetic algorithm based text clustering using parts of speech and outlier elimination",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877703472&partnerID=40&md5=5042f879fc3c0a31472d6e04121e4e9f"
    },
    {
      "abstract": "The semantic knowledge of Wikipedia has proved to be useful for many tasks, for example, named entity disambiguation. Among these applications, the task of identifying the word sense based on Wikipedia is a crucial component because the output of this component is often used in subsequent tasks. In this article, we present a two-stage framework (called TSDW) for word sense disambiguation using knowledge latent in Wikipedia. The disambiguation of a given phrase is applied through a two-stage disambiguation process: (a) The first-stage disambiguation explores the contextual semantic information, where the noisy information is pruned for better effectiveness and efficiency",
      "title": "15424 TSDW: Two-stage word sense disambiguation using Wikipedia",
      "url": ""
    },
    {
      "abstract": "This paper presents another approach for determining documents semantic orientation process. It includes a brief introduction describing the area of application of opinion mining, and some definitions useful in the field. The most commonly used methods are mentioned and some alternative ones are described. Experiment results are presented which show that kNN algorithm gives similar results to proportional algorithm.",
      "title": "15425 Performance of K-nearest neighbors algorithm in opinion classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880870292&partnerID=40&md5=85d70feb8754322b134c1c4e5541f8df"
    },
    {
      "abstract": "The Latent Semantic Indexing (LSI) is a commonly used dimensionality reduction methods in text categorization",
      "title": "15429 Dimensionality reduction by combining category information and latent semantic index for text categorization",
      "url": ""
    },
    {
      "abstract": "In order to improve the clustering result of semi-structured texts, it needs to reduce the dimension and sparsity. To reduce the dimensions of semi-structured texts clustering, aimed at meta-data of semi-structured texts, we build the metadata feature vectors. Based on the domain concepts model, we build domain vector based on the domain concepts tree (set). With the help of the WordNet, we compute semantic similarity between the metadata feature vector and the domain vector. Finally, the clustering algorithm is designed to cluster semi-structured texts based on the semantic similarity between metadata feature vectors and domain vectors. The analysis shows that the clustering algorithm is feasible and has higher clustering accurate rate. It can ease the problem of lacking domain ontology and has the ability to improve the clustering quality.  2005 - 2013 JATIT & LLS. All rights reserved.",
      "title": "15430 A semi-structured texts clustering algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877047197&partnerID=40&md5=467cc9bebc051552c65a54583bf2bca2"
    },
    {
      "abstract": "Text classification and information mining are two significant objectives of natural language processing. Whereas handcrafting rules for both tasks has an extensive convention, learning strategies increased much attention in the past. Existing work presented concept based mining model for text, sentence mining and does not support text classification. To enhance the text clustering approach, we first presented Conceptual Rule Mining On Text clusters to evaluate the more related and influential sentences contributing the document topic. But this model might discriminate terms with semantic variation and negligible authority on the sentence meaning. In addition, we plan to extend conceptual text clustering to web documents, by assigning sentence weights based on conditional probability. Probability ratio is identified for the sentence similarity from which unique sentence meaning contributing to the document topic are listed. In this work, our plan is to implement ranking of the sentences which are calculated using the weights assigned to each and every sentences. With sentence rank conceptual rules are defined for the text cluster documents. The conceptual rule depicts finer tuned document topic and sentence meaning utilized to evaluate the global document contribution. Experiments are conducted with the web documents extracted from the research repositories to evaluate the efficiency of the proposed efficient conceptual rule mining on web document clusters using sentence ranking conditional probability [CRMSRCP] and compared with an existing Model for Concept Based Clustering and Classification and our previous works in terms of Sentence Term Relation, Cluster Object weights, and cluster quality.  2013 IEEE.",
      "title": "15431 Mining conceptual rules for web document using sentence ranking conditional probability",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876769508&partnerID=40&md5=e11197f6fccc44cd37b602408da92c6c"
    },
    {
      "abstract": "We show that a task that typically involves rather deep semantic processing of text-being recognizing textual entailment our case study - can be successfully solved without any tools at all specific for the language of the texts on which the task is performed. Instead, we automatically translate the text into English using a standard machine translation system, and then perform all linguistic processing, including syntactic and semantic levels, using only English language linguistic tools. In this case study we use Italian annotated data. Textual entailment is a relation between two texts. To detect it, we use various measures, which allow us to make entailment decision in the two-way classification task (yes / no). We set up various heuristics and measures for evaluating the entailment between two texts based on lexical relations. To make entailment judgments, the system applies named entity recognition module, chunking, part-of-speech tagging, n-grams, and text similarity modules to both texts, all those modules being for English and not for Italian. Rules have been developed to perform the two-way entailment classification. Our system makes entailment judgments basing on the entailment scores for the text pairs. The system was evaluated on Italian textual entailment data sets: we trained our system on Italian development datasets using the WEKA machine learning toolset and tested it on Italian test data sets. The accuracy of our system on the development corpus is 0.525 and on the test corpus is 0.66, which is a good result given that no Italian-specific linguistic information was used.  2013 Springer-Verlag.",
      "title": "15434 Recognizing textual entailment in non-English text via automatic translation into English",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875871109&partnerID=40&md5=8e0c73b426f6df3a9ed4e50a7e5f3b38"
    },
    {
      "abstract": "The analysis of discourse phenomena is essential in many natural language processing (NLP) applications. The growing diversity of available corpora and NLP tools brings a multitude of representation formats. In order to alleviate the problem of incompatible formats when constructing complex text mining pipelines, the Unstructured Information Management Architecture (UIMA) provides a standard means of communication between tools and resources. U-Compare, a text mining workflow construction platform based on UIMA, further enhances interoperability through a shared system of data types, allowing free combination of compliant components into workflows. Although U-Compare and its type system already support syntactic and semantic analyses, support for the analysis of discourse phenomena was previously lacking. In response, we have extended the U-Compare type system with new discourse-level types. We illustrate processing and visualisation of discourse information in U-Compare by providing several new deserialisation components for corpora containing discourse annotations. The new U-Compare is downloadable from http://nactem.ac.uk/ ucompare.  2013 Springer-Verlag.",
      "title": "15438 Facilitating the analysis of discourse phenomena in an interoperable NLP platform",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875539491&partnerID=40&md5=ea8bf0f51e21d917fdb87d224988fec2"
    },
    {
      "abstract": "We describe the latent semantic indexing subspace signature model (LSISSM) for semantic content representation of unstructured text. Grounded on singular value decomposition, the model represents terms and documents by the distribution signatures of their statistical contribution across the top-ranking latent concept dimensions. LSISSM matches term signatures with document signatures according to their mapping coherence between latent semantic indexing (LSI) term subspace and LSI document subspace. LSISSM does feature reduction and finds a low-rank approximation of scalable and sparse term-document matrices. Experiments demonstrate that this approach significantly improves the performance of major clustering algorithms such as standard K-means and self-organizing maps compared with the vector space model and the traditional LSI model. The unique contribution ranking mechanism in LSISSM also improves the initialization of standard K-means compared with random seeding procedure, which sometimes causes low efficiency and effectiveness of clustering. A two-stage initialization strategy based on LSISSM significantly reduces the running time of standard K-means procedures.  2013 ASIS&T.",
      "title": "15445 Document clustering using the LSI subspace signature model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876680603&partnerID=40&md5=d9021d6aa06caf61f69ac8de5aef9d26"
    },
    {
      "abstract": "Many national and international governments establish organizations for applied science research funding. For this, several organizations have defined procedures for identifying relevant projects that based on prioritized technologies. Even for applied science research projects, which combine several technologies it is difficult to identify all corresponding technologies of all research-funding organizations. In this paper, we present an approach to support researchers and to support research-funding planners by classifying applied science research projects according to corresponding technologies of research-funding organizations. In contrast to related work, this problem is solved by considering results from literature concerning the application based technological relationships and by creating a new approach that is based on latent semantic indexing (LSI) as semantic text classification algorithm. Technologies that occur together in the process of creating an application are grouped in classes, semantic textual patterns are identified as representative for each class, and projects are assigned to one of these classes. This enables the assignment of each project to all technologies semantically grouped by use of LSI. This approach is evaluated using the example of defense and security based technological research. This is because the growing importance of this application field leads to an increasing number of research projects and to the appearance of many new technologies.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15446 Technology classification with latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872016537&partnerID=40&md5=32a51a8fd9f1670fdb8deb25b99904a8"
    },
    {
      "abstract": "This paper addresses an information-extraction problem that aims to identify semantic relations among medical concepts (problems, tests, and treatments) in clinical text. The objectives of the paper are twofold. First, we extend an earlier one-page description (appearing as a part of [5]) of a top-ranked model in the 2010 I2B2 NLP Challenge to a necessary level of details, with the belief that feature design is the most crucial factor to the success of our system and hence deserves a more detailed discussion. We present a precise quantification of the contributions of a wide variety of knowledge sources. In addition, we show the end-to-end results obtained on the noisy output of a top-ranked concept detector, which could help construct a more complete view of the state of the art in the real-world scenario. As the second major objective, we reformulate our models into a composite-kernel framework and present the best result, according to our knowledge, on the same dataset.  2012.",
      "title": "15448 Detecting concept relations in clinical text: Insights from a state-of-the-art model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875601114&partnerID=40&md5=6927fdc4ed63c2eeddd421d61745f21d"
    },
    {
      "abstract": "Unlike full reading, skim-reading involves the process of looking quickly over information in an attempt to cover more material whilst still being able to retain a superficial view of the underlying content. Within this work, we specifically emulate this natural human activity by providing a dynamic graph-based view of entities automatically extracted from text using superficial text parsing / processing techniques. We provide a preliminary web-based tool (called SKIMMR) that generates a network of inter-related concepts from a set of documents. In SKIMMR, a user may browse the network to investigate the lexically-driven information space extracted from the documents. When a particular area of that space looks interesting to a user, the tool can then display the documents that are most relevant to the displayed concepts. We present this as a simple, viable methodology for browsing a document collection (such as a collection scientific research articles) in an attempt to limit the information overload of examining that document collection. This paper presents a motivation and overview of the approach, outlines technical details of the preliminary SKIMMR implementation, describes the tool from the users perspective and summarises the related work.",
      "title": "15450 SKIMMR: Machine-aided skim-reading",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875828157&partnerID=40&md5=f109ea038d6dcb441d1bb0ba1a80ba7c"
    },
    {
      "abstract": "We are building a topic-based, interactive visual analytic tool that aids users in analyzing large collections of text. To help users quickly discover content evolution and significant content transitions within a topic over time, here we present a novel, constraint-based approach to temporal topic segmentation. Our solution splits a discovered topic into multiple linear, non-overlapping sub-topics along a timeline by satisfying a diverse set of semantic, temporal, and visualization constraints simultaneously. For each derived subtopic, our solution also automatically selects a set of representative keywords to summarize the main content of the sub-topic. Our extensive evaluation, including a crowd-sourced user study, demonstrates the effectiveness of our method over an existing baseline. Copyright  2013 ACM.",
      "title": "15451 Optimizing temporal topic segmentation for intelligent text visualization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875866160&partnerID=40&md5=807543e4c644cfe8cbc8a7770ff4597a"
    },
    {
      "abstract": "The tradition methods about feature selection are based on mathematical statistics and lack of semantic information. LDA(Latent Dirichlet Allocation) model is a three-level hierarchical Bayesian and a kind of unsupervised learning model. With the ability to express the text topic which has been put forward in recent years, it can explore the semantic information which has been hidden in the documents without the help of outer knowledge bases. This paper presents a kind of text classification method based on the feature selection and LDA model, blends the digital representation of tests which have been obtained from the mathematical statistics and LDA model and then classify the texts. This method can combine the external and internal information presented by the texts well. Experiments on Sogou Corgus show the method can improve text classification effectiveness: On micro F1 measures, it approaches an improvement of 9.4% and 2.84% respectively on contrast experiments. Copyright  2013 Binary Information Press.",
      "title": "15452 Text classification based on feature selection and LDA model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876030117&partnerID=40&md5=b0db7b6c3bdd421639f533d7fb8cc8ac"
    },
    {
      "abstract": "Conventional document clustering techniques are mainly based on the existence of keywords and the number of occurrences of it. Most of the term frequency based clustering techniques consider the documents as bag-of-words and ignore the important relationships between the words in the document. Phrase based clustering techniques also capture only the order in which the words occur in a sentence rather than the semantics behind the words. Hence a concept based clustering technique is proposed in this paper. It uses Medical Subject Headings MeSH ontology for concept extraction and concept weight calculation based on the identity and synonymy relationships. K-means algorithm is used for clustering the documents based on the semantic similarity and the results are analyzed.  2013 IEEE.",
      "title": "15454 Biomedical document clustering using ontology based concept weight",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874726371&partnerID=40&md5=cb22c2241b39b7e3c6c49a12ce09980c"
    },
    {
      "abstract": "The text data being unstructured pose multiple research issues in document classification. Relevant feature extraction is the foremost problem in the preprocessing stage. SentiWordNet is an ontology that includes numeric scores related to the positive or negative aspects of the words. The work in this paper explores the use of SentiWordNet to extract sentiment features of the words in the song lyrics. The experiments are carried out on a collection of 185 lyrics each belonging to one of the four classes. Three classification algorithms namely, Naive Bayesian (NB), k-Nearest Neighbor (KNN) and Support Vector Machine (SVM) using six measures for attribute relevance analysis namely, Principal Component Analysis (PCA), Latent Semantic Analysis (LSA), Chi-Square (CS), Information Gain (IG), GINI Index (GI) and Gain Ratio (GR) have been applied to model the classifiers. The experiments examine the relevance of the sentiment features for classification. The ratio of the positive and negative scores, normalized ratio, and average of the positive and negative scores are three sentiment features. The experimental results indicate that the Naive Bayesian classifier using the average of the positive and negative score as sentiment feature, and gain ratio as feature selection criteria achieve 78.27% accuracy based on top 10% of the features. The second best accuracy has been achieved by SVM-based classifiers using the average of the positive and negative score as sentiment feature and top 10% features applying all feature selection criteria except CS.  2013 IEEE.",
      "title": "15455 Mood classifiaction of lyrics using SentiWordNet",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874689476&partnerID=40&md5=1d895fa7d11cdb7e30feab984879b20a"
    },
    {
      "abstract": "In recent years, hashing-based methods for large-scale similarity search have sparked considerable research interests in the data mining and machine learning communities. While unsupervised hashing-based methods have achieved promising successes for metric similarity, they cannot handle semantic similarity which is usually given in the form of labeled point pairs. To overcome this limitation, some attempts have recently been made on semi-supervised hashing which aims at learning hash functions from both metric and semantic similarity simultaneously. Existing semi-supervised hashing methods can be regarded as passive hashing since they assume that the labeled pairs are provided in advance. In this paper, we propose a novel framework, called active hashing, which can actively select the most informative labeled pairs for hash function learning. Specifically, it identifies the most informative points to label and constructs labeled pairs accordingly. Under this framework, we use data uncertainty as a measure of informativeness and develop a batch mode algorithm to speed up active selection. We empirically compare our method with a state-of-the-art passive hashing method on two benchmark data sets, showing that the proposed method can reduce labeling cost as well as overcome the limitations of passive hashing.  2012 The Author(s).",
      "title": "15458 Active hashing and its application to image and text retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872416414&partnerID=40&md5=d8fe1bd8990f0a8e70911f6b15d9f663"
    },
    {
      "abstract": "This paper presents a novel unsupervised method for mining time series based on two generative topic models, i.e., probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA). The proposed method treats each time series as a text document, and extracts a set of local patterns from the sequence as words by sliding a short temporal window along the sequence. Motivated by the success of latent topic models in text document analysis, latent topic models are extended to find the underlying structure of time series in an unsupervised manner. The clusters or categories of unlabeled time series are automatically discovered by the latent topic models using bag-of-patterns representation. The proposed method was experimentally validated using two sets of time series data extracted from a public Electrocardiography (ECG) database through comparison with the baseline k-means and the Normalized Cuts approaches. In addition, the impact of the bag-of-patterns parameters was investigated. Experimental results demonstrate that the proposed unsupervised method not only outperforms the baseline k-means and the Normalized Cuts in learning semantic categories of the unlabeled time series, but also is relatively stable with respect to the bag-of-patterns parameters. To the best of our knowledge, this work is the first attempt to explore latent topic models for unsupervised mining of time series data.  2012 Elsevier B.V.",
      "title": "15459 Unsupervised mining of long time series based on latent topic model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870388533&partnerID=40&md5=85f19fdb877c27d5227f889814d717a2"
    },
    {
      "abstract": "We build an open-source toolkit which implements deterministic learning to support search and text classification tasks. We extend the mechanism of logical generalization towards syntactic parse trees and attempt to detect weak semantic signals from them. Generalization of syntactic parse tree as a syntactic similarity measure is defined as the set of maximum common sub-trees and performed at a level of paragraphs, sentences, phrases and individual words. We analyze semantic features of such similarity measure and compare it with semantics of traditional anti-unification of terms. Nearest-neighbor machine learning is then applied to relate a sentence to a semantic class. Using syntactic parse tree-based similarity measure instead of bag-of-words and keyword frequency approach, we expect to detect a weak semantic signal otherwise unobservable. The proposed approach is evaluated in a four distinct domains where a lack of semantic information makes classification of sentences rather difficult. We describe a toolkit which is a part of Apache Software Foun-dation project OpenNLP, designed to aid search engineers in tasks requiring text relevance assessment.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15460 Machine learning of syntactic parse trees for search and classification of text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873997220&partnerID=40&md5=c96a2a51736d831093a53d04bf45150c"
    },
    {
      "abstract": "Modern information technologies and Internet services are suffering from the problem of selecting and managing a growing amount of textual information, to which access is often critical. Machine learning techniques have recently shown excellent performance and flexibility in many applications, such as artificial intelligence and pattern recognition. Question answering (QA) is a method of locating exact answer sentences from vast document collections. This paper presents a machine learning-based question-answering framework, which integrates a question classifier, simple document/passage retrievers, and the proposed context-ranking models. The question classifier is trained to categorize the answer type of the given question and instructs the context-ranking model to re-rank the passages retrieved from the initial retrievers. This method provides flexible features to learners, such as word forms, syntactic features, and semantic word features. The proposed context-ranking model, which is based on the sequential labeling of tasks, combines rich features to predict whether the input passage is relevant to the question type. We employ TREC-QA tracks and question classification benchmarks to evaluate the proposed method. The experimental results show that the question classifier achieves 85.60% accuracy without any additional semantic or syntactic taggers, and reached 88.60% after we employed the proposed term expansion techniques and a predefined related-word set. In the TREC-10 QA task, by using the gold TREC-provided relevant document set, the QA model achieves a 0.563 mean reciprocal rank (MRR) score, and a 0.342 MRR score is achieved after using the simple document and passage retrieval algorithms.  2012 Elsevier Inc. All rights reserved.",
      "title": "15462 A support vector machine-based context-ranking model for question answering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871008442&partnerID=40&md5=3818f5acbda2023d3e87c9b8d6d5232c"
    },
    {
      "abstract": "We present a minimally-supervised approach for learning part-whole relations from texts. Unlike previous techniques, we focused on sparse, domain-specific texts. The novelty in our approach lies in the use of Wikipedia as a knowledge-base, from which we first acquire a set of reliable patterns that express part-whole relations. This is achieved by a minimally-supervised algorithm. We then use the patterns acquired to extract part-whole relation triples from a collection of sparse, domain-specific texts. Our strategy, of learning in one domain and applying the knowledge in another domain is based upon the notion of domain-adaption. It allows us to overcome the challenges of learning the relations directly from the sparse, domain-specific corpus. Our experimental evaluations reveal that, despite its general-purpose nature, Wikipedia can be exploited as a source of knowledge for improving the performance of domain-specific part-whole relation extraction. As our other contributions, we propose a mechanism that mitigates the negative impact of semantic-drift on minimally-supervised algorithms. Also, we represent the patterns in the extracted relations using sophisticated syntactic structures that avoid the limitations of traditional surface string representations. In addition, we show that domain-specific part-whole relations cannot be conclusively classified in existing taxonomies.  2012 Elsevier B.V. All rights reserved.",
      "title": "15463 Minimally-supervised extraction of domain-specific part-whole relations using Wikipedia as knowledge-base",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875252029&partnerID=40&md5=28b707b57022f3236d4b12280b876223"
    },
    {
      "abstract": "Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations become bigger calling for more sophisticated methods, in particular a combination of methods from different research areas including information retrieval, machine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Automatic text analysis become an integral part of many systems, pushing boundaries of research capabilities towards what one can refer to as an artificial intelligence dream - never ending learning from text aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.",
      "title": "15465 Automatic text analysis by artificial intelligence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877024501&partnerID=40&md5=6973beed3d95d2ff12efa106e1018796"
    },
    {
      "abstract": "Data mining algorithms such as data classification or clustering methods exploit features of entities to characterise, group or classify them according to their resemblance. In the past, many feature extraction methods focused on the analysis of numerical or categorical properties. In recent years, motivated by the success of the Information Society and the WWW, which has made available enormous amounts of textual electronic resources, researchers have proposed semantic data classification and clustering methods that exploit textual data at a conceptual level. To do so, these methods rely on pre-annotated inputs in which text has been mapped to their formal semantics according to one or several knowledge structures (e.g. ontologies, taxonomies). Hence, they are hampered by the bottleneck introduced by the manual semantic mapping process. To tackle this problem, this paper presents a domain-independent, automatic and unsupervised method to detect relevant features from heterogeneous textual resources, associating them to concepts modelled in a background ontology. The method has been applied to raw text resources and also to semi-structured ones (Wikipedia articles). It has been tested in the Tourism domain, showing promising results.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15466 An automatic approach for ontology-based feature extraction from heterogeneous textual resources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873992874&partnerID=40&md5=da69fc51296924b53bafc31f840abb54"
    },
    {
      "abstract": "Wikipedia is an important human generated knowledge base containing over 21 million articles organized by millions of categories. In this paper, we exploit Wikipedia for a new task of text mining: Context-aware Concept Categorization. In the task, we focus on categorizing concepts according to their context. We exploit article link feature and category structure in Wikipedia, followed by introducing Wiki3C, an unsupervised and domain independent concept categorization approach based on context. In the approach, we investigate two strategies to select and filter Wikipedia articles for the category representation. Besides, a probabilistic model is employed to compute the semantic relatedness between two concepts in Wikipedia. Experimental evaluation using manually labeled ground truth shows that our proposed Wiki3C can achieve a noticeable improvement over the baselines without considering contextual information.  2013 ACM.",
      "title": "15467 Wiki3C: Exploiting wikipedia for context-aware concept categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874259244&partnerID=40&md5=0d346ac65ff1ca6362db085e04a79829"
    },
    {
      "abstract": "Learning low dimensional representations of text corpora is critical in many content analysis and data mining applications. It is even more desired and challenging to learn a sparse representation in practice for large scale text modeling. However, traditional probabilistic topic models (PTM) lack a mechanism to directly control the posterior sparsity of the inferred representations",
      "title": "15468 Group sparse topical coding: From code to topic",
      "url": ""
    },
    {
      "abstract": "The many endless rivers of text now available present a serious challenge in the task of gleaning, analyzing and discovering useful information. In this paper, we describe a methodology for visualizing text streams in real-time modeled as a dynamic graph and its derived map. The approach automatically groups similar messages into countries, with keyword summaries, using semantic analysis, graph clustering and map generation techniques. It handles the need for visual stability across time by dynamic graph layout and Procrustes projection techniques, enhanced with a novel stable component packing algorithm. The result provides a continuous, succinct view of evolving topics of interest. To make these ideas concrete, we describe their application to an online service called TwitterScope.  2013 Springer-Verlag.",
      "title": "15471 Visualizing streaming text data with dynamic graphs and maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874179419&partnerID=40&md5=a244776fb73835d6b78375d21344a5c9"
    },
    {
      "abstract": "Knowledge representation (KR) is an important area in artificial intelligence (AI) and is often related to specific domains. The representation of knowledge in domain-specific contexts makes it desirable to capture semantics as domain experts would. This motivates the development of semantics-preserving standards for KR within the given domain. In addition to the storage and analysis of information using such standards, the effect of globalization today necessitates the publishing of information on the Web. Thus, it is advisable to use formats that make the information easily publishable and accessible while developing KR standards. In this article, we propose such a standard called Quenching Markup Language (QuenchML). This follows the syntax of the eXtensible Markup Language and captures the semantics of the quenching domain within the heat treating of materials. We describe the development of QuenchML, a multidisciplinary effort spanning the realms of AI, database management, and materials science, considering various aspects such as ontology, data modeling, and domain-specific constraints. We also explain the usefulness of QuenchML in semantics-preserving information retrieval and in text mining guided by domain knowledge. Furthermore, we outline the significance of this work in software tools within the field of AI. Copyright  2013.Cambridge University Press.",
      "title": "15475 QuenchML: A semantics-preserving markup language for knowledge representation in quenching",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872453833&partnerID=40&md5=5ec8bc6e1f0541afde00ef637d9d8fc1"
    },
    {
      "abstract": "This paper describes an approach to assertion classification and an empirical study on the impact this task has on phenotype identification, a real world application in the clinical domain. The task of assertion classification is to assign to each medical concept mentioned in a clinical report (e.g., pneumonia, chest pain) a specific assertion category (e.g., present, absent, and possible). To improve the classification of medical assertions, we propose several new features that capture the semantic properties of special cue words highly indicative of a specific assertion category. The results obtained outperform the current state-of-the-art results for this task. Furthermore, we confirm the intuition that assertion classification contributes in significantly improving the results of phenotype identification from free-text clinical records.  2012 Elsevier Inc.",
      "title": "15476 Assertion modeling and its role in clinical phenotype identification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873102400&partnerID=40&md5=68131dcfde3b03a680cf99ba04a58ee2"
    },
    {
      "abstract": "With an overwhelming amount of published biomedical research, the underlying biomedical knowledge is expanding at an exponential rate. This expansion makes it very difficult to find interested genetics knowledge. And therefore, there is an urgent need for developing text mining approaches to discover new knowledge from publications. This paper presents a text mining approach for multiclass biomedical relations based on predicate argument structure (PAS) and shallow parsing. The approach can mine explicit biomedical relations with semantic enrichment, and visualize relations with semantic network. It first identifies noun phrases based on shallow parsing, and then filters arguments from noun phrases via biomedical ontology dictionary. We have implemented BRES, a text mining system, based on our proposed approach. Our results obtained 67.7% F-measure, 62.5% precision and 73.8% recall for the test dataset. This also shows our proposed approach is promising for developing biomedical text mining technology. Highlights: Mining multiclass biomedical relations",
      "title": "15477 BRES: Extracting multiclass biomedical relations with semantic network",
      "url": ""
    },
    {
      "abstract": "There is a growing interest in efficient models of text mining and an emergent need for new data structures that address word relationships. Detailed knowledge about the taxonomic environment of keywords that are used in text documents can provide valuable insight into the nature of the subject matter contained therein. Such insight may be used to enhance the data structures used in the text data mining task as relationships become usefully apparent. A popular scalable technique used to infer these relationships, while reducing dimensionality, has been Latent Semantic Analysis. We present a new approach, which uses an ontology of lexical abstractions to create abstraction profiles of documents and uses these profiles to perform text organization based on a process that we call frequent abstraction analysis. We introduce TATOO, the Text Abstraction TOOlkit, which is a full implementation of this new approach. We present our data model via an example of how taxonomically derived abstractions can be used to supplement semantic data structures for the text classification task.  2012 Wiley Periodicals, Inc.",
      "title": "15478 Abstracting for dimensionality reduction in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870931082&partnerID=40&md5=22ae72d4609de6b86f357c6883dbf52e"
    },
    {
      "abstract": "Semantic similarity is vital to many areas, such as information retrieval. Various methods have been proposed with a focus on comparing unstructured text documents. Several of these have been enhanced with ontology",
      "title": "15480 Semantic similarity of ontology instances using polarity mining",
      "url": "Review"
    },
    {
      "abstract": "Objective: To identify Common Data Elements (CDEs) in eligibility criteria of multiple clinical trials studying the same disease using a human-computer collaborative approach. Design: A set of free-text eligibility criteria from clinical trials on two representative diseases, breast cancer and cardiovascular diseases, was sampled to identify disease-specific eligibility criteria CDEs. In this proposed approach, a semantic annotator is used to recognize Unified Medical Language Systems (UMLSs) terms within the eligibility criteria text. The Apriori algorithm is applied to mine frequent disease-specific UMLS terms, which are then filtered by a list of preferred UMLS semantic types, grouped by similarity based on the Dice coefficient, and, finally, manually reviewed. Measurements: Standard precision, recall, and F-score of the CDEs recommended by the proposed approach were measured with respect to manually identified CDEs. Results: Average precision and recall of the recommended CDEs for the two diseases were 0.823 and 0.797, respectively, leading to an average F-score of 0.810. In addition, the machine-powered CDEs covered 80% of the cardiovascular CDEs published by The American Heart Association and assigned by human experts. Conclusion: It is feasible and effort saving to use a human-computer collaborative approach to augment domain experts for identifying disease-specific CDEs from free-text clinical trial eligibility criteria.  2012 Elsevier Inc.",
      "title": "15481 A human-computer collaborative approach to identifying common data elements in clinical trial eligibility criteria",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873106410&partnerID=40&md5=d94580560425f8bde421b9875c84106a"
    },
    {
      "abstract": "This paper proposed a method of polarity analysis of stock comments in Chinese based on word clustering, to avoid effect of noisy information without emotional tendency. This method mines semantic association between words to construct the concept dictionary in specific areas dynamically and describes the text feature with the concept constructed. The experimental results show that this method can not only reduces dimensionality of feature space, speed up the process of sentiment classification, but also improve the performance of sentiment classification to a certain extent.  2013 Asian Network for Scientific Information.",
      "title": "15482 Polarity analysis of stock comments in chinese based on word clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872982211&partnerID=40&md5=46d4c5ae4e59f0164a509b609bf2da1b"
    },
    {
      "abstract": "This article proposes a novel approach for text categorization based on a regularization extreme learning machine (RELM) in which its weights can be obtained analytically, and a bias-variance trade-off could be achieved by adding a regularization term into the linear system of single-hidden layer feedforward neural networks. To fit the input scale of RELM, the latent semantic analysis was used to represent text for dimensionality reduction. Moreover, a classification algorithm based on RELM was developed including the uni-label (i. e., a document can only be assigned to a unique category) and multi-label (i. e., a document can be assigned to multiple categories simultaneously) situations. The experimental results in two benchmarks show that the proposed method can produce good performance in most cases, and it could learn faster than popular methods such as feedforward neural networks or support vector machine.  2012 Springer-Verlag London Limited.",
      "title": "15491 Text categorization based on regularization extreme learning machine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874022486&partnerID=40&md5=7db2ceceae10b533babbff501275539b"
    },
    {
      "abstract": "Motivated from related entity finding problem, in this paper, we introduce a novel approach to query answering called NMiner. NMiner takes advantage of heuristics to find answers to complex semantic queries. It uses a combination of natural language processing techniques to parse sentences and extract entities, hypertext structure of the documents to derive relational information, and semantic web data to extract relevant entities as search result candidates. Further, a bimodal network of sentences and entities is created from the search result candidates. Content Centric Ranking (CCR) and Cumulative Structural Similarity (CSS), are proposed to rank the candidate entities. Our empirical study on the ClueWeb09 corpus (with approximately 25 terabytes of web documents) shows that both CSS and CCR outperform PageRank and HITS. Moreover, NMiner proved to be significant in solving the problem of answering complex queries performed against a largely unstructured corpus of text documents.  Springer-Verlag 2012.",
      "title": "15493 NMiner: A system for finding related entities by mining a bimodal network",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871551488&partnerID=40&md5=8194c5142d56c36f9fa681acd01028d9"
    },
    {
      "abstract": "The current trend in image analysis and multimedia is to use information extracted from text and text processing techniques to help vision-related tasks, such as automated image annotation and generating semantically rich descriptions of images. In this work, we claim that image analysis techniques can return the favor to the text processing community and be successfully used for a general-purpose representation of word meaning. We provide evidence that simple low-level visual features can enrich the semantic representation of word meaning with information that cannot be extracted from text alone, leading to improvement in the core task of estimating degrees of semantic relatedness between words, as well as providing a new, perceptually-enhanced angle on word semantics. Additionally, we show how distinguishing between a concept and its context in images can improve the quality of the word meaning representations extracted from images.  2012 ACM.",
      "title": "15496 Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871385304&partnerID=40&md5=6a08a5e3a5740a5a1c8aa9ff8edcd65a"
    },
    {
      "abstract": "Despite various advantages of traditional feature vector model for document representation, the well-known inherent deficiency in this model is sovereign term assumption. This deficiency makes it impossible to identify syntactically different but semantically related terms. In this paper, we demonstrate the use of semantic similarity measure for quantifying the relationship between related terms. Identifying such relationships help in reducing the difference between related documents. In this work, we use only noun terms for enriching the representation model. The natural visualization of clusters is investigated in this study using Emergent Self Organizing Map (ESOM). Experimental results show that incorporation of semantic relationship enhances the accuracy of clustering results.  2012 Springer-Verlag.",
      "title": "15498 Emergent self organizing maps for text cluster visualization by incorporating ontology based descriptors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871386978&partnerID=40&md5=fc9fbaddb4406c5032a407e29d4947b5"
    },
    {
      "abstract": "Traditional document indexing techniques store documents using easily accessible representations, such as inverted indices, which can efficiently scale for large document sets. These structures offer scalable and efficient solutions in text document management tasks, though, they omit the cornerstone of the documents purpose: meaning. They also neglect semantic relations that bind terms into coherent fragments of text that convey messages. When semantic representations are employed, the documents are mapped to the space of concepts and the similarity measures are adapted appropriately to better fit the retrieval tasks. However, these methods can be slow both at indexing and retrieval time. In this paper we propose SemaFor, an indexing algorithm for text documents, which uses semantic spanning forests constructed from lexical resources, like Wikipedia, and WordNet, and spectral graph theory in order to represent documents for further processing.  2012 ACM.",
      "title": "15499 SemaFor: Semantic document indexing using semantic forests",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871071778&partnerID=40&md5=e42e93c7dd1afccdec26b368e326f926"
    },
    {
      "abstract": "Extracting semantical relations between concepts from texts is an important research issue in text mining and ontology construction. This paper presents a machine learning-based approach to semantic relation discovery using prepositional phrases. The semantic relations are characterized by the prepositions and the semantic classes of the concepts in the prepositional phrase. WordNet and word sense disambiguation are used to extract semantic classes of concepts. Preliminary experimental results are reported here showing the promise of the proposed method.  2012 Springer-Verlag.",
      "title": "15500 Discovering semantic relations using prepositional phrases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870907974&partnerID=40&md5=a648e61075307b5ce3fa32df496e4ae1"
    },
    {
      "abstract": "Purpose - Semantic similarity measures are very important in many computer-related fields. Previous works on applications such as data integration, query expansion, tag refactoring or text clustering have used some semantic similarity measures in the past. Despite the usefulness of semantic similarity measures in these applications, the problem of measuring the similarity between two text expressions remains a key challenge. This paper aims to address this issue. Design/methodology/approach - In this article, the authors propose an optimization environment to improve existing techniques that use the notion of co-occurrence and the information available on the web to measure similarity between terms. Findings - The experimental results using the Miller and Charles and Gracia and Mena benchmark datasets show that the proposed approach is able to outperform classic probabilistic web-based algorithms by a wide margin. Originality/value - This paper presents two main contributions. The authors propose a novel technique that beats classic probabilistic techniques for measuring semantic similarity between terms. This new technique consists of using not only a search engine for computing web page counts, but a smart combination of several popular web search engines. The approach is evaluated on the Miller and Charles and Gracia and Mena benchmark datasets and compared with existing probabilistic web extraction techniques.  Emerald Group Publishing Limited.",
      "title": "15501 Smart combination of web measures for solving semantic similarity problems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870901699&partnerID=40&md5=aeb85698da34bbe4fcd60318de43fcdc"
    },
    {
      "abstract": "Nowadays, people frequently use search engines in order to find the information they need on the Web. Especially Web search constitutes a basic tool used by million researchers in their everyday work. A very popular indexing engine, concerning life sciences and biomedical research is PubMed. PubMed is a free database accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The present search engines usually return search results in a global ranking making it difficult to the users to browse in different topics or subtopics that they query. Because of this mixing of results belonging to different topics, the average users spend a lot of time to find Web pages, best matching their query. In this paper, we propose a novel system to address this problem. We present and evaluate a methodology that exploits semantic text clustering techniques in order to group biomedical document collections in homogeneous topics. In order to provide more accurate clustering results, we utilize various biomedical ontologies, like MeSH and GeneOntology. Finally, we embed the proposed methodology in an online system that post-processes the PubMed online database in order to provide to users the retrieved results according to well formed topics.  2012 IFIP International Federation for Information Processing.",
      "title": "15502 On topic categorization of PubMed query results",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870923524&partnerID=40&md5=57a286c5a542178fb467ca9af46a54ae"
    },
    {
      "abstract": "Intelligent methods for automatic text processing require linking between lexical resources (texts) and ontologies that define semantics. However, one of the main problems is that while building ontologies, the main effort is put to the construction of the conceptual part, whereas the lexical aspects of ontologies are usually diminished. Therefore, analyzing texts, it is usually difficult to map words to concepts from the ontology. Usually one should consider various linguistic relationships, such as homonymy, synonymy, etc. However, they are not clearly reflected in the conceptual part. We propose LEXO - a special lexical layer, which is thought as a bridge between text and the conceptual core of the ontology. LEXO is dedicated to storing linguistic relationships along with textual evidence for the relationships (as discovered in the text mining process). In addition, we present an algorithm based on LEXO for determining meaning of a given term in an analyzed text.  2012 Springer-Verlag.",
      "title": "15504 Lexical ontology layer - A bridge between text and concepts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870914302&partnerID=40&md5=b0fbe6f82733acae426be6e503da792c"
    },
    {
      "abstract": "Multilevel security (MLS) is specifically created to protect information from unauthorized access. In MLS, documents are assigned to a security label by a trusted subject e.g. an authorized user and based on this assignment",
      "title": "15505 Improved multilevel security with latent semantic indexing",
      "url": ""
    },
    {
      "abstract": "We investigate e-commerce success factors concerning their impact on the success of commerce transactions between businesses companies. In scientific literature, many e-commerce success factors are introduced. Most of them are focused on companies website quality. They are evaluated concerning companies success in the business to consumer (B2C) environment where consumers choose their preferred e-commerce websites based on these success factors e.g. website content quality, website interaction, and website customization. In contrast to previous work, this research focuses on the usage of existing e-commerce success factors for predicting successfulness of business-to-business (B2B) ecommerce. The introduced methodology is based on the identification of semantic textual patterns representing success factors from the websites of B2B companies. The successfulness of the identified success factors in B2B ecommerce is evaluated by regression modeling. As a result, it is shown that some B2C e-commerce success factors also enable the predicting of B2B e-commerce success while others do not. This contributes to the existing literature concerning ecommerce success factors. Further, these findings are valuable for B2B e-commerce websites creation.  2012 IEEE.",
      "title": "15507 Using webcrawling of publicly-available websites to assess e-commerce relationships",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874809994&partnerID=40&md5=aa7f941b2dc29eed27590417f5f6f895"
    },
    {
      "abstract": "In information retrieval, one of the main problems is to retrieve a set of documents that is semantically related to a given user query. Efficient estimation of semantic similarity between words is critical for various natural language processing tasks such as Word Sense Disambiguation (WSD), textual entailment and automatic text summarization. We propose an empirical method to estimate semantic similarity using Fuzzy Formal Concept Analysis. Grouping the different lexical patterns enable us to represent a semantic relation between two words accurately. Specifically, we define various word cooccurrence measures using page counts and integrate those with lexical patterns extracted from text snippets. To identify the numerous semantic relations that exist between two given words, we propose a novel pattern extraction algorithm and a pattern clustering algorithm. The optimal combination of page counts-based co-occurrence measures and lexical pattern clusters is learned using support vector machines. The proposed method outperforms various baselines and previously proposed web-based semantic similarity measures on three benchmark data sets showing a high correlation with human ratings. Moreover, the proposed method significantly improves the accuracy in a community mining task. Copyright  2012 ACM.",
      "title": "15510 Context similarity measure using fuzzy formal concept analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870664604&partnerID=40&md5=bdd5364b6adf26036fa1b23be006b8b2"
    },
    {
      "abstract": "We propose a novel framework that enables multifaceted analysis of news articles. This system uses semantic annotated information (e.g., person, place) as facets and can be used to construct structured queries for comparing the differences between sets of articles.",
      "title": "15514 Multifaceted analysis of news articles by using semantic annotated information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870560036&partnerID=40&md5=ed465f433bb9f042b6061f70db7a12c7"
    },
    {
      "abstract": "In this paper, we design a classifier based-on topic for food complain documents, and take a series of measures to the implementation process. In order to accomplish feature reduction, the filter method named term filtering for independent topic features is proposed to compress each topic feature vector. We introduce the created food ontology as background knowledge and to expand the semantic of complaint documents with the aid of HowNet. So we can supplement effective information and improve the effect of text classification. In addition, we take account of different importance between title and body in the text, considering that title can stand out the topic of text better than the textual body. Consequently, we separately calculate the word frequency which words are in textual title and body. The experiments show that it is necessary to consider the different importance between textual title and body, and we can achieve good feature reduction effect using the proposed filter method, and the classification performance get obvious improvement after the process of term expanding.  2012 ACADEMY PUBLISHER.",
      "title": "15515 Research on food complains document classification based-on topic",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870338123&partnerID=40&md5=7556ba71af038684c6196e90eed6f185"
    },
    {
      "abstract": "Natural language processing applications invariably perform word sense disambiguation as one of its processing steps. The accuracy of sense disambiguation depends upon an efficient algorithm as well as a reliable knowledge-base in the form of annotated corpus and/or dictionaries in machine readable form. Algorithms working on corpus for sense disambiguation are generally employed as supervised machine learning systems. But such systems need ample training on the corpus before being applied on the actual data set. This paper discusses an unsupervised approach of a graph-based technique that solely works on a machine-readable dictionary as the knowledge source. This approach can improve the bottleneck problem that persists in corpus-based word sense disambiguation. The method described here attempts to make the algorithm more intelligent by considering various WordNet semantic relations and auto-filtration of content words before graph generation.  2012 IEEE.",
      "title": "15517 Adding intelligence to non-corpus based word sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874134370&partnerID=40&md5=d9385347ac919dfb4f8785a9d0e78810"
    },
    {
      "abstract": "Word clouds have become one of the most widely accepted visual resources for document analysis and visualization, motivating the development of several methods for building layouts of keywords extracted from textual data. Existing methods are effective to demonstrate content, but are not capable of preserving semantic relationships among keywords while still linking the word cloud to the underlying document groups that generated them. Such representation is highly desirable for exploratory analysis of document collections. In this paper we present a novel approach to build document clouds, named ProjCloud that aim at solving both semantical layouts and linking with document sets. ProjCloud generates a semantically consistent layout from a set of documents. Through a multidimensional projection, it is possible to visualize the neighborhood relationship between highly related documents and their corresponding word clouds simultaneously. Additionally, we propose a new algorithm for building word clouds inside polygons, which employs spectral sorting to maintain the semantic relationship among words. The effectiveness and flexibility of our methodology is confirmed when comparisons are made to existing methods. The technique automatically constructs projection based layouts the user may choose to examine in the form of the point clouds or corresponding word clouds, allowing a high degree of control over the exploratory process.  2012 The Author(s).",
      "title": "15518 Semantic wordification of document collections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876017650&partnerID=40&md5=66a67094d9f9cc30f91c43b3fe452066"
    },
    {
      "abstract": "This paper concerns supervised classification of text. Rocchio, the method we choose for its efficiency and extensibility, is tested on three reference corpora 20NewsGroups, OHSUMED and Reuters, using several similarity measures. Analyzing statistical results, many limitations are identified and discussed. In order to overcome these limitations, this paper presents two main solutions: first constituting Rocchio-based classifier committees, and then using semantic resources (ontologies) in order to take meaning into consideration during text classification. These two approaches can be combined in a Rocchio-based semantic classifier committee.  2012 The Authors and IOS Press.",
      "title": "15520 Towards a semantic classifier committee based on Rocchio",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84879069126&partnerID=40&md5=fc9ec77b9b9f27af766863d7aa114fea"
    },
    {
      "abstract": "Explosive growth of data on the web demand techniques, which would enable the user to access desired information. In Information retrieval Document Classification is prerequisite. In practice many classification techniques were and are in use. Term Frequency-Inverse Document Frequency (TF-IDF) is an approach which represents documents based on the frequency of terms in documents. Limitation of this approach is high dimensionality of data. Moreover it does not consider the relations among the terms, resulting in less precise and noisy end result. In our approach we are using weighted Concept Frequency-Inverse Document Frequency (CF-IDF) with background knowledge of domain Ontology, for classification of RSS feed News Items. Metadata information of news items has been used to assign weight to the identified concepts. No trained classifiers are required as Ontology itself acts as a classifier. We have designed ontology based on news industry standards. This classification approach considers relations among the concepts and properties. It results in reduction of noise in final output. It considers only the key concepts of a domain for classification instead of all the terms, which curbs the problem of dimensionality. Evaluation of experimental results reveals that proposed approach gives better classification results.  2012 IEEE.",
      "title": "15522 Classification of RSS feed news items using ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874367704&partnerID=40&md5=f4fd7a89064f6f6c06141f28c184b915"
    },
    {
      "abstract": "This paper presents a vocabulary co-occurrence feature based on vector space model for sentiment classification of Chinese movie reviews. It takes the amount of emotional words for different subject words as the features, and constructed low dimension vector space model by introducing the semantic information into feature extraction. In order to better describe the semantic information, we use term frequency weight instead of Bool weight. In the experiment, we take the N-gram feature and part of speech feature as the reference and use LIBSVM for classification. The result shows that the vocabulary co-occurrence feature not only achieves high accuracy in all the datasets, but also has a high training and predicting speed.",
      "title": "15524 Vocabulary co-occurrence feature based on vector space model for chinese movie reviews sentiment classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871910455&partnerID=40&md5=680e083f337c38dca393f60659c5a5a8"
    },
    {
      "abstract": "The ability to connect the dots in structured background knowledge and also across scientific literature has been demonstrated as a critical aspect of knowledge discovery. It is not unreasonable therefore to expect that connecting-the-dots across massive amounts of healthcare data may also lead to new insights that could impact diagnosis, treatment and overall patient care. Of critical importance is the observation that while structured Electronic Medical Records (EMR) are useful sources of health information, it is often the unstructured clinical texts such as progress notes and discharge summaries that contain rich, updated and granular information. Hence, by coupling structured EMR data with data from unstructured clinical texts, more holistic patient records, needed for connecting the dots, can be obtained. Unfortunately, free-text progress notes are fraught with a lack of proper grammatical structure, and contain liberal use of jargon and abbreviations, together with frequent misspellings. While these notes still serve their intended purpose for medical care, automatically extracting semantic information from them is a complex task. Overcoming this complexity could mean that evidence-based support for structured EMR data using unstructured clinical texts, can be provided. In this work therefore, we explore a pattern-based approach for extracting Smoker Semantic Types (SST) from unstructured clinical notes, in order to enable evidence-based resolution of SSTs asserted in structured EMRs using SSTs extracted from unstructured clinical notes. Our findings support the notion that information present in unstructured clinical text can be used to complement structured healthcare data. This is a crucial observation towards creating comprehensive longitudinal patient models for connecting-the-dots and providing better overall patient care.  2012 IEEE.",
      "title": "15525 Towards comprehensive longitudinal healthcare data capture",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875608230&partnerID=40&md5=679460ea2d5aa0e68d21b875b9613aa2"
    },
    {
      "abstract": "Link-based applications like Wikipedia are becoming increasingly popular because they provide users with an efficient way to find needed knowledge, such as searching for definitions and information about a particular topic, and exploring articles on related topics. This work introduces a semantics-based navigation application called WNavi s, to facilitate information-seeking activities in internal link-based websites in Wikipedia. WNavi s is based on the theories and techniques of link mining, semantic relatedness analysis and text summarization. Our goal is to develop an application that helps users find related articles for a seed query (topic) easily and then quickly check the content of articles to explore a new concept or topic in Wikipedia. Technically, we construct a preliminary topic network by analyzing the internal links of Wikipedia and applying the normalized Google distance algorithm to quantify the strength of the semantic relationships between articles via key terms. Because not all the content of articles in Wikipedia is relevant to users information needs, it is desirable to locate specific information for users and enable them to quickly explore and read topic-related articles. Accordingly, we propose an SNA-based single and multiple-document summarization technique that can extract meaningful sentences from articles. We applied a number of intrinsic and extrinsic evaluation methods to demonstrate the efficacy of the summarization techniques in terms of precision, and recall. The results suggest that the proposed summarization technique is effective. Our findings have implications for the design of a navigation tool that can help users explore related articles in Wikipedia quickly.  2012 Elsevier B.V. All rights reserved.",
      "title": "15526 WNavi s: Navigating Wikipedia semantically with an SNA-based summarization technique",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868659043&partnerID=40&md5=60d1ff1230fd6583189438425951f859"
    },
    {
      "abstract": "Short text classification problem as text classification a branch, in addition to the same with traditional text classification to a certain degree, still need to face some special problems to be solved, because of short text length, features sparse, measuring the words difficultly. Due to the nature of ontology emphasize related field concept, which has the defect of few characteristics in short text, it is necessary to emphasize the relationship between semantic. This paper uses the feature extended method based on theme Ontology. As considering the semantic relations, it can get better classification performance compared to the conventional method. Meanwhile, using Case-Base Maintenance learning via the GC (Generalization Capability) algorithm, which can reduce the case number into K-NN algorithm, can improve efficiency when indexing near neighbor in K-Nearest Neighbor algorithm. The numerical experiments prove the validity of this learning algorithm.  2012 IEEE.",
      "title": "15527 Feature extended short text categorization based on theme ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872960239&partnerID=40&md5=12081414a7c70e2164fa21bc40858bf2"
    },
    {
      "abstract": "Domain ontologies facilitate the organization, sharing and reuse of domain knowledge, and enable various vertical domain applications to operate successfully. Most methods for automatically constructing ontologies focus on taxonomic relations, such as is-kind-of and is-part-of relations. However, much of the domain specific semantics is ignored. This work proposes a semi-unsupervised approach for extracting semantic relations from domain-specific text documents. The approach effectively utilizes text mining and existing taxonomic relations in domain ontologies to discover candidate keywords that can represent semantic relations. A preliminary experiment on the natural science domain (Taiwan K9 education) indicates that the proposed method yields valuable recommendations. This work enriches domain ontologies by adding distilled semantics.  Springer Science+Business Media, LLC 2012.",
      "title": "15529 Extracting semantic relations to enrich domain ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869504357&partnerID=40&md5=ef27a5e1a1d7bf6c8373681f722b276a"
    },
    {
      "abstract": "There are many popular models available for classification of documents like Naive Bayes Classifier, k-Nearest Neighbors and Support Vector Machine. In all these cases, the representation is based on the Bag of words model. This model doesnt capture the actual semantic meaning of a word in a particular document. Semantics are better captured by proximity of words and their occurrence in the document. We propose a new Bag of Phrases model to capture this discriminative power of phrases for text classification. We present a novel algorithm to extract phrases from the corpus using the well known topic model, Latent Dirichlet Allocation(LDA), and to integrate them in vector space model for classification. Experiments show a better performance of classifiers with the new Bag of Phrases model against related representation models.  2012 ICPR Org Committee.",
      "title": "15530 Efficient classification using phrases generated by topic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874562822&partnerID=40&md5=7f845f1c89608a423b7fe87f0c19c0de"
    },
    {
      "abstract": "Main microblog research is focus on the structural analysis of social networks, rather than the text and topic analysis. Traditional topic detection methods could not be applied due to the microblog short text features and structural characteristics. We taken advantage of availability of latent dirichlet allocation (LDA) to expand the text feature space, and used frequency statistics for our topic ranking, and improved it based on the microblog nontext element data and word element. We taken into account both the text context similarity and semantic similarity in order to make it possible that the traditional clustering method can make difference in the microblog text topic analysis. Experimental studies show our method works well on microblog dataset.  2012 IEEE.",
      "title": "15532 Hotspots detection on microblog",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873121656&partnerID=40&md5=39dee50ba52ca3e12aa554581e6e56ad"
    },
    {
      "abstract": "In this paper, a text document categorization method called Theme Word Subspace (TWS) learning is presented, which utilizes theme words jointly express class-semantic information for document classification. In a class corpus, the theme words with high probability distribution in topic structure are extracted firstly, and then these words as important theme element span class subspaces to jointly represent semantic and distribution of the class. For document categorization processing, a text document is belonged to the nearest subspace whose theme words have the best representation for test document. In our TWS, L1, L2 norm are separately used for measuring the distances of a test document to subspaces. Experiments on a large Chinese text corpus, the proposed TWS learning methods exhibit comparable performances for text document category.",
      "title": "15536 Theme word subspace method for text document categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877323640&partnerID=40&md5=ce39be113c067f90b8cf1e0ed4483ecb"
    },
    {
      "abstract": "Social tags have been acknowledged as a highly useful resource in retrieving music by moods or topics. However, since social tags are open for labeling, some social tags are inaccurate. In this paper, we present a new framework to identify accurate social tags of songs. In our framework, we first clean and filter music tags. Then we apply an improved hierarchical clustering algorithm to group the tags to build a tag category. Based on the category, we classify music songs using lyrics. In order to extend the semantic information of lyrics, we apply CLOPE to cluster lyrics and use the centroid of the corresponding cluster to represent the lyrics. Based on the Naive Bayes method, the probability of assigning lyrics to particular class is predicted. The classification result is then used to determine whether a social tag is accurate. The experimental results show that the proposed framework is effective and encouraging.  2012 IEEE.",
      "title": "15537 Identifying accuracy of social tags by using clustering representations of song lyrics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873577224&partnerID=40&md5=db91bcf82baa16c07805b7fef61bb131"
    },
    {
      "abstract": "Emotion detection from text is a relatively new classification task. This paper proposes a novel unsupervised context-based approach to detecting emotion from text at the sentence level. The proposed methodology does not depend on any existing manually crafted affect lexicons such as Word Net-Affect, thereby rendering our model flexible enough to classify sentences beyond Ekmans model of six basic emotions. Our method computes an emotion vector for each potential affect bearing word based on the semantic relatedness between words and various emotion concepts. The scores are then fine tuned using the syntactic dependencies within the sentence structure. Extensive evaluation on various data sets shows that our framework is a more generic and practical solution to the emotion classification problem and yields significantly more accurate results than recent unsupervised approaches.  2012 IEEE.",
      "title": "15538 Unsupervised emotion detection from text using semantic and syntactic relations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84878442848&partnerID=40&md5=a7c6e0f179091decd343af4de18853b7"
    },
    {
      "abstract": "Driving simulators, crash databases, and more recently, naturalistic studies all help understand how changes to vehicle design affect driving safety. The rapid computerization of cars makes it increasingly important to capitalize on these sources and exploit others. The present study explores a rarely analyzed data source on traffic fatalities: National Highway Traffic Safety Administrations vehicle owners complaint database. The textual data within the event description field of each complaint is extracted and analyzed using a text mining approach that involves the use of latent semantic analysis (LSA) for reducing the dimensionality of the problem. Hierarchical clustering is then employed to identify clusters of complaints that share content. Clusters are described in terms of the most frequent terms and the time trends of the complaints within them. The analysis highlights how text mining analysis can help unlock the wealth of information contained in consumer complaint databases. Copyright 2012 by Human Factors and Ergonomics Society, Inc. All rights reserved.",
      "title": "15540 Consumer complaints and traffic fatalities: Insights from the NHTSA vehicle owners complaint database",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873440070&partnerID=40&md5=11caaf849f216f26ba8ef819364b08d6"
    },
    {
      "abstract": "We present a study about the influence of sentiment polarity (positive, negative and neutral) in the Textual Entailment Recognition. The main idea of this paper is guided to identify the behavior of the sentiment polarity (obtained by a method of sentiment polarity classification based at the construction of Relevant Polarity Trees) on the Recognition of textual Entailment. Our analysis was conducted from a semantic conceptual point of view using a multidimensional resource. We also describe an experiment to evaluate the proposal in the Textual Entailment Recognition tack. The well knows dataset from Pascal Challenger RTE-1, RTE-2, RTE-3, RT4 and others like BPI, ENGARTE2 and Lexical were used as a corpus for testing our idea. Considering that only the sentiment polarity is utilized, we get an acceptable 66% of accuracy detecting the entailment relation between text-hypothesis pairs.",
      "title": "15545 Approaching textual entailment with sentiment polarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875155251&partnerID=40&md5=cfb43b22c06c236d98277e16f06e0eea"
    },
    {
      "abstract": "This paper introduces a workbench called SINDI-WALKS capable of extracting inherent valuable information from scientific literatures such as articles and patents. The system identifies PLOTs",
      "title": "15547 SINDI-WALKS: Workbench for PLOT-based technological information extraction and management",
      "url": "Conference Paper"
    },
    {
      "abstract": "The traditional supervised classifier for Text Categorization (TC) is learned from a set of hand-labeled documents. However, the task of manual data labeling is labor intensive and time consuming, especially for a complex TC task with hundreds or thousands of categories. To address this issue, many semi-supervised methods have been reported to use both labeled and unlabeled documents for TC. But they still need a small set of labeled data for each category. In this paper, we propose a Fully Automatic Categorization approach for Text (FACT), where no manual labeling efforts are required. In FACT, the lexical databases serve as semantic resources for category name understanding. It combines the semantic analysis of category names and statistic analysis of the unlabeled document set for fully automatic training data construction. With the support of lexical databases, we first use the category name to generate a set of features as a representative profile for the corresponding category. Then, a set of documents is labeled according to the representative profile. To reduce the possible bias originating from the category name and the representative profile, document clustering is used to refine the quality of initial labeling. The training data are subsequently constructed to train the discriminative classifier. The empirical experiments show that one variant of our FACT approach outperforms the state-of-the-art unsupervised TC approach significantly. It can achieve more than 90% of F1 performance of the baseline SVM methods, which demonstrates the effectiveness of the proposed approaches.  Springer Science+Business Media, LLC 2012.",
      "title": "15549 Exploiting semantic resources for large scale text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869499954&partnerID=40&md5=504a1de378c062fc4fc0957456088925"
    },
    {
      "abstract": "Scientific discovery increasingly requires collaboration between scientific sub-domains that often have different representations for their data. To bridge gaps between varying domain representations, researchers are developing metadata and semantic representations meaningful to broader communities. Through exploiting these representations we propose a logical model and architecture by which cross-domain researchers can more easily discover, use, and eventually archive, data. In this paper we present an architecture, intermediate data model, and methodology for mapping diverse social-ecological data sources stored in relational databases to a common representation, and for classifying textual data using machine learning. The results are visualized through client views that are built against the general logical model, and applied against a longitudinal database from social-ecological research. 2012 IEEE.",
      "title": "15551 Generalized representation and mapping for social-ecological data: Freeing data from the database",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873645917&partnerID=40&md5=5d1fb9342c4b8c7466ea19ac3d2ad29f"
    },
    {
      "abstract": "Extracting important relations between biological components and semantic events involving genes or proteins from literature has become a focus for the biomedical text mining community. In this paper, we review a subgraph matching-based approach proposed in our previous work for mining relations and events in the biomedical literature. Our subgraph matching algorithm is formally presented, along with a detailed analysis of its complexity. We present three different relation/event extraction tasks in which our approach has been successfully applied. Our approach is of considerable value in extracting highly precise, binary relations when appropriate training data is available. Copyright  2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "15553 Subgraph matching-based literature mining for biomedical relations and events",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875583566&partnerID=40&md5=6ae386ed52c1f7dc7a93693f88b26053"
    },
    {
      "abstract": "A novel text association rule approach FHAR algorithm is presented. To overcome the defect of traditional keywords which does not take into account the semantic relation between keywords, FHAR algorithm in the paper is based on concept vector. The density of semantic field and the weight of meaning are used to determine the concept of the keywords, which not only adds the texts semantic, but also reduces vector dimensions",
      "title": "15554 FHAR: A new text association rule algorithm based on concept vector and its application",
      "url": ""
    },
    {
      "abstract": "One of the important Natural Language Processing applications is Text Summarization, which helps users to manage the vast amount of information available, by condensing documents content and extracting the most relevant facts or topics included. Text Summarization can be classified according to the type of summary: extractive, and abstractive. Extractive summary is the procedure of identifying important sections of the text and producing them verbatim while abstractive summary aims to produce important material in a new generalized form. In this paper, a novel approach is presented to create an abstractive summary for a single document using a rich semantic graph reducing technique. The approach summaries the input document by creating a rich semantic graph for the original document, reducing the generated graph, and then generating the abstractive summary from the reduced graph. Besides, a simulated case study is presented to show how the original text was minimized to fifty percent.  2012 IEEE.",
      "title": "15556 Semantic graph reduction approach for abstractive Text Summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874045370&partnerID=40&md5=551da3c758f0ceb1e7bc88132f716795"
    },
    {
      "abstract": "Neuroimaging studies have over the past decades established that language is grounded in sensorimotor areas of the brain. Not only action verbs related to face and hand motion but also emotional expressions activate premotor systems in the brain. Hypothesizing that patterns of neural activation might be reflected in the latent semantics of words, we apply hierarchical clustering and network graph analysis to quantify the interaction of emotion and motion related action verbs based on two large-scale text corpora. Comparing the word topologies to neural networks we suggest that the co-activation of associated word forms in the brain resemble the latent semantics of action verbs, which may in turn reflect parameters of force and spatial differentiation underlying action based language.  2012 IEEE.",
      "title": "15557 On an emotional node: Modeling sentiment in graphs of action verbs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872140912&partnerID=40&md5=721ea81adf7bf7a88f86c77f822fa8eb"
    },
    {
      "abstract": "We analyze the impact of textual information from e-commerce companies websites on their commercial success. The textual information is extracted from web content of e-commerce companies divided into the Top 100 worldwide most successful companies and into the Top 101 to 500 worldwide most successful companies. It is shown that latent semantic concepts extracted from the analysis of textual information can be adopted as success factors for a Top 100 e-commerce company classification. This contributes to the existing literature concerning e-commerce success factors. As evaluation, a regression model based on these concepts is built that is successful in predicting the commercial success of the Top 100 companies. These findings are valuable for e-commerce websites creation.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15559 Predicting e-commerce company success by mining the text of its publicly-accessible website",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865034916&partnerID=40&md5=06df8965b97d8f2a79261fae56a3c9e6"
    },
    {
      "abstract": "Chinese radicals play important roles in forming Chinese characters semantic meaning. The semantic properties of radicals make them a promising source of information to be analyzed in text mining and content extraction. However, until recently there is little research work concentrating on using the radical set in text mining related tasks. We investigate the roles of radicals in Chinese text classification tasks. In the task, texts are transformed into vectors of radicals, characters and words. Radicals are further pruned by their semantic strengths and network traits. We carry out experiments with real data from Open Directory Project. The experiments results justify Chinese radicals as important features for semantic processing in Chinese text mining tasks.  2012 IEEE.",
      "title": "15562 Radical features for Chinese text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872940252&partnerID=40&md5=442861ce8a97f2698e7bd9350da7e262"
    },
    {
      "abstract": "This manuscript presents the study and application of the method of principal component analysis (PCA) in the field of text mining. We began by studying the theoretical basis behind this method and we have focused on two of its variants namely the neural PCA and kernel PCA. We used neural PCA for automatic categorization of text documents through an extraction of semantic concepts. The second contribution of our work is the use of PCA (neuronal and kernel) for the dimension reduction of textual documents through the automatic classification.  2012 IEEE.",
      "title": "15563 Principal component analysis neural network for textual document categorization and dimension reduction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875740244&partnerID=40&md5=eda17bb9c1adc63a21836786682f0d79"
    },
    {
      "abstract": "Representation of semantic information contained in the words is needed for any Arabic Text Mining applications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Stemming, and Light Stemming techniques for measuring the semantic between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results",
      "title": "15565 Stemming versus Light Stemming for measuring the simitilarity between Arabic Words with Latent Semantic Analysis model",
      "url": ""
    },
    {
      "abstract": "In text categorization, the dimensionality reduction methods, such as latent semantic indexing and nonnegative matrix factorization, commonly yield the dense representation that is not consistent with our common knowledge. On the other hand, the popular sparse coding methods are time-consuming and their dictionaries might contain negative entries, which is difficulty to interpret the semantic meaning of text. This paper proposes a novel Non-negative Sparse Semantic Coding (NSSC) approach for text reprentation. NSSC provides an efficient algorithm to construct a set of non-negative basis vectors that span a low dimensional semantic subspace, where each document obtains a non-negative sparse representation corresponding to these basis vectors. Extensive experimental results have shown that the proposed approach achieves a good performance and presents more interpretability with respect to these semantic concepts.  2012 ICPR Org Committee.",
      "title": "15566 Non-negative Sparse Semantic Coding for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874573284&partnerID=40&md5=3cbdcf791a3747454077b90c38708495"
    },
    {
      "abstract": "Multi-document summarization aims to extract information from the original multiple unstructured text documents or other types of multimedia about the same topic and this paper will focus on eventbased multi-document summarization. Event-based extractive summarization attempts to extract sentences and re-organize them in a summary according to the important events that the sentences describe. Events are defined by the event terms and the associated entities at the sentence level. In this paper, we emphasize on the event semantic relations derived from external linguistic resource. Firstly, the graph based on the event semantic relations is constructed and the events in the graph are grouped into clusters using the revised DBSCAN clustering algorithm. Then, we select one event as the representative event for each cluster or one cluster to present the main topic of the documents. Lastly, we generate the summary by extracting the sentences which contain more informative representative events from the documents. The evaluation on the DUC 2001 document sets shows it is necessary to take the semantic relations among the events into consideration and our summarization approach based on event semantic relation graph clustering is effective.",
      "title": "15568 Multi-document extractive summarization using event semantic relation graph clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871454566&partnerID=40&md5=c5e2d1303c85ca752d33b2522c74d618"
    },
    {
      "abstract": "The aim of this paper is to propose a supervised text classification method for the biomedical domain using semantic resources. We choose the traditional text classification method, Rocchio, for its scalability and extendibility with semantic knowledge. This paper proposes to integrate semantic aspects into Rocchio through a conceptualization task. This conceptualization is realized by mapping terms that are extracted from text to their corresponding concepts in the UMLS Metathesaurus in order to take meaning into consideration during text classification. The proposed classifier is tested on the Ohsumed text corpus, which is composed of abstracts of biomedical articles retrieved from the MEDLINE database. The effects of Conceptualization on Rocchios performance are discussed according to different standard similarity measures and to a variety of conceptualization strategies.  2012 IEEE.",
      "title": "15571 Conceptualization effects on MEDLINE documents classification using Rocchio method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84878456239&partnerID=40&md5=e2054dd68ce553a7249f5b99038bd1b0"
    },
    {
      "abstract": "In this work we propose Inclusive vector to keep the key words available in natural language database. The inclusive vectors are generated by the process of extraction of words given in the source and the cited items of records published in the ISI Thompson Citation Indexes. The proposed inclusive vector exhibits related words and the degree of their relationships. In this work we present the results of the implications of using vectors on the automatic classification of natural language text. In this system, preprocessed documents, extra words as well as word stems are at first found. We have used an enhanced algorithm to bring further semantic relations between the cited and source items in citation databases.  2012 IEEE.",
      "title": "15573 Building semantic richness among natural language content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874499013&partnerID=40&md5=ae90f8618334f9823a9b6ac293230ce7"
    },
    {
      "abstract": "Processing short texts is becoming a trend in information retrieval. Since the text has rarely external information, it is more challenging than document. In this paper, keyword clustering is studied for automatic categorization. To obtain semantic similarity of the keywords, a broad-coverage lexical resource WordNet is employed. We introduce a semantic hierarchical clustering. For automatic keyword categorization, a validity index for determining the number of clusters is proposed. The minimum value of the index indicates the potentially appropriate categorization. We show the result in experiments, which indicates the index is effective.  2012 ICPR Org Committee.",
      "title": "15577 Keyword clustering for automatic categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874577727&partnerID=40&md5=1142c4805b45077447317cd2c6d3c5e9"
    },
    {
      "abstract": "We propose the Least Information theory (LIT) to quantify meaning of information in probability distributions and derive a new document representation model for text classification. By extending Shannon entropy to accommodate a non-linear relation between information and uncertainty, LIT offers an information-centric approach to weight terms based on probability distributions in documents vs. in the collection. We develop two term weight quantities in the document classification context: 1) LI Binary (LIB) which quantifies (least) information due to the observation of a terms (binary) occurrence in a document",
      "title": "15578 Least information document representation for automated text classification",
      "url": ""
    },
    {
      "abstract": "The current work aims to validate, by means of a computational model, an empirical database of free word association norms of Mexican Spanish. Specifically, this work has two main goals: (1) to detect the associated weight of word word pairs, and (2) to provide an understanding of a lexical network formed beyond an input-output word pair, similar to the mediated priming effect reported experimentally. We used the Term Frequency-Inverse Document Frequency weighting (TFIDF) to obtain the associated weight between an input output word pair and to calculate the TFIDF-Matrix which is used as an input in the Latent Semantic Analysis (LSA) Model. The LSA model is a semantic representation at the lexical level that allows us to understand semantic relationships beyond input-output word pairs. Our computational model replicates and further explains previous experimental work on lexical networks.  2012 IEEE.",
      "title": "15579 Latent semantic analysis model as a representation of free-association word norms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872558581&partnerID=40&md5=eb8ae079d7041eb1b51b33915bbf1999"
    },
    {
      "abstract": "The aim of text document classification is to automatically group a document to a predefined class. The main problem of document classification is high dimensionality and sparsity of the data matrix. A new feature selection technique using the google distance have been proposed in this article to effectively obtain a feature subset which improves the classification accuracy. Normalized google distance can automatically extract the meaning of terms from the world wide web. It utilizes the advantage of number of hits returned by the google search engine to compute the semantic relation between two terms. In the proposed approach, only the distance function of google distance is used to develop a relation between a feature and a class for document classification and it is independent of google search results. Every feature will generate a score based on their relation with all the classes and then all the features will be ranked accordingly. The experimental results are presented using knn classifier on several TREC and Reuter data sets. Precision, recall, f-measure and classification accuracy are used to analyze the results. The proposed method is compared with four other feature selection methods for document classification, document frequency thresholding, information gain, mutual information and Chi2 statistic. The empirical studies have shown that the proposed method have effectively done feature selection in most of the cases with either an improvement or no change of classification accuracy.  Springer-Verlag 2012.",
      "title": "15581 A feature selection method for improved document classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872696074&partnerID=40&md5=0a524c5d497a2b480ef9779dc4c85926"
    },
    {
      "abstract": "This paper focuses on the semantic collocation errors of Chinese text. Based on study of the related properties of The Semantic Knowledge-base of Modern Chinese (SKCC/SKMC), I put forward a method to use semantic classification for automatic error-detection. This paper mainly introduces the method of constructing semantic knowledge base, and the text automatic error-detection algorithm based on this knowledge base. The experimental results show, this method has good experimental results and application prospects.  2012 IEEE.",
      "title": "15582 Study of automatic semantic errors checking for Chinese text based on SKCC",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872980099&partnerID=40&md5=7543a4fb23fcd31b3d7e805722c2bb6b"
    },
    {
      "abstract": "inference in texts (RITE) attracts growing attention of natural language processing (NLP) researchers in recent years. In this article we propose a novel approach to recognize inference with probabilistic logical reasoning. Our approach is built on Markov logic networks (MLNs) framework which is a Probabilistic Extension Of First-order Logic. We Design Specific Semantic Rules Based On The Surface Syntactic Semantic Representations Of Texts Map These Rules To Logical Representations. We Also Extract Information From Some Knowledge Bases As Common Sense Logic Rules. Then We Utilize MLNs Framework To Make Predictions With Combining Statistical Logical Reasoning. Experiment Results Shows That Our System Can Achieve Better Performance Than State-of-the-art RITE Systems.  2012 ACM 1530-0226/2012/12-ART14 $15.00.",
      "title": "15583 Recognizing inference in texts with markov logic networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871185450&partnerID=40&md5=fcd2aa0246ce924a38d4e56059f157f0"
    },
    {
      "abstract": "Twitter has become a significant means by which people communicate with the world and describe their current activities, opinions and status in short text snippets. Tweets can be analyzed automatically in order to derive much potential information such as, interesting topics, social influence, users communities, etc. Community extraction within social networks has been a focus of recent work in several areas. Different from the most community discovery methods focused on the relations between users, we aim to derive users communities based on common topics from users tweets. For instance, if two users always talk about politic in their tweets, thus they can be grouped in the same community which is related to politic topic. To achieve this goal, we propose a new approach called CETD: Community Extraction based on Topic-Driven-Model. This approach combines our proposed model used to detect topics of the users tweets based on a semantic taxonomy together with a community extraction method based on the hierarchical clustering technique. Our experimentation on the proposed approach shows the relevant of the users communities extracted based on their common topics and domains.  Springer-Verlag 2012.",
      "title": "15584 Community extraction based on topic-driven-model for clustering users tweets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872704923&partnerID=40&md5=b88f0505047006df9a18a2450d5758ad"
    },
    {
      "abstract": "The number of scientific publications available in multiple repositories is huge and rapidly growing. Accessing this information is of critical importance to conducting research and designing experiments. However, retrieving data of particular interest for a specific research field in such a large volume of publications is often like looking for a needle in a haystack. We present a web platform that supports researchers in navigating and curating biochemical literature. Our platform provides a single-point of access to abstracts of publications harvested from multiple databases and supports further analysis of these abstracts. It also allows users to obtain a personalized view of the literature and its semantic analysis results.  2012 IEEE.",
      "title": "15586 Personalized semantic assistance for the curation of biochemical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872576060&partnerID=40&md5=367265a6ca4a91b768488cb434217940"
    },
    {
      "abstract": "This paper proposes a strategy of semantic processing implemented in an Indonesian text understanding evaluation system. It uses component that already developed in Institut Teknologi Bandung consists of POS Tagger and Syntactic Parser.  2012 IEEE.",
      "title": "15587 Implementation of semantic analyzer in Indonesian text-understanding evaluation system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874566858&partnerID=40&md5=a3973b3db672f1812b52cd0113732f97"
    },
    {
      "abstract": "Semantic tuples are core component of text mining and knowledge extraction systems in biomedicine. The practical success of these systems significantly depends on the correctness and quality of the extracted semantic tuples. The quality and correctness of the semantic predictions can be measured against a benchmark semantic structure. In this article, we presented an approach for constructing a reference semantic tuple structure based on the existing biomedical knowledge sources in which the evaluation is based on the UMLS knowledge sources. In the evaluation, 7400 semantic triples are extracted from UMLS knowledge sources and the semantic predictions are constructed using the proposed approach. In the semantic triples, 87 concepts are found redundantly classified and 207 pair of semantic triples showed hierarchically inconsistent. 128 are found to be non-taxonomically inconsistent. The quality of the semantic triple is also judged using expert evaluators. The Cohens kappa coefficient is used to measure the degree of agreement between two evaluators and the result is promising (0.9).  2012 The COLING.",
      "title": "15588 Constructing reference semantic predictions from biomedical knowledge sources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876792926&partnerID=40&md5=7d73e72a2a86b098337f7c317d7269be"
    },
    {
      "abstract": "In this paper, we present a new algorithm based on the LDA (Latent Dirichlet Allocation) and the Support Vector Machine (SVM) used in the classification of Arabic texts. Current research usually adopts Vector Space Model to represent documents in Text Classification applications. In this way, document is coded as a vector of words",
      "title": "15593 Arabic text classification framework based on Latent Dirichlet Allocation",
      "url": "Article"
    },
    {
      "abstract": "service classification approach that is based on text mining, semantic processing and machine learning is proposed. Specifically, WordNet is used to standardize service specifications and the classification approach applied is based on suffix tree clustering. The major advantage of this classification approach is that this approach can obtain the category description of classification while others can not do. In addition, it also has a better performance.  2012 IEEE.",
      "title": "15596 A service classification approach based on suffix tree clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874070164&partnerID=40&md5=83b9aeeba041101c67060cecf94e9615"
    },
    {
      "abstract": "The paper is dedicated to constructing of the new classes of expert and logical-analytical systems, based on the knowledge structures. For this the means of knowledge representation (the extended semantic networks - ESN) and the tools of their processing (the language of logical programming DEKL) have been designed They have been used as basis for creating the new technologies, which provide the following functions: the automatic extraction of the knowledge from natural language texts, forming the Knowledge Base and the solution of the most complex problems of the logical-analytical processing by transformation and comparison of the knowledge structures. On this basis many intellectual systems for different applications have been designed.",
      "title": "15597 Technological peculiarity of knowledge extraction for logical-analytical systems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875170651&partnerID=40&md5=7489c212373c29181979a518d78818eb"
    },
    {
      "abstract": "This paper presents our experimental work on evaluation of Machine Learning based classification approaches (Naive Bayes and SVM) with the Unsupervised Semantic Orientation based SO-PMI-IR algorithm for sentiment analysis of movie review texts. We have used both pre-existing data sets and our own dataset collection comprising of a large number of user reviews for Hindi movies. The Naive Bayes and SVM approaches were implemented in multiple folds. The results, in addition to presenting a detailed comparative view of these techniques, demonstrate that with suitable selection of features the Naive Bayes algorithm performs reasonably well and at times matches the popularly believed superior performance level of SVM, at least for sentiment analysis task. The SO-PMI-IR algorithm produces substantially accurate sentiment classification without the requirement of any prior training. The accuracy of SO-PMI-IR however depends on POS tags used as features and thresholding/ aggregation schemes used.  2012 IEEE.",
      "title": "15598 Evaluating Machine Learning and Unsupervised Semantic Orientation approaches for sentiment analysis of textual reviews",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877934478&partnerID=40&md5=24ec19bc8bd8078186358816e0f826ec"
    },
    {
      "abstract": "Studying the text messages of a user such as his posts in Facebook or his tweets in Twitter can help in detecting his topics of interests. User in Social Network Systems (SNS) posts text messages about a wide diverse of topics. Posts usually written in a non-standard language, which make it not applicable to the standard Natural Language Processing (NLP) techniques used to catch the relations between words in text. In many cases there are semantic relations between the contained entities of posts that can infer the interest of the user. Bag-Of-Words (BOW) based text classification techniques classify this kind of messages to a wide diverse of topics, but they fail in catching the implicit semantic relation between the contained entities. In this paper we propose a technique to discover the implicit semantic relations between entities in text messages, which can infer the interests of a user. The proposed technique based on a semantically enriched graph representation of entities contained in text messages generated by a user, a new algorithm (Root-Path-Degree) is invented and used to find the most representative sub-graph that reflects the semantic implicit interests of the user. An evaluation was done using manually annotated posts of 687 Facebook users. Precision and Recall results showed our technique performs better than the standard BOW technique.  2012 IEEE.",
      "title": "15600 An interests discovery approach in social networks based on semantically enriched graphs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874249018&partnerID=40&md5=54471525fcaab1d53ab42b4ca5f2c66f"
    },
    {
      "abstract": "Mining retrospective events from text streams has been an important research topic. Classic text representation model (i.e., vector space model) cannot model temporal aspects of documents. To address it, we proposed a novel burst-based text representation model, denoted as BurstVSM. BurstVSM corresponds dimensions to bursty features instead of terms, which can capture semantic and temporal information. Meanwhile, it significantly reduces the number of non-zero entries in the representation. We test it via scalable event detection, and experiments in a 10-year news archive show that our methods are both effective and efficient.  2012 Association for Computational Linguistics.",
      "title": "15606 A novel burst-based text representation model for scalable event detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84878168940&partnerID=40&md5=2aa98e89f3a8d0af456da77fe3e9f79b"
    },
    {
      "abstract": "In this paper, a text document categorization method called Theme Word Subspace (TWS) learning is presented, which utilizes theme words jointly express class-semantic information for document classification. In a class corpus, the theme words with high probability distribution in topic structure are extracted firstly, and then these words as important theme element span class subspaces to jointly represent semantic and distribution of the class. For document categorization processing, a text document is belonged to the nearest subspace whose theme words have the best representation for test document. In our TWS, L1, L2 norm are separately used for measuring the distances of a test document to subspaces. Experiments on a large Chinese text corpus, the proposed TWS learning methods exhibit comparable performances for text document category.",
      "title": "15607 From D-I-K to wisdom and meta-synthesis of wisdom",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84877329353&partnerID=40&md5=69225b480d0a5c4c9052da1c0ede8d01"
    },
    {
      "abstract": "Among the various security threats in online games, the use of game bots is the most serious problem. Previous studies on game bot detection have proposed many methods to find out discriminable behaviors of bots from humans based on the fact that a bots playing pattern is different from that of a human. In this paper, we look at the chatting data that reflects gamers communication patterns and propose a communication pattern analysis framework for online game bot detection. In massive multi-user online role playing games (MMORPGs), game bots use chatting message in a different way from normal users. We derive four features",
      "title": "15608 Chatting pattern based game BOT detection: Do they talk like us?",
      "url": ""
    },
    {
      "abstract": "As the wealth of structured repositories of educational content for agricultural object is increasing, the problem of heterogeneity between them on a semantic level is becoming more prominent. Ontology matching is a technique that helps to identify the correspondences on the description schemas of different sources and provide the basis for interesting applications that exploit the information in a linked fashion. The present paper presents a data-driven approach for discovering matches between different classification schemas. The approach is based on content analysis and linguistic processing in order to extract information in the form of relation tuples, use the extracted information to associate the content of different repositories and match their underlying classification schemas based on the degree of content similarity. The preliminary results verified the validity of the approach, as both experiments produced a semantically valid matching in 68% of the examined classes. The results also exposed the need for refinements on the linguistic processing of the available textual information and on the definition of relation similarity, as well as, the need to exploit structural information in order to move from discovering semantically valid matches to effectively handling class specializations and generalizations.  2012 Springer-Verlag.",
      "title": "15609 Data-driven schema matching in agricultural learning object repositories",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869801929&partnerID=40&md5=8d910d460c513fcdd70614d54d1f46c8"
    },
    {
      "abstract": "Keyphrases, synonymously spoken as keywords, represent semantic metadata and play an important role to capture the main theme represented by a large text data collection. Although authors provide a list of about five to ten keywords in scientific publications that are used to map them to respective domains, due to exponential growth of non-scientific documents either on the World Wide Web or in textual databases, an automatic mechanism is sought to identify keyphrases embedded within them. In this paper, we propose the design of a light-weight machine learning approach to identify feasible keyphrases in text documents. The proposed method mines various lexical and semantic features from texts to learn a classification model. The efficacy of the proposed system is established through experimentation on datasets from three different domains.  2012 ICIC International.",
      "title": "15615 A supervised learning approach for automatic keyphrase extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869836101&partnerID=40&md5=a63556913a8faf832184deaf8d63fd13"
    },
    {
      "abstract": "Nowadays open source software has become an indispensable basis for both individual and industrial software engineering. Various kinds of labeling mechanisms like categories and tags are used in open source communities to annotate projects and facilitate the discovery of certain software However as large amounts of software are attached with no/few labels or the existing labels are from different ontology space, it is still hard to retrieve potentially topic-relevant software. This paper highlights the valuable semantic information of project descriptions and labels, proposes labeled software topic detection LSTD a hybrid approach combining topic models and ranking mechanisms to detect and enrich the topics of software by mining the large amount of textual software profiles, which can be employed to do software categorization and tag recommendation. LSTD makes use of labeled LDA to capture the semantic correlations between labels and descriptions and then construct the label-based topic-word matrix. Based on the generated matrix and the generality of labels, LSTD designs a simple yet eficient algorithm to detect the latent topics of software that expressed as relevant and popular labels. Comprehensive evaluations are conducted on the large-scale datasets of representative open source communities and the results validate the effectiveness of LSTD.",
      "title": "15616 Labeled topic detection of open source software from mining mass textual project profiles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869796077&partnerID=40&md5=6110457fed08d79950f852d6a2c77f81"
    },
    {
      "abstract": "In this paper we propose a novel comparative web search system - BiCWS, which can mine cognitive differences from web search results in a multi-language setting. Given a topic represented by two queries (they are the translations of each other) in two languages, the corresponding web search results for the two queries are firstly retrieved by using a general web search engine, and then the bilingual facets for the topic are mined by using a bilingual search results clustering algorithm. The semantics in Wikipedia are leveraged to improve the bilingual clustering performance. After that, the semantic distributions of the search results over the mined facets are visually presented, which can reflect the cognitive differences in the bilingual communities. Experimental results show the effectiveness of our proposed system.  2012 Springer-Verlag.",
      "title": "15618 BiCWS: Mining cognitive differences from bilingual web search results",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869438366&partnerID=40&md5=3f1463b715f79ea63d5c42867c84a119"
    },
    {
      "abstract": "Existing ontology-based knowledge representations systems have achieved considerable success in semantic querying on large biomedical text corpora over keyword-based systems. However, their query expressivity is limited due to the lack of cross-ontology integration and semantic relations. We present a System for Multiple-Ontology Knowledge Representation (SMOKR) to alleviate the problem. The system first performs annotations of phrases and the semantic relations between them using different domain ontologies, before instantiating the ontologies with the annotated phrases. It then integrates the ontologies by matching their instances using simple NLP techniques, and also by matching their concepts using the state-of-the-art Biomedical Ontology Alignment Tool (BOAT). SMOKR performs inconsistency detection to remove conflicting axioms in order to create a consistent ontology for querying. We evaluate the performance of the system by testing it with a set of semantic queries, and the results are compared to a keyword-based search engine, Lucene, and a hybrid system, SSOKR Luc, which combines a knowledge representation system using a single ontology and the keyword-based search engine, Lucene. SMOKR shows the best performance of F-Measures 0.7 and 0.87 on the GRO Corpus and the GENIA Corpus, respectively, compared to that of SSOKR Luc at 0.62 and 0.33, and that of Lucene at 0.36 and 0.12. Copyright  2012 ACM.",
      "title": "15620 Semantic querying over knowledge in biomedical text corpora annotated with multiple ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869388607&partnerID=40&md5=b625162581706ba0c91e16605b16bb46"
    },
    {
      "abstract": "Aiming at more efficient search on the Internet, it seems adequate to deploy classification techniques using semantic resources restricting this search to the users domain of interest. In this work, we try to assess the impact of integrating semantic knowledge on text classification. This integration can be realized in different ways. The one we choose in this paper is the conceptualization. We examine the impact of the different conceptualization strategies on text classification using three traditional text classification methods: Rocchio, Support Vector Machines (SVMs) and Naive Bayes (NB). We restrain our experimentation to the biomedical domain so conceptualization is applied on OHSUMED corpus, mapping terms in text to their corresponding concepts in UMLS Metathesaurus in order to take their meaning into consideration during text classification. Rocchio, SVMs, and NB are tested using different conceptualization strategies in order to evaluate their effect on classification. Preliminary results demonstrate promising improvements.  2012 Springer-Verlag.",
      "title": "15621 The impact of conceptualization on text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869472893&partnerID=40&md5=fa49b21960323131ebd4531fff3c4ac7"
    },
    {
      "abstract": "To solve the problem of sparse keywords and similarity drift in short text segments, this paper proposes short text clustering algorithm with feature keyword expansion (STCAFKE). The method can realize short text clustering by expanding feature keyword based on HowNet and combining K-means algorithm and density algorithm. It may add the number of text keyword with feature keyword expansion and increase text semantic features to realize short text clustering. Experimental results show that this algorithm has increased the short text clustering quality on precision and recall. (2012) Trans Tech Publications, Switzerland.",
      "title": "15626 Short text clustering algorithm with feature keyword expansion",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868651383&partnerID=40&md5=04aab313df3c81f0b93a358b9fed839b"
    },
    {
      "abstract": "The principles of the immune system and Monoclonal were introduced briefly. Focused on the text expressed by the vector space model which was processed by semantic computation, an adaptive polyclonal clustering algorithm was proposed. Firstly, the calculation method was defined for the affinity between antibody and antigens and the affinity of antibodies, the genetic operation factors were designed, replacement, inverse, colonel, crossover, mutation, death, concatenate and clustering included, secondly, the process was given, and lastly, the clustering processes and analysis were done based on the text sets in a corpora. The experiments verifies that the algorithm proposed above can get the rational clustering number and have a better correct identification rate and recall rate.  2012 IEEE.",
      "title": "15627 Immune network based text clustering algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868517875&partnerID=40&md5=256e10badf4c3e2581d3c9e1f241819c"
    },
    {
      "abstract": "Event Extraction is a complex and interesting topic in Information Extraction that includes event extraction methods from free text or web data. The result of event extraction systems can be used in several fields such as risk analysis systems, online monitoring systems or decide support tools. In this paper, we introduce a method that combines lexico - semantic and machine learning to extract event from Vietnamese news. Furthermore, we concentrate to describe event online monitoring system named VnLoc based on the method that was proposed above to extract event in Vietnamese language. Besides, in experiment phase, we have evaluated this method based on precision, recall and F1 measure. At this time of experiment, we on investigated on three types of event: FIRE, CRIME and TRANSPORT ACCIDENT.  2012 IEEE.",
      "title": "15629 VnLoc: A real-time news event extraction framework for Vietnamese",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868314079&partnerID=40&md5=457b2c459c5135508bf574211e655b13"
    },
    {
      "abstract": "In this paper, we describe how we improve our system for Chinese Textual Entailment Recognition by a monolingual machine translation system. Previously, our approach is based on the standard supervised learning classification. We integrate the result of monolingual machine translation system with the other available computational linguistic resources of Chinese language processing to build the system for the natural language processing application. We observed the training corpus and list all possible features. The features include surface text, semantic and syntactical information, such as POS tagging, synonym substitution, and dependency relation. The annotated data is used in training statistical models and build the classifier for the Binary-class Chinese textual Entailment Recognition task. The experiment result shows that the monolingual machine translation technology can improve the system performance in both 10-fold cross validation and open test.  2012 IEEE.",
      "title": "15630 Improving Binary-class Chinese Textural Entailment by monolingual machine translation technology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868325372&partnerID=40&md5=6be085057fdd598f016a04f767e83d3a"
    },
    {
      "abstract": "The task of automatically determining the correct sense of a polysemous word has remained a challenge to this day. In our research, we introduce Concept-Based Disambiguation (CBD), a novel framework that utilizes recent semantic analysis techniques to represent both the context of the word and its senses in a high-dimensional space of natural concepts. The concepts are retrieved from a vast encyclopedic resource, thus enriching the disambiguation process with large amounts of domain-specific knowledge. In such concept-based spaces, more comprehensive measures can be applied in order to pick the right sense. Additionally, we introduce a novel representation scheme, denoted anchored representation, that builds a more specific text representation associated with an anchoring word. We evaluate our framework and show that the anchored representation is more suitable to the task of wordsense disambiguation (WSD). Additionally, we show that our system is superior to state-of-the-art methods when evaluated on domain-specific corpora, and competitive with recent methods when evaluated on a general corpus. Copyright  2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",
      "title": "15632 Concept-based approach to word-sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868268853&partnerID=40&md5=994661fb871fa564b08ad209855a8851"
    },
    {
      "abstract": "We modelled disaster risk attitudes using top down and bottom up approaches. Top down, we constructed an attitudinal model to comprise of affect, behavior and cognition. Bottom-up, we mined ABC semantics from narratives of disaster experiences and gathered ABC data in an online survey. This paper reports two studies: Study 1 identified ABC semantics through text mining",
      "title": "15633 Measuring affect, behavior and cognition for modeling disaster risk attitudes",
      "url": ""
    },
    {
      "abstract": "This paper proposes a novel text information retrieval and visualization approach. This method allows users to browse and query texts in databases in a simple and efficient way. To facilitate content identification, the attractive design of tag clouds has been combined with an innovative text representation structure developed by our research group. This structure stems from the semantics in text representation since it keeps terms together and does not activate them separately. This structure has also been improved by the inclusion of both term order and term weight. Our proposal thus provides a more accurate form of querying and visualization by means of tag clouds.  2012 Springer-Verlag Berlin Heidelberg.",
      "title": "15635 Text retrieval and visualization in databases using tag clouds",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868024346&partnerID=40&md5=afd4062bcf6ba6271516e36625d3cb47"
    },
    {
      "abstract": "Microblogs are a rising social network with distinguishing features such as simplicity and convenience and has already attracted a large number of users and triggered massive information explosion concerning individuals own statuses and opinions. While sentiment analysis of the messages in microblogs is of great value, most of present studies are on English microblogs and few are on Chinese microblogs. Compared to English, Chinese has its unique expression style, such as no spaces or other word delimiters. Furthermore, Chinese short text also has its own properties. Thus we are inspired to explore effective features for sentiment classification of Chinese short text. In this paper, we propose to study userrelated sentiment classification of Chinese microblogs in terms of the statistical and semantic characteristics, and deisgn the corresponding features: ratio of positive words and negative words (PNR), position feature (POS ), collocation of verbs (COL), auxiliary words (AU). Then we employ an SVM-based method to classify the sentiment. Experiments show that the features we design is effective in recognizing the sentiment of messages in microblogs. Copyright  2012 The Institute of Electronics, Information and Communication Engineers.",
      "title": "15637 Affect computation of chinese short text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868259390&partnerID=40&md5=6bddabb51958bfd12e75de63b1bcf639"
    },
    {
      "abstract": "With the growth rate of information repositories, most of the current research effort are focusing on improving the accuracy in searching and managing information (especially text data), because of lacking of adaptive knowledge representation to the information content of these systems. Besides, domain knowledge is evolving and consequently, ontologies should be automatically built and extended. Thus, introducing modularity paradigm in ontology engineering is now important to tackle scalability problems. In this paper, we address the problem of representing modular ontologies at an abstract level that can improve the traditional information system with higher efficiency, in the context of previous work aiming at integrating ontology learning in traditional Information Retrieval systems on the web. The contribution consists in organizing ontology elements into semantic three-layered ontology warehouse (topic classification, domain knowledge representation, and module representation). The proposed model has been applied for textual content semantic search and relevance improvement has been observed.  2012 Springer-Verlag.",
      "title": "15641 Modular ontological warehouse for adaptative information search",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867853309&partnerID=40&md5=dea74d04e1a8ba020c9fb93c119540e6"
    },
    {
      "abstract": "Online databases and search engines usually return a (long) list of hits that satisfy the users search criteria. The returned list of hits is often too long for the user to review every hit if he/she does not know exactly what he/she wants and/or lacks time. Our focus is on biomedical literature search - a healthcare provider needs to find important articles while a patient is waiting for the providers diagnosis or treatment decision. In this paper, we developed a fuzzy logic-based ranking approach for biomedical literature search using relevance feedback with the help of Unified Medical Language System (UMLS). UMLS is a biomedical term database that classifies and defines the biomedical language. Relevance feedback refers to an interactive process that helps to improve the retrieval efficiency via user feedback. UMLS provides meaning and semantic type methods that can be used for search result ranking, but they sometimes do not rank the search result accurately. To preliminarily evaluate our proposed approach, we created a document set containing 10 biomedical papers and 20 synthesized documents from them. We designed experiments to: 1) compare the performance of fuzzy ranking method with UMLS meaning and semantic type methods, and 2) evaluate the effectiveness of using relevance feedback in the search process. Our experiments showed that 1) the fuzzy ranking approach improved the average ranking order accuracy by 3.35% and 29.55% as compared with UMLS meaning and semantic type methods respectively, and 2) better ranking result using relevance feedback in the search process.  2012 IEEE.",
      "title": "15642 Ranking biomedical literature search result based on relevance feedback using fuzzy logic and unified medical language system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867693313&partnerID=40&md5=6bd82c2124aeccb7571a7397b643b1f7"
    },
    {
      "abstract": "We investigate a semi-automated identification of technical problems occurred by armed forces weapon systems during mission of war. The proposed methodology is based on a semantic analysis of textual information in reports from soldiers (war logs). Latent semantic indexing (LSI) with non-negative matrix factorization (NMF) as technique from multivariate analysis and linear algebra is used to extract hidden semantic textual patterns from the reports. NMF factorizes the term-by-war log matrix - that consists of weighted term frequencies - into two non-negative matrices. This enables natural parts-based representation of the report information and it leads to an easy evaluation by human experts because human brain also uses parts-based representation. For an improved research and technology planning, the identified technical problems are a valuable source of information. A case study extracts technical problems from military logs of the Afghanistan war. Results are compared to a manual analysis written by journalists of Der Spiegel.  2012 Springer-Verlag.",
      "title": "15643 Using NMF for analyzing war logs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867688363&partnerID=40&md5=245727a3aaafe445d0c128ecff08dc32"
    },
    {
      "abstract": "The availability of digitized collections of historical data, such as newspapers, increases every day. With that, so does the wish for historians to explore these collections. Methods that are traditionally used to examine a collection do not scale up to todays collection sizes. We propose a method that combines text mining with exploratory search to provide historians with a means of interactively selecting and inspecting relevant documents from very large collections. We assess our proposal with a case study on a prototype system.  2012 Springer-Verlag.",
      "title": "15645 Semantic document selection historical research on collections that span multiple centuries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867679186&partnerID=40&md5=a76b0cd0346a59ea3d874f193be1b0c5"
    },
    {
      "abstract": "Event network is a new semantic-based and event-oriented text representation model, the operation on event network is a good form of semantic computation, which can provide support for text semantic information processing. The paper proposes a new three-step matching algorithm for event network: event matching based on maximum similarity priority, relation matching based on isotropic-relational-distance matrix, and event network matching by integrating event matching and relation matching. The papers experimental results show that the method is feasible and reasonable.  2012 Springer-Verlag.",
      "title": "15647 A novel event network matching algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867659369&partnerID=40&md5=da6e5edbe9f2dfb927f5196d7f4bb7b5"
    },
    {
      "abstract": "The article improved the traditional web text mining technology which can not understand the text semantics. The author discusses the web text mining methods based on the ontology. The author set up the web ontology structure at first, then introduced the concept-concept similarity matrix, and described the relations among the concepts",
      "title": "15648 Research on web text mining",
      "url": ""
    },
    {
      "abstract": "Text mining on a lexical basis is quite well developed for the English language. In compounding languages, however, lexicalized words are often a combination of two or more semantic units. New words can be built easily by concatenating existing ones, without putting any white spaces in between. That poses a problem to existing search algorithms: Such compounds could be of high interest for a search request, but how can be examined whether a compound comprises a given lexeme? A string match can be considered as an indication, but does not prove semantic relation. The same problem is faced when using lexicon based approaches where signal words are defined as lexemes only and need to be identified in all forms of appearance, and hence also as component of a compound. This paper explores the characteristics of compounds and their constituent elements for German, and compares seven algorithms with regard to runtime and error rates. The results of this study are relevant to query analysis and term weighting approaches in information retrieval system design.  2012 ACM.",
      "title": "15650 Algorithms for the verification of the semantic relation between a compound and a given lexeme",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867471847&partnerID=40&md5=acfdf78657b41fd7d18b78327dfadcc3"
    },
    {
      "abstract": "Business intelligence aims to support better business decision making. Customer survey is priceless asset for intelligent business decision-making. However, business analysts usually have to read hundreds of textual comments and tabular data in survey to manually dig out the necessary information to feed business intelligence models and tools. This paper introduces a business intelligence system to solve this problem by extensively utilizing Semantic Web technologies. Ontology based knowledge extraction is the key to extract interesting terms and understand the logic concept of them. All knowledge extracted forms a semantic knowledge base. Flexible user queries and intelligent analysis can be easily issued to the system over the semantic data store through standard protocol. Besides resolving problems in theory, we designed a flexible, intuitive user interaction interface to explain and present the analysis result for business analysts. Through the real usage of this system, it is validated that our system gives good solution for semantic mining on customer survey for business intelligence. Copyright 2012 ACM.",
      "title": "15651 Semantic mining on customer survey",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867482620&partnerID=40&md5=c8849427eb86664e52640885bda9d9f3"
    },
    {
      "abstract": "Online hotel searching is a daunting task due to the wealth of online information. Reviews written by other travelers replace the wordof- mouth, yet turn the search into a time consuming task. Users do not rate enough hotels to enable a collaborative filtering based recommendation. Thus, a cold start recommender system is needed. In this work we design a cold start hotel recommender system, which uses the text of the reviews as its main data. We define context groups based on reviews extracted from TripAdvisor.com and Venere.com. We introduce a novel weighted algorithm for text mining. Our algorithm imitates a user that favors reviews written with the same trip intent and from people of similar background (nationality) and with similar preferences for hotel aspects, which are our defined context groups. Our approach combines numerous elements, including unsupervised clustering to build a vocabulary for hotel aspects, semantic analysis to understand sentiment towards hotel features, and the profiling of intent and nationality groups. We implemented our system which was used by the public to conduct 150 trip planning experiments. We compare our solution to the top suggestions of the mentioned web services and show that users were, on average, 20% more satisfied with our hotel recommendations. We outperform these web services even more in cities where hotel prices are high. Copyright  2012 by the Association for Computing Machinery, Inc. (ACM).",
      "title": "15653 Finding a needle in a haystack of reviews: Cold start context-based hotel recommender system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867365565&partnerID=40&md5=97856e3263a6915dc7e02e72a7cd426c"
    },
    {
      "abstract": "This paper presents a research on the construction of a new unsupervised model for learning a semantic similarity measure from text corpora. Two main components of the model are a semantic interpreter of texts and a similarity function whose properties are derived from data. The first one associates particular documents with concepts defined in a knowledge base corresponding to the topics covered by the corpus. It shifts the representation of a meaning of the texts from words that can be ambiguous to concepts with predefined semantics. With this new representation, the similarity function is derived from data using a modification of the dynamic rule-based similarity model, which is adjusted to the unsupervised case. The adjustment is based on a novel notion of an information bireduct having its origin in the theory of rough sets. This extension of classical information reducts is used in order to find diverse sets of reference documents described by diverse sets of reference concepts that determine different aspects of the similarity. The paper explains a general idea of the approach and also gives some implementation guidelines. Additionally, results of some preliminary experiments are presented in order to demonstrate usefulness of the proposed model.",
      "title": "15662 Unsupervised similarity learning from textual data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866843829&partnerID=40&md5=780593263130fe5364b23d14e6e5e06e"
    },
    {
      "abstract": "Cross-document knowledge discovery is dedicated to exploring meaningful (but maybe unapparent) information from a large volume of textual data. The sparsity and high dimensionality of text data present great challenges for representing the semantics of natural language. Our previously introduced Concept Chain Queries (CCQ) was specifically designed to discover semantic relationships between two concepts across documents where relationships found reveal semantic paths linking two concepts across multiple text units. However, answering such queries only employed the Bag of Words (BOW) representation in our previous solution, and therefore terms not appearing in the text literally are not taken into consideration. Explicit Semantic Analysis (ESA) is a novel method proposed to represent the meaning of texts in a higher dimensional space of concepts which are derived from large-scale human built repositories such as Wikipedia. In this paper, we propose to integrate the ESA technique into our query processing, which is capable of using vast knowledge from Wikipedia to complement existing information from text corpus and alleviate the limitations resulted from the BOW representation. The experiments demonstrate the search quality has been greatly improved when incorporating ESA into answering CCQ, compared with using a BOW-based approach.  2012 Springer-Verlag.",
      "title": "15663 Improving cross-document knowledge discovery using explicit semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866667751&partnerID=40&md5=15bd6d8c8f9a40e26fafbcb9a231d170"
    },
    {
      "abstract": "In this study we present novel feature engineering techniques that leverage the biomedical domain knowledge encoded in the Unified Medical Language System (UMLS) to improve machine-learning based clinical text classification. Critical steps in clinical text classification include identification of features and passages relevant to the classification task, and representation of clinical text to enable discrimination between documents of different classes. We developed novel information-theoretic techniques that utilize the taxonomical structure of the Unified Medical Language System (UMLS) to improve feature ranking, and we developed a semantic similarity measure that projects clinical text into a feature space that improves classification. We evaluated these methods on the 2008 Integrating Informatics with Biology and the Bedside (I2B2) obesity challenge. The methods we developed improve upon the results of this challenges top machine-learning based system, and may improve the performance of other machine-learning based clinical text classification systems. We have released all tools developed as part of this study as open source, available at http://code.google.com/p/ytex.  2012 Elsevier Inc..",
      "title": "15666 Ontology-guided feature engineering for clinical text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865991155&partnerID=40&md5=eafdd10a4b753d0ea12886c9e5788133"
    },
    {
      "abstract": "Collecting design rationale (DR) and making it available in a well-organized manner will better support product design, innovation and decision-making. Many DR systems have been developed to capture DR since the 1970s. However, the DR capture process is heavily human involved. In addition, with the increasing amount of DR available in archived design documents, it has become an acute problem to research a new computational approach that is able to capture DR from free textual contents effectively. In our previous study, we have proposed an ISAL (issue, solution and artifact layer) model for DR representation. In this paper, we focus on algorithm design to discover DR from design documents according to the ISAL modeling. For the issue layer of the ISAL model, we define a semantic sentence graph to model sentence relationships through language patterns. Based on this graph, we improve the manifold-ranking algorithm to extract issue-bearing sentences. To discover solution-reason bearing sentences for the solution layer, we propose building up two sentence graphs based on candidate solution-bearing sentences and reason-bearing sentences respectively, and propagating information between them. For artifact information extraction, we propose two term relations, i.e. positional term relation and mutual term relation. Using these relations, we extend our document profile model to score the candidate terms. The performance and scalability of the algorithms proposed are tested using patents as research data joined with an example of prior art search to illustrate its application prospects.  2011 Elsevier Ltd. All rights reserved.",
      "title": "15669 Learning the whys: Discovering design rationale using text mining - An algorithm perspective",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862291090&partnerID=40&md5=39614e3fe76e94a15962ac272afaaec6"
    },
    {
      "abstract": "Semantic search results clustering is one of the most wanted functionalities of many information retrieval systems including general web search engines as well as domain specific article portals or digital libraries. It may advice the users to describe the need for information in a more precise way. In this paper, we discuss a framework of document description extension which utilizes domain knowledge and semantic similarity. Our idea is based on application of Tolerance Rough Set Model, semantic information extracted from source text and domain ontology to approximate concepts associated with documents and to enrich the vector representation. Some document representation models including document meta-data, citations and semantic information build using MeSH ontology. We compare those models in a search result clustering problem over the freely accessed biomedical research articles from Pubmed Cetral (PMC) portal. The experimental results are showing the advantages of the proposed models. Copyright  2012 ACM.",
      "title": "15670 Enhancing search result clustering with semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866595519&partnerID=40&md5=6c3e3b920d9fc0d2be7a1cc5d55d05d4"
    },
    {
      "abstract": "Exploratory search, in which a user investigates complex concepts, is cumbersome with todays search engines. We present a new exploratory search approach that generates interactive visualizations of query concepts using thematic cartography (e.g. choropleth maps, heat maps). We show how the approach can be applied broadly across both geographic and non-geographic contexts through explicit spatialization, a novel method that leverages any figure or diagram - from a periodic table, to a parliamentary seating chart, to a world map - as a spatial search environment. We enable this capability by introducing explanatory semantic relatedness measures. These measures extend frequently-used semantic relatedness measures to not only estimate the degree of relatedness between two concepts, but also generate human-readable explanations for their estimates by mining Wikipedias text, hyperlinks, and category structure. We implement our approach in a system called Atlasify, evaluate its key components, and present several use cases.  2012 ACM.",
      "title": "15671 Explanatory semantic relatedness and explicit spatialization for exploratory search",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866623262&partnerID=40&md5=711d7cf1d3c984c3851c23ac4848df29"
    },
    {
      "abstract": "Topic modeling can reveal the latent structure of text data and is useful for knowledge discovery, search relevance ranking, document classification, and so on. One of the major challenges in topic modeling is to deal with large datasets and large numbers of topics in real-world applications. In this paper, we investigate techniques for scaling up the non-probabilistic topic modeling approaches such as RLSI and NMF. We propose a general topic modeling method, referred to as Group Matrix Factorization (GMF), to enhance the scalability and efficiency of the non-probabilistic approaches. GMF assumes that the text documents have already been categorized into multiple semantic classes, and there exist class-specific topics for each of the classes as well as shared topics across all classes. Topic modeling is then formalized as a problem of minimizing a general objective function with regularizations and/or constraints on the class-specific topics and shared topics. In this way, the learning of class-specific topics can be conducted in parallel, and thus the scalability and efficiency can be greatly improved. We apply GMF to RLSI and NMF, obtaining Group RLSI (GRLSI) and Group NMF (GNMF) respectively. Experiments on a Wikipedia dataset and a real-world web dataset, each containing about 3 million documents, show that GRLSI and GNMF can greatly improve RLSI and NMF in terms of scalability and efficiency. The topics discovered by GRLSI and GNMF are coherent and have good readability. Further experiments on a search relevance dataset, containing 30,000 labeled queries, show that the use of topics learned by GRLSI and GNMF can significantly improve search relevance.  2012 ACM.",
      "title": "15673 Group matrix factorization for scalable topic modeling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866634599&partnerID=40&md5=eadcbf529bb2ffa704175d40e8ae91b7"
    },
    {
      "abstract": "Background: Online psychiatric texts are natural language texts expressing depressive problems, published by Internet users via community-based web services such as web forums, message boards and blogs. Understanding the cause-effect relations embedded in these psychiatric texts can provide insight into the authors problems, thus increasing the effectiveness of online psychiatric services. Methods: Previous studies have proposed the use of word pairs extracted from a set of sentence pairs to identify cause-effect relations between sentences. A word pair is made up of two words, with one coming from the cause text span and the other from the effect text span. Analysis of the relationship between these words can be used to capture individual word associations between cause and effect sentences. For instance, (broke up, life) and (boyfriend, meaningless) are two word pairs extracted from the sentence pair: I broke up with my boyfriend. Life is now meaningless to me. The major limitation of word pairs is that individual words in sentences usually cannot reflect the exact meaning of the cause and effect events, and thus may produce semantically incomplete word pairs, as the previous examples show. Therefore, this study proposes the use of inter-sentential language patterns such as broke up, boyfriend<, >life, meaningless to detect causality between sentences. The inter-sentential language patterns can capture associations among multiple words within and between sentences, thus can provide more precise information than word pairs. To acquire inter-sentential language patterns, we develop a text mining framework by extending the classical association rule mining algorithm such that it can discover frequently co-occurring patterns across the sentence boundary. Results: Performance was evaluated on a corpus of texts collected from PsychPark (http://www.psychpark. org), a virtual psychiatric clinic maintained by a group of volunteer professionals from the Taiwan Association of Mental Health Informatics. Experimental results show that the use of inter-sentential language patterns outperformed the use of word pairs proposed in previous studies. Conclusions: This study demonstrates the acquisition of inter-sentential language patterns for causality detection from online psychiatric texts. Such semantically more complete and precise features can improve causality detection performance.  2012 Wu et al.",
      "title": "15674 Detecting causality from online psychiatric texts using inter-sentential language patterns",
      "url": ""
    },
    {
      "abstract": "Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.  2012 ACM.",
      "title": "15675 An algorithm for fuzzy-based sentence-level document clustering for micro-level contradiction analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866073354&partnerID=40&md5=4f10226a223769203ad087a09371d1e0"
    },
    {
      "abstract": "Entity synonyms are critical for many applications like information retrieval and named entity recognition in documents. The current trend is to automatically discover entity synonyms using statistical techniques on web data. Prior techniques suffer from several limitations like click log sparsity and inability to distinguish between entities of different concept classes. In this paper, we propose a general framework for robustly discovering entity synonym with two novel similarity functions that overcome the limitations of prior techniques. We develop efficient and scalable techniques leveraging the MapReduce framework to discover synonyms at large scale. To handle long entity names with extraneous tokens, we propose techniques to effectively map long entity names to short queries in query log. Our experiments on real data from different entity domains demonstrate the superior quality of our synonyms as well as the efficiency of our algorithms. The entity synonyms produced by our system is in production in Bing Shopping and Video search, with experiments showing the significance it brings in improving search experience.  2012 ACM.",
      "title": "15676 A framework for robust discovery of entity synonyms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866022707&partnerID=40&md5=3849090bbabed1e98b87b63643f621da"
    },
    {
      "abstract": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.  2012 ACM.",
      "title": "15677 Large-scale learning of word relatedness with constraints",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866033702&partnerID=40&md5=783d89c5bfd3f52b3f17d7692c55dc78"
    },
    {
      "abstract": "Automatic detection of text scope is now crucial for information retrieval tasks owing to semantic, linguistic, and unexpressive content problems, which has increased the demand for uncomplicated, language-independent, and scope-based strategies. In this paper, we extend the vector of documents with exerting impressive words to simplify expressiveness of each document from extracted essential words of related documents and then analyze the network of these words to detect words that share meaningful concepts related to exactly our document. In other words, we analyze each document in only one topic: the topic of that document. We changed measures of social network analysis according to weights of the document words. The impression of these new words to the document can be exerted as changing the document vector weights or inserting these words as metadata to the document. As an example, we classified documents and compared effectiveness of our Intelligent Information Retrieval (IIR) model.  2012 Springer-Verlag.",
      "title": "15683 Extending information retrieval by adjusting text feature vectors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865606983&partnerID=40&md5=c766459c1a0af78ce95daacc91b88600"
    },
    {
      "abstract": "Semantic-based computer-assisted automated question generator has been increasingly popular as a tool for creating personalized assessment questions.Various question generator tools have been proposed, such as those which generate structured question from a text file, generate Multiple Choice Question (MCQ) from a text file or from ontology-based knowledge representation. A comparison framework and evaluation methodology is required to evaluate different question generator tools. This paper discusses the requirement and criteria to carry out performance comparison of different MCQ generator. A feature comparison of Question Generation (QG) tool, namely Mimos-QG with the existing QG tools is presented. We have evaluated our QG tool based on several standard criteria such as the correctness of (a) distractor generation, (b) answer choice grouping strategy and (c) syntactical and pedagogical quality on three different domain ontologies. The experimental result indicated that Mimos-QG is capable of producing good quality direct type and grouping type multiple choice questions.  2012 Springer-Verlag.",
      "title": "15684 Evaluating multiple choice question generator",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865599315&partnerID=40&md5=5ff83a84bed59913ce35bb3cef8e4f18"
    },
    {
      "abstract": "Data sparseness, the evident characteristic of short text, has always been regarded as the main cause of the low accuracy in the classification of short texts using statistical methods. Intensive research has been conducted in this area during the past decade. However, most researchers failed to notice that ignoring the semantic importance of certain feature terms might also contribute to low classification accuracy. In this paper we present a new method to tackle the problem by building a strong feature thesaurus (SFT) based on latent Dirichlet allocation (LDA) and information gain (IG) models. By giving larger weights to feature terms in SFT, the classification accuracy can be improved. Specifically, our method appeared to be more effective with more detailed classification. Experiments in two short text datasets demonstrate that our approach achieved improvement compared with the state-of-the-art methods including support vector machine (SVM) and Naive Bayes Multinomial.  Zhejiang University and Springer-Verlag Berlin Heidelberg 2012.",
      "title": "15688 Short text classification based on strong feature thesaurus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866516645&partnerID=40&md5=41415f514bd9d46fec97928e1d98d67a"
    },
    {
      "abstract": "Semantic similarity plays a vital role in Q & A systems, Text Mining, Language modeling, Information Retrieval, Natural Language Processing (NLP), text-related research and applications. Measuring Semantic similarity between sentences is closely related to Semantic similarity between words. Key word extraction is useful to understand the important information contained in a document or in a short text. This paper proposes two strategies for: (i) finding the similarity and context between two sentences. (ii) Extending this approach for a paragraph of sentences using the WordNet lexical database.  2012 Springer-Verlag.",
      "title": "15689 Ontology based text processing for context, similarity and key word extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865300785&partnerID=40&md5=16815b17662a232caaa5a6f7b2847cd2"
    },
    {
      "abstract": "In software industry a lot of effort is spent in analyzing the bug report to classify the bugs. This Classification helps in assigning the bugs to the specific team for Bug Fixing according to the nature of the bug. In this paper, we have proposed a data mining technique applying syntactic and semantic Feature Extraction to assist developers in bug Classification. Extracted features are organized into different feature groups then a specific preprocessing technique is applied to each feature group. The applied methods have reduced the noise in the bug data compared to traditional approach of word frequency for text categorization. We have analyzed our approach on a collection of bug reports collected from a networking based organization (CISCO).The experiments are performed using Naive Bayes Multinomial Model and Support Vector Machine on features obtained after preprocessing.  2012 Springer-Verlag.",
      "title": "15690 Syntactic and semantic feature extraction and preprocessing to reduce noise in bug classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865283250&partnerID=40&md5=fb9965384444df591dc791913b97bdc3"
    },
    {
      "abstract": "Text clustering can be considered as a four step process consisting of feature extraction, text representation, document clustering and cluster interpretation. Most text clustering models consider text as an unordered collection of words. However the semantics of text would be better captured if word sequences are taken into account. In this paper we propose a sequence based text clustering model where four novel sequence based components are introduced in each of the four steps in the text clustering process. Experiments conducted on the Reuters dataset and Sydney Morning Herald (SMH) news archives demonstrate the advantage of the proposed sequence based model, in terms of capturing context with semantics, accuracy and speed, compared to clustering of documents based on single words and n-gram based models.  2012 IEEE.",
      "title": "15693 A sequence based dynamic SOM model for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865086618&partnerID=40&md5=e67a7f2d43df8ecb1fbd3f3d4933c0cf"
    },
    {
      "abstract": "A substantial amount of subjectivity is involved in how people use language and conceptualize the world. Computational methods and formal representations of knowledge usually neglect this kind of individual variation. We have developed a novel method, Grounded Intersubjective Concept Analysis (GICA), for the analysis and visualization of individual differences in language use and conceptualization. The GICA method first employs a conceptual survey or a text mining step to elicit from varied groups of individuals the particular ways in which terms and associated concepts are used among the individuals. The subsequent analysis and visualization reveals potential underlying groupings of subjects, objects and contexts. One way of viewing the GICA method is to compare it with the traditional word space models. In the word space models, such as latent semantic analysis (LSA), statistical analysis of word-context matrices reveals latent information. A common approach is to analyze term-document matrices in the analysis. The GICA method extends the basic idea of the traditional term-document matrix analysis to include a third dimension of different individuals. This leads to a formation of a third-order tensor of size subjects X objects X contexts. Through flattening into a matrix, these subject-object-context (SOC) tensors can again be analyzed using various computational methods including principal component analysis (PCA), singular value decomposition (SVD), independent component analysis (ICA) or any existing or future method suitable for analyzing high-dimensional data sets. In order to demonstrate the use of the GICA method, we present the results of two case studies. In the first case, GICA of health-related concepts is conducted. In the second one, the State of the Union addresses by US presidents are analyzed. In these case studies, we apply multidimensional scaling (MDS), the self-organizing map (SOM) and Neighborhood Retrieval Visualizer (NeRV) as specific data analysis methods within the overall GICA method. The GICA method can be used, for instance, to support education of heterogeneous audiences, public planning processes and participatory design, conflict resolution, environmental problem solving, interprofessional and interdisciplinary communication, product development processes, mergers of organizations, and building enhanced knowledge representations in semantic web.  2012 IEEE.",
      "title": "15695 Subjects on objects in contexts: Using GICA method to quantify epistemological subjectivity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865062431&partnerID=40&md5=4eb62f83215f23817755d987c81d56ed"
    },
    {
      "abstract": "In this paper, we propose a model for semantic clustering of entities extracted from a text, and we apply it to a Proper Noun classification task. This model is based on a new method to compute the similarity between the entities. Indeed, the classical way of calculating similarity is to build a feature vector or Bag-of-Features for each entity and then use classical similarity functions like Cosine. In practice, the features are contextual, such as words around the different occurrences of each entity. Here, we propose to use an alternative representation for entities, called Bag-of-Vectors, or Bag-of-Bags-of-Features. In this new model, each entity is not defined as a unique vector but as a set of vectors, in which each vector is built based on the contextual features of one occurrence of the entity. In order to use Bag-of-Vectors for clustering, we introduce new versions of classical similarity functions such as Cosine and Scalar Products. Experimentally, we show that the Bag-of-Vectors representation always improve the clustering results compared to classical Bag-of-Features representations. Copyright  2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",
      "title": "15696 Proper noun semantic clustering using bag-of-vectors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864980945&partnerID=40&md5=ae3f52ad5c1e334c8f18a2d2952f4089"
    },
    {
      "abstract": "Measuring inter-document similarity is one of the most essential steps in text document clustering. Traditional methods rely on representing text documents using the simple Bag-of-Words (BOW) model. A document is an organized structure consisting of various text segments or passages. Such single term analysis of the text treats whole document as a single semantic unit and thus, ignores other semantic units like sentences, passages etc. In this paper, we attempt to take advantage of underlying subtopic structure of text documents and investigate whether clustering of text documents can be improved if text segments of two documents are utilized, while calculating similarity between them. We concentrate on examining effects of combining suggested inter-document similarities (based on inter-passage similarities) with traditional inter-document similarities following a simple approach for the same. Experimental results on standard data sets suggest improvement in clustering of text documents.  2012 Springer-Verlag.",
      "title": "15697 Investigating usage of text segmentation and inter-passage similarities to improve text document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864925071&partnerID=40&md5=d4eeff0455ab407b3ba020b4a8fe67e8"
    },
    {
      "abstract": "The quality of extracted features is the key issue to text mining due to the large number of terms, phrases, and noise. Most existing text mining methods are based on term-based approaches which extract terms from a training set for describing relevant information. However, the quality of the extracted terms in text documents may be not high because of lot of noise in text. For many years, some researchers make use of various phrases that have more semantics than single words to improve the relevance, but many experiments do not support the effective use of phrases since they have low frequency of occurrence, and include many redundant and noise phrases. In this paper, we propose a novel pattern discovery approach for text mining. This approach first discovers closed sequential patterns in text documents for identifying the most informative contents of the documents and then utilise the identified contents to extract useful features for text mining. We develop a novel fusion method based on Dempster-Shafers evidential reasoning which allows to combine the pieces of document to discover the knowledge (features). To evaluate the proposed approach, we adopt the feature extraction method for information filtering (IF). The experimental results conducted on Reuters Corpus Volume 1 and TREC topics confirm that the proposed approach could achieve excellent performance.  2012 Springer-Verlag.",
      "title": "15698 A pattern discovery model for effective text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864948676&partnerID=40&md5=52152267f2fec59974632ccfbca0941d"
    },
    {
      "abstract": "Recent years have seen a vast amount of data generated by various biological and biomedical experiments. The storage, management and analysis of this data, is done by means of the modern bioinformatics applications and tools. One of the bioinformatics instruments used for solving these tasks, are ontologies and the apparatus they provide. Ontology as a modeling tool is a specification of a conceptualization meaning that an ontology is a formal description of the concepts and relationships that can exist for a given software system or software agent (8, 10). Anatomical (phenotypic) ontologies of various species nowadays typically contain from few thousands to few tens of thousands of terms and relations (which is a very small number compared to the count of objects and the amount of data produced by biological experiments at the molecular level, for example) but usually the semantics employed in them is enormous in scale. The major problem when using such ontologies is that they lack intelligent tools for cross-species literature searches (text mining) as well as tools aiding the design of new biological and biomedical experiments with other (not yet tested) species/organisms, based on available information about experiments already performed on certain model species/organisms. This is where the process of merging anatomical ontologies comes into use. Using specific models and algorithms for merging of such ontologies is a matter of choice. In this work a novel approach for solving this task, based on two directed acyclic graph (DAG) models and three original algorithmic procedures is presented. Based on them, an intelligent software system for merging two (and possibly more) input/source anatomical ontologies into one output/target super-ontology was designed and implemented. This system was named AnatOM (an abbreviation from Anatomical Ontologies Merger). In this work a short overview of ontologies is provided describing what ontologies are and why they are widely used as a tool in bioinformatics. The problem of merging anatomical ontologies of two or more different organisms is introduced and some effort has been put into explaining why it is important. A general outline is presented of the models and the method that have been developed for solving the ontologies merging problem. A high-level overview of the AnatOM program implemented by the authors as part of this work is also provided. To achieve the degree of intelligence that is needed, the AnatOM program utilizes the large amount of high-quality data (knowledge) available in several widely popular and generally recognized knowledge bases such as UMLS, FMA, and WordNet. The last one of these is a general-purpose i.e. non-specialized knowledge source. The first two are biological/biomedical ones. Their choice was based on the fact that they provide a very good foundation for building an intelligent system that performs certain comparative anatomy tasks including mapping and merging of anatomical ontologies (23).",
      "title": "15700 An intelligent system approach for integrating anatomical ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84865463330&partnerID=40&md5=e7b6d93e38bbfacb200dffb63cc172ed"
    },
    {
      "abstract": "Document similarity measures are crucial components of many text-analysis tasks, including information retrieval, document classification, and document clustering. Conventional measures are brittle: They estimate the surface overlap between documents based on the words they mention and ignore deeper semantic connections. We propose a new measure that assesses similarity at both the lexical and semantic levels, and learns from human judgments how to combine them by using machine-learning techniques. Experiments show that the new measure produces values for documents that are more consistent with peoples judgments than people are with each other. We also use it to classify and cluster large document sets covering different genres and topics, and find that it improves both classification and clustering performance.  2012 ASIS&T.",
      "title": "15702 Learning a concept-based document similarity measure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864406780&partnerID=40&md5=b46e28a1f3269f273f09d58b2f5152e5"
    },
    {
      "abstract": "Automatic emotion sensing in textual data is crucial for the development of intelligent interfaces in interactive computer applications. This paper reports a high-precision, domain-independent approach for automatic emotion sensing for events embedded in sentences. The proposed approach is based on the common action distribution between the subject and object of an event. We have incorporated semantic labeling and web-based text mining techniques, together with a number of reference entity pairs and hand-crafted emotion generation rules to realize an event emotion detection system. Moreover, a hybrid emotion detection engine is presented by incorporating a set of predefined emotion keywords and the proposed event-level emotion detection engine. The evaluation outcome reveals a rather satisfactory result with about 73% accuracy for detecting the Happy, Sad, Fear, Angry, Surprise, Disgust, and Neutral.  2012 Springer-Verlag.",
      "title": "15703 Event-level textual emotion sensing based on common action distributions between event participants",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864370623&partnerID=40&md5=858ced99f769c1f35ea840e2ddb47f61"
    },
    {
      "abstract": "Topic detection is a hot topic in the field of text mining. In this paper, focusing on the Chinese interactive text, we explored a novel topic detection method, named SDD-PLSA, which integrates Semantic Dependency Distance (SDD) and PLSA. It not only has the advantages of PLSA, which is an efficient, effective method and is widely used in text mining, but also considers the semantic and syntax information. Thus, the problem of lacking semantic information in PLSA can be avoided. SDD-PLSA has two main steps. The first is using SDD to classify the sentences that have a high similarity in semantics into several groups according to semantic feature extraction of the interactive text. Then, a PLSA classifier is used upon the result of the first step. The experiments show that the accuracy of detection on love topic has been improved to 64.8% when using SDD-PLSA, better than 55.4% when using PLSA.  2012 IEEE.",
      "title": "15709 A topic detection method based on Semantic Dependency Distance and PLSA",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864210080&partnerID=40&md5=86ded9975cead093bb47950443ca58ba"
    },
    {
      "abstract": "This paper proposes a novel representation for Authorship Attribution (AA), based on Concise Semantic Analysis (CSA), which has been successfully used in Text Categorization (TC). Our approach for AA, called Document Author Representation (DAR), builds document vectors in a space of authors, calculating the relationship between textual features and authors. In order to evaluate our approach, we compare the proposed representation with conventional approaches and previous works using the c50 corpus. We found that DAR can be very useful in AA tasks, because it provides good performance on imbalanced data, getting comparable or better accuracy results.  2012 Springer-Verlag Berlin Heidelberg.",
      "title": "15710 A new document author representation for authorship attribution",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864258583&partnerID=40&md5=ffdd6d635053de68d15b6fccc71f33f1"
    },
    {
      "abstract": "Text feature is usually expressed as a matrix of huge dimensionality in text mining, and common clustering algorithm are not stable and cannot obtain clustering solution efficiently. Latent Semantic Analysis can reduce dimensionality effectively, and emerges the semantic relations between texts and terms. Clustering ensemble can get better clustering solution than single clustering method. A text clustering ensemble based on genetic algorithms is presented, which combines Latent Semantic Analysis and Clustering ensemble based on genetic algorithms. Experiments have demonstrated that text clustering ensemble based on genetic algorithms can effectively improve the clustering performance.  2012 IEEE.",
      "title": "15711 Text clustering ensemble based on genetic algorithms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864183975&partnerID=40&md5=c54c5e942e6a26d3eafea5e65d7ce33e"
    },
    {
      "abstract": "This paper aims to design a system model that analyzes the unstructured data inside the posts about electronic products on social networking websites. For the purposes of this study, posts on social networking websites have been mined and the keywords are extracted from such posts. The extracted keywords and the ontologies of electronic products and emotions form the base for the text-mining model which is used to understand online consumer behavior in the market.  2012 IEEE.",
      "title": "15713 Ontology-based text-mining model for social network analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864198284&partnerID=40&md5=b7e8889e25724f6b66ce41941a6fd483"
    },
    {
      "abstract": "This paper proposes a new approach to evaluate future technological value of patents using TRIZ evolution trends. Previous studies using TRIZ evolution trends have determined patents with high evolutionary potential as high value technology in the future without considering relative importance of each TRIZ trend in a specific technology domain. Thus, previous approaches have limitations in that the importance of TRIZ evolution trends can be different according to the technology domain and current stage of the technology cycle of the technology domain. To overcome such limitations, we propose a method which can consider the priority of importance of TRIZ evolution trends and the technology cycle. To this end, we adopted an SAO-based text mining approach and semantic similarity measurement method.  2012 IEEE.",
      "title": "15714 An SAO-based approach to patent evaluation using TRIZ evolution trends",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864212949&partnerID=40&md5=2fa8b31b76f2e1c04c24717af0fccdc6"
    },
    {
      "abstract": "Due to proliferation of Web 2.0, there is an exponential growth in user generated contents in the form of customer reviews on the Web, containing precious information useful for both customers and manufacturers. However, most of the contents are stored in either unstructured or semi-structured format due to which distillation of knowledge from this huge repository is a challenging task. In this paper, we propose a text mining approach to mine product features, opinions and their reliability scores from Web opinion sources. A rule-based system is implemented, which applies linguistic and semantic analysis of texts to mine feature-opinion pairs that have sentence-level co-occurrence in review documents. The extracted feature-opinion pairs and source documents are modeled using a bipartite graph structure. Considering feature-opinion pairs as hubs and source documents as authorities, Hyperlink-Induced Topic Search (HITS) algorithm is applied to generate reliability score for each feature-opinion pair with respect to the underlying corpus. The efficacy of the proposed system is established through experimentation over customer reviews on different models of electronic products. Copyright 2012 ACM.",
      "title": "15721 Mining feature-opinion pairs and their reliability scores from web opinion sources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863926200&partnerID=40&md5=d791e19229f9bddee4d9c4a2622a09cf"
    },
    {
      "abstract": "The amount of data available in semi-structured or unstructured format grows exponentially. The area of text mining aims at discovering knowledge from data of this type. Most work in this area uses the model known as bag of words to represent the texts. This form of representation, although effective, minimizes the quality of knowledge discovered because it is not able to capture essential characteristics of this type of data such as semantics and context. The paradigm of granular computing has been shown effective in the treatment of complex problems of information processing and can produce significant results in large-scale environments such as the Web. This paper explores the granulation process of words with a view to its application in the subsequent improvement in text representation. We use fuzzy relations and spectral clustering in this process and present some results.  2012 Springer-Verlag.",
      "title": "15723 Granules of words to represent text: An approach based on fuzzy relations and spectral clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863915100&partnerID=40&md5=1894cceabeadfc45c5647dda1e673aa6"
    },
    {
      "abstract": "Text classification constitutes a popular task in Web research with various applications that range from spam filtering to sentiment analysis. To address it, patterns of co-occurring words or characters are typically extracted from the textual content of Web documents. However, not all documents are of the same quality",
      "title": "15724 Representation models for text classification: A comparative analysis over three Web document types",
      "url": "Conference Paper"
    },
    {
      "abstract": "This paper presents initial research in the area of sentiment analysis in Czech. We will describe a method for annotating Czech evaluative structures and analyze existing results for both manual and automatic annotation of the plain text data which represents the basis for further subjectivity clues learning. Copyright 2012 ACM.",
      "title": "15727 Sentence-level sentiment analysis in Czech",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863971753&partnerID=40&md5=defd7dacf27571fad82da293005d8c60"
    },
    {
      "abstract": "Hermes is a Web-based framework designed to build personalized news services using Semantic Web technologies. It makes use of ontologies for knowledge representation, natural language processing techniques for semantic text analysis, and semantic query languages for specifying the desired information. This paper presents the Hermes Graphical Query Language (HGQL). HGQL makes it possible to create structured queries in Hermes. Structured queries use disjunctive, conjunctive, negation, and pattern operators. In addition, this paper presents a ranking algorithm based on the queries made using HGQL.  2012 ACM.",
      "title": "15731 Querying and ranking news items in the Hermes framework",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863569598&partnerID=40&md5=d94c51e78a6a329b928c0e9542624c8b"
    },
    {
      "abstract": "For most, the web is the first source to answer a question formulated by curiosity, need, or research reasons. This phenomenon is due to the internets ubiquitous access, ease of use, and the extensive and ever expanding content. The problem is no longer the need to acquire content to encourage use, but to provide organizational tools to support content categorization that will facilitate improved access methods. This paper presents the results of a new text characterization algorithm that combines semantic and linguistic techniques utilizing domain-based ontology background knowledge. It explores the combination of meronym, synonym, and hypernym linguistic relationships to create a set of concept chains used to represent concepts found in a document. The experiments show improved accuracy over bag-of-words based term weighting methods and reveal characteristics of the meronym contribution to document representation.  2012 ACM.",
      "title": "15732 Concept chaining utilizing meronyms in text characterization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863549997&partnerID=40&md5=1c9084cf323fe6ccaf82962a88c351aa"
    },
    {
      "abstract": "Named Entity Recognition (NER) is a subtask of information extraction and aims to identify atomic entities in text that fall into predefined categories such as person, location, organization, etc. Recent efforts in NER try to extract entities and link them to linked data entities. Linked data is a term used for data resources that are created using semantic web standards such as DBpedia. There are a number of online tools that try to identify named entities in text and link them to linked data resources. Although one can use these tools via their APIs and web interfaces, they use different data resources and different techniques to identify named entities and not all of them reveal this information. One of the major tasks in NER is disambiguation that is identifying the right entity among a number of entities with the same names",
      "title": "15735 Named entity recognition and disambiguation using linked data and graph-based centrality scoring",
      "url": ""
    },
    {
      "abstract": "The user generated content on the web grows rapidly in this emergent information age. The evolutionary changes in technology make use of such information to capture only the users essence and finally the useful information are exposed to information seekers. In this paper we detect online hotspot forums by computing sentiment analysis for text data available in each forum. This approach analyzes the forum text data and computes sentiment score for each word or phrase of text. Then we propose two text mining approaches like K-means clustering and Support Vector Machine with Particle Swarm Optimization (PSO-SVM) classification algorithms that can be used to group into two forum clusters forming hotspot forums and non-hotspot forums within each time window. The text datasets that we use in our experimental research are collected from forums.digitalpoint.com and after data cleaning they are formatted to 37 different forums and 1616 threads. The experiment helps to identify that K-means and PSO-SVM together achieve highly consistent results.  2012 IEEE.",
      "title": "15736 A semantic enhanced approach for online hotspot forums detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862876862&partnerID=40&md5=a4beae595b82a1eee0768076e4eacb4d"
    },
    {
      "abstract": "Estimation of the semantic likeness between words is of great importance in many applications dealing with textual data such as natural language processing, knowledge acquisition and information retrieval. Semantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent years, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an structured knowledge representation. Thanks to the possibilities that ontologies enable regarding semantic interpretation of terms many ontology-based similarity measures have been developed. According to the principle in which those measures base the similarity assessment and the way in which ontologies are exploited or complemented with other sources several families of measures can be identified. In this paper, we survey and classify most of the ontology-based approaches developed in order to evaluate their advantages and limitations and compare their expected performance both from theoretical and practical points of view. We also present a new ontology-based measure relying on the exploitation of taxonomical features. The evaluation and comparison of our approachs results against those reported by related works under a common framework suggest that our measure provides a high accuracy without some of the limitations observed in other works.  2012 Elsevier Ltd. All rights reserved.",
      "title": "15741 Ontology-based semantic similarity: A new feature-based approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858332971&partnerID=40&md5=841c5dad1517db7a5818d0495365361f"
    },
    {
      "abstract": "The last several years have seen the emergence of digital libraries from which documents are harvested using the OAI-PMH protocol. Considering the volume of data provided by these repositories, we are interested in the exploitation of the full text content of scientific publications. Our aim is to bring new value to scientific publications by automatic extraction and semantic analysis. The identification of bibliographic references in texts makes it possible to localize specific text segments that carry linguistic markers in order to annotate a set of semantic categories related to citations. This work uses a categorization of surface linguistic markers organized in a linguistic ontology. The semantic annotations are used to enrich the document metadata and to provide new types of visualizations in an information retrieval context. We present the system architecture as well as some experimental results.  2012 Marc Bertin and Iana Atanassova.",
      "title": "15742 Semantic enrichment of scientific publications and metadata: Citation analysis through contextual and cognitive analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866780795&partnerID=40&md5=f17b1262fcc1fe5a61d0f6cef735e68c"
    },
    {
      "abstract": "Software Requirements Specifications (SRS) documents are important artifacts in the software industry. A SRS contains all the requirements specifications for a software system, either as functional requirements (FR) or non-functional requirements (NFR). FRs are the features of the system-to-be, whereas NFRs define its quality attributes. NFRs impact the system as a whole and interact both with each other and with the functional requirements. SRS documents are typically written in informal natural language [1], which impedes their automated analysis. The goal of this work is to support software engineers with semantic analysis methods that can automatically extract and analyze requirements written in natural language texts, in order to (i) make SRS documents machine-processable by transforming them into an ontological representation",
      "title": "15747 Semantic analysis of functional and non-functional requirements in software requirements specifications",
      "url": "Conference Paper"
    },
    {
      "abstract": "Document clustering generates clusters from the whole document collection automatically and is used in many fields, including data mining and information retrieval. Clustering text data faces a number of new challenges. Among others, the volume of text data, dimensionality, sparsity and complex semantics are the most important ones. These characteristics of text data require clustering techniques to be scalable to large and high dimensional data, and able to handle sparsity and semantics. In the traditional vector space model, the unique words occurring in the document set are used as the features. But because of the synonym problem and the polysemous problem, such a bag of original words cannot represent the content of a document precisely. Most of the existing text clustering methods use clustering techniques which depend only on term strength and document frequency where single terms are used as features for representing the documents and they are treated independently which can be easily applied to non-ontological clustering. To overcome the above issues, this paper makes a survey of recent research done on ontology or thesaurus based document clustering.  2005 - 2012 JATIT & LLS. All rights reserved.",
      "title": "15749 Study of ontology or thesaurus based document clustering and information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862857733&partnerID=40&md5=dc762b6f672479ec059b552191afd7a2"
    },
    {
      "abstract": "Despite the advantages of the traditional vector space model (VSM) representation, there are known deficiencies concerning the term independence assumption. The high dimensionality and sparsity of the text feature space and phenomena such as polysemy and synonymy can only be handled if a way is provided to measure term similarity. Many approaches have been proposed that map document vectors onto a new feature space where learning algorithms can achieve better solutions. This paper presents the global term context vector-VSM (GTCV-VSM) method for text document representation. It is an extension to VSM that: (i) it captures local contextual information for each term occurrence in the term sequences of documents",
      "title": "15750 Text document clustering using global term context vectors",
      "url": ""
    },
    {
      "abstract": "We present the Document-Entity-Topic (DET) model for semantic social network analysis which tries to find out the interested entities through the topics we aim at, detect groups according to the entities which concern the similar topics, and rank the plentiful entities in a document to figure out the most valuable ones. DET model learns the topic distributions by the literal descriptions of entities. The model is similar to Author-Topic (AT) model, adding the key attribute that the distribution of entities in a document is not uniform but Dirichlet allocation. We experiment on the Libya Event data set which is collected from the Internet. DET model increases the precision on tasks of social network analysis and gives much lower perplexity than AT model.  2012 Springer-Verlag.",
      "title": "15752 Semantic social network analysis with text corpora",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861429279&partnerID=40&md5=1b73cf10a904827825d13eec77f86a67"
    },
    {
      "abstract": "Text document clustering is a popular task for understanding and summarizing large document collections. Besides the need for efficiency, document clustering methods should produce clusters that are readily understandable as collections of documents relating to particular contexts or topics. Existing clustering methods often ignore term-document semantics while relying upon geometric similarity measures. In this paper, we present an efficient iterative partitional clustering method, CDIM, that maximizes the sum of discrimination information provided by documents. The discrimination information of a document is computed from the discrimination information provided by the terms in it, and term discrimination information is estimated from the currently labeled document collection. A key advantage of CDIM is that its clusters are describable by their highly discriminating terms - terms with high semantic relatedness to their clusters contexts. We evaluate CDIM both qualitatively and quantitatively on ten text data sets. In clustering quality evaluation, we find that CDIM produces high-quality clusters superior to those generated by the best methods. We also demonstrate the understandability provided by CDIM, suggesting its suitability for practical document clustering.  2012 Springer-Verlag.",
      "title": "15753 Clustering and understanding documents via discrimination information maximization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861427881&partnerID=40&md5=2b75547624af7eef6dee3e9ed6a67b21"
    },
    {
      "abstract": "The rapid increase in the amount of textual data has brought forward a growing research interest towards mining text to detect deviations. Specialized methods for specific domains have emerged to satisfy various needs in discovering rare patterns in text. This paper focuses on a graph-based approach for text representation and presents a novel error tolerance dissimilarity algorithm for deviation detection. We resolve two non-trivial problems, i.e. semantic representation of text and the complexity of graph matching. We employ conceptual graphs interchange format (CGIF) - a knowledge representation formalism to capture the structure and semantics of sentences. We propose a novel error tolerance dissimilarity algorithm to detect deviations in the CGIFs. We evaluate our method in the context of analyzing real world financial statements for identifying deviating performance indicators. We show that our method performs better when compared with two related text based graph similarity measuring methods. Our proposed method has managed to identify deviating sentences and it strongly correlates with expert judgments. Furthermore, it offers error tolerance matching of CGIFs and retains a linear complexity with the increasing number of CGIFs.  2012 - IOS Press and the authors. All rights reserved.",
      "title": "15754 Deviation detection in text using conceptual graph interchange format and error tolerance dissimilarity function",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861398942&partnerID=40&md5=65ed2a4dd2a1c4782562aeec8e783d89"
    },
    {
      "abstract": "Contextual advertising is an important revenue source for major service providers on the Web. Ads classification is one of main tasks in contextual advertising, and it is used to retrieve semantically relevant ads with respect to the content of web pages. However, it is difficult for traditional text classification methods to achieve satisfactory performance in ads classification due to scarce term features in ads. In this paper, we propose a novel ads classification method that handles the lack of term features for classifying ads with short text. The proposed method utilizes a vocabulary expansion technique using semantic associations among terms learned from large-scale search query logs. The evaluation results show that our methodology achieves 4.0% ~ 9.7% improvements in terms of the hierarchical f-measure over the baseline classifiers without vocabulary expansion.  2012 KSII.",
      "title": "15755 Vocabulary expansion technique for advertisement classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861896975&partnerID=40&md5=555855bdc1500cea2110e8e3ae7461cf"
    },
    {
      "abstract": "In this paper we propose KOIOS++, which automatically processes natural language queries provided by handwritten input. The system integrates several recent achievements in the area of handwriting recognition, natural language processing, information retrieval, and human computer interaction. It uses a knowledge base described by the resource description framework (RDF). Our generic approach first generates a lexicon as background information for the handwritten text recognition. After recognizing a handwritten query, several output hypotheses are sent to a natural language processing system in order to generate a structured query (SPARQL query). Subsequently, the query is applied to the given knowledge base and a result graph visualizes the retrieved information. At all stages, the user can easily adjust the intermediate results if there is any undesired outcome. The system is implemented as a web-service and therefore works for handwritten input on digital paper as well as on input on Pen-enabled interactive surfaces. Furthermore, we build on the generic RDF-representation of semantic knowledge which is also used by the linked open data (LOD) initiative. As such, our system works well in various scenarios. We have implemented prototypes for querying company knowledge bases, the DBPedia1, the DBLP computer science bibliography2, and a knowledge base of the DAS 2012.  2012 IEEE.",
      "title": "15759 Koios++: A query-answering system for handwritten input",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862081741&partnerID=40&md5=af124782de74ad504f7896eacb487666"
    },
    {
      "abstract": "Humans are able to easily judge if a pair of concepts are related in some way. Understanding of how humans are able to perform this task is not easy. Semantic similarity denotes computing the similarity between concepts, having the same meaning or related information, which are not necessarily lexically similar. Semantic similarity between concepts plays an important role in Semantic Web, knowledge sharing, Web mining, semantic sense understanding and text summarization. This also is an important problem in Natural Language Processing and Information Retrieval Researches. These techniques are becoming important components of most of the Information Retrieval (IR), Information Extraction (IE) and other intelligent knowledge based systems. Therefore it has received considerable attention in the literature. Ontology has a good hierarchical structure of concepts. In the ontology, semantic information can be realized through the semantic relationship of concepts. Ontology-based semantic similarity techniques can estimate the semantic similarity between two hierarchically expressed concepts in a given ontology or taxonomy. Semantic similarity is usually computed by mapping concepts to ontology and by examining their relationships in it. The most popular semantic similarity methods are implemented and evaluated using WordNet and MeSH. Several algorithmic approaches for computing semantic similarity have been proposed. This paper discusses the various approaches used for identifying semantically similar concepts in ontology.  Maxwell Scientific Organization, 2012.",
      "title": "15760 Studying of semantic similarity methods in ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862060791&partnerID=40&md5=2575ac1bce535cce6527660c9868f447"
    },
    {
      "abstract": "Background: Research into event-based text mining from the biomedical literature has been growing in popularity to facilitate the development of advanced biomedical text mining systems. Such technology permits advanced search, which goes beyond document or sentence-based retrieval. However, existing event-based systems typically ignore additional information within the textual context of events that can determine, amongst other things, whether an event represents a fact, hypothesis, experimental result or analysis of results, whether it describes new or previously reported knowledge, and whether it is speculated or negated. We refer to such contextual information as meta-knowledge. The automatic recognition of such information can permit the training of systems allowing finer-grained searching of events according to the meta-knowledge that is associated with them.Results: Based on a corpus of 1,000 MEDLINE abstracts, fully manually annotated with both events and associated meta-knowledge, we have constructed a machine learning-based system that automatically assigns meta-knowledge information to events. This system has been integrated into EventMine, a state-of-the-art event extraction system, in order to create a more advanced system (EventMine-MK) that not only extracts events from text automatically, but also assigns five different types of meta-knowledge to these events. The meta-knowledge assignment module of EventMine-MK performs with macro-averaged F-scores in the range of 57-87% on the BioNLP09 Shared Task corpus. EventMine-MK has been evaluated on the BioNLP09 Shared Task subtask of detecting negated and speculated events. Our results show that EventMine-MK can outperform other state-of-the-art systems that participated in this task.Conclusions: We have constructed the first practical system that extracts both events and associated, detailed meta-knowledge information from biomedical literature. The automatically assigned meta-knowledge information can be used to refine search systems, in order to provide an extra search layer beyond entities and assertions, dealing with phenomena such as rhetorical intent, speculations, contradictions and negations. This finer grained search functionality can assist in several important tasks, e.g., database curation (by locating new experimental knowledge) and pathway enrichment (by providing information for inference). To allow easy integration into text mining systems, EventMine-MK is provided as a UIMA component that can be used in the interoperable text mining infrastructure, U-Compare.  2012 Miwa et al.",
      "title": "15761 Extracting semantically enriched events from biomedical literature",
      "url": ""
    },
    {
      "abstract": "On the basis of discussing the information spreading mechanism under Internet environment, we have studied on how to build a public opinion monitoring model according to the semantic content or text mining in recent years. A micro-blog public opinion corpus named MPO Corpus on the content of micro-blog information as a test data set has been constructed by our research team. In this paper, it proposes a quick emergency response model (QERM) for micro-blog public opinion crisis oriented to Mobile Internet services. Firstly, it describes the micro-blog cases and emergency response plan library using web ontology language (OWL), which makes the transitive logical reason capacity among micro-blog subjects, microblog cases and emergency plans. Secondly, it proposes an algorithm to calculate the sentiment intensity of micro-blogs from three levels on words, sentences and documents based on HowNet Knowledge-base respectively. Thirdly, we continue to study on how to update cases under the subjects and quick response processes for micro-blog case base. Finally, we design a test experiment which shows some merits of QERM in time, which basically meets the quick emergency response demand on the micro-blog public opinions crisis under Mobile Internet environment. Thus, it will provide more efficient support to the government and related monitoring departments involved with the public opinions crisis.  2012 ACADEMY PUBLISHER.",
      "title": "15762 A quick emergency response model for microblog public opinion crisis based on text sentiment intensity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861113623&partnerID=40&md5=cbac8d134688ce22c9380129b56bb4df"
    },
    {
      "abstract": "This paper describes the new techniques developed to extract and compute the domain-specific knowledge implicitly embedded in a highly structural ontology-based information system for TV Electronic Programming Guide (EPG). The domain knowledge represented by a set of mutually related n-gram data sets is then enriched by exploring the explicit structural dependencies and implicit semantic association between the data entities in the domain and the domain-independent texts from the Google 1 trillion 5-grams corpus created from general WWW documents. The knowledge-based enrichment process creates the language models required for a natural language based EPG search system that outperform the baseline model created only from the original EPG data source by a significant margin measured by an absolute improvement of 14.1% on the model coverage (recall accuracy) using large-scale test data collected from a real-world EPG search application.  2012 Springer-Verlag Berlin Heidelberg.",
      "title": "15763 Enriching domain-specific language models using domain independent WWW n-gram corpus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861069949&partnerID=40&md5=99bc8e8fe2591c5401642ee8ce5f1486"
    },
    {
      "abstract": "This work presents a Sentence Hashing Algorithm for Plagiarism Detection - SHAPD. To present a user with the best results the algorithm makes use of special trait of the written texts - their natural sentence fragmentation, later employing a set of special techniques for text representation. Results obtained demonstrate that the algorithm delivers solution faster than the alternatives. Its algorithmic complexity is logarithmic, thus its performance is better than most algorithms using dynamic programming used to find the longest common subsequence.  2012 Springer-Verlag Berlin Heidelberg.",
      "title": "15764 Fast plagiarism detection by Sentence Hashing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861073259&partnerID=40&md5=ae73be0787988867ffee342941dfafea"
    },
    {
      "abstract": "This paper describes the patents retrieval prototype developed within the MOLTO project. The prototype aims to provide a multilingual natural language interface for querying the content of patent documents. The developed system is focused on the biomedical and pharmaceutical domain and includes the translation of the patent claims and abstracts into English, French and German. Aiming at the best retrieval results of the patent information and text content, patent documents are preprocessed and semantically annotated. Then, the annotations are stored and indexed in an OWLIM semantic repository, which contains a patent specific ontology and others from different domains. The prototype, accessible online at http://molto-patents.ontotext.com, presents a multilingual natural language interface to query the retrieval system. In MOLTO, the multilingualism of the queries is addressed by means of the GF Tool, which provides an easy way to build and maintain controlled language grammars for interlingual translation in limited domains. The abstract representation obtained from the GF is used to retrieve both the matched RDF instances and the list of patents semantically related to the users search criteria. The online interface allows to browse the retrieved patents and shows on the text the semantic annotations that explain the reason why any particular patent has matched the users criteria. Copyright is held by the International World Wide Web Conference Committee (IW3C2).",
      "title": "15765 The patents retrieval prototype in the MOLTO project",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861016496&partnerID=40&md5=c68bca81e8db5bb5021cf60d4c63d59c"
    },
    {
      "abstract": "Currently, patent data have gained increasing attention in the data mining area. However, the traditional method was set by many factors in patent classification, such as professional terms and high dimensions. In this article, we propose a word sense disambiguation model, which would effectively reduce the structure of the traditional machine word. The experimental result shows that the semantic disambiguation and feature reduction strategy can effectively improve the classification accuracy.  2011 by Binary Information Press.",
      "title": "15768 Chinese patent classification based on sense disambiguation and manifold learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862676495&partnerID=40&md5=6b7367ec488c31526710bcd619dda761"
    },
    {
      "abstract": "Classification plays a vital role in many information management and retrieval tasks. This paper studies classification of text document. Text classification is a supervised technique that uses labeled training data to learn the classification system and then automatically classifies the remaining text using the learned system. In this paper, we propose a mining model consists of sentence-based concept analysis, document-based concept analysis, and corpus-based concept-analysis. Then we analyze the term that contributes to the sentence semantics on the sentence, document, and corpus levels rather than the traditional analysis of the document only. After extracting feature vector for each new document, feature selection is performed. It is then followed by K-Nearest Neighbour classification. The approach enhances the text classification accuracy.  2012 IEEE.",
      "title": "15769 An enhanced data mining model for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84860648436&partnerID=40&md5=5afc835510d9f9689434d4b6d4686292"
    },
    {
      "abstract": "Automatic Text Classification is a semi-supervised machine learning task that automatically assigns a given text document to a set of pre-defined categories based on the features extracted from its textual content. This paper attempts to automatically classify the textual entries made by bloggers on various sports blogs, to the appropriate category of sport by following steps like pre-processing, feature extraction and Naive Bayesian classification. Empirical evaluation of this technique has resulted in a classification accuracy of approximately 87% over the test set. In addition to classifying the textual entries of sports blogs, it is proposed that the extracted features themselves be further classified under more meaningful heads which results in generation of a semantic resource that lends greater understanding to the classification task. This semantic resource can be used for data mining requirements that arise in the future.  2012 IEEE.",
      "title": "15771 Automatic text classification of sports blog data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84860439065&partnerID=40&md5=3dbae59ef85edee3ccdf79920b63837b"
    },
    {
      "abstract": "Web information extraction is a very important and difficulty research subject which involves lots of fields, such as artificial intelligence, machinery learning, etc. As a modeling tool in describing the concept model of information systems at the semantic and knowledge level, ontology is widely used in many areas of computer science in recent years. A new method using ontology to extract valuable information from web documents was proposed in this paper. Firstly, according to the characteristics of the websites and web pages, the text content of web pages was extracted by locating the pagesregional position. Secondly, on the basis of the traditional vector space model as well as the domain ontology, the concept vectors were generated according to the weightings of the concept vectors combining with the level structure feature of the ontology. Thus, the instances of ontology knowledge base were created semi-automatically, and the text of non-structured web page was turned into semantic structured information which can be understood by the machine.",
      "title": "15773 A web information extraction method based on ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861506281&partnerID=40&md5=f20a6b29c3b7bda80d60e54b0e4256af"
    },
    {
      "abstract": "In this paper, we design and develop a movie-rating and review-summarization system in a mobile environment. The movie-rating information is based on the sentiment-classification result. The condensed descriptions of movie reviews are generated from the feature-based summarization. We propose a novel approach based on latent semantic analysis (LSA) to identify product features. Furthermore, we find a way to reduce the size of summary based on the product features obtained from LSA. We consider both sentiment-classification accuracy and system response time to design the system. The rating and review-summarization system can be extended to other product-review domains easily.  2012 IEEE.",
      "title": "15776 Movie rating and review summarization in mobile environment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84860216754&partnerID=40&md5=34ea0082dbf5a88ab3605f98c248541e"
    },
    {
      "abstract": "We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW 08), and robust with respect to concept drifts and input sources.  2012 Springer-Verlag Berlin Heidelberg.",
      "title": "15777 Classification of short texts by deploying topical annotations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84860206957&partnerID=40&md5=ad6e6d9bd190bc59953c5e9e7f77e418"
    },
    {
      "abstract": "Text similarity computing is widely applied in Natural Language Processing. This paper proposes a novel text similarity computing method based on thematic term set. It firstly extracts thematic terms based on position weight, and merges thematic terms using thesaurus. Then it calculates the semantic similarity between thematic terms using HowNet to get the maximal semantic similarity of each pair of thematic terms. Finally, it utilizes the semantic similarity between two sets of thematic terms as text similarity. Experiment results show that the proposed method performs well, achieving 77.65% precision in text classification.",
      "title": "15782 Text similarity computing based on thematic term set",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859813791&partnerID=40&md5=8f702c0d6df2a289323c2be345421905"
    },
    {
      "abstract": "In this paper, we describe and evaluate a system that extracts clinical findings and body locations from radiology reports and correlates them. The system uses Medical Language Extraction and Encoding System (MedLEE) to map the reports free text to structured semantic representations of their content. A lightweight reasoning engine extracts the clinical findings and body locations from MedLEEs semantic representation and correlates them. Our study is illustrative for research in which existing natural language processing software is embedded in a larger system. We manually created a standard reference based on a corpus of neuro and breast radiology reports. The standard reference was used to evaluate the precision and recall of the proposed system and its modules. Our results indicate that the precision of our system is considerably better than its recall (82.32-91.37% vs. 35.67-45.91%). We conducted an error analysis and discuss here the practical usability of the system given its recall and precision performance.  Society for Imaging Informatics in Medicine 2011.",
      "title": "15783 Automatically correlating clinical findings and body locations in radiology reports using MedLEE",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861639366&partnerID=40&md5=b450d92f265ed5fd33347cdae4c8585c"
    },
    {
      "abstract": "The representation of word meaning in texts is a central problem in Computational Linguistics. Geometrical models represent lexical semantic information in terms of the basic co-occurrences that words establish each other in large-scale text collections. As recent works already address, the definition of methods able to express the meaning of phrases or sentences as operations on lexical representations is a complex problem, and a still largely open issue. In this paper, a perspective centered on Convolution Kernels is discussed and the formulation of a Partial Tree Kernel that integrates syntactic information and lexical generalization is studied. The interaction of such information and the role of different geometrical models is investigated on the question classification task where the state-of-the-art result is achieved.  2012 Springer-Verlag.",
      "title": "15786 Distributional models and lexical semantics in convolution kernels",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858325371&partnerID=40&md5=149fd6c953c10f2cb9b38305a62c71e6"
    },
    {
      "abstract": "As we strive for sophisticated machine translation and reliable information extraction, we have launched a subproject pertaining to the practical elaboration of intensional levels of discourse referents in the framework of a representational dynamic discourse semantics, the DRT-based [14] Realis [2], and the implementation of resulting representations within a complete model of communicating interpreters minds as it is captured formally in Realis by means of functions delta, alpha, gamma and kappa [5]. We show analyses of chiefly Hungarian linguistic data, which range from revealing complex semantic contribution of small affixes through pointing out the multiply intensional nature of certain (pre)verbs to studying the embedding of whole discourses in information state. An outstanding advantage of our method, due to our theoretical basis, is that not only sentences / discourses are assigned semantic representations but relevant factors of speakers information states can also be revealed and implemented.  2012 Springer-Verlag.",
      "title": "delta Multiple level of referents in information state",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858315060&partnerID=40&md5=7ec1a2e37a07fd2c9ad4d484c60a0f4d"
    },
    {
      "abstract": "Recently there has been an increase in interest towards clustering short text because it could be used in many NLP applications. According to the application, a variety of short text could be defined mainly in terms of their length (e.g. sentence, paragraphs) and type (e.g. scientific papers, newspapers). Finding a clustering method that is able to cluster short text in general is difficult. In this paper, we cluster 4 different corpora with different types of text with varying length and evaluate them against the gold standard. Based on these clustering experiments, we show how different similarity measures, clustering algorithms, and cluster evaluation methods effect the resulting clusters. We discuss four existing corpus based similarity methods, Cosine similarity, Latent Semantic Analysis, Short text Vector Space Model, and Kullback-Leibler distance, four well known clustering methods, Complete Link, Single Link, Average Link hierarchical clustering and Spectral clustering, and three evaluation methods, clustering F-measure, adjusted Rand Index, and V. Our experiments show that corpus based similarity measures do not significantly affect the clusters and that the performance of spectral clustering is better than hierarchical clustering. We also show that the values given by the evaluation methods do not always represent the usability of the clusters.  2012 Springer-Verlag.",
      "title": "15789 Clustering short text and its evaluation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858304474&partnerID=40&md5=f5b5e52e6e6ed4d3e4556c1749295e28"
    },
    {
      "abstract": "Knowledge bases extracted from Wikipedia are particularly useful for various NLP and Semantic Web applications due to their co- verage, actuality and multilingualism. This has led to many approaches for automatic knowledge base extraction from Wikipedia. Most of these approaches rely on the English Wikipedia as it is the largest Wikipedia version. However, each Wikipedia version contains socio-cultural knowledge, i.e. knowledge with relevance for a specific culture or language. In this work, we describe a method for extracting a large set of hyponymy relations from the Wikipedia category system that can be used to acquire taxonomies in multiple languages. More specifically, we describe a set of 20 features that can be used for for Hyponymy Detection without using additional language-specific corpora. Finally, we evaluate our approach on Wikipedia in five different languages and compare the results with the WordNet taxonomy and a multilingual approach based on interwiki links of the Wikipedia.  2012 Springer-Verlag.",
      "title": "15790 Automatic taxonomy extraction in different languages using wikipedia and minimal language-specific information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858323267&partnerID=40&md5=8cbd4b6ba93b83903caff1f22a4fbf8d"
    },
    {
      "abstract": "This paper presents a weakly-supervised transfer learning based text categorization method, which does not need to tag new training documents when facing classification tasks in new area. Instead, we can take use of the already tagged documents in other domains to accomplish the automatic categorization task. By extracting linguistic information such as part-of-speech, semantic, co-occurrence of keywords, we construct a domain-adaptive transfer knowledge base. Relation experiments show that, the presented method improved the performance of text categorization on traditional corpus, and our results were only about 5% lower than the baseline on cross-domain classification tasks. And thus we demonstrate the effectiveness of our method.  2012 Springer-Verlag.",
      "title": "15791 Research on text categorization based on a weakly-supervised transfer learning method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863377354&partnerID=40&md5=4092f5d40495771879bb18f72c1b8562"
    },
    {
      "abstract": "Empirical distributional methods account for the meaning of syntactic structures by combining word vectors according to algebraic operators. In this paper, a novel approach for semantic composition based on space projection techniques over lexical vector representations is proposed. In line with the principle of compositionality, the meaning of a phrase is modeled in terms of the subset of properties shared by co-occurring words. Syntactic bi-grams are thus projected in the so called Support Subspace, corresponding to such properties. State-of-the-art results are achieved in a well known phrase similarity task, used as a benchmark for this class of methods.  2012 Springer-Verlag.",
      "title": "15793 Space projections as distributional models for semantic composition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858307156&partnerID=40&md5=9f0284fe04ea5cd4698a45b5e6ab8669"
    },
    {
      "abstract": "In this paper, we propose a novel method for conceptual hierarchical clustering of documents using knowledge extracted from Wikipedia. The proposed method overcomes the classic bag-of-words models disadvantages through the exploitation of Wikipedia textual content and link structure. A robust and compact document representation is built in real-time using the Wikipedia application programmers interface, without the need to store locally any Wikipedia information. The clustering process is hierarchical and extends the idea of frequent items by using Wikipedia article titles for selecting cluster labels that are descriptive and important for the examined corpus. Experiments show that the proposed technique greatly improves over the baseline approach, both in terms of F-measure and entropy on the one hand and computational cost on the other.  2011 The Author. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.",
      "title": "15802 Exploiting wikipedia knowledge for conceptual hierarchical clustering of documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858212665&partnerID=40&md5=fe5d62ee03910a72f8c056b3b88af0e4"
    },
    {
      "abstract": "Ontology of a domain mainly consists of concepts, taxonomical (hierarchical) relations and non-taxonomical relations. Automatic ontology construction requires methods for extracting both taxonomical and non-taxonomical relations. Compared to extensive works on concept extraction and taxonomical relation learning, little attention has been given on identification and labeling of non-taxonomical relations in text mining. In this paper, we propose an unsupervised technique for extracting non-taxonomical relations from domain texts. We propose the VFICF metric for measuring the importance of a verb as a representative relation label, in much the same spirit as the TFIDF measure in information retrieval. Domain-relevant concepts (nouns) are extracted using techniques developed earlier. Candidate non-taxonomical relations are generated as (SVO) triples of the form (subject, verb, object) from domain texts. A statistical method with log-likelihood ratios is used to estimate the significance of relationships between concepts and to select suitable relation labels. Texts from two domains, the Electronic Voting (EV) domain texts and the Tenders and Mergers (TNM) domain texts are used to compare our method with one of the existing approaches. Experiments showed that our method achieved better performance in both domains.  2011 Springer Science+Business Media, LLC.",
      "title": "15808 Learning non-taxonomical semantic relations from domain texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856218022&partnerID=40&md5=44b057f6f4d8fd94ad1c8d07bc74a73d"
    },
    {
      "abstract": "Extracting concepts (such as drugs, symptoms, and diagnoses) from clinical narratives constitutes a basic enabling technology to unlock the knowledge within and support more advanced reasoning applications such as diagnosis explanation, disease progression modeling, and intelligent analysis of the effectiveness of treatment. The recent release of annotated training sets of de-identified clinical narratives has contributed to the development and refinement of concept extraction methods. However, as the annotation process is labor-intensive, training data are necessarily limited in the concepts and concept patterns covered, which impacts the performance of supervised machine learning applications trained with these data. This paper proposes an approach to minimize this limitation by combining supervised machine learning with empirical learning of semantic relatedness from the distribution of the relevant words in additional unannotated text.The approach uses a sequential discriminative classifier (Conditional Random Fields) to extract the mentions of medical problems, treatments and tests from clinical narratives. It takes advantage of all Medline abstracts indexed as being of the publication type  clinical trials to estimate the relatedness between words in the i2b2/VA training and testing corpora. In addition to the traditional features such as dictionary matching, pattern matching and part-of-speech tags, we also used as a feature words that appear in similar contexts to the word in question (that is, words that have a similar vector representation measured with the commonly used cosine metric, where vector representations are derived using methods of distributional semantics). To the best of our knowledge, this is the first effort exploring the use of distributional semantics, the semantics derived empirically from unannotated text often using vector space models, for a sequence classification task such as concept extraction. Therefore, we first experimented with different sliding window models and found the model with parameters that led to best performance in a preliminary sequence labeling task.The evaluation of this approach, performed against the i2b2/VA concept extraction corpus, showed that incorporating features based on the distribution of words across a large unannotated corpus significantly aids concept extraction. Compared to a supervised-only approach as a baseline, the micro-averaged F-score for exact match increased from 80.3% to 82.3% and the micro-averaged F-score based on inexact match increased from 89.7% to 91.3%. These improvements are highly significant according to the bootstrap resampling method and also considering the performance of other systems. Thus, distributional semantic features significantly improve the performance of concept extraction from clinical narratives by taking advantage of word distribution information obtained from unannotated data.  2011 Elsevier Inc.",
      "title": "15811 Enhancing clinical concept extraction with distributional semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856376731&partnerID=40&md5=ff03b2cf13a24433b6775a59ce437ff9"
    },
    {
      "abstract": "Document clustering has been investigated for use in a number of different areas of text mining and information retrieval. Initially, document clustering was investigated for improving the precision or recall in information retrieval systems and as an efficient way of finding the nearest neighbors of a document. More recently, clustering has been proposed for use in browsing a collection of documents or in organizing the results returned by a search engine in response to a users query. This paper presents a new semantic synonym based correlation indexing method in which documents are clustered based on nearest neighbors from the document collection and then further refined by semantically relating the query term with the retrieved documents by making use of a thesaurus or ontology model to improve the performance of Information Retrieval System (IRS) by increasing the number of relevant documents retrieved. Results show that the proposed method achieves significant improvement than the existing methods and may generate the more relevant document in the top rank.  2005 - 2012 JATIT & LLS. All rights reserved.",
      "title": "15813 Improving information retrieval using document clusters and semantic synonym extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84857954842&partnerID=40&md5=0a5f6aefe4458e6cc719d2d743b36f22"
    },
    {
      "abstract": "The recent influx in generation, storage, and availability of textual data presents researchers with the challenge of developing suitable methods for their analysis. Latent Semantic Analysis (LSA), a member of a family of methodological approaches that offers an opportunity to address this gap by describing the semantic content in textual data as a set of vectors, was pioneered by researchers in psychology, information retrieval, and bibliometrics. LSA involves a matrix operation called singular value decomposition, an extension of principal component analysis. LSA generates latent semantic dimensions that are either interpreted, if the researchers primary interest lies with the understanding of the thematic structure in the textual data, or used for purposes of clustering, categorization, and predictive modeling, if the interest lies with the conversion of raw text into numerical data, as a precursor to subsequent analysis. This paper reviews five methodological issues that need to be addressed by the researcher who will embark on LSA. We examine the dilemmas, present the choices, and discuss the considerations under which good methodological decisions are made. We illustrate these issues with the help of four small studies, involving the analysis of abstracts for papers published in the European Journal of Information Systems.  2012 Operational Research Society Ltd. All rights reserved.",
      "title": "15816 Latent semantic analysis: Five methodological recommendations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863011331&partnerID=40&md5=7919402fb8a813b976e202b04c06875a"
    },
    {
      "abstract": "Unstructured text data, such as emails, blogs, contracts, academic publications, organizational documents, transcribed interviews, and even tweets, are important sources of data in Information Systems research. Various forms of qualitative analysis of the content of these data exist and have revealed important insights. Yet, to date, these analyses have been hampered by limitations of human coding of large data sets, and by bias due to human interpretation. In this paper, we compare and combine two quantitative analysis techniques to demonstrate the capabilities of computational analysis for content analysis of unstructured text. Specifically, we seek to demonstrate how two quantitative analytic methods, viz., Latent Semantic Analysis and data mining, can aid researchers in revealing core content topic areas in large (or small) data sets, and in visualizing how these concepts evolve, migrate, converge or diverge over time. We exemplify the complementary application of these techniques through an examination of a 25-year sample of abstracts from selected journals in Information Systems, Management, and Accounting disciplines. Through this work, we explore the capabilities of two computational techniques, and show how these techniques can be used to gather insights from a large corpus of unstructured text.  2012 Operational Research Society Ltd. All rights reserved.",
      "title": "15825 Quantitative approaches to content analysis: Identifying conceptual drift across publication outlets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856994229&partnerID=40&md5=a3374d211882be153796e14d6179b4eb"
    },
    {
      "abstract": "The advent of Web 2.0 has led to an increase in user-generated content on the Web. This has provided an extensive collection of free-style texts with opinion expressions that could influence the decisions and actions of their readers. Providers of such content exert a certain level of influence on the receivers and this is evident from blog sites having effect on their readers purchase decisions, political view points, financial planning, and others. By detecting the opinion expressed, we can identify the sentiments on the topics discussed and the influence exerted on the readers. In this paper, we introduce an automatic approach in deriving polarity pattern rules to detect sentiment polarity at the phrase level, and in addition consider the effects of the more complex relationships found between words in sentiment polarity classification. Recent sentiment analysis research has focused on the functional relations of words using typed dependency parsing, providing a refined analysis on the grammar and semantics of textual data. Heuristics are typically used to determine the typed dependency polarity patterns, which may not comprehensively identify all possible rules. We study the use of class sequential rules (CSRs) to automatically learn the typed dependency patterns, and benchmark the performance of CSR against a heuristic method. Preliminary results show CSR leads to further improvements in classification performance achieving over 80% F1 scores in the test cases. In addition, we observe more complex relationships between words that could influence phrase sentiment polarity, and further discuss on possible approaches to handle the effects of these complex relationships.  2012 Springer Science+Business Media, LLC & Science Press, China.",
      "title": "15826 Phrase-level sentiment polarity classification using rule-based typed dependencies and additional complex phrases consideration",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861739949&partnerID=40&md5=953090424796db677123ae62d83a2154"
    },
    {
      "abstract": "Bag-of-words is the most common-used method in text mining tasks and many other applications. However, this method has some obvious shortcomings, such as ignoring semantic information. While in document analysis, semantic information always plays a more important role than individual words. To tackle this problem, we need to borrow semantic information from ontologies to learn the text information better. An expert-edited ontology is usually well structured and is more authoritative than an online cyclopedia. On the other hand, due to the costly editing, it is rather difficult for expert-edited ontologies to keep up with a deluge of new words. In this paper, we propose a method to construct a Chinese ontology to keep the carefully-designed structure of an expert-edited ontology, meanwhile embody new vocabulary from an online cyclopedia. We name the enhanced ontology as Chinese Concept Encyclopedia (CCE) and employ it in some text mining applications. The experimental results show that CCE outperforms the expert-edited ontology Chinese Concept Dictionary (CCD).  2011 Springer-Verlag.",
      "title": "15828 CCE: A Chinese concept encyclopedia incorporating the expert-edited chinese concept dictionary with online cyclopedias",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84255176305&partnerID=40&md5=7a868fec6f349ae03df32aa8462a560f"
    },
    {
      "abstract": "The conventional algorithms for text clustering that are based on the bag of words model, fail to fully capture the semantic relations between the words. As a result, documents describing an identical topic may not be categorized into same clusters if they use different sets of words. A generic solution for this issue is to utilize background knowledge to enrich the document contents. In this research, we adopt a language modeling approach for text clustering and propose to smooth the document language models using Wikipedia articles in order to enhance text clustering performance. The contents of Wikipedia articles as well as their assigned categories are used in three different ways to smooth the document language models with the goal of enriching the document contents. Clustering is then performed on a document similarity graph constructed on the enhanced document collection. Experiment results confirm the effectiveness of the proposed methods.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "15829 Wikipedia-based smoothing for enhancing text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84255194247&partnerID=40&md5=b9640a3b7c7bb3059021b4c79d8cd3de"
    },
    {
      "abstract": "For the problems of Chinese text similarity calculation based on word frequency statistics, this paper proposed a method by using machine translation to translate Chinese text into English text, indirectly calculate similarity of given texts. This method can avoid some shortcomings of Chinese word segmentation and utilize the advantages of the natural word segmentation of English, and also can use machine translation to indirectly take the semantics of part of words into account. The experiments compared it with the way of directly using Chinese, and a detailed analysis was performed. Experiments show that this method can improve most of social texts similarity computation as well as increase the accuracy of the computation as a whole.  2011 IEEE.",
      "title": "15834 Empirical study of Chinese text similarity computation based on machine translation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84055223157&partnerID=40&md5=9662865fa44bf789266908ea67f9fcb8"
    },
    {
      "abstract": "When dealing with the high dimensions and large-scale multi-class textual data, it is commonly to ignore the semantic relation between words with the traditional feature selection method. In order to solve the problem, we introduce the categories information into the existing LDA model feature selection algorithm, and construct SVM multi-class classifier on the implicit topic-text matrix. Experimental results show that this method can improve classification accuracy and the dimensionality is reduced availably, the value of F1, Macro-F1, and Micro-F1 are obtained improvement.  2011 Published by Elsevier Ltd.",
      "title": "15835 Multi-class text categorization based on LDA and SVM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84055189202&partnerID=40&md5=7cf7693fc7ad64082bcf148465931915"
    },
    {
      "abstract": "The main existed problem in the traditional text classification methods is cant use the rich semantic information in training data set. This paper proposed a new text classification model based SUMO (The Suggested Upper Merged Ontology) and WordNet ontology integration. This model utilizes the mapping relations between WordNet synsets and SUMO ontology concepts to map terms in document-words vector space into the corresponding concepts in ontology, forming document-concepts vector space, based this, we carry out a text classification experiment. Experiment results show that the proposed method can greatly decrease the dimensionality of vector space and improve the text classification performance.  2011 Springer-Verlag.",
      "title": "15836 A novel web pages classification model based on integrated ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83755170323&partnerID=40&md5=1c78e51eceb2fb1759361244c78b37d4"
    },
    {
      "abstract": "In this paper we describe the possibility of constructing the well-known small world topology for an ordinary document, based on the actual document structure. Sentences in such a graph are represented by nodes, which are connected if and only if the corresponding sentences are neighbors or share at least one common keyword. This graph is built using a carefully selected one-parameter set of keywords. By varying this parameter - the level of meaningfulness - we transition the document-representing graph from a trivial path graph into a large random graph. During such a conversion, as the parameter is varied over its range, the graph becomes a small world. This in turn opens the possibility of applying many well-established ranking algorithms to the problem of ranking sentences and paragraphs in text documents. These rankings are, in turn, crucial for document understanding, summarization and information extraction. These graphs can also serve as a source of interesting small world graphs for the theory of complex networks.  2011 IEEE.",
      "title": "15838 Document sentences as a small world",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83755174368&partnerID=40&md5=faa70042664103031ec547c2970cdad5"
    },
    {
      "abstract": "This paper discribes InSciTe Advanced, a decision-making support service, based on TOD(technology opportunity discovery) model. TOD model is a logical model for discovery of emerging technologies and prediction of phase and speed on a technology life cycle. InSciTe Advanced is based on semantic technologies such as ontology, semantic repository and inference as well as text mining. It aims to provide multi-facet services on emerging technologies, their elements and alternations in all domain. InSciTe Advanced has major services such as trends and predictions, technology levels, relationship paths, roadmaps and competitiors and collaborators.  2011 Springer-Verlag.",
      "title": "15839 Decision-making support service based on technology opportunity discovery model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83755196207&partnerID=40&md5=6cc05c73241cd52197d7cc167dc86904"
    },
    {
      "abstract": "Ontology is an effective formal representation of knowledge used commonly in artificial intelligence, semantic web, software engineering and information retrieval. Typically, ontologies are constructed by domain experts using domain knowledge and domain documents. However, manual acquisition of ontologies from domain documents consumes high costs. We present a support system for Vietnamese ontology construction using pattern-based mechanisms of discovering Vietnamese concepts and conceptual relations from Vietnamese text documents. As there are very few existing taxonomies constructed in Vietnamese, we use non-taxonomy based approach. The combination of association rule mining and lexical pattern based learning was used as our major method of concept extraction and conceptual relation detection. From the experiments, we show that this is a feasible solution.  2011 IEEE.",
      "title": "15840 A preliminary study on semi-automatic construction of Vietnamese ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83755196474&partnerID=40&md5=cb21523fa583bf16c180d44c85f7644a"
    },
    {
      "abstract": "Nowadays, users of computers store a lot of information on the Web. For this reason, the Internet is a good place to search information on any subject. Due to the large amount of information, some users would search information on specific websites that they consider interesting (e.g. www.wikipedia.com, news sites, etc.). Traditional models represent webpages by using the frequency of terms or the structure of links in order to assign weight to terms of webpages. This paper presents a semantic information retrieval to represent specific websites. This proposal integrates text mining algorithms based on natural language processing and traditional representation models with the aim to improve the quality of webpages recovered by searching. Each webpage of the website is represented as a vector of topics, instead of a vector of terms. In a similar way, the query is represented as a vector of topics. Thus, a similarity measure can be applied over this vector and vectors of documents to retrieve the most relevant documents.  2011 IEEE.",
      "title": "15841 A semantic information retrieval model for focused crawling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83755225295&partnerID=40&md5=465d53e9407adad377513824840c8601"
    },
    {
      "abstract": "It has never been easy to search relevant information from ever increasing corpus of academic literatures. Large volume of research exists concerning this problem. The previous solutions are put on either ends of spectrum: general-purpose search and domain-specific deep search systems. The general-purpose search systems such as PubMed offer flexible query interface, but churn out a list of matching documents that users have to digest in order to find the answers to their queries. On the other hand, the deep search systems such as PPI Finder and iHOP return the precompiled results in a structured way. Their results, however, are often found only within some predefined contexts. In order to address this problem, we introduce a new search engine, BOSS, for search on biomedical objects. Unlike the conventional search systems, BOSS indexes segments, rather than documents. A segment refers to a minimal semantic unit such as phrase, clause or sentence that is semantically coherent in the given context (e.g., biomedical objects or their relations). For a user query, BOSS finds all matching segments, identifies the objects appearing in the segments, and aggregates the segments for each object. Finally, it turns up for the user the ranked list of the objects along with their matching segments. BOSS fills the gap between either ends of the spectrum by allowing users to pose context-free queries and by returning a structured set of results. Furthermore, BOSS exhibits the characteristic of good scalability, just as with conventional document search engines, because as it is designed to use a standard document-indexing model with minimal modifications. Considering the features, BOSS is believed to notch up the technological level of traditional solutions for search on biomedical information. BOSS is accessible at http://boss.korea.ac.kr.  2011 ACM.",
      "title": "15846 BOSS: A biomedical object search system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83255166529&partnerID=40&md5=7ee856398c44a9bcf99ef29098184bd0"
    },
    {
      "abstract": "With the flourishing of community-based question answering (cQA) services like Yahoo! Answers, more and more web users seek their information need from these sites. Understanding users information need expressed through their search questions is crucial to information providers. Question classification in cQA is studied for this purpose. However, there are two main difficulties in applying traditional methods (question classification in TREC QA and text classification) to cQA: (1) Traditional methods confine themselves to classify a text or question into two or a few predefined categories. While in cQA, the number of categories is much larger, such as Yahoo! Answers, there contains 1,263 categories. Our empirical results show that with the increasing of the number of categories to moderate size, the performance of the classification accuracy dramatically decreases. (2) Unlike the normal texts, questions in cQA are very short, which cannot provide sufficient word co-occurrence or shared information for a good similarity measure due to the data sparseness. In this paper, we propose a two-stage approach for question classification in cQA that can tackle the difficulties of the traditional methods. In the first stage, we preform a search process to prune the large-scale categories to focus our classification effort on a small subset. In the second stage, we enrich questions by leveraging Wikipedia semantic knowledge to tackle the data sparseness. As a result, the classification model is trained on the enriched small subset. We demonstrate the performance of our proposed method on Yahoo! Answers with 1,263 categories. The experimental results show that our proposed method significantly outperforms the baseline method (with error reductions of 23.21%).  2011 ACM.",
      "title": "15850 Large-scale question classification in cQA by leveraging Wikipedia semantic knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055161663&partnerID=40&md5=736d4b0c5f05e57989dc3b32af275479"
    },
    {
      "abstract": "Hand-crafted textual patterns have been the mainstay device of practical relation extraction for decades. However, there has been little work on reducing the manual effort involved in the discovery of effective textual patterns for relation extraction. In this paper, we propose a clustering-based approach to facilitate the pattern discovery for relation extraction. Specifically, we define the notion of semantic signature to represent the most salient features of a textual fragment. We then propose a novel clustering algorithm based on semantic signature, S2C, and its enhancement S2C+. Experiments on two real-world data sets show that, when compared with k-means clustering, S2C and S2C+ are at least an order of magnitude faster, while generating high quality clusters that are at least comparable to the best clusters generated by k-means without requiring any manual tuning. Finally, a user study confirms that our clustering-based approach can indeed help users discover effective textual patterns for relation extraction with only a fraction of the manual effort required by the conventional approach.  2011 ACM.",
      "title": "15851 Facilitating pattern discovery for relation extraction with semantic-signature-based clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055161653&partnerID=40&md5=5053ddd1a27401b94faad4b2c258897c"
    },
    {
      "abstract": "With the rapid growth of social Web applications such as Twitter and online advertisements, the task of understanding short texts is becoming more and more important. Most traditional text mining techniques are designed to handle long text documents. For short text messages, many of the existing techniques are not effective due to the sparseness of text representations. To understand short messages, we observe that it is often possible to find topically related long texts, which can be utilized as the auxiliary data when mining the target short texts data. In this article, we present a novel approach to cluster short text messages via transfer learning from auxiliary long text data. We show that while some previous work exists that enhance short text clustering with related long texts, most of them ignore the semantic and topical inconsistencies between the target and auxiliary data and hurt the clustering performance. To accommodate the possible inconsistency between source and target data, we propose a novel topic model - Dual Latent Dirichlet Allocation (DLDA) model, which jointly learns two sets of topics on short and long texts and couples the topic parameters to cope with the potential inconsistency between data sets. We demonstrate through large-scale clustering experiments on both advertisements and Twitter data that we can obtain superior performance over several state-of-art techniques for clustering short text documents.  2011 ACM.",
      "title": "15852 Transferring topical knowledge from auxiliary long texts for short text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055191296&partnerID=40&md5=3e9fa5f6caa3a93bebd291f0e5ea1ef5"
    },
    {
      "abstract": "In web search, understanding the user intent plays an important role in improving search experience of the end users. Such an intent can be represented by the categories which the user query belongs to. In this work, we propose an information retrieval based approach to query categorization with an emphasis on learning category rankings. To carry out categorization we first represent a category by web documents (from Open Directory Project) that describe the semantics of the category. Then, we learn the category rankings for the queries using learning to rank techniques. To show that the results obtained are consistent and do not vary across datasets, we evaluate our approach on two datasets including the publicly available KDD Cup dataset. We report an overall improvement of 20% on all evaluation metrics (precision, recall and F-measure) over two baselines: a text categorization baseline and an unsupervised IR baseline.  2011 ACM.",
      "title": "15853 Learning to rank categories for web queries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055186784&partnerID=40&md5=e8bc5f4ae01315263e217edfd2b2c14a"
    },
    {
      "abstract": "In this paper we present a novel approach to learning semantic models for multiple domains, which we use to categorize Wikipedia pages and to perform domain Word Sense Disambiguation (WSD). In order to learn a semantic model for each domain we first extract relevant terms from the texts in the domain and then use these terms to initialize a random walk over the WordNet graph. Given an input text, we check the semantic models, choose the appropriate domain for that text and use the best-matching model to perform WSD. Our results show considerable improvements on text categorization and domain WSD tasks.  2011 ACM.",
      "title": "15854 Two birds with one stone: Learning semantic models for text categorization and word sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055165906&partnerID=40&md5=02320330a4a702b38968eafa6bfbd975"
    },
    {
      "abstract": "The volume of microblogging messages is increasing exponentially with the popularity of microblogging services. With a large number of messages appearing in user interfaces, it hinders user accessibility to useful information buried in disorganized, incomplete, and unstructured text messages. In order to enhance user accessibility, we propose to aggregate related microblogging messages into clusters and automatically assign them semantically meaningful labels. However, a distinctive feature of microblogging messages is that they are much shorter than conventional text documents. These messages provide inadequate term co occurrence information for capturing semantic associations. To address this problem, we propose a novel framework for organizing unstructured microblogging messages by transforming them to a semantically structured representation. The proposed framework first captures informative tree fragments by analyzing a parse tree of the message, and then exploits external knowledge bases (Wikipedia and WordNet) to enhance their semantic information. Empirical evaluation on a Twitter dataset shows that our framework significantly outperforms existing state-of-the-art methods.  2011 ACM.",
      "title": "15855 Enhancing accessibility of microblogging messages using semantic knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055179194&partnerID=40&md5=6eac20ffbd23f2479696c06f7d82b6ac"
    },
    {
      "abstract": "Sentiment classification is becoming attractive in recent years because of its potential commercial applications. It exploits supervised learning methods to learn the classifiers from the annotated training documents. The challenge in sentiment classification lies in that the sentiment domains are diverse, heterogeneous and fast-growing. The classifiers trained on one domain (source domain) could not classify a document from another domain (target domain). The domain adaptation technique is to address the problem by making use of labeled samples in the source domain, and unlabeled samples in the target domain. This paper presents a new solution, a cross-domain topic indexing (CDTI) method, with which a common semantic space is found from the prior between-domain term correspondences and the term co-occurrences in the cross-domain documents. These observations are characterized with the mixture model in CDTI, with each component being a possible topic shared by the source and target domains. Such common topics are found to index the cross-domain content. We evaluate the algorithms on a multi-domain sentiment classification task, which shows that CDTI outperforms the state-of-the-art domain adaptation method, i.e. spectral feature alignment (SFA), and the traditional latent semantic indexing method.  2011 ACM.",
      "title": "15856 A cross-domain adaptation method for sentiment classification using probabilistic latent analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-83055161732&partnerID=40&md5=4ecb07ea6ad33163a4d903a9342a0a8a"
    },
    {
      "abstract": "Existing anti-plagiarism tools are, in fact, text matching systems but do not make accurate judgments about plagiarism. Texts that are acceptable to be redundant and texts that are cited properly are all highlighted as plagiarism, and the real decision of plagiarism is left up to the user. To reduce the human input and to give more reliance to automatic plagiarism detectors, we propose an Intelligent Plagiarism Reasoner (iPlag), which works by combining several analytical procedures. Scholarly documents under investigation are segmented into logical tree-structured representation using a procedure called D-SEGMENT. Statistical methods are utilised to assign numerical weights to structural components under a technique called C-WEIGHT. Relevance ranking (R-RANK) and plagiarism screening approaches (P-SCREEN) are adjusted to incorporate structural weights, citation evidences, syntax-based and semantic-based methods into plagiarism detection results. We encourage current plagiarism detection systems to adapt the proposed framework.  2011 IEEE.",
      "title": "15862 iPlag: Intelligent plagiarism reasoner in scientific publications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84857157439&partnerID=40&md5=2d0d7c5b6dfd2caa2b56282d1467ab36"
    },
    {
      "abstract": "The human mind creates and interprets beautiful literature. Perhaps one of the greatest challenges for Machine Intelligence is the replication of those skills. Semantic Category Theory indicates that human language cognition is based upon four distinct data types that can be combined in one of two ways. The analytical adjunct to Semantic Category Theory, Semantic Category Analysis, determines the specific data types and interaction modes involved in any spoken or written text. This paper illustrates the Semantic Category Analysis of human-created Poetry and Prose. The paper further demonstrates how the tenets of Semantic Category Theory form a basis for the Machine-creation of Poetry and Prose",
      "title": "15865 Semantic category analysis of poetry and prose",
      "url": ""
    },
    {
      "abstract": "An investigation into the use of negation in Inductive Rule Learning (IRL) for text classification is described. The use of negated features in the IRL process has been shown to improve effectiveness of classification. However, although in the case of small datasets it is perfectly feasible to include the potential negation of all possible features as part of the feature space, this is not possible for datasets that include large numbers of features such as those used in text mining applications. Instead a process whereby features to be negated can be identified dynamically is required. Such a process is described in the paper and compared with established techniques (JRip, NaiveBayes, Sequential Minimal Optimization (SMO), OlexGreedy). The work is also directed at an approach to text classification based on a bag of phrases representation",
      "title": "15867 Using negation and phrases in inducing rules for text classification",
      "url": ""
    },
    {
      "abstract": "Semantic-based document clustering has been a challenging problem over the past few years and its execution depends on modeling the underlying content and its similarity metrics. Existing metrics evaluate pair wise text similarity based on text content, which is referred as content similarity. The performances of these measures are based on co-occurrences, and ignore the semantics among words. Although, several research works have been carried out to solve this problem, we propose a novel similarity measure by exploiting external knowledge base-Wikipedia to enhance document clustering task. Wikipedia articles and the main categories were used to predict and affiliate them to their semantic concepts. In this measure, we incorporate context similarity by constructing a vector with each dimension representing contents similarity between a document and other documents in the collection. Experimental result conducted on TREC blog dataset confirms that the use of context similarity measure, can improve the precision of document clustering significantly.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "15868 Clustering blogs using document context similarity and spectral graph partitioning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84555204281&partnerID=40&md5=1b6d2de5f1ff50a52e5c714d711e8158"
    },
    {
      "abstract": "We present new ways of detecting semantic relations between learning resources, e.g. for recommendations, by only taking their usage but not their content into account. We take concepts used in linguistic lexicology and transfer them from their original field of application, i.e. sequences of words, to the analysis of sequences of resources extracted from user activities. In this paper we describe three initial experiments, their evaluation and further work.  2011 ACM.",
      "title": "15869 Usage contexts for object similarity: Exploratory investigations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856360356&partnerID=40&md5=719308465418397d4f4d463f52e294b9"
    },
    {
      "abstract": "We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.  2011 Association for Computational Linguistics.",
      "title": "15871 Identifying the semantic orientation of foreign words",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859095231&partnerID=40&md5=f74874e04ce40a6036fa5fdabcfde516"
    },
    {
      "abstract": "The volume and diversity of documents available in todays world is increasing daily. It is therefore difficult for a single classifier to efficiently handle multi-level categorization of such a varied document space. In this paper we analyse methods to enhance the efficiency of a single classifier for two-level classification by combining it with classifiers of other types. We use the maximum significance value as an indicator for the subspace of a test document. We represent the documents using the conditional significance vector which increases the distinction between classes within a subspace. Our experiments show that dividing a document space into different semantic subspaces increases the efficiency of such hybrid classifier combinations. Applying different types of classifiers on different subspaces substantially improves overall learning.  2011 IEEE.",
      "title": "15872 Hybrid classifiers for improved semantic subspace learning of news documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856735140&partnerID=40&md5=b9f5398847392c91abd693ba454f3de4"
    },
    {
      "abstract": "We propose a method enabling automatic extraction of protein-specific residues from the biomedical literature. We aim to associate mentions of specific amino acids to the protein of which the residue forms a part. The methods presented in this work will enable improved protein functional site extraction from articles, ultimately supporting protein function prediction. Our method made use of linguistic patterns for identifying the amino acid residue mentions in text. Further, we applied an automated graph-based method to learn syntactic and semantic patterns corresponding to protein-residue pairs mentioned in the text. On a new automatically generated data set of high confidence protein-residue relationship sentences, established through distant supervision, the method achieved a F-measure of 0.78. This work will pave the way to improved extraction of protein functional residues from the literature.  2011 IEEE.",
      "title": "15873 Pattern learning through distant supervision for extraction of protein-residue associations in the biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863271012&partnerID=40&md5=cb2b3349dc5ca806a4f8407714d4682b"
    },
    {
      "abstract": "Social Media is becoming major and popular technological platform that allows users discussing and sharing information. Information is generated and managed through either computer or mobile devices by one person and consumed by many other persons. Most of these user generated content are textual information, as Social Networks(Facebook, LinkedIn), Microblogging(Twitter), blogs(Blogspot, Wordpress). Looking for valuable nuggets of knowledge, such as capturing and summarizing sentiments from these huge amount of data could help users make informed decisions. In this paper, we develop a sentiment identification system called SES which implements three different sentiment identification algorithms. We augment basic compositional semantic rules in the first algorithm. In the second algorithm, we think sentiment should not be simply classified as positive, negative, and objective but a continuous score to reflect sentiment degree. All word scores are calculated based on a large volume of customer reviews. Due to the special characteristics of social media texts, we propose a third algorithm which takes emoticons, negation word position, and domain-specific words into account. Furthermore, a machine learning model is employed on features derived from outputs of three algorithms. We conduct our experiments on user comments from Facebook and tweets from twitter. The results show that utilizing Random Forest will acquire a better accuracy than decision tree, neural network, and logistic regression. We also propose a flexible way to represent document sentiment based on sentiments of each sentence contained. SES is available online.  2011 IEEE.",
      "title": "15874 SES: Sentiment elicitation system for social media data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863163463&partnerID=40&md5=1afb3d18c2d70667453853a2aaf22675"
    },
    {
      "abstract": "Many developers of biomedical knowledge bases typically validate and update formalized knowledge based on reviews of full-text scientific articles, but finding text relevant to domain concepts can be tedious and prone to errors. Prior methods have automated this process by matching term-based patterns within a single sentence. In our work developing a knowledge base of autism phenotypes, specified using Semantic Web standards, we are interested in finding multi-sentence sections of text that contains complex phenotype definitions. In this paper, we present a text-mining method that incorporates both ontology- and rule-based semantics to determine which section is relevant. We evaluated our method in undertaking text extraction for the set of full-text articles used to create the knowledge base. We show that our method has higher precision and recall than a term-based approach in identifying definitions that contain complex patterns and occur across sentence boundaries.",
      "title": "15875 Ontology-based text mining of concept definitions in biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871273742&partnerID=40&md5=30c1c3be0396b3debedcecfdf48f718a"
    },
    {
      "abstract": "A current problem in text processing is the inability to make accurate unsupervised semantic classification systems. In this research we study the unsupervised semantic classification problem using several approaches. We find that morphological and semantic hints can be translated into effective rules within semantic classification. Our results showed a 66% recall rate and a 70% precision rate. We also observed that using raw contextual words as a metric for observing similarity between concepts is minimally effective. Finally we propose further research topics that may be able to improve recall and precision rates of unsupervised semantic classification systems.  2011 IEEE.",
      "title": "15877 Unsupervised semantic classification methods",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863035834&partnerID=40&md5=acfe1d4d519d796d82c3326a1775dd4f"
    },
    {
      "abstract": "Recently, due to the widespread on-line availability of syntactically annotated text corpora, some automated tools for searching in such text corpora have gained great attention. Generally, those conventional corpus search tools use a decomposition-matching-merging method based on relational predicates for matching a tree pattern query to the desired parts of text corpora. Thus, their query formulation and expressivity are often complicated due to poorly understood query formalisms, and their searching tasks may require a big computational overhead due to a large number of repeated trials of matching tree patterns. To overcome these difficulties, we present TPEMatcher, a tool for searching in parsed text corpora. TPEMatcher provides not only an efficient way of query formulation and searching but also a good query expressivity based on concise syntax and semantics of tree pattern query. We also demonstrate that TPEMatcher can be effectively used for a text mining in practice with its useful interface providing in-depth details of search results.  2011 Elsevier B.V. All rights reserved.",
      "title": "15878 TPEMatcher: A tool for searching in parsed text corpora",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051471698&partnerID=40&md5=b01532a52884a78eb5771586de5ab4fd"
    },
    {
      "abstract": "The ability to understand natural language text is far from being emulated in machines. One of the main hurdles to overcome is that computers lack both the common and the common sense knowledge humans normally acquire during the formative years of their lives. If we want machines to really understand natural language, we need to provide them with this kind of knowledge rather than relying on the valence of keywords and word co-occurrence frequencies. In this work, we blend the largest existing taxonomy of common knowledge with a natural-language-based semantic network of common sense knowledge, and use multi-dimensionality reduction techniques on the resulting knowledge base for opinion mining and sentiment analysis.  2011 IEEE.",
      "title": "15882 Isanette: A common and common sense knowledge base for opinion mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863140492&partnerID=40&md5=2d504a6bb0422f83d1b23040eae8688a"
    },
    {
      "abstract": "Identifying the semantic relatedness of two words is an important task for the information retrieval, natural language processing, and text mining. However, due to the diversity of meaning for a word, the semantic relatedness of two words is still hard to precisely evaluate under the limited corpora. Nowadays, Wikipedia is now a huge and wiki-based encyclopedia on the internet that has become a valuable resource for research work. Wikipedia articles, written by a live collaboration of user editors, contain a high volume of reference links, URL identification for concepts and a complete revision history. Moreover, each Wikipedia article represents an individual concept that simultaneously contains other concepts that are hyperlinks of other articles embedded in its content. Through this, we believe that the semantic relatedness between two words can be found through the semantic relatedness between two Wikipedia articles. Therefore, we propose an Editor-Contribution-based Rank (ECR) algorithm for ranking the concepts in the articles content through all revisions and take the ranked concepts as a vector representing the article. We classify four types of relationship in which the behavior of addition and deletion maps appropriate and inappropriate concepts. ECR ranks those concepts depending on the mutual signed-reinforcement relationship between the concepts and the editors. The results reveal that our method leads to prominent performance improvement and increases the correlation coefficient by a factor ranging from 4% to 23% over previous methods that calculate the relatedness between two articles.  2011 IEEE.",
      "title": "15885 Measuring semantic relatedness using wikipedia revision information in a signed network",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84862973145&partnerID=40&md5=36387476fc47be93e0cc3a917e96e9db"
    },
    {
      "abstract": "Tag cloud, also known as word cloud, are very useful for quickly perceiving the most prominent terms embedded within a text collection to determine their relative prominence. The effectiveness of tag clouds to conceptualize a text corpus is directly proportional to the quality of the keyphrases extracted from the corpus. Although, authors provide a list of about five to ten keywords in scientific publications that are used to map them into their respective domain, due to exponential growth in non-scientific documents on the World Wide Web, an automatic mechanism is sought to identify keyphrases embedded within them for tag cloud generation. In this paper, we propose a web content mining technique to extract keyphrases from web documents for tag cloud generation. Instead of using partial or full parsing, the proposed method applies n-gram technique followed by various heuristics-based refinements to identify a set of lexical and semantic features from text documents. We propose a rich set of domain-independent features to model candidate keyphrases very effectively for establishing their keyphraseness using classification models. We also propose a font-determination function to determine the relative font-size of keyphrases for tag cloud generation. The efficacy of the proposed method is established through experimentation. The proposed method outperforms the popular keyphrase extraction system KEA.  2011 ACM.",
      "title": "15886 A web content mining approach for tag cloud generation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856837496&partnerID=40&md5=37125a6993dbefefce1dd30cb8938c27"
    },
    {
      "abstract": "Service integration is central to joined-up government initiatives and requires information on the collaborators and the services they offer, roles of different actors, the resources required, and their goals (individual and shared). These information are largely available in unstructured forms on government portals, publications and other textural sources. This paper explores semantic text mining for extracting service-related information from such sources using Natural Language Processing techniques supported by Service-Oriented Process Ontologies. Our solution framework consists of the following steps: (1) creating domain and service-oriented process ontology, (2) extracting service related information from textual sources based on the ontology, and finally (3) mining relationship among the services based on the extracted information in Step 2 linked with a pre-defined hierarchy of service delivery goals specifying the objective(s) to be achieved among the orchestrated services. We describe our approach to these tasks and discuss the progress of the work, our experiences and the challenges encountered so far. Copyright 2011 ACM.",
      "title": "15892 Mining service integration opportunities towards joined-up government",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855396930&partnerID=40&md5=029f5df5a26c0e4b040d92257dd4508e"
    },
    {
      "abstract": "In order to overcome the SVM for text classification ignoring the context of semantic information and the use of a community to text classification, one boundary point can only belong to a community of view, the concept of contribution and overlapping coefficient based on the complex network diagram is introduced. And feature selection algorithm based on community discovery is proposed. Experiments show that the algorithm can remain the text in the context of semantic information, which is more reasonable to divide the boundary points into the community. Then in the Bayesian classifier validate the algorithm performance in the recall, precision and F1 values. The result shows that the algorithm performance is superior to MIDF, and has a good flexibility in text classification.  2011 IEEE.",
      "title": "15895 Feature selection algorithm based on the Community discovery",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856536018&partnerID=40&md5=fcd51b4bc3660c46675dbe4b90b092e9"
    },
    {
      "abstract": "Ontology evolution is the process of incrementally and consistently adapting an existing ontology to changes in the relevant domain. Semantic drift refers to how ontology concepts intentions gradually change as the domain evolves. Normally, a semantic drift captures small domain changes that are hard to detect with traditional ontology management tools or ontology learning methods, but may be important to the maintenance of the ontology. This paper discusses a new approach to detecting semantic drift that makes use of concept signatures reflecting the textual references to concepts over time. Comparing how signatures change over time, we see how concepts semantic content evolves and how their relationships to other concepts gradually reflect these changes. An experiment with the DNVs business sector ontology from 2004 and 2008 demonstrates the value of this approach to ontology evolution.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "15901 Concept signatures and semantic drift",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84876223325&partnerID=40&md5=200ab6c5a09aa84a8c8362e4e2b16088"
    },
    {
      "abstract": "This contributions deals with the methodological study of a generative approach for the analysis of textual data. Instead of creating heuristic rules for the representation of documents and word counts, we employ a distribution able to model words along text considering different topics. In this regard, following Minka proposal, we implement a Dirichlet Compound Multinomial distribution that is a mixture of random variables over words and topics.Moving from such approach we propose an extension called sbDCM that takes into account the different latent topics that compound the document. The number of topics to be inserted can be known or unknown in advance, on the basis of the application context.Without losing in generality we present the case where the number and characteristics of topics are properly evaluated on the basis of available data.  Springer-Verlag Berlin Heidelberg 2011.",
      "title": "15903 A semantic based Dirichlet Compound Multinomial model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888272061&partnerID=40&md5=9afb8c70cc7ad05353ab08457ecdea26"
    },
    {
      "abstract": "The objective of this paper is to discuss the term ontology with reference to Philosophy and Information Science. The rationale which constitutes the term is analyzed in relation to the present use by researchers of different areas, but always related to the computer context. A formal definition of ontology is presented by utilizing Logic and Mathematics, which are essential for automation of computerized procedures. Some models proposed are interrelated for construction of ontology and consolidation of a vision which can be used in Information Systems. Arguments are presented for the automatic construction of ontology from the text as a way of making the production of ontology feasible and as fast as required for the present days and so providing a quality to information structures spread out in the web digital world.",
      "title": "15908 From the text to ontology: A perspective for information science",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866365322&partnerID=40&md5=44e193964b8872dd73d379b7fcd9a96f"
    },
    {
      "abstract": "This paper constructs a knowledge representation model that describes structural relationships among participants in an event. The model is used to represent the internal structure of the frames in Chinese FrameNet and thus describes the inference knowledge in Chinese texts. This paper applies the model in the commerce event and shows it provides a flexible means of expressing linguistic perspectives and semantic inference relationships between them. The resulting representations should be useful for a variety of NLP applications, including question answering and information extraction.  2011 by Binary Information Press.",
      "title": "15909 A Chinese event representation model based on CFN",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84155196056&partnerID=40&md5=ac4c5d2e0b12718fbffc9db3fa9d0805"
    },
    {
      "abstract": "The digitization of books can not only provide a electronic way to read but also make it possible to establish information systems via information extraction and correlation. This paper proposed a new approach that build up a traditional Chinese medicine (TCM) diagnosis system by extracting information from textbooks. First of all, we design a semantic ontology which can achieve targeted and precise extraction by combining it with SVM classification and regular expression match. After the popularization of the ontology, data and information in the database can be correlated automatically. Moreover, aiming at the domain of TCM textbooks, a new symptom identification algorithm is adopted. Eventually, a structural knowledge database is constructed and the experiment system shows that our method can be useful to provide new services for digital library.  2011 IEEE.",
      "title": "15910 A TCM diagnosis system based on textbook information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863298558&partnerID=40&md5=4fc14b8fc418c0b2ecaaf6b24f372ed3"
    },
    {
      "abstract": "In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet.  2011 ACM.",
      "title": "15913 Text mining and probabilistic language modeling for online review spam detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859702088&partnerID=40&md5=398dc5c6eafbf6fbcf8e092f4606d019"
    },
    {
      "abstract": "Nowadays, most of information saved in companies are as unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the storage efficiency and unstructured data analysis. Merrill Lynch recently estimated that more than 80% of all potentially useful business information is unstructured data. The large number and complexity of unstructured data opens up many new possibilities for the analyst. We analyze both structured and unstructured data individually and collectively. Text mining and natural language processing are two techniques with their methods for knowledge discovery form textual context in documents. In this study, text mining and natural language techniques will be illustrated. The aim of this work comparison and evaluation the similarities and differences between text mining and natural language processing for extraction useful information via suitable themselves methods.  2011 IEEE.",
      "title": "15922 Analysis and evaluation of unstructured data: Text mining versus natural language processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855923552&partnerID=40&md5=a0b9ac0b6f06a431e997ef4b35d01213"
    },
    {
      "abstract": "This paper explores the use of a graph decomposition technique applied to an ontology, designed as a basis for understanding natural language text. It demonstrates how the technique can help with ontology verification, detecting common human errors in concept acquisition, as well as having some ramifications for semantic processing of multiple noun expressions, among other natural language entities. Ontology verification with the help of graph decomposition is illustrated on the phenomena of inverse properties, transitive properties, and hierarchical (ancestor/descendant) properties. The ontology on which the technique has been tested is that of Ontological Semantic Technology, a significantly modified version of Ontological Semantics [5].",
      "title": "15923 Graph decomposition and its use for ontology verification and semantic representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866124939&partnerID=40&md5=410b07b51c055226d1d07d6611f698e9"
    },
    {
      "abstract": "We propose and evaluate a systematic approach to detect and classify Patient/Problem, Intervention, Comparison and Outcome (PICO) from the medical literature. The training and test corpora were generated systematically and automatically from structured PubMed abstracts. 23,472 sentences by exact pattern match of head words of P-I-O categories. Afterward, the terms with top frequencies were used as the features of Naive Bayesian classifier. This approach achieves F-measure values of 0.91 for Patient/Problem, 0.75 for Intervention and 0.88 for Outcome, comparable to previous studied based on mixed textural, paragraphical, and semantic features. In conclusion, we show that by stricter pattern matching criteria of training set, detection and classification of PICO elements can be reproducible with minimal expert intervention. The results of this work are higher than previous studies.  2011 IEEE.",
      "title": "15926 Classification of PICO elements by text features systematically extracted from PubMed abstracts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863028228&partnerID=40&md5=c1531b78cccee254f58a1d054aa673e3"
    },
    {
      "abstract": "In this paper we propose a novel framework for recognizing complex opinion attributes from product reviews. Instead of focusing on linguistic properties of text fragments and their direct representations, we focus on these fragments similarities which we obtain from multiple sources of lexical and semantic information. The problem is formulated as that of multiclass classification and is based on multiple similarity matrices. We apply multiple kernel learning algorithm which seeks optimal combinations of matrices using linear programming and support vector machines for classification. Experiments demonstrate benefits from multiple sources of information. Overall, the approach is promising especially in the case of reviews of product types with complex and wordy attribute expressions.  2011 IEEE.",
      "title": "15929 Mining opinion attributes from texts using multiple kernel learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84857159021&partnerID=40&md5=61c603cf40a3515559a8767c9120d1e7"
    },
    {
      "abstract": "This paper explores how background knowledge from freely available web resources can be utilised for Textual Case Based Reasoning. The work reported here extends the existing Explicit Semantic Analysis approach to representation, where textual content is represented using concepts with correspondence to Wikipedia articles. We present approaches to identify Wikipedia pages that are likely to contribute to the effectiveness of text classification tasks. We also study the effect of modelling semantic similarity between concepts (amounting to Wikipedia articles) empirically. We conclude with the observation that integrating background knowledge from resources like Wikipedia into TCBR tasks holds a lot of promise as it can improve system effectiveness even without elaborate manual knowledge engineering. Significant performance gains are obtained using a very small number of features that have very strong correspondence to how humans describe the domain.  2011 Springer-Verlag.",
      "title": "15933 Selective integration of background knowledge in TCBR systems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856842326&partnerID=40&md5=6c46133d94047c3d279f2c317500f0b0"
    },
    {
      "abstract": "Patent databases provide valuable information for technology management. However, the rapid growth of patent documents, the lengthy text and the rich of content in technical terminology, and the complicated relationships among the patents, make it taking a lot of human effort for conducting analyses. As a result, an automated system for assisting the inventors in patent analysis as well as providing support in technological innovation is in great demand. In this paper, a Semantic-based Intellectual Property Management System (SIPMS) has been developed for supporting the management of intellectual properties (IP). It incorporates semantic analysis and text mining techniques for processing and analyzing the patent documents. The method differentiates itself from the traditional technological management tools in its knowledge base. Instead of eliciting knowledge from domain experts, the proposed method adopts global patent databases as sources of knowledge. The system enables users to search for existing patent documents or relevant IP documents which are related to a potential new invention and to support invention by providing the relationships and patterns among a group of IP documents. The method has been evaluated by benchmarking with the performance against traditional text mining technique and has successfully been implemented at a selected reference site.  2011 Elsevier Ltd. All rights reserved.",
      "title": "15937 A Semantic-based Intellectual Property Management System (SIPMS) for supporting patent analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054685358&partnerID=40&md5=cfcb736a95bda4a777e3dc395c0d3e1e"
    },
    {
      "abstract": "The knowledge-based information extraction approach has been proved effectively in the recognition and classification of Web texts. However, manual creation of knowledge base is time consuming and error prone, even for a small application domain. For improving the automatic construction ability of knowledge base in the process of IE, this paper presents a web text extraction tool based on case similarity calculation (named TECS). TECS facilitates the process of constructing case knowledge base and strengthens the knowledge representation ability of single case by calculating case similarity and merging the cases which have similar semantic relationships.  2011 IEEE.",
      "title": "15939 TECS: A web text extraction tool based on semantic similarity calculation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855793654&partnerID=40&md5=a5c8e2c04b37f8984d368314f04493b4"
    },
    {
      "abstract": "With the advent of the Semantic Web aims to inject a meaning in the current Web, application of ontology as one of its layers, emerged more than past. Lack of the necessary tools and researches in this field reveals the need of this work. Due to problems with the methods based on syntactic - lexical patterns, the need for this research in order to find a method, based on ranking of the concepts is more visible. Method presented in this paper, could be applied in the fields of, information retrieval, text mining, query systems developers, developing semantic dictionaries such as WorldNet and etc. In this paper a new method for assigning instances to the concepts in ontology will be presented.  2011 AICIT.",
      "title": "15943 Proposing a new method for ontology population",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855862383&partnerID=40&md5=354cf76ab1367cea828ed7ced703d830"
    },
    {
      "abstract": "Expressiveness of natural language is a challenge for text representation since the same idea can be expressed in many different ways. Therefore, terms in a document should not be treated independently of one another since together they help to disambiguate and establish meaning. Term-similarity measures are often used to improve representation by capturing semantic relationships between terms. Another consideration for representation involves the importance of terms. Feature selection techniques address this by using statistical measures to quantify term usefulness for retrieval. In this paper we present a framework that combines term-similarity and weighting for text representation. This allows us to comparatively study the impact of term similarity, term weighting and any synergistic effect that may exist between them. Study of term similarity is based on approaches that exploit term co-occurrences within document and sentence contexts whilst term weighting uses the popular Chi-squared test. Our results on text classification tasks show that the combined effect of similarity and weighting is superior to each technique independently and that this synergistic effect is obtained regardless of co-occurrence context granularity.  2011 Springer-Verlag.",
      "title": "15945 Term similarity and weighting framework for text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856832940&partnerID=40&md5=c4471f144aeee254f51d543597c01e93"
    },
    {
      "abstract": "Reviews are text-based feedback provided by reviewers to authors. The quality of a review can be determined by identifying how relevant it is to the work that the review was written for as well as its similarity to existing well-written and coherent reviews. Relevance between two pieces of text can be determined by identifying semantic and syntactic similarities between them. In this paper, we make use of string-based metrics that incorporate concepts of paraphrasing and plagiarism to determine matching between texts. We use a graph-based text representation technique. We use the k-nearest neighbor classification algorithm to build a supervised model and classify text as LOW, MEDIUM or HIGH based on values of the metrics. We evaluate our approach on three data sets from student assignments and show that our model achieves an average accuracy of 63%.  2011 IEEE.",
      "title": "15946 Determining degree of relevance of reviews using a graph-based text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855795324&partnerID=40&md5=09fb5eb72f19bac859d501974611f7c3"
    },
    {
      "abstract": "Governments strive to achieve improvements in delivering public services, developing and implementing public policies, responding to crisis situations, and optimizing the use of public resources, among others. Achieving such goals requires collaboration across different levels and functions of government, and across public and private sectors in a Joined-Up Government. Establishing such collaboration requires information on prospective participants including their goals, resources, processes and services. Such information is rarely available in structured forms e.g. in databases, but instead scattered over government portals, publications and other textual sources. This paper proposes the use of semantic text mining for extracting collaboration-related information (focusing on government collaboration) from unstructured data sources. The proposed solution applies natural language processing techniques supported by the relevant domain and process ontologies. It consists of three steps: 1) extracting process-related information from textual sources, 2) creating process ontology instances from extracted information and 3) mining shared and integrated processes based on process instances and the service goal hierarchy in the domain ontology. The paper describes the rationale of and approach adopted in this research, the progress achieved in implementing step 1, the challenges encountered and how we intend to address them in pursuing subsequent steps.  2011 IFIP International Federation for Information Processing.",
      "title": "15947 Mining collaboration opportunities to support Joined-Up Government",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84864886782&partnerID=40&md5=bff109dac141ec05e8c9544998014ac1"
    },
    {
      "abstract": "Clustering text documents is an important step in mining useful information on the Web or other text-based resources. The common task in text clustering is to handle text in a multi-dimensional space, and to partition documents into groups, where each group contains documents that are similar to each other. However, this strategy lacks a comprehensive view for humans since it cannot explain the subject of each cluster. Utilizing semantic information such as an ontology can solve this problem, but it needs a well-defined database or pre-labeled gold standard set. In this paper, we present a theme-based clustering algorithm for text documents. Given text, subject terms are extracted and used for clustering documents in a probabilistic framework. An EM approach is used to ensure documents are assigned to correct themes, hence it converges to an optimal solution. The proposed method is distinctive because its results are sufficiently explanatory for human understanding as well as efficient for usual clustering performance. The experimental results show that the proposed method provides competitive performance compared to other state-of-the-art approaches. In addition, the extracted themes represent well the topics of clusters on the MEDLINE dataset.  2011 IEEE.",
      "title": "15949 An EM clustering algorithm which produces a dual representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863241839&partnerID=40&md5=0d8e0b1211446fc43c03de8c5346b7b1"
    },
    {
      "abstract": "Recognizing Textual Entailment (RTE) is one of the fundamental problems in many natural language processing applications. In this paper, a novel feature extraction method is proposed for the Chinese textual entailment recognition task in NTCIR. Firstly, we extract lexical and semantic features. Two lexical semantic dictionaries are used, and a novel semantic feature extraction method based on Stanford Parser and part-of-speech tagging is proposed. Furthermore, three different classification algorithms are applied and compared, rule based algorithm, SVM and C4.5. Experiment results show that C4.5 gives the best performance. Evaluation of the proposed approach on NTICR-9 RITE development data set shows promising precisions of 70.1% in binary classification and 55.9% in 5-class classification.  2011 IEEE.",
      "title": "15950 Chinese textual entailment recognition model based on lexical and semantic matching",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863129209&partnerID=40&md5=d3f65ac03d7466726874c25939a171d2"
    },
    {
      "abstract": "Most of the knowledge intensive organizations are having their information resided in large text document repositories and most of these text repositories and databases are either unstructured or semi-structured. Recently various soft computing techniques have been used to improve information retrieval efficiency. More specifically genetic algorithms have been used for various information retrieval components like matching function learning, documents clustering, information extraction, query optimization [1 - 6]. In most of the cases in information retrieval matching function is based on term frequency. But the problem with this approach is that the syntactic information of the text document is lost and phrases are also not considered, so results in poor accuracy. In this paper we have proposed a new semantic based similarity measure in which each term can be a phrase or a single word and the weight assigned to each term is based on its semantic importance considering each sentence. We have used this semantic similarity measure along with other standard similarity measure as Jaccard and cosine to form the semantic-based-combined-similarity- measure. Standard genetic algorithm has been used to optimize the weight given for each similarity measure.  2011 IEEE.",
      "title": "15951 Enhancing information retrieval efficiency using semantic-based-combined- similarity-measure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855960440&partnerID=40&md5=d61b19162f0d963fa0de446ba78f8e13"
    },
    {
      "abstract": "Today in media companies there is a serious problem for cataloging news due to the large number of articles received by the documentation departments. That manual labor is subject to many errors and omissions because of the different points of view and expertise level of each staff member. There is also an additional difficulty due to the large size of the list of words in a thesaurus. In this paper, we present a new method for solving the problem of text categorization over a corpus of newspaper articles where the annotation must be composed of thesaurus elements. The method consists of applying lemmatization, obtaining keywords and named entities, and finally using a combination of Support Vector Machines (SVM), ontologies and heuristics to infer appropriate tags for the annotation. We carried out a detailed evaluation of our method with real newspaper articles, and we compared out tagging with the annotation performed by a real documentation department, obtaining really promising results.  2011 IEEE.",
      "title": "15953 NASS: News annotation semantic system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84855759229&partnerID=40&md5=7304aa033223b4ec13f776d52211954a"
    },
    {
      "abstract": "In this paper we present a method for getting topics from aims and scopes in DBLP journals and following construction of their hierarchical order. We focused on semantic relations between topics. Our method is fully automatic but manual cleaning of topic database would lead to much better accuracy. Another purpose of our work is to provide similar system to ACM classification system. We want to provide better, newer and fully automatic system contrary to ACM.",
      "title": "15955 Modeling of lexical relations between topics retrieved from DBLP journals",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868703921&partnerID=40&md5=b3cee046df5c9f46c2b14ec841c7d036"
    },
    {
      "abstract": "To impart a congruent emotional quality to synthetic speech, it is expedient to leverage the overall polarity of the input text. This is feasible inasmuch as speech generation complies with the outcome of sentiment analysis. We have recently introduced latent affective mapping [1]-[3], a new approach to emotion detection which exploits two separate levels of semantic information: one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. The ensuing framework exposes the emergent relationship between these two levels in order to advantageously inform affective evaluation. This paper applies latent affective mapping to the narrower problem of sentiment analysis, in order to achieve a more robust identification of the polarity of textual data. Empirical evidence gathered on the Affective Text portion of the SemEval-2007 corpus [4] shows that this approach is promising for automatic sentiment prediction in text. This bodes well as a first step in ensuring emotional congruence in text-to-speech synthesis.  2011 IEEE.",
      "title": "15956 Sentiment analysis of text-to-speech input using latent affective mapping",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858992913&partnerID=40&md5=1643b58dfbe454dd6637092680171774"
    },
    {
      "abstract": "Sentiment analysis is the procedure by which information is extracted from the opinions, appraisals and emotions of people in regards to entities, events and their attributes. In decision making, the opinions of others have a significant effect on customers ease in making choices regards to online shopping, choosing events, products, entities. In this paper, the rule based domain independent sentiment analysis method is proposed. The proposed method classifies subjective and objective sentences from reviews and blog comments. The semantic score of subjective sentences is extracted from SentiWordNet to calculate their polarity as positive, negative or neutral based on the contextual sentence structure. The results show the effectiveness of the proposed method and it outperforms the machine learning methods. The proposed method achieves an accuracy of 87% at the feedback level and 83% at the sentence level.  2011 IEEE.",
      "title": "15957 Sentiment classification using sentence-level semantic orientation of opinion terms from blogs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84857095086&partnerID=40&md5=5c975862d79130d6dbe37ada46b4d9e3"
    },
    {
      "abstract": "One common approach in hierarchical text classification (HTC) involves associating classifiers with nodes in the category tree T and classifying documents in a top-down manner. Training a hierarchical classifier is to choose the suitable classifier and its parameters at each node of T because the features of data in different classes are usually distinct. Until now all known approaches suffer from so-called blocking problem and ignore the hierarchical semantic relationships among subcategories in training classifier. In this paper we propose a novel methodology to train a hierarchical classifier that takes both above two problems into account. The approach will punish the classifier that wrongly classified documents in the last run and update the weights of the wrongly classified samples according to the levels they locate at T. The aim of the training process is to let the defined whole loss function of the classification be the minimized. Experiments on two collections show that the trained classifier outperforms the traditional AdaBoost in terms of accuracy, precision, recall, F1 and microPrecision.  2011 by Binary Information Press.",
      "title": "15959 Training a hierarchical classifier based on the global information of the category tree",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84155181853&partnerID=40&md5=0897d4aca332fa8a109070230dae4851"
    },
    {
      "abstract": "This paper proposes an innovative approach to improve the performance of Persian text classification. The proposed method uses a thesaurus as a helpful knowledge to obtain the real frequencies of words in the corpus. Three types of relationships are considered in our thesaurus. This is the first attempt to use a Persian thesaurus in the field of Persian information retrieval. Experimental results show a significant improvement in the case of employing Persian thesaurus rather common methods.  2011 Springer-Verlag.",
      "title": "15969 Improving Persian text classification using Persian thesaurus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-81855177140&partnerID=40&md5=b49a3d91f6b51326203cdfc045b1107e"
    },
    {
      "abstract": "As one foundational technology of cloud computing, services computing is playing a critical role to enable provisioning of software as a service (SaaS). However, how to effectively and efficiently discover proper available services from the cloud of resources remains a big challenge. This paper reports our continuous efforts on semantic services discovery. We extend the Support Vector Machine (SVM)-based text clustering technique in the context of serviceoriented categorization in a service repository, and propose an iterative process to incrementally enrich domain ontology. A popular Web 2.0 mashup platform is used as a testbed",
      "title": "15970 Leveraging fragmental semantic data to enhance services discovery",
      "url": "Trata-se de evoluÃ§Ã£o de ontologias."
    },
    {
      "abstract": "Geographic feature categorization from text addresses the need for querying and finding geographic features from text documents. Although many text classification techniques have been developed, there are limitations to apply to geographic features due to the uniqueness of the geography features. In this paper we propose a method to classify geographic features based on latent semantic analysis and domain knowledge. The empirical experiment indicates that the proposed method achieves satisfactory categorizing effectiveness.  2011 IEEE.",
      "title": "15974 A latent semantic analysis-based approach to geographic feature categorization from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-81255128243&partnerID=40&md5=e5f5417dd56470330709ca23800993f4"
    },
    {
      "abstract": "In the relation extraction of semantic relations, it is not uncommon to face settings in which the training data provides very few instances of some relation classes. This is mostly due to the high cost of producing such data and to the class imbalance problem, which may result in some classes presenting small frequencies even with a large annotated corpus. This work thus presents a semi-supervised bootstrapped method to expand this initial training dataset, using pattern matching to extract new candidate instances from the Web. The core of this process uses a multiview feature distance-based framework, which allows quantitative and qualitative analysis of intermediate steps of the process. Experimental results show that this framework provides better results in the relation classification task than the baseline, and the bootstrapped architecture improves the relation classification task as a whole for these low-frequency semantic relations settings.  2011 IEEE.",
      "title": "15975 Feature distance-based framework for classification of low-frequency semantic relations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-81255147433&partnerID=40&md5=0372ff9da60aecaae7612b7d4ab76e09"
    },
    {
      "abstract": "The exponential growth of the Web is the most influential factor that contributes to the increasing importance of cross-lingual text retrieval and filtering systems. Indeed, relevant information exists in different languages, thus users need to find documents in languages different from the one the query is formulated in. In this context, an emerging requirement is to sift through the increasing flood of multilingual text: this poses a renewed challenge for designing effective multilingual Information Filtering systems. Content-based filtering systems adapt their behavior to individual users by learning their preferences from documents that were already deemed relevant. The learning process aims to construct a profile of the user that can be later exploited in selecting/recommending relevant items. User profiles are generally represented using keywords in a specific language. For example, if a user likes movies whose plots are written in Italian, a content-based filtering algorithm will learn a profile for that user which contains Italian words, thus failing in recommending movies whose plots are written in English, although they might be definitely interesting. Moreover, keywords suffer of typical Information Retrieval-related problems such as polysemy and synonymy. In this paper, we propose a language-independent content-based recommender system, called MARS (MultilAnguage Recommender System), that builds cross-language user profiles, by shifting the traditional text representation based on keywords, to a more complex language-independent representation based on word meanings. The proposed strategy relies on a knowledge-based word sense disambiguation technique that exploits MultiWordNet as sense inventory. As a consequence, content-based user profiles become language-independent and can be exploited for recommending items represented in a language different from the one used in the content-based user profile. Experiments conducted in a movie recommendation scenario show the effectiveness of the approach.  2011 ACM.",
      "title": "15978 Learning semantic content-based profiles for cross-language recommendations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80955125630&partnerID=40&md5=06a3680c00c3c4bd7c800628076c3ce6"
    },
    {
      "abstract": "This thesis presents novel measures to estimate the degree of semantic similarity between words using one or more knowledge sources. Several evaluations show that they improve the accuracy of related works. These measures have been applied to clustering to compute the similarity/distance between individuals described by textual attributes. Clustering results show that a proper interpretation of textual data at a semantic level improves the quality of the clusters and ease their interpretation.  2011 IOS Press and the authors. All rights reserved.",
      "title": "15979 Ontology-based semantic clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80955140428&partnerID=40&md5=d5fa7ec35b673ee816cc3a2ac201779c"
    },
    {
      "abstract": "Recent sentiment analysis research has focused on the functional relations of words using typed dependency parsing as this provides a refined analysis on the grammar and semantics of the textual data, which could improve performance. However, typed dependencies only provide the grammatical relationships between individual words while there exist more complex relationships between words that could influence a sentence sentiment polarity. In this paper, we propose a linguistic approach, called Polarity Prediction Model (PPM), that combines typed dependencies and subjective phrase analysis to detect sentence-level sentiment polarity. Our approach also considers the intensity of words and domain terms that could influence the sentiment polarity output. PPM is shown to provide a fine-grained analysis for handling and explaining the complex relationships between words in detecting a sentence sentiment polarity. PPM was found to consistently outperform a baseline model by 5% in terms of overall F1-score, and exceeding 10% in terms of positive F1-score when compared to a Typed-dependency only approach.  2011 Springer-Verlag.",
      "title": "15982 Sentence-level sentiment polarity classification using a linguistic approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80455140260&partnerID=40&md5=88bafacc2e44d0f0f4ef32ddac14643f"
    },
    {
      "abstract": "Language modeling plays a critical role for automatic extraction. Typically, the statistical model of automatic extraction suffers from the lack of the subject semantic consistency between sentences and the redundancy of information. In this study, we first introduce our work on automatic extraction, and then analyze the disadvantages of different extracting models. We then present a advanced mathematical model to overcome these lacks based on computational linguistics. As shown by experiments, the proposed modeling and methods can significantly reduce the redundancy of information and increase the subject semantic consistency between sentences of automatic abstraction with moderate computational cost.  2011 IEEE.",
      "title": "15984 An improved sentence polishing model used in automatic extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80155198242&partnerID=40&md5=d86d9956595b6eff163efa9ae0e2f18d"
    },
    {
      "abstract": "For the purpose of measuring semantic orientation of documents, we implemented an opinion mining tool which hybrids three different methods: The first one is based on semantic patterns, which simplify the structure of the natural language syntax",
      "title": "15985 Analysis of three methods for web-based opinion mining",
      "url": "Conference Paper"
    },
    {
      "abstract": "The identification of communities in social networks is a common problem that researchers have been dealing using network analysis properties. However, in environments where community members are connected by digital documents, most researchers have either emphasize to solve the community discovery problem computing structural properties of networks, ignoring the underlying semantic information from digital documents. In this paper, we propose a novel approach to combine traditional network analysis methods for community detection with text mining techniques. This way, extracted communities can be labeled according to latent semantic information within documents, called topics. Our proposal was evaluated in Plexilandia, a virtual community of practice with more than 2,500 members and 9 years of commentaries.  2011 IEEE.",
      "title": "15986 Enhancing community discovery and characterization in VCoP using topic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80155202847&partnerID=40&md5=ec79e76aec3258f57e345284c31d17c1"
    },
    {
      "abstract": "The blooming of the Internet information has made fast text categorization very essential. Generally, in order to accelerate the classification process, the classifier needs to be simplified as much as possible",
      "title": "15988 Fast text categorization based on collaborative work in the semantic and class spaces",
      "url": ""
    },
    {
      "abstract": "Many NLP and IR applications require semantic classification knowledge of words. However, manually constructing semantic classes is a time-consuming and labor-intensive task. In this paper, we present an algorithm for induction of Chinese semantic classes from natural language text based on coordinate patterns. First, several coordinate patterns are proposed to harvest high-quality coordinate instance. Second, an iterative clustering process is used to cluster words into semantic classes. The clustering process mainly used coordinate relation between words. Experiment results show that the proposed approach performs relatively well and achieves 53.2% in terms of precision. Finally, a thesaurus containing about 15000 Chinese words is generated automatically.  2011 IEEE.",
      "title": "15989 Induction of semantic classes based on coordinate patterns",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80155139255&partnerID=40&md5=7ec7a0d62c00479cd18b04f3895968ce"
    },
    {
      "abstract": "This paper describes how natural language processing and ontologies are exploited for automatic text categorisation. The approach introduced is part of the MANENT system, an infrastructure for integrating, structuring and searching Digital Libraries. The procedure of structural information extraction, and of the automatic classification of the records according to natural language understanding and the WordNet Domains taxonomy is discussed. A comparison between two versions of the classification algorithm is conducted and the improvements of the new approach are articulated. In particular, using semantic connections between words refines the classification results while reducing misclassification to non classification.  2011 IEEE.",
      "title": "15990 When you doubt, abstain: From misclassification to epoche in automatic text categorisation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80155180750&partnerID=40&md5=09c8a4765b5abfff624f214afe3b61d8"
    },
    {
      "abstract": "Since the beginning of mankind, there has been a need to display knowledge in such a way that it could be easily understood. In todays highly technological world we face an overwhelming amount of information from a variety of sources, namely: business applications such as customer comments and communications, trade publications, internal research reports and competitor web sites. Most of this knowledge is stored in textual format so in order to conceptually visualize this information, we need to extract the knowledge it contains and then try to comprehend it. However, one of the characteristics of textual information is its unstructured format which is not readily assimilated and understood. This paper presents a summary of current techniques used in information visualization. We present some of the main visual text analytics systems, highlighting their techniques, in addition to providing a brief overview of visual text analytics, semantic analysis of text and ontology that could be used to facilitate the process of cognitive visualization.  2011 IEEE.",
      "title": "15992 Overview of cognitive visualisation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80055031247&partnerID=40&md5=e79c7792dc8775ef45ea3bdbeae6f337"
    },
    {
      "abstract": "The increasing volume of information generated on microblogging sites such as Twitter raises several challenges to traditional text mining techniques. First, most texts from those sites are abbreviated due to the constraints of limited characters in one post",
      "title": "15993 Transfer latent semantic learning: Microblog mining with less supervision",
      "url": ""
    },
    {
      "abstract": "In this paper we consider the problem of clustering snippets returned from search engines. We propose a technique to invoke semantic similarity in the clustering process. Our technique improves on the well-known STC method, which is a highly efficient heuristic for clustering web search results. However, a weakness of STC is that it cannot cluster semantic similar documents. To solve this problem, we propose a new data structure to represent suffixes of a single string, called a Semantic Suffix Net (SSN). A generalized semantic suffix net is created to represent suffixes of a set of strings by using a new operator to partially combine nets. A key feature of this new operator is to find a joint point by using semantic similarity and string matching",
      "title": "15994 Applying Semantic Suffix Net to suffix tree clustering",
      "url": ""
    },
    {
      "abstract": "Text information processing depends critically on the proper representation of documents. Traditional models, like the vector space model, have significant limitations because they do not consider semantic relations amongst terms. Global Association Distance Model (GADM) is an alternative that includes this consideration for document representation, assuming basically that two documents should be closer if the shortest formal distances amongst terms in each document are similar. The association strength function used to model the semantic relations among terms, based on its formal distances is a critical feature of GADM. In this paper the association strength function is analyzed, a family of piecewise association strength functions is proposed and a Simulated Annealing algorithm is used to tune it and to obtain an optimal model of semantic relation. We evaluate this significance for topic classification task.  2011 IEEE.",
      "title": "15995 Tuning semantic association for modelling textual data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80055029219&partnerID=40&md5=b80a51f8e2659043eb2a23b141ee6092"
    },
    {
      "abstract": "We approached the problems of event detection, argument identification, and negation and speculation detection in the BioNLP09 information extraction challenge through concept recognition and analysis. Our methodology involved using the OpenDMAP semantic parser with manually written rules. The original OpenDMAP system was updated for this challenge with a broad ontology defined for the events of interest, new linguistic patterns for those events, and specialized coordination handling. We achieved state-of-the-art precision for two of the three tasks, scoring the highest of 24 teams at precision of 71.81 on Task 1 and the highest of 6 teams at precision of 70.97 on Task 2. We provide a detailed analysis of the training data and show that a number of trigger words were ambiguous as to event type, even when their arguments are constrained by semantic class. The data is also shown to have a number of missing annotations. Analysis of a sampling of the comparatively small number of false positives returned by our system shows that major causes of this type of error were failing to recognize second themes in two-theme events, failing to recognize events when they were the arguments to other events, failure to recognize nontheme arguments, and sentence segmentation errors. We show that specifically handling coordination had a small but important impact on the overall performance of the system. The OpenDMAP system and the rule set are available at.  2011 Wiley Periodicals, Inc.",
      "title": "15998 High-precision biological event extraction: Effects of system and of data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-82455189734&partnerID=40&md5=1e12ecc0771142c6983abf246d774dfc"
    },
    {
      "abstract": "Measuring the semantic similarity between words is an important component in various tasks on the web such as relation extraction, community mining, document clustering, and automatic metadata extraction. Despite the usefulness of semantic similarity measures in these applications, accurately measuring semantic similarity between two words (or entities) remains a challenging task. We propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search engine for two words. Specifically, we define various word co-occurrence measures using page counts and integrate those with lexical patterns extracted from text snippets. To identify the numerous semantic relations that exist between two given words, we propose a novel pattern extraction algorithm and a pattern clustering algorithm. The optimal combination of page counts-based co-occurrence measures and lexical pattern clusters is learned using support vector machines. The proposed method outperforms various baselines and previously proposed web-based semantic similarity measures on three benchmark data sets showing a high correlation with human ratings. Moreover, the proposed method significantly improves the accuracy in a community mining task.  2006 IEEE.",
      "title": "15999 A web search engine-based approach to measure semantic similarity between words",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054884899&partnerID=40&md5=cb68b88e2cfd8b12fae95544b9f2c849"
    },
    {
      "abstract": "This work proposes a comparative study of algorithms used for attribute selection in text clusterization in the scientific literature with the Cassiopeia algorithm. The aim of the Cassiopeia model is to allow for knowledge Discovery in textual bases in distinct and/or antagonistic domains using both Summarization and Clusterizations as part of the process of obtaining this knowledge. Hence, our intention is to achieve an improvement in the measurement of clusters as well as to solve the problem of high dimensionality in the knowledge discovery of textual bases.  2011 IEEE.",
      "title": "16000 The Cassiopeia Model: Using summarization and clusterization for semantic knowledge management",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054914578&partnerID=40&md5=37667511d6a369e56a374ba72b0b7cef"
    },
    {
      "abstract": "Aimed at the problem of document automatic classification, an improved KNN algorithm is proposed based on LSA reduced dimensionality. It advances the KNN algorithms efficiency and classifiers precision by using LSA to reduce dimensionality of text feature matrix. The experiment result shows that the improved KNN algorithm has good performance.  2011 IEEE.",
      "title": "16004 KNN text categorization algorithm based on LSA reduce dimensionality",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054723155&partnerID=40&md5=c825e2ef4e0d7e25349eff7cea3c67ae"
    },
    {
      "abstract": "There is a desire to extract and make better use of unstructured textual information available on the web. Semantic cognition opens new avenues in the utilization of this information. In this research, we extended the Hubel Wiesel model of hierarchical visual representation to extract semantic information from text. The unstructured text was preprocessed to a suitable input for Hubel Wiesel model. The threshold at each layer for neuronal growth was chosen as a ramp function of the level. Probabilistic approach was used for all post processing steps like prediction, word association, labeling, gist extraction etc. Equivalence with the Topics model was used to arrive at conditional probabilities in our model. We validated our model on three datasets and the model generated reasonable semantic associations. We evaluated the model based on top level clustering, label generation and word association.  2011 IEEE.",
      "title": "16005 Utilizing Hubel Wiesel models for semantic associations and topics extraction from unstructured text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054760679&partnerID=40&md5=cae785ec3fbd986953f07da5ea28a05a"
    },
    {
      "abstract": "In terms of the current information society, where news and information generally run at a remarkable speed, it is imperative that the events of capital importance for the world economy (and others fields) to be available and disseminated in real time. Thus, we refer to news that may influence in a large manner the stock quotes, forex, oil quotes and so on. In view of the fact that the Internet abounds in news, in our on-going research (under grant TE 316 Intelligent methods for decision fundamentation on stock market transactions, based on public information) we seek to propose a solution for extracting news vital information, its interpretation in real time, followed by suggesting possible decisional alternatives to the representatives involved in the stock exchange market. We put forward in the present paper our starting ideas for a conceptual model for a semantic Web news search, which encompasses multi-agent technologies, semantic Web, text search and mining, RSS news feeds.  2011 Springer-Verlag.",
      "title": "16006 Ontology importance towards enhancing suggestions in a news recommender system for a financial investor",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054060303&partnerID=40&md5=3e0d2e9244abcc598391440ae3b0e6b6"
    },
    {
      "abstract": "Text clustering plays an important role in many real-world applications, but it is faced with various challenges, such as, curse of dimensionality, complex semantics and large volume. A lot of researches paid attention to deal with such problems by designing new text representation models and clustering algorithms. However, text clustering still remains a research problem due to the complicated properties of text data. In this paper, a text clustering procedure is proposed based on the principle of granular computing with the aid of Wikipedia. The proposed clustering method firstly identifies the text granules, especially focusing on concepts and words with the aid of Wikipedia. And then, it mines the latent patterns based on the computation of such granules. Experimental results on benchmark data sets (20Newsgroups and Reuters-21578) have shown that the proposed method improves the performance of text clustering by comparing with the existing clustering algorithm together with the existing representation models.  2011 Springer-Verlag.",
      "title": "16007 Text clustering based on granular computing and Wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054070985&partnerID=40&md5=b28d5270c382507e41565fd3e9b38642"
    },
    {
      "abstract": "Dimensionality reduction can efficiently improve computing performance of classifiers in text categorization, and non-negative matrix factorization could map the high dimensional term space into a low dimensional semantic subspace easily. Meanwhile, the non-negative of the basis vectors could provide a meaningful explanation for the semantic subspace. However, it usually could not achieve a satisfied classification performance because it is sensitive to the noise, data missing and outlier as a linear reconstruction method. This paper proposes a novel approach in which the train text and its category information are fused and a transformation matrix that maps the term space into a semantic subspace is obtained by a basis orthogonality non-negative matrix factorization and truncation. Finally, the dimensionality can be reduced aggressively with these transformations. Experimental results show that the proposed approach remains a good classification performance in a very low dimensional case.  2011 Springer-Verlag.",
      "title": "16008 Dimensionality reduction with category information fusion and non-negative matrix factorization for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054080739&partnerID=40&md5=b6295f83c96287253282efa8c2ba4e39"
    },
    {
      "abstract": "A case study of semantic clustering of scientific articles related to Rough Sets is presented. The proposed method groups the documents on the basis of their content and with assistance of DBpedia knowledge base. The text corpus is first treated with Natural Language Processing tools in order to produce vector representations of the content and then matched against a collection of concepts retrieved from DBpedia. As a result, a new representation is constructed that better reflects the semantics of the texts. With this new representation, the documents are hierarchically clustered in order to form partition of papers that share semantic relatedness. The steps in textual data preparation, utilization of DBpedia and clustering are explained and illustrated with results of experiments performed on a corpus of scientific documents about rough sets.  2011 Springer-Verlag.",
      "title": "16009 Clustering of rough set related documents with use of knowledge from DBpedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054071399&partnerID=40&md5=1dabe2cacf6650dc6ea99472844ffeed"
    },
    {
      "abstract": "Text Classification is a daunting task because it is difficult to extract the semantics of natural language texts. Many problems must be resolved before natural-language processing techniques can be effectively applied to a large collection of texts. A significant one is to extract semantic information from corpus in plan text. In Vector Space Model, a document is conceptually represented by a vector of terms extracted from each document, with associated weights representing the importance of each term in the document and within the whole document collection. Likewise, an unclassified document is also modeled as a list of terms with associated weights representing the importance of the terms in it. Many techniques introduces much statistical information of terms to represent their semantic information. However, as always, document title is not taken into special consideration, while it obviously contains much semantic information. This paper proposes Title Vector to address this issue.  2011 IEEE.",
      "title": "16010 Improve VSM text classification by title vector based document representation method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054018687&partnerID=40&md5=0ab769fc91451d2ee9d43f102c3bb301"
    },
    {
      "abstract": "Background: Identification of transcription factors (TFs) responsible for modulation of differentially expressed genes is a key step in deducing gene regulatory pathways. Most current methods identify TFs by searching for presence of DNA binding motifs in the promoter regions of co-regulated genes. However, this strategy may not always be useful as presence of a motif does not necessarily imply a regulatory role. Conversely, motif presence may not be required for a TF to regulate a set of genes. Therefore, it is imperative to include functional (biochemical and molecular) associations, such as those found in the biomedical literature, into algorithms for identification of putative regulatory TFs that might be explicitly or implicitly linked to the genes under investigation.Results: In this study, we present a Latent Semantic Indexing (LSI) based text mining approach for identification and ranking of putative regulatory TFs from microarray derived differentially expressed genes (DEGs). Two LSI models were built using different term weighting schemes to devise pair-wise similarities between 21,027 mouse genes annotated in the Entrez Gene repository. Amongst these genes, 433 were designated TFs in the TRANSFAC database. The LSI derived TF-to-gene similarities were used to calculate TF literature enrichment p-values and rank the TFs for a given set of genes. We evaluated our approach using five different publicly available microarray datasets focusing on TFs Rel, Stat6, Ddit3, Stat5 and Nfic. In addition, for each of the datasets, we constructed gold standard TFs known to be functionally relevant to the study in question. Receiver Operating Characteristics (ROC) curves showed that the log-entropy LSI model outperformed the tf-normal LSI model and a benchmark co-occurrence based method for four out of five datasets, as well as motif searching approaches, in identifying putative TFs.Conclusions: Our results suggest that our LSI based text mining approach can complement existing approaches used in systems biology research to decipher gene regulatory networks by providing putative lists of ranked TFs that might be explicitly or implicitly associated with sets of DEGs derived from microarray experiments. In addition, unlike motif searching approaches, LSI based approaches can reveal TFs that may indirectly regulate genes.  2011 Roy et al",
      "title": "16011 Latent Semantic Indexing of PubMed abstracts for identification of transcription factor candidates from microarray derived gene sets",
      "url": ""
    },
    {
      "abstract": "Chinese language has been generally regarded as a Subject-Verb -Object (SVO) language and the basic semantic unit is the Chinese word that is usually consisted by two or more Chinese characters. However, word-centered structure of Chinese language has been controversial in linguistics. Some recent research in computational linguistics in Chinese language suggests that the character-based models perform better than the word-based models in some applications such word segmentation. In this paper, the word-based topic models and the character-based models are tested for modeling Chinese language, respectively. By empirical studies, we demonstrated the effectiveness of using Chinese characters as the basic semantic units. These two models have close performance in text classifications while the character-based model has a better quality in language modeling and a much smaller vocabulary. By testing on a bilingual corpus, three independent topic models based on Chinese words, Chinese characters and English words are trained and compared to each other. we verify the capability of topic models in modeling semantics by experiments across Chinese and English. The classification accuracy can also be boosted up by aggregating the classification results from the three independent topic models.  2011 Springer-Verlag.",
      "title": "16012 What is the basic semantic unit of chinese language? A computational approach based on topic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054023437&partnerID=40&md5=8b911314eddd249f750278fb676ed49a"
    },
    {
      "abstract": "A text mining model for topical evolutionary analysis was proposed through a text latent semantic analysis process on textual data. Analyzing topic evolution through tracking the topic different trends over time. Using the LDA model for the corpus and text to get the topics, and then using Clarity algorithm to measure the similarity of topics in order to identify topic mutation and discover the topic hidden in the text. Experiments show that the proposed model can discover meaningful topical evolution.  2011 IEEE.",
      "title": "16013 LDA-based model for topic evolution mining on text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054030170&partnerID=40&md5=7a71657ed7f669b925a95ef7d0b00e8f"
    },
    {
      "abstract": "In recent years, a number of machine learning approaches to literature-based gene function annotation have been proposed. However, due to issues such as lack of labeled data, class imbalance and computational cost, they have usually been unable to surpass simpler approaches based on string-matching. In this paper, we investigate the use of semantic kernels as a way to address the tasks inherent data scarcity and we propose a simple yet effective solution to deal with class imbalance. From experiments on the TREC Genomics Track data, our approach achieves better F 1-score than two state-of-the-art approaches based on string-matching and cross-species information.  2011 Springer-Verlag.",
      "title": "16014 Application of semantic kernels to literature-based gene function annotation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054001378&partnerID=40&md5=99baa9b49fa3d65816fe0f1c13db3656"
    },
    {
      "abstract": "Item descriptions on an online e-Commerce site such as eBay consist of item-specific information along with generic information such as shipping and return policies, requests for feedback, and contact information. Extracting these textual segments from the item descriptions is non-trivial as they contain html markups, advertisements, templates, and navigational elements. Since sellers have considerable editorial freedom in how to describe their items, many of the descriptions lack homogeneity and compactness. Very often, the relevant information has to be extracted from incomplete, ill-formed discourse units adding to the challenge of finding coherent segments. In this paper we describe an approach that identifies item-specific text segments from eBay descriptions. This approach uses a bootstrapping technique to learn high-quality semantic lexicons for item-agnostic text segments. We first extract useful text by removing html markups using a boiler-plate removal technique that preserves markup information and captures visual segmentation. Each segment is further processed to extract discourse units that play the same role as sentences. This is followed by a clustering technique that identifies thematic breaks to extract coherent segments. We evaluate our approach on a diverse set of descriptions and show that our approach outperforms a commonly-used approach that relies only on the title keywords. Copyright  2011 ACM.",
      "title": "16016 Segmenting eBay item descriptions into coherent sections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053643917&partnerID=40&md5=b7d1c18435389841957a2c79e244eae4"
    },
    {
      "abstract": "Background: Due to the rapidly expanding body of biomedical literature, biologists require increasingly sophisticated and efficient systems to help them to search for relevant information. Such systems should account for the multiple written variants used to represent biomedical concepts, and allow the user to search for specific pieces of knowledge (or events) involving these concepts, e.g., protein-protein interactions. Such functionality requires access to detailed information about words used in the biomedical literature. Existing databases and ontologies often have a specific focus and are oriented towards human use. Consequently, biological knowledge is dispersed amongst many resources, which often do not attempt to account for the large and frequently changing set of variants that appear in the literature. Additionally, such resources typically do not provide information about how terms relate to each other in texts to describe events. Results: This article provides an overview of the design, construction and evaluation of a large-scale lexical and conceptual resource for the biomedical domain, the BioLexicon. The resource can be exploited by text mining tools at several levels, e.g., part-of-speech tagging, recognition of biomedical entities, and the extraction of events in which they are involved. As such, the BioLexicon must account for real usage of words in biomedical texts. In particular, the BioLexicon gathers together different types of terms from several existing data resources into a single, unified repository, and augments them with new term variants automatically extracted from biomedical literature. Extraction of events is facilitated through the inclusion of biologically pertinent verbs (around which events are typically organized) together with information about typical patterns of grammatical and semantic behaviour, which are acquired from domain-specific texts. In order to foster interoperability, the BioLexicon is modelled using the Lexical Markup Framework, an ISO standard. Conclusions: The BioLexicon contains over 2.2 M lexical entries and over 1.8 M terminological variants, as well as over 3.3 M semantic relations, including over 2 M synonymy relations. Its exploitation can benefit both application developers and users. We demonstrate some such benefits by describing integration of the resource into a number of different tools, and evaluating improvements in performance that this can bring.  2011 Thompson et al",
      "title": "16019 The BioLexicon: A large-scale terminological resource for biomedical text mining",
      "url": "Trata-se da construÃ§Ã£o de um recurso lexico."
    },
    {
      "abstract": "The reasonable classification of components is the basis and key of component efficient retrieval. In order to overcome the shortcomings of faceted classification method widely used, we adopt a method combing faceted classification with full-text retrieval to describe components. Based on that description, a component cluster index tree is proposed which uses cluster analysis technique and semantic analysis technique. And the experiments prove that the index tree is feasible, which can effectively overcome the shortcomings of faceted classification method. Meanwhile to some extent, it can achieve the component semantic retrieval and has higher component recall ratio and precision ratio. Moreover, the description of retrieval conditions is no longer limited by restrictive terms so as to be convenient for general users.  2011 Springer-Verlag.",
      "title": "16020 A component clustering index tree based on semantic",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053607708&partnerID=40&md5=634af77cf287af12c49b491393638378"
    },
    {
      "abstract": "Reading comprehension is one of the main concerns for educational institutions, as it forges the students ability to comprehend and learn accurately a given information source (e.g. textbooks, articles, papers, etc.). However, there are few approaches that integrates digital sources of educational information with automated systems to detect whether an individual has comprehended a given reading task. This work main contribution is a text comprehension classification methodology for the detection of reading comprehension failures in educational institutions. The proposed approach relates situational model theories and latent semantic analysis from fields of psycholinguistics and natural language processing respectively. A numerical characterization of students documents using structural information, such as the usage of text connectors, and latent semantic features are used as input for traditional classification algorithms. Therefore, an automated classifier is built to determine whether a given student could or not comprehend the information in the given stimulus documents. For the evaluation of the proposed methodology, using a set of stimulus documents, a set of questions must be answered by an experimental group of students. We have performed experiments using first year students from Engineering and Linguistics undergraduate schools at the University of Chile with promising results.",
      "title": "16021 An automatic text comprehension classifier based on mental models and latent semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053579554&partnerID=40&md5=57f71203fde884dfa6c5920b84e3d015"
    },
    {
      "abstract": "This paper proposes an automated grading model (AGM) for English text. The AGM combines the Latent Semantic Analysis (LSA) with Text Clustering (TC) to measure the content quality of English text. In the AGM, LSA represents the meaning of words as vectors in semantic space using statistical analysis technique applied to large corpus, and TC is applied to classify the texts in the corpus into different categories. By comparing the similarity of a text with reference texts which are belong to the same cluster on basis of semantic content, our model can grade the text in the same cluster. In addition, the AGM can judge the to-be-graded text whether related to the given subject. Our experiments show that the AGM is competent in grading the content of texts.  2011 Springer-Verlag.",
      "title": "16022 An automated grading model integrated LSA and text clustering together for English text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053583351&partnerID=40&md5=3eef9dd7e8a0596e90986e202b352ccc"
    },
    {
      "abstract": "Background: Biomedical papers contain rich information about entities, facts and events of biological relevance. To discover these automatically, we use text mining techniques, which rely on annotated corpora for training. In order to extract protein-protein interactions, genotype-phenotype/gene-disease associations, etc., we rely on event corpora that are annotated with classified, structured representations of important facts and findings contained within text. These provide an important resource for the training of domain-specific information extraction (IE) systems, to facilitate semantic-based searching of documents. Correct interpretation of these events is not possible without additional information, e.g., does an event describe a fact, a hypothesis, an experimental result or an analysis of results? How confident is the author about the validity of her analyses? These and other types of information, which we collectively term meta-knowledge, can be derived from the context of the event. Results: We have designed an annotation scheme for meta-knowledge enrichment of biomedical event corpora. The scheme is multi-dimensional, in that each event is annotated for 5 different aspects of meta-knowledge that can be derived from the textual context of the event. Textual clues used to determine the values are also annotated. The scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed in the text. We report here on both the main features of the annotation scheme, as well as its application to the GENIA event corpus (1000 abstracts with 36,858 events). High levels of inter-annotator agreement have been achieved, falling in the range of 0.84-0.93 Kappa. Conclusion: By augmenting event annotations with meta-knowledge, more sophisticated IE systems can be trained, which allow interpretative information to be specified as part of the search criteria. This can assist in a number of important tasks, e.g., finding new experimental knowledge to facilitate database curation, enabling textual inference to detect entailments and contradictions, etc. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event.  2011 Thompson et al",
      "title": "16023 Enriching a biomedical event corpus with meta-knowledge annotation",
      "url": ""
    },
    {
      "abstract": "The unique characteristic of short text makes short text classification quite different from traditional long text processing. The feature space of short text is so sparse, which makes it notoriously difficult to extract sufficient and effective features. In this paper, aiming to classify the short text on web forum accurately, a novel short-text-processing method based on semantic extension is introduced to enhance the content of the original short text, which effectively solves the problem of feature sparse. In addition, we put forward the concept of Key-Pattern (KP) and propose a new text feature representation approach based on KP, which extracts phrase with powerful semantic information as the text features. Traditional classifier model are applied to estimate the texts classification, experimental results show that the proposed method is effective to improve the accuracy and recall of short text classification.  2011 IEEE.",
      "title": "16024 Research on short text classification for web forum",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053428354&partnerID=40&md5=60318709e622d5144b9e7b6fefc453f4"
    },
    {
      "abstract": "Text clustering has been recognized as an important component in data mining. Self-Organizing Map (SOM) based models have been found to have certain advantages for clustering sizeable text data. However, current existing approaches lack in providing an adaptive hierarchical structure within in a single model. This paper presents a new method of hierarchical text clustering based on combination of latent semantic analysis (LSA) and hierarchical GSOM, which is called LSA-HGSOM method. The text clustering result using traditional methods can not show hierarchical structure. However, the hierarchical structure is very important in text clustering. The LSA-HGSOM method can automatically achieve hierarchical text clustering, and establishes vector space model (VSM) of term weight by using the theory of LSA, then semantic relation is included in the vector space model. Both theory analysis and experimental results confirm that LSA-HGSOM method decreases the number of vector, and enhances the efficiency and precision of text clustering.  2011 Springer-Verlag.",
      "title": "16025 Text clustering based on LSA-HGSOM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053404461&partnerID=40&md5=0294b7b2b555e82169eba0a95b37283b"
    },
    {
      "abstract": "The viability of cloud computing for information-intensive tasks arising in real-time opinion mining and sentiment analysis of large online text streams is described. We show how a smart distributed architecture enables an efficient and scalable design for opinion mining on internet-based content that answers key challenges, such as integrating heterogeneous data sources and adapting to events through dynamic system configuration. In particular, we present a novel approach of semantic complex event processing in a cloud environment capturing different levels of information, such as event data (e.g. content from various heterogeneous, distributed sources) as well as associations identified during the opinion mining and sentiment analysis process (e.g. dynamic co-reference resolution).  2011 IEEE.",
      "title": "16026 Cloud-based event-processing architecture for opinion mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053392570&partnerID=40&md5=833b907a78564227e3e03dd6208adc2c"
    },
    {
      "abstract": "This paper introduces three classic models of statistical topic models: Latent Semantic Indexing (LSI), Probabilistic Latent Semantic Indexing (PLSI) and Latent Dirichlet Allocation (LDA). Then a method of text classification based on LDA model is briefly described, which uses LDA model as a text representation method. Each document means a probability distribution of fixed latent topic sets. Next, Support Vector Machine (SVM) is chose as classification algorithm. Finally, the evaluation parameters in classification system of LDA with SVM are higher than other two methods which are LSI with SVM and VSM with SVM, showing a better classification performance.  2011 IEEE.",
      "title": "16027 Performance evaluation of Latent Dirichlet Allocation in text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053434228&partnerID=40&md5=3946f46649d7eeb3a47dd310d7993c65"
    },
    {
      "abstract": "This article firstly discusses the situation of Chinese vocabulary in semantic space (categorical space), then research short message text word by word, by which we can treat short message text as a vocabulary. Next, it discusses the meaning similarity of vocabulary and the selection of clustering features, and presents a method on short message text clustering.  2011 IEEE.",
      "title": "16028 Research on clustering based on short message text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053229212&partnerID=40&md5=2f7550cf1ea5c96ed4b9122a3f53ecaa"
    },
    {
      "abstract": "Polarity classification of opinionated sentences with both positive and negative senti-ments 1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cue-phrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.  2011 Association for Computational Linguistics.",
      "title": "16029 Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053239308&partnerID=40&md5=e691634834d93db02ddf8d1ca7543421"
    },
    {
      "abstract": "Medical Subject Headings (MeSH) are used to index the majority of databases generated by the National Library of Medicine. Essentially, MeSH terms are designed to make information, such as scientific articles, more retrievable and assessable to users of systems such as PubMed. This paper proposes a novel method for automating the assignment of biomedical publications with MeSH terms that takes advantage of citation references to these publications. Our findings show that analysing the citation references that point to a document can provide a useful source of terms that are not present in the document. The use of these citation contexts, as they are known, can thus help to provide a richer document feature representation, which in turn can help improve text mining and information retrieval applications, in our case MeSH term classification. In this paper, we also explore new methods of selecting and utilising citation contexts. In particular, we assess the effect of weighting the importance of citation terms (found in the citation contexts) according to two aspects: (i) the section of the paper they appear in and (ii) their distance to the citation marker.We conduct intrinsic and extrinsic evaluations of citation term quality. For the intrinsic evaluation, we rely on the UMLS Metathesaurus conceptual database to explore the semantic characteristics of the mined citation terms. We also analyse the  informativeness of these terms using a class-entropy measure. For the extrinsic evaluation, we run a series of automatic document classification experiments over MeSH terms. Our experimental evaluation shows that citation contexts contain terms that are related to the original document, and that the integration of this knowledge results in better classification performance compared to two state-of-the-art MeSH classification systems: MeSHUP and MTI. Our experiments also demonstrate that the consideration of Section and Distance factors can lead to statistically significant improvements in citation feature quality, thus opening the way for better document feature representation in other biomedical text processing applications.  2011 Elsevier Inc.",
      "title": "16030 Improving MeSH classification of biomedical articles using citation contexts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052871937&partnerID=40&md5=5bdbc27ce1933369e7776793c37c31b0"
    },
    {
      "abstract": "Semantic analysis of corpora containing heavy usage of jargon words and phrases introduces problems not commonly addressed by Natural Language Processing methods. Modern semantic analysis relies on data from unedited websites or other expertly written sources, which lack similar usage of jargon words and phrases. This paper presents a system of semi-supervised lexicon learning algorithms that collate several manually labeled and clustered data sources, such as thesauri. In addition, this paper demonstrates an improvement in performance of these subjectivity classifiers by applying a boosting method. This paper presents a method of automatic Aviation Safety Reporting System (ASRS) shaping factor classification based on the most relevant words from a subjectivity lexicon.  2011 IEEE.",
      "title": "16037 Subjectivity classification and analysis of the ASRS corpus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053155452&partnerID=40&md5=ca28dcd7c48c0094b61b56dffde39b8c"
    },
    {
      "abstract": "We report on advances in deep linguistic parsing of the full textual content of 8200 papers from the ACL Anthology, a collection of electronically available scientific papers in the fields of Computational Linguistics and Language Technology. We describe how - by incorporating new techniques - we increase both speed and robustness of deep analysis, specifically on long sentences where deep parsing often failed in former approaches. With the current open source HPSG (Head-driven phrase structure grammar) for English (ERG), we obtain deep parses for more than 85% of the sentences in the 1.5 million sentences corpus, while the former approaches achieved only approx. 65% coverage. The resulting sentence-wise semantic representations are used in the Scientists Workbench, a platform demonstrating the use and benefit of natural language processing (NLP) to support scientists or other knowledge workers in fast and better access to digital document content. With the generated NLP annotations, we are able to implement important, novel applications such as robust semantic search, citation classification, and (in the future) question answering and definition exploration.  2011 Springer-Verlag.",
      "title": "16042 Advances in deep parsing of scholarly paper content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053041122&partnerID=40&md5=2d478f9da174d02dee7580f2d27007af"
    },
    {
      "abstract": "Topic Models like Latent Dirichlet Allocation have been widely used for their robustness in estimating text models through mixtures of latent topics. Although LDA has been mostly used as a strictly lexicalized approach, it can be effectively applicable to a much richer set of linguistic structures. A novel application of LDA is here presented that acquires suitable grammatical generalizations for semantic tasks tightly dependent on NL syntax. We show how the resulting topics represent suitable generalizations over syntactic structures and lexical information as well. The evaluation on two different classification tasks, such as predicate recognition and question classification, shows that state of the art results are obtained.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16043 Latent topic models of surface syntactic information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053021414&partnerID=40&md5=b4c088d00a6f8698e1a72e70f6b90c2b"
    },
    {
      "abstract": "Facing the enormous text on the Internet, automatic topic discovery out of large text corpus becomes an important task for advanced intelligence information analysis, such as opinion recognition, Web user interest analysis, etc. Although many topic mining methods have shown great success in dealing with topic-based analysis tasks, it is desired to discover meaningful topic descriptions for informatics analysis. To avoid words with different granularity to explain a topic, a mechanism for separating text corpus into two subsets with equal semantic topics is proposed. EM algorithm is employed to infer topics models for the subsets. Then a merging process is devised to generate topic descriptions based on the output of EM. Experiments on standard AP text corpus shows that the proposed topic discovery method can achieve better perplexity, which means better ability in predicting topics. Furthermore, a test of topics extraction on a collection of news documents about recent Expo 2010 Shanghai China shows that the description key words in topics are more meaningful and reasonable than that of tradition topic mining method.  2011 IEEE.",
      "title": "16046 Topic discovery based on dual em merging",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052879437&partnerID=40&md5=052e4ff48313a560c3fad98e5dea211d"
    },
    {
      "abstract": "We describe in this paper, a system that groups, classifies and finds the latent semantic features in a database composed of a large number of documents. The database will be constantly growing as users who co-create it will be adding more and more new documents. Users require a system to provide them information, both about a specific document, and about the entire set of documents. This information includes statistical data about words in documents, information about aspects in which this words appears, classification, clustering, etc. To meet these expectations we propose using methods for searching for hidden patterns in multivariable data. We apply machine learning algorithms for data analysis, useful in identifying local patterns in multivariate data. We consider two different algorithms described in the literature (1) Probabilistic Latent Semantic Analysis Method [2] and (2) Nonnegative Matrix Factorization algorithm described in [4] and used in the text analysis system [1].  2011 Springer-Verlag.",
      "title": "16048 Efficient system for clustering of dynamic document database",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052815131&partnerID=40&md5=3be6f6ded26c1446f0bd52309935f555"
    },
    {
      "abstract": "We developed a learning-based question classifier for question answering systems. A question classifier tries to predict the entity type of the possible answers to a given question written in natural language. We extracted several lexical, syntactic and semantic features and examined their usefulness for question classification. Furthermore we developed a weighting approach to combine features based on their importance. Our result on the well-known trec questions dataset is competitive with the state-of-the-art on this task.  2011 Springer-Verlag.",
      "title": "16050 Question classification by weighted combination of lexical, syntactic and semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052752198&partnerID=40&md5=499963de50c62158a946b3167a11463a"
    },
    {
      "abstract": "Text clustering can effectively improve search results and user experience of information retrieval system. Traditional text clustering approaches are based on vector space model, in which a document is represented as a vector using term frequency based weighting scheme. The main disadvantage of this model is that it cannot fully exploit semantic correlations between social annotations and document contents because term frequency based weighting scheme only captures the number of occurrences of terms in the document. However, social annotation of web pages implicates fundamental and valuable semantic information thus can be fully utilized to improve information retrieval system. In this paper, we investigate and evaluate several extended vector space models which can combine social annotation and web page text. In particular, we propose a novel vector space model by computing the semantic correlations between social annotations and web page words. Comparing with other vector space models, our experiments show that using semantic correlations between social tags and web page words improves the clustering accuracy with RI score increase of 4% ~ 7%.  2011 Springer-Verlag.",
      "title": "16052 A new vector space model exploiting semantic correlations of social annotations for web page clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052778565&partnerID=40&md5=55760e3616fb6cba551f3577ca83c229"
    },
    {
      "abstract": "As a well-known semantic repository, WordNet is widely used in many applications. However, due to costly edit and maintenance, WordNets capability of keeping up with the emergence of new concepts is poor compared with on-line encyclopedias such as Wikipedia. To keep WordNet current with folk wisdom, we propose a method to enhance WordNet automatically by merging Wikipedia entities into WordNet, and construct an enriched ontology, named as WorkiNet. WorkiNet keeps the desirable structure of WordNet. At the same time, it captures abundant information from Wikipedia. We also propose a learning approach which is able to generate a tailor-made semantic concept collection for a given document collection. The learning process takes the characteristics of the given document collection into consideration and the semantic concepts in the tailor-made collection can be used as new features for document representation. The experimental results show that the adaptively generated feature space can outperform a static one significantly in text mining tasks, and WorkiNet dominates WordNet most of the time due to its high coverage. Copyright 2011 ACM.",
      "title": "16054 Ontology enhancement and concept granularity learning: Keeping yourself current and adaptive",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052667871&partnerID=40&md5=60b4a0f36876ebdd9ba12025ea02166a"
    },
    {
      "abstract": "With the development of Web applications, textual documents are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about text-rich heterogeneous information networks. Topic models have been proposed and shown to be useful for document analysis, and the interactions among multi-typed objects play a key role at disclosing the rich semantics of the network. However, most of topic models only consider the textual information while ignore the network structures or can merely integrate with homogeneous networks. None of them can handle heterogeneous information network well. In this paper, we propose a novel topic model with biased propagation (TMBP) algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way. The underlying intuition is that multi-typed objects should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network. A simple and unbiased topic propagation across such a heterogeneous network does not make much sense. Consequently, we investigate and develop two biased propagation frameworks, the biased random walk framework and the biased regularization framework, for the TMBP algorithm from different perspectives, which can discover latent topics and identify clusters of multi-typed objects simultaneously. We extensively evaluate the proposed approach and compare to the state-of-the-art techniques on several datasets. Experimental results demonstrate that the improvement in our proposed approach is consistent and promising. Copyright 2011 ACM.",
      "title": "16055 Probabilistic topic models with biased propagation on heterogeneous information networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052674988&partnerID=40&md5=9fca54b69e7a8e0e854671e7d195a9a7"
    },
    {
      "abstract": "The reach of ubiquitous social networking tools enabled through new technology is evident in the recent regime change and the increased unrest within the Middle East. Such rapid dissemination of information may engender an equally rapid emergence of virtual communities of interest - some with potentially hostile intent. Twitters accessibility and popularity have attracted a large number of automated programs, known as bots, which can serve as a double-edged sword within Twitter. While legitimate bots generate a large amount of benign tweets delivering news and updating information, malicious bots spread misinformation with potentially virulent results. This paper blends methods from traditional statistical classification methods with latent Dirichlet allocation with techniques of social network analysis to underpin threat assessment based upon similarities discerned among topic sets that have distilled by semantic analysis of Twitter intercepts.  2011 EWI.",
      "title": "16056 Hybrid Elicitation of Latent Intent in Open Societies (HELIOS)",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052613892&partnerID=40&md5=f91cfba5e6fd7cb808c0ebb45ea520a8"
    },
    {
      "abstract": "In this paper, we present a formalization of an Index Assignment process that was used against documents stored in a text database. The process uses key phrases or terms from a hierarchical thesaurus or ontology and is based on the new notion of entropy on ontology for terms and their weights that is an extension of the Shannon concept of entropy in Information Theory and the Resnik semantic similarity measure for terms on ontology. Introduced notion provides a measure of closeness or semantic similarity for a set of terms in ontology and their weights and allows creation of a clustering algorithm that constructively resolves index assignment task. The algorithm was tested on 30,000 documents randomly extracted from MEDLINE biomedicine database that are manually indexed by professional indexers. The main output from experiments shows that after all 30,000 documents were processed in seven topics out of ten the presented algorithm and human indexers have the same understanding of documents.",
      "title": "16057 Entropy on ontology and indexing in information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052589285&partnerID=40&md5=92b4a0355af55df4ea649d335b34d639"
    },
    {
      "abstract": "The representation of document content is very important factors in retrieval process. The failure to create a good knowledge representation will definitely lead to failure in terms of its retrieval no matter how good the retrieval engine is. Therefore, this research focused on creating a reliable knowledge representation for our retrieval engine. We are using skolem to capture the information conveyed by multiple text documents and used skolem as an index language. This research also focuses on utilizing the skolem index as its knowledge representation in its question answering system. The system is capable of retrieving the answer as well as states the exact document in which the answer is derived from.  2011 IEEE.",
      "title": "16059 Retrieving answers from multiple documents using semantic skolem indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052603226&partnerID=40&md5=a4379aec38ea45606d236e642505ddc0"
    },
    {
      "abstract": "When Support Vector Machines (SVM) are exploited for automatic text categorization, text representation and term weighting have a significant impact on the performance of text classification. Conventional supervised weighting methods only focus on the frequency characteristics of feature terms, without consideration of semantic characteristics of them. Inspired by supervised weighting method, semantic distance between terms and categories is introduced into term weights calculation. The first step is modeling each category with two vectors of feature terms, which are called category core terms, and acquiring these terms by machine learning methods. Second, the semantic distance between feature terms and category core terms is calculated based on semantic database. Third, the global weight factor is replaced by the sematic distance to calculate the weight of every term. Based on the standard benchmark Reuters-21578, this kind of term weighting schemas can generally produce satisfied results of classification using SVMlight as classifier with default parameters.  2011 Springer Science+Business Media B.V.",
      "title": "16065 An extended supervised term weighting method for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052139243&partnerID=40&md5=d22161a88ffb414b98f84a09ccd5fd9d"
    },
    {
      "abstract": "In the research of text emotional classification, although the method based on using characteristic words list and affective words list to identify the text emotional tendencies is simple and quick, but the accuracy is often poor. Thats more the method cannot judgment the rhetorical relations. To solve this problem, this paper from the semantic comprehension point of view, using the dependency parsing method to process the sentiment phrases, feature phrase and negative sentence, and makes the text emotional classification possible. The experimental results show that this method can meet the practical need.  2011 IEEE.",
      "title": "16066 Research on recognition of the emotional tendencies in web data mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052154514&partnerID=40&md5=308083ebb6d198e3327105d8ddbf82c9"
    },
    {
      "abstract": "Automatic Document Summarization is a highly interdisciplinary research area related with computer science as well as cognitive psychology. This Summarization is to compress an original document into a summarized version by extracting almost all of the essential concepts with text mining techniques. This research focuses on developing a statistical automatic text summarization approach, K-mixture probabilistic model, to enhancing the quality of summaries. KSRS employs the K-mixture probabilistic model to establish term weights in a statistical sense, and further identifies the term relationships to derive the semantic relationship significance (SRS) of nouns. Sentences are ranked and extracted based on their semantic relationship significance values. The objective of this research is thus to propose a statistical approach to text summarization. We propose a K-mixture semantic relationship significance (KSRS) approach to enhancing the quality of document summary results. The K-mixture probabilistic model is used to determine the term weights. Term relationships are then investigated to develop the semantic relationship of nouns that manifests sentence semantics. Sentences with significant semantic relationship, nouns are extracted to form the summary accordingly.  2011 IEEE.",
      "title": "16068 A statistical approach for automatic text summarization by extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052133202&partnerID=40&md5=c19ad89b96efb1a14d2aaab018083e21"
    },
    {
      "abstract": "Documents clustering become an essential technology with the popularity of the Internet. That also means that fast and high-quality document clustering technique play core topics. Text clustering or shortly clustering is about discovering semantically related groups in an unstructured collection of documents. Clustering has been very popular for a long time because it provides unique ways of digesting and generalizing large amounts of information. One of the issues of clustering is to extract proper feature (concept) of a problem domain. The existing clustering technology mainly focuses on term weight calculation. To achieve more accurate document clustering, more informative features including concept weight are important. Feature Selection is important for clustering process because some of the irrelevant or redundant feature may misguide the clustering results. To counteract this issue, the proposed system presents the concept weight for text clustering system developed based on a k-means algorithm in accordance with the principles of ontology so that the important of words of a cluster can be identified by the weight values. To a certain extent, it has resolved the semantic problem in specific areas.",
      "title": "16069 Ontology-based concept weighting for text documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053448057&partnerID=40&md5=04b03f0bcd4cc6845010650ec0de5a42"
    },
    {
      "abstract": "Recently, more and more short texts (e.g., ads, tweets) appear on the Web. Classifying short texts into a large taxonomy like ODP or Wikipedia category system has become an important mining task to improve the performance of many applications such as contextual advertising and topic detection for micro-blogging. In this paper, we propose a novel multi-stage classification approach to solve the problem. First, explicit semantic analysis is used to add more features for both short texts and categories. Second, we leverage information retrieval technologies to fetch the most relevant categories for an input short text from thousands of candidates. Finally, a SVM classifier is applied on only a few selected categories to return the final answer. Our experimental results show that the proposed method achieved significant improvements on classification accuracy compared with several existing state of art approaches.",
      "title": "16070 Towards effective short text deep classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052123411&partnerID=40&md5=6810f58c1631806738d6e3b4d79eb752"
    },
    {
      "abstract": "Literature-related discovery (LRD) is the linking of two or more previously disjoint concepts in order to produce novel, interesting, plausible, and intelligible connections (i.e., potential discovery). LRD has been used to identify potential treatments or preventative actions for challenging medical problems, among myriad other applications. Severe acute respiratory syndrome (SARS) was the first pandemic of the 21st century. SARS was eventually controlled through increased hygienic measures (e.g., face mask protection, frequent hand washing, living quarter disinfection), travel restrictions, and quarantine. According to recent reviews of SARS, none of the drugs that were used during the pandemic worked. For the present paper, SARS was selected as the first application of LRD to an infectious disease. The main goal of this research was to identify non-drug non-surgical treatments that would 1) prevent the occurrence, or 2) reduce the progression rate, or 3) stop/reverse the progression of SARS. The MeSH taxonomy of Medline was used to restrict potential discoveries to selected semantic classes, and to identify potential discoveries efficiently. To enhance the volume of potential discovery, databases were used in addition to Medline. These included the Science Citation Index (SCI) and, in contrast to previous work, a full text database. Because of the richness of the full text, surgical queries were developed that targeted the exact types of potential discovery of interest while eliminating clutter more efficiently.  2011 Elsevier Inc.",
      "title": "16071 Literature-related discovery: Potential treatments and preventatives for SARS",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051579728&partnerID=40&md5=4be4a9204b6a8e7423a7c26afba72a21"
    },
    {
      "abstract": "Retrieving similar documents from a large-scale text corpus according to a given document is a fundamental technique for many applications. However, most of existing indexing techniques have difficulties to address this problem due to special properties of a document query, e.g. high dimensionality, sparse representation and semantic issue. Towards addressing this problem, we propose a two-level retrieval solution based on a document decomposition idea. A document is decomposed to a compact vector and a few document specific keywords by a dimension reduction approach. The compact vector embodies the major semantics of a document, and the document specific keywords complement the discriminative power lost in dimension reduction process. We adopt locality sensitive hashing (LSH) to index the compact vectors, which guarantees to quickly find a set of related documents according to the vector of a query document. Then we re-rank documents in this set by their document specific keywords. In experiments, we obtained promising results on various datasets in terms of both accuracy and performance. We demonstrated that this solution is able to index large-scale corpus for efficient similarity-based document retrieval.",
      "title": "16072 Query by document via a decomposition-based two-level retrieval approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052122520&partnerID=40&md5=f2756985e73b4d26bd98891f6c556fc8"
    },
    {
      "abstract": "Relation extraction is the task of finding semantic relations between two entities in text, and is often cast as a classification problem. In contrast to the significant achievements on English language, research progress in Chinese relation extraction is relatively limited. In this article, we present a novel Chinese relation extraction framework, which is mainly based on a 9-position structure. The design of this proposed structure is motivated by the fact that there are some obvious connections between relation types/subtypes and position structures of two entities. The 9-position structure can be captured with less effort than applying deep natural language processing, and is effective to relieve the class imbalance problem which often hurts the classification performance. In our framework, all involved features do not require Chinese word segmentation, which has long been limiting the performance of Chinese language processing. We also utilize some correction and inference mechanisms to further improve the classified results. Experiments on the ACE 2005 Chinese data set show that the 9-position structure feature can provide strong support for Chinese relation extraction. As well as this, other strategies are also effective to further improve the performance.  2011 ACM.",
      "title": "16073 Developing position structure-based framework for chinese entity relation extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053183514&partnerID=40&md5=f1ed99eeb5a1e55a0067479bd52b9f2c"
    },
    {
      "abstract": "Natural language text describes the nature of peoples internal representation of space. It is investigated that 80% of unstructured text has location expressions e.g. place names and spatial relations. In the past few years, text has become a most important geospatial data resource as well as survey, map, satellite images and GPS. The most previous research focused on the recognition of place names in text and its integration with map services. Spatial relations play an important role in the fields of spatial data modelling, spatial query, spatial analysis, spatial reasoning and map generalization. Spatial relations in text are described in natural language with qualitative spatial expressions including place names, spatial terms, prepositions, verbs and so on. And these expressions are combined with certain syntactic patterns to represent their semantic functions. An instance of spatial relation in text can be simply formalized as (C1, P1, C2, P2, C3), where P1 and P2 are place names, and C1, C2 and C3 are the context. Support Vector Machine (SVM) is a pattern recognition method popularly used in information extraction from text. This paper investigates the extraction of spatial relations based on SVM model which can implement the recognition of spatial expressions and their classification synchronously. For the SVM model, a set of feature vectors are specified, such as lexical tokens, spatial terms, syntactic structures and geographical feature types of place names, and a multi-label classifier is presented to solve the multi-classification problem. Finally, an experimental evaluation is explored in a Chinese annotation corpus. This study proves that spatial terms are important indicators for identification of spatial relations in text. However, there is serious ambiguity of their classification. Therefore, integration of much more context information could potentially improve the performance of extraction of spatial relations in text.  2011 IEEE.",
      "title": "16074 SVM based extraction of spatial relations in text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052118071&partnerID=40&md5=81b38fcb63f2b4ab01d1f0b08ae23e3e"
    },
    {
      "abstract": "E-Discovery Review is a type of legal service that aims at finding relevant electronically stored information (ESI) in a legal case. This requires manual reviewing of large number of documents by legal analysts, thus involving huge costs. in this paper, we investigate the use of IT, specifically text mining techniques, for improving the efficiency and quality of the e-discovery review service. We employ near duplicate detection and automatic classification techniques that can be used to create coherent groups of documents. Since a group characterizes a syntactic or a semantic theme all the documents in a group can be reviewed together. This leads to a faster and more consistent review of documents. Our experimental results on the publicly available Enron email corpus show that we can achieve high precision and recall in identifying the syntactic and semantic groups. We also conduct a user study that demonstrates 80% reduction in the review time and improved consistency in the review results, leading to better service quality.  2011 IEEE.",
      "title": "16075 Improving the efficiency of legal e-discovery services using text mining techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051939118&partnerID=40&md5=adc4e5b84ba664b813ebf5bd03fed784"
    },
    {
      "abstract": "Finding relevant scientific documents from a huge set of academic papers is a challenging task and with the tremendous growth in electronic publication, locating the most relevant and related scientific documents when going through a new research paper is becoming even more challenging. In this paper, we present a new way of indexing and searching the scientific documents to assist researchers in finding relevant documents when coming across a new research document. In particular, we explored how DT-Tree (DocumentTerm-Tree) - a new structure for the representation of scientific documents - can be used to create an index of scientific documents. We used MVP-Tree to create index using DT-Tree representation of the documents. We then performed search experiments, using new scientific documents as queries, to show that relevant documents are retrieved when DT-Tree structures are used to create MVP-Tree.  2011 IEEE.",
      "title": "16076 Using semantic and structural similarities for indexing and searching scientific papers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051892977&partnerID=40&md5=5e637836bd5b08c8eb8ba34ca8cfdb10"
    },
    {
      "abstract": "This paper presents a cognitive-based emotion classifier of Chinese vocabulary, which inherits the advantages of traditional statistical linguistics model. The concept of cognitive prototype theory in cognitive linguistics was applied for the filter of text characteristics, while HowNet, which can provide the interface of the calculation of semantic similarity, and The Corpus Annotation of Harbin Institute of Technology(HIT) were also used in this classifier, accordingly this paper build a new classification model. It is used in binary classification of word emotion and the experimental results show that the accuracy of the classifier for the word emotion was significantly higher than the traditional classifier based solely on statistics, and on the other hand it greatly reduced the computational cost. Although the recall rate was slightly lower, it can be solved with improvement suggestion in the end of this paper.  2010 IEEE.",
      "title": "16078 Cognitive-based emotion classifier of Chinese vocabulary design",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79961191699&partnerID=40&md5=65c0a4e5444f3a8bdba2d2e8e696bd82"
    },
    {
      "abstract": "This paper proposes a model of text categorization named Alida, which combines a model of categorization inspired of the classical cognitive models of categorization of Nosofsky, with a semantic space model as system of semantic knowledge representation. The model addresses large-scale text categorization applications in opinion mining in different domains and different languages. The performance in the text-mining campaign DEFT09 shows that the model can compete with existing Natural Language Processing and Information Retrieval models.  2011 IEEE.",
      "title": "16080 Alida, a cognitive approach of text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79961157733&partnerID=40&md5=06a964eef759c0cf895cce7418346f93"
    },
    {
      "abstract": "Increasingly, user-generated product reviews serve as a valuable source of information for customers making product choices online. The existing literature typically incorporates the impact of product reviews on sales based on numeric variables representing the valence and volume of reviews. In this paper, we posit that the information embedded in product reviews cannot be captured by a single scalar value. Rather, we argue that product reviews are multifaceted, and hence the textual content of product reviews is an important determinant of consumers choices, over and above the valence and volume of reviews. To demonstrate this, we use text mining to incorporate review text in a consumer choice model by decomposing textual reviews into segments describing different product features. We estimate our model based on a unique data set from Amazon containing sales data and consumer review data for two different groups of products (digital cameras and camcorders) over a 15-month period. We alleviate the problems of data sparsity and of omitted variables by providing two experimental techniques: clustering rare textual opinions based on pointwise mutual information and using externally imposed review semantics. This paper demonstrates how textual data can be used to learn consumers relative preferences for different product features and also how text can be used for predictive modeling of future changes in sales.  2011 INFORMS.",
      "title": "16083 Deriving the pricing power of product features by mining consumer reviews",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051648956&partnerID=40&md5=d5b43c39e60a702f86006b261e2c3f2b"
    },
    {
      "abstract": "In order to improve the semantic description of items in SVM, and overcome the defect that semantic units are independent of each other, this paper proposed a feature granulation description method based on phrases. This method refered to text representation and organization among feature items, identified base phrases through syntactic cues, and built the relation tree which contained feature items and head verb, then replaced words in BOW with base phrases. Experimental results indicates that the new approach improves the performance of the classifier, increases the relationship between terms, overcomes the defect of mutual independence between feature items, and keeps favourable effect even if the number of feature items is small.",
      "title": "16085 Text representation combining syntax in vector space model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052571017&partnerID=40&md5=3992910fffbc3ed19e0d23ed6648ecdc"
    },
    {
      "abstract": "The state-of-the-art text clustering methods suffer from the huge size of documents with high-dimensional features. In this paper, we studied fast SOM clustering technology for Text Information. Our focus is on how to enhance the efficiency of text clustering system whereas high clustering qualities are also kept. To achieve this goal, we separate the system into two stages: offline and online. In order to make text clustering system more efficient, feature extraction and semantic quantization are done offline. Although neurons are represented as numerical vectors in high-dimension space, documents are represented as collections of some important keywords, which is different from many related works, thus the requirement for both time and space in the offline stage can be alleviated. Based on this scenario, fast clustering techniques for online stage are proposed including how to project documents onto output layers in SOM, fast similarity computation method and the scheme of Incremental clustering technology for real-time processing, We tested the system using different datasets, the practical performance demonstrate that our approach has been shown to be much superior in clustering efficiency whereas the clustering quality are comparable to traditional methods.  2011 Published by Elsevier Ltd.",
      "title": "16086 Research of fast SOM clustering for text information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79953721150&partnerID=40&md5=9cc448925ee89a041da4f070a5a3bc79"
    },
    {
      "abstract": "Semantic loss between feature words is identified as the main reason for the low quality of text summary, however, the root reason for this phenomenon is the low segmentation accuracy of lexical analysis system and the orthogonal assumptions existed in text representation model. To solve this issue, we proposed an integrated with semantic method that is able to generate high quality text summary in this paper. This method formulates the amendment rules which are used to the sentences formed after partition process of the words to regulate the vocabulary combination, according to the part of speech, making the words in the same sentence semantically related to each other can establish their appropriate links. External evaluation of text summarization shows that the summary generated using the new method could better cover the full text content than the traditional approach.",
      "title": "16087 Text summarization method applying vocabulary combination into sentence extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80052555169&partnerID=40&md5=68e94982833788133bd9e554f243b1dd"
    },
    {
      "abstract": "This paper presents a novel approach for resolving ambiguities in concepts that already reside in semantic databases such as Freebase and DBpedia. Different from standard dictionaries and lexical databases, semantic databases provide a rich hierarchy of semantic relations in ontological structures. Our disambiguation approach decides on the implied sense by computing concept similarity measures as a function of semantic relations defined in ontological graph representation of concepts. Our similarity measures also utilize Wikipedia descriptions of concepts. We performed a preliminary experimental evaluation, measuring disambiguation success rate and its correlation with input text content. The results show that our method outperforms well-known disambiguation methods.  2011 ACM.",
      "title": "16090 Concept disambiguation exploiting semantic databases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960692742&partnerID=40&md5=52f351c0d3b5502cc27d747e60b125cf"
    },
    {
      "abstract": "Why people process text with computers? It all started many years ago, with the main goal in minds of researchers, to understand the text. In the meantime, the area of text processing developed in many different directions whereby the original goals were often forgotten. Funny enough, it seems, in several decades of computerized processing of textual data, the solution to the text understanding problem didnt evolve much compared to some other, easier and often more profitable problems to deal with (such as information retrieval/search, machine translation or information extraction). In this paper we touch various aspects of text processing along several dimensions: (a) how we represent the textual data, (b) what kind of algorithms and techniques we use, and (c) what kind of problems we solve on the top of text. Finally, it is interesting to observe various research communities dealing with textual data in different ways. Most of them are still rather fragmented and dont learn enough from each other - many of the ideas developed within one community dont cross borders of that community for too long.  2011 ACM.",
      "title": "16092 Many faces of text processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960611027&partnerID=40&md5=2767018a6745ae5c915a1e405283d537"
    },
    {
      "abstract": "When recommending news items, most of the traditional algorithms are based on TF-IDF, i.e., a term-based weighting method which is mostly used in information retrieval and text mining. However, many new technologies have been made available since the introduction of TF-IDF. This paper proposes a new method for recommending news items based on TF-IDF and a domain ontology. It is demonstrated that adapting TF-IDF with the semantics of a domain ontology, resulting in Concept Frequency - Inverse Document Frequency (CF-IDF), yields better results than using the original TF-IDF method. CF-IDF is built and tested in Athena, a recommender extension to the Hermes news personalization framework. Athena employs a user profile to store concepts or terms found in news items browsed by the user. The framework recommends new articles to the user using a traditional TF-IDF recommender and the CF-IDF recommender. A statistical evaluation of both methods shows that the use of an ontology significantly improves the performance of a traditional recommender.  2011 ACM.",
      "title": "16095 News personalization using the CF-IDF semantic recommender",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960611031&partnerID=40&md5=aaf9a9cd2035decc27e16d216125b15e"
    },
    {
      "abstract": "Sentiment classification has attracted increasing interest from natural language processing. The goal of sentiment classification is to automatically identify whether a given piece of text expresses positive or negative opinion on a topic of interest. Latent semantic analysis (LSA) has been shown to be extremely useful in information retrieval. In this paper, we propose a new method, based on LSA and support vector machine (SVM) to improve the sentiment classification performance. This method takes the advantage of both LSA and SVM. During the training process, SVM makes use of its excellent classification ability to conduct the sentiment classification first. To show that our method is feasible and effective, we designed two experiments and the experimental result shows that the introduction of SVM outperforms the other experiment in precision of the polarity analysis.  2011 Springer-Verlag.",
      "title": "16096 Sentiment classification of documents based on latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960499323&partnerID=40&md5=268312c4242703759fea6f48dced9ce0"
    },
    {
      "abstract": "This paper researches on ontology auto-extension, which is a hot issue of semantic web. In this paper, technology related to information processing is used to automatically extend ontology instances from free text. Firstly, three tasks are identified after analyzing on ontology auto-extension. Secondly, a new binary tree (BT) is constructed based on the ontology taxonomy, and then it is used as the training and learning strategy of support vector machine (SVM) classifier. And the full strategy is regarded as Onto-BT-SVM model. The main advantage of this strategy is that the semantic of ontology is made full use of. Finally, Different multi-class classification strategies are compared to verify their impacts on classification effects. Experiment results show that the recall rate of Onto-BT-SVM model is 90.5% and the accuracy rate is 93.5%, which is satisfactory.  2011 IEEE.",
      "title": "16097 Ontology auto-extension based on improved SVM algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960405627&partnerID=40&md5=1799e4a25a7441364c8a5a3d9b27f8b8"
    },
    {
      "abstract": "By mapping messages into a large context, we can compute the distances between them, and then classify them. We test this conjecture on Twitter messages: Messages are mapped onto their most similar Wikipedia pages, and the distances between pages are used as a proxy for the distances between messages. This technique yields more accurate classification of a set of Twitter messages than alternative techniques using string edit distance and latent semantic analysis.  2011 Springer-Verlag.",
      "title": "16100 Discovering context: Classifying tweets through a semantic transform based on wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960319229&partnerID=40&md5=ae0e939e2b59fd0185d0a5bb727ffdcb"
    },
    {
      "abstract": "Many attempts have been made to extract structured data from Web resources, exposing them as RDF triples and interlinking them with other RDF datasets: in this way it is possible to create clouds of highly integrated Semantic Web data collections. In this paper we describe an approach to enhance the extraction of semantic contents from unstructured textual documents, in particular considering Wikipedia articles and focusing on event mining. Starting from the deep parsing of a set of English Wikipedia articles, we produce a semantic annotation compliant with the Knowledge Annotation Format (KAF). We extract events from the KAF semantic annotation and then we structure each event as a set of RDF triples linked to both DBpedia and WordNet. We point out examples of automatically mined events, providing some general evaluation of how our approach may discover new events and link them to existing contents.  2011 Springer-Verlag.",
      "title": "16102 Extracting events from Wikipedia as RDF triples linked to widespread semantic web datasets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960296511&partnerID=40&md5=ad48db3670c9cbe4b11e78e9dccc567b"
    },
    {
      "abstract": "Due to the popularity of link-based applications like Wikipedia, one of the most important issues in online research is how to alleviate information overload on the World Wide Web (WWW) and facilitate effective information-seeking. To address the problem, we propose a semantically-based navigation application that is based on the theories and techniques of link mining, semantic relatedness analysis and text summarization. Our goal is to develop an application that assists users in efficiently finding the related subtopics for a seed query and then quickly checking the content of articles. We establish a topic network by analyzing the internal links of Wikipedia and applying the Normalized Google Distance algorithm in order to quantify the strength of the semantic relationships between articles via key terms. To help users explore and read topic-related articles, we propose a SNA-based summarization approach to summarize articles. To visualize the topic network more efficiently, we develop a semantically-based WikiMap to help users navigate Wikipedia effectively.  2011 Springer-Verlag.",
      "title": "16103 An exploratory study of navigating Wikipedia semantically: Model and application",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960318383&partnerID=40&md5=55920ffd3ada1ed092d72a6d327f45c3"
    },
    {
      "abstract": "Online microblogging services such as Twitter allow users to post very short messages related to everything ranging from mundane daily life routines to breaking news events. This phenomenon has changed the way for information acquisition. In this paper, we present an instinctive method with a time-decay function which corresponds to the natural propagation of social networks for clustering real-time text streams collected from Twitter. Compared to most previous studies, we follow natural cascading behaviors of event lifecycle to develop a self-adaptive clustering model for online event detection. Also, we construct an expandable similarity matrix which is capable of evaluating microblogging posts with incomplete semantic features. Experimental results show that the proposed method is a sensible solution to monitoring momentous real-time events and utilizing the text streams to facilitate the management of social networking data.  2011 Springer-Verlag.",
      "title": "16105 A self-adaptive clustering scheme with a time-decay function for microblogging text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960138366&partnerID=40&md5=9ae925cf5d0dadc69aecdeabb6a7f485"
    },
    {
      "abstract": "This paper describes the an approach to indexing texts by their conceptual content using ontologies. Central to this approach is a two-phase extraction principle divided into a syntactic annotation phase and a semantic generation phase drawing on lexico-syntactic information and semantic role assignment provided by existing lexical resources. Meaningful chunks of text are transformed into conceptual feature structures and mapped into concepts in a generative ontology. By this approach, synonymous but linguistically quite distinct expressions are extracted and mapped to the same concept in the ontology, providing a semantic indexing which enables content-based search.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16106 Extracting conceptual feature structures from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960149610&partnerID=40&md5=5873b1380d1a9db92bddceb782581adf"
    },
    {
      "abstract": "In information society, there are now worldwide economic network that are expanding prodigiously. There could be argued that social structure is in dire need of supporting technology innovation. This paper seeks to analysis and how to generate of knowledge model that supporting policy making of technology innovation. We let the user focus on some interested parts a whole decision making and to analysis using case and how to generate knowledge model and network through advanced semantic mining and formal language. Knowledge model is created to topic network that compositing driven keyword through text mining from natural language in document. And we show that the way of analyzing knowledge model and automatically generating feature keyword and relation properties into topic networks. In order that we analyze the semantic relationship and knowledge properties using CTL(Computational Tree Logic) on organized topic map and Kripke structure.  2011 Springer-Verlag.",
      "title": "16108 Optimization and generation of knowledge model for supporting technology innovation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960117050&partnerID=40&md5=7c68700dc9a4148ce3eb8c7f70631d5d"
    },
    {
      "abstract": "Keyphrases are useful for variety of purposes including: text clustering, classification, content-based retrieval, and automatic text summarization. A small amount of documents have author-assigned keyphrases. Manual assignment of the keyphrases to existing documents is a tedious task, therefore, automatic keyphrase extraction has been extensively used to organize documents. Existing automatic keyphrase extraction algorithms are limited in assigning semantically relevant keyphrases to documents. In this paper we have proposed a methodology to assign keyphrases to digital documents. Our approach exploits semantic relationships and hierarchical structure of the classification scheme to filter out irrelevant keyphrases suggested by Keyphrase Extraction Algorithm (KEA++). Experiments demonstrate that the refinement improves the precision of extracted keyphrases from 0.19% to 0.38% while maintains the same recall.  2011 Springer-Verlag.",
      "title": "16109 Automatic documents annotation by keyphrase extraction in digital libraries using taxonomy",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960133299&partnerID=40&md5=7e1ed21bbcb942816f9f575848665d6a"
    },
    {
      "abstract": "Searching for relevant documents in large sets of documents is one of the key tasks in the areas of semantic web and knowledge technologies. This paper deals with analysis and design of improvement for information retrieval (IR) using specific conceptual model automatically created from semantically non-annotated set of text documents. This conceptual model combines locally applied Formal Concept Analysis (FCA) and agglomerative clustering of particular models into one structure, which is suitable to support information retrieval process and can be combined with standard full-text search. Formal Concept Analysis (FCA) is one of the approaches which can be applied in process of conceptual modeling in domain of text documents. Extension of classic FCA (binary table data) is one-sided fuzzy version that works with real values in the object-attribute table (document-term matrix in case of vector representation of text documents). In our approach, starting set of documents is decomposed to smaller sets of similar documents with the use of some partitional clustering algorithm. Then one concept lattice is built for every cluster using FCA method and these FCA-based models are combined to hierarchy of concept lattices using agglomerative clustering algorithm. Finally, we define basic details and methods of IR system that combines standard full-text search and conceptual search (using extracted concept hierarchy).  2011 IEEE.",
      "title": "16111 Hierarchical FCA-based conceptual model of text documents used in information retrieval system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79959990080&partnerID=40&md5=0ba503c0cf897f81e6ded2645ce81ffb"
    },
    {
      "abstract": "We present NOAM, an integrated platform for the monitoring and analysis of news media content. NOAM is the data management system behind various applications and scientific studies aiming at modelling the mediasphere. The system is also intended to address the need in the AI community for platforms where various AI technologies are integrated and deployed in the real world. It combines a relational database (DB) with state of the art AI technologies, including data mining, machine learning and natural language processing. These technologies are organised in a robust, distributed architecture of collaborating modules, that are used to populate and annotate the DB. NOAM manages tens of millions of news items in multiple languages, automatically annotating them in order to enable queries based on their semantic properties. The system also includes a unified user interface for interacting with its various modules.  2011 Authors.",
      "title": "16112 NOAM: News outlets analysis and monitoring system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79959992240&partnerID=40&md5=c5ec1a14449803140983829d47ba17d9"
    },
    {
      "abstract": "As Internet usage has heavily increased within recent years, money launderers have started to take advantage of Online Financial Transaction (OFT) services to facilitate their money laundering activities. However, law enforcement has struggled to understand and detect OFT services that criminals use for money laundering. To assist law enforcement in its efforts to identify and monitor OFT services, we have designed the Online Financial Transaction Services Identification Tool (OFTSIT), which crawls the Internet and determines the probability that they are OFT services. OFTSIT analyzes a websites content and extracts textual features using latent semantic indexing (LSI). LSI is a text mining approach that can extract a small number (< 10) of features from more than 40,000 possible words on a website. OFTSIT inputs the LSI discovered features into a generalized linear model to produce the probability that a website is an OFT service. Testing showed that OFTSIT outperforms current method of manual searching. This paper describes the system architecture, algorithms employed to classify OFT services from other websites, and performance testing to demonstrate OFTSITs operational relevance.  2011 IEEE.",
      "title": "16113 Identifying and tracking online financial services through web mining and latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79959964017&partnerID=40&md5=efeef9e759148915bff9742c49492ac8"
    },
    {
      "abstract": "Purpose - The goal of the research is to explore whether the use of higher-level semantic features can help us to build better self-organising map (SOM) representation as measured from a human-centred perspective. The authors also explore an automatic evaluation method that utilises human expert knowledge encapsulated in the structure of traditional textbooks to determine map representation quality. Design/methodology/approach - Two types of document representations involving semantic features have been explored - i.e. using only one individual semantic feature, and mixing a semantic feature with keywords. Experiments were conducted to investigate the impact of semantic representation quality on the map. The experiments were performed on data collections from a single book corpus and a multiple book corpus. Findings - Combining keywords with certain semantic features achieves significant improvement of representation quality over the keywords-only approach in a relatively homogeneous single book corpus. Changing the ratios in combining different features also affects the performance. While semantic mixtures can work well in a single book corpus, they lose their advantages over keywords in the multiple book corpus. This raises a concern about whether the semantic representations in the multiple book corpus are homogeneous and coherent enough for applying semantic features. The terminology issue among textbooks affects the ability of the SOM to generate a high quality map for heterogeneous collections. Originality/value - The authors explored the use of higher-level document representation features for the development of better quality SOM. In addition the authors have piloted a specific method for evaluating the SOM quality based on the organisation of information content in the map.  2011 Emerald Group Publishing Limited.",
      "title": "16115 Improving self-organising information maps as navigational tools: A semantic approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79959916028&partnerID=40&md5=a991e1dc5d0ae4f502ad32a6861799a5"
    },
    {
      "abstract": "The rapid growth of biomedical literature prompts pervasive concentrations of biomedical text mining community to explore methodology for accessing and managing this ever-increasing knowledge. One important task of text mining in biomedical literature is gene mention normalization which recognizes the biomedical entities in biomedical texts and maps each gene mention discussed in the text to unique organic database identifiers. In this work, we employ an information retrieval based method which extracts gene mentions semantic profile from PubMed abstracts for gene mention disambiguation. This disambiguation method focuses on generating a more comprehensive representation of gene mention rather than the organic clues such as gene ontology which has fewer co-occurrences with the gene mention. Furthermore, we use an existing biomedical resource as another disambiguation method. Then we extract features from gene mention detection systems outcome to build a false positive filter according to Wikipedias retrieved documents. Our system achieved F-measure of 83.1% on BioCreative II GN test data.  2011 Elsevier Ltd. All rights reserved.",
      "title": "16118 Combining multiple disambiguation methods for gene mention normalization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952441028&partnerID=40&md5=3804afa44b9f6d15bcce557e7e33658f"
    },
    {
      "abstract": "Deep web provides tremendous structured data with high quality. In order to retrieve deep web data, one important task is to classify the domains of deep web automatically. In this paper, an approach based on domain feature text (DFT) is presented to classify the deep web. In the phase of DFT selection, a semantic abstract method based on ontology knowledge and a quantitative criteria for DFT selection based on domain correlation is proposed, which enhances the representational ability of DFT and avoids the subjectivity and uncertainty of manual selection as well. In the process of the interface vector construction, an improved weighting method is given to evaluate the different roles of DFT. Finally, a KNN algorithm is used to classify these interface vectors. Experiments on 160 query interfaces in 4 typical domains demonstrate the feasibility and effectiveness of our proposed approach.",
      "title": "16119 Deep web classification based on domain feature text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79961049710&partnerID=40&md5=74ffd7199b571ecc146486f9b8ef50cc"
    },
    {
      "abstract": "Feature weighting plays an important role in text clustering. Traditional feature weighting is determined by the syntactic relationship between feature and document (e.g. TF-IDF). In this paper, a semantically enriched feature weighting approach is proposed by introducing the semantic relationship between feature and document, which is implemented by taking account of the local feature relatedness - the relatedness between feature and its contextual features within each individual document. Feature relatedness is measured by two methods, document collection-based implicit relatedness measure and Wikipedia link-based explicit relatedness measure. Experimental results on benchmark data sets show that the new feature weighting approach surpasses traditional syntactic feature weighting. Moreover, clustering quality can be further improved by linearly combining the syntactic and semantic factors. The new feature weighting approach is also compared with two existing feature relatedness-based approaches which consider the global feature relatedness (feature relatedness in the entire feature space) and the inter-document feature relatedness (feature relatedness between different documents) respectively. In the experiments, the new feature weighting approach outperforms these two related work in clustering quality and costs much less computational complexity.  2011 Springer-Verlag.",
      "title": "16126 Unsupervised feature weighting based on local feature relatedness",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957956600&partnerID=40&md5=d05dc9120769fbfb1eb35b30a91cb8a0"
    },
    {
      "abstract": "Semantic web-based approaches and computational intelligence can be merged in order to get useful tools for several data mining issues. In this work a web-based tagging process followed by a validation step is carried to tag WordNet adjectives with positive, neutral or negative moods. This tagged WordNet is used to define a semantic metric for text documents clustering. Experimental results on movie reviews prove that the introduced semantically oriented metric is extremely fast and gives improved results with respect to the classical frequency based text mining metric from the accuracy point of view.  2011 Springer-Verlag.",
      "title": "16130 Semantic oriented clustering of documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957789137&partnerID=40&md5=772292fe08926d37730ec57c8f90db04"
    },
    {
      "abstract": "Rich information on mutations and their impacts is scattered across scientific texts and literature. Reuse of mutation impact annotations requires grounding mutations to the correct positions on sequences extracted from protein databases as a critical step. This paper presents a generic method for grounding textual mentions of mutation entities to protein sequences, that is based on an OWL-DL ontology driven workflow that integrates text and sequence information in a semantically consistent way. Mutation mentions mined from texts are iteratively mapped onto candidate proteins, and an ontology mining algorithm facilitates their correct grounding to a protein sequence. Using a gold standard corpus of full text articles and corresponding protein sequences we show the proposed method is promising compared to existing approaches.  2011 IEEE.",
      "title": "16131 Extraction and grounding of protein mutations via semantic integration of text and sequence information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957707160&partnerID=40&md5=0efd6f34556362c7fdf38469a1f8b176"
    },
    {
      "abstract": "Facilitating class discussions effectively is a critical yet challenging component of instruction, particularly in online environments where student and faculty interaction is limited. Our goals in this research were to identify facilitation strategies that encourage productive discussion, and to explore text mining techniques that can help discover meaningful patterns in the discussions more efficiently at scale. Based on a close reading of selected discussion threads from online undergraduate science classes, we observed a variety of facilitation strategies associated with discussion quality. These observations informed our selection of a larger dataset of discussion threads to analyze via text mining techniques. Using latent semantic analysis to produce topic models of the content of the discussions, we constructed visualizations of the topical and temporal development of those discussions among students and faculty. These visualizations revealed patterns that appeared to correspond with specific facilitation styles and with the extent to which discussions remained focused on particular topics. From a case study focusing on six of these discussions, we documented distinct patterns in the types of facilitation strategies employed and the character of the discussions that followed. In our conclusion, we discuss potential applications of these analytical techniques for helping students, faculty, and faculty developers become more aware of their participation and influence in online discussions, thereby improving their value as a learning environment.",
      "title": "16132 Using text mining to characterize online discussion facilitation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84872349697&partnerID=40&md5=72e901493259795266b193bbe3e89cdb"
    },
    {
      "abstract": "With increasing digital information availability, semantic web technologies have been employed to construct semantic digital libraries in order to ease information comprehension. The use of semantic web enables users to search or visualize resources in a semantic fashion. Semantic web generation is a key process in semantic digital library construction, which converts metadata of digital resources into semantic web data. Many text mining technologies, such as keyword extraction and clustering, have been proposed to generate semantic web data. However, one important type of metadata in publications, called affiliation, is hard to convert into semantic web data precisely because different authors, who have the same affiliation, often express the affiliation in different ways. To address this issue, this paper proposes a clustering method based on normalized compression distance for the purpose of affiliation disambiguation. The experimental results show that our method is able to identify different affiliations that denote the same institutes. The clustering results outperform the well-known k-means clustering method in terms of average precision, F-measure, entropy, and purity.  2011 ASIS&T.",
      "title": "16134 Affiliation disambiguation for constructing semantic digital libraries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79956068336&partnerID=40&md5=3218e7374c65ebb7057f8b79d4313d60"
    },
    {
      "abstract": "The emotion tendency of sentiment word is divided into two types: static emotion tendency and dynamic emotion tendency. Basic semantic lexicon is static emotion tendency, in the real context, but it is different between static emotion tendency and dynamic emotion tendency. The paper proposes a method based on degree lexicon, negative lexicon and dependence relationship of sentence elements. The experimental results show dynamic emotion tendency even more reflects text emotion tendency and get best performance in text sentiment classification.  2011 IEEE.",
      "title": "16135 A dynamic adjustment algorithm research of sentiment word weight based on context",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957562576&partnerID=40&md5=f7aec7028a07704f4936330b73123dd2"
    },
    {
      "abstract": "Background: Applications of Natural Language Processing (NLP) technology to biomedical texts have generated significant interest in recent years. In this paper we identify and investigate the phenomenon of linguistic subdomain variation within the biomedical domain, i.e., the extent to which different subject areas of biomedicine are characterised by different linguistic behaviour. While variation at a coarser domain level such as between newswire and biomedical text is well-studied and known to affect the portability of NLP systems, we are the first to conduct an extensive investigation into more fine-grained levels of variation.Results: Using the large OpenPMC text corpus, which spans the many subdomains of biomedicine, we investigate variation across a number of lexical, syntactic, semantic and discourse-related dimensions. These dimensions are chosen for their relevance to the performance of NLP systems. We use clustering techniques to analyse commonalities and distinctions among the subdomains.Conclusions: We find that while patterns of inter-subdomain variation differ somewhat from one feature set to another, robust clusters can be identified that correspond to intuitive distinctions such as that between clinical and laboratory subjects. In particular, subdomains relating to genetics and molecular biology, which are the most common sources of material for training and evaluating biomedical NLP tools, are not representative of all biomedical subdomains. We conclude that an awareness of subdomain variation is important when considering the practical use of language processing applications by biomedical researchers.  2011 Lippincott et al",
      "title": "16136 Exploring subdomain variation in biomedical language",
      "url": ""
    },
    {
      "abstract": "Answer Set Programming (ASP) is a declarative language for knowledge representation and reasoning. After its proposal, in a seminal paper by Michael Gelfond and Vladimir Lifschitz dated in 1988, ASP has been the subject of a broad theoretical research-work, ranging from linguistic extensions and semantic properties to evaluation algorithm and optimization techniques. Later on, the availability of a number of efficient systems made ASP a powerful tool for the fast development of knowledge-based applications. In this paper, we report on the ongoing effort aimed at the industrial exploitation of DLV - one of the most popular ASP systems - in the area of Knowledge Management. Two spin-off companies of University of Calabria are involved in such an exploitation: Dlvsystem Srl and Exeura Srl. They have specialized DLV into some Knowledge Management products for Text Classification, Information Extraction, and Ontology Representation and Reasoning, which have allowed to develop a number of successful real-world applications.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16137 ASP at work: Spin-off and applications of the DLV system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79956312585&partnerID=40&md5=9339b4978c0b01b079501dd4bc21b697"
    },
    {
      "abstract": "Document clustering techniques mostly rely on single term analysis which can not reveal the potential semantic relationship between terms. To better capture the semantic subject of documents, this study proposes weighted conceptual model for document presentation. The new model divides the document concepts into centroid concepts and peripheral concepts due to their semantic relations to subject. The semantic similarity between two documents is calculated by centroid concepts and peripheral concepts respectively. A fuzzy semantic clustering method is put forward bases on the new semantic model. Experimental results show that the method enhances semi-structured document clustering quality significantly and outperforms K-Means and Fuzzy C-Means.  2011 Asian Network for Scientific Information.",
      "title": "16139 Fuzzy document clustering using weighted conceptual model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79955929085&partnerID=40&md5=e8a6a45358083a885f8b65b664efd212"
    },
    {
      "abstract": "Similarity measurement plays the fundamental role in the classification of information resources and transmission of network information. According to the research of text-based similarity algorithm on three-layer structure, add the word difference factors to the measurement method of the original text similarity factor, thereby reducing the similarity measurement error resulted by semantics and words difference. The results demonstrate that compare with the improved algorithm and the similarity measurement method base on the original three-layer structure, the measurement accuracy can be improved.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16141 Research on the text lengths effect of the text similarity measurement",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79955416617&partnerID=40&md5=8bbb10519e3b7b0cb54765795eaed129"
    },
    {
      "abstract": "Traditional text similarity measurements use TF-IDF method to model text documents as term frequency vectors, and compute similarity between text documents by using cosine similarity. These methods ignore semantic information of text documents, and semantic information enhanced methods distinguish between text documents poorly because extended vectors with semantic similar terms aggravate the curse of dimensionality. This paper proposes a similarity measurement, which is based on TF-IDF method, and analyzes similarity between important terms in text documents. This approach uses NLP technology to pre-process text, and uses TF-IDF method to filter those key terms that have higher TF-IDF value than other common terms. With the proposed data structure TSWT (Term Similarity Weight Tree) and the definition of semantic similarity, this paper resolves the semantic information of those key terms to compute similarities between text documents. Finally, several K-Means clustering methods is used for evaluating performance of the new text document similarity. By comparing with TF-IDF and another the-state-of-art semantic information based similarity method, experimental results on benchmark corpus demonstrate that it can promote the evaluation metrics of F-Measure.",
      "title": "16143 A text similarity measurement combining word semantic information with TF-IDF method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79958006205&partnerID=40&md5=f6fa761ec1ccc8f40e63e6520d1bf1f3"
    },
    {
      "abstract": "We describe a framework to digest news webpages in finer granularity: to extract event snippets from contexts. Events are atomic text snippets and a news article is constituted by more than one event snippet. Event Snippet Extraction (ESE) aims to mine these snippets out. The problem is important because its solutions may be applied to many information mining and retrieval tasks. The challenge is to exploit rich features to detect snippet boundaries, including various semantic, syntactic and visual features. We run experiments to present the effectiveness of our approaches.  2011 Authors.",
      "title": "16145 A finegrained digestion of news webpages through Event Snippet Extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79955140622&partnerID=40&md5=bab4cb1afed55daf1a2e11da1f3aefd5"
    },
    {
      "abstract": "With the development of the techniques of Event Detection and Tracking, it is feasible to gather text information from many sources and structure it into events which are constructed online automatically and updated temporally. There are always diversified versions to describe an event and users usually are eager to know all the versions. With the huge quantity of documents, it is almost impossible for users to read all of them. In this paper, we formally define the problem of event diversified versions discovery. We introduce a novel and principled model (called DVD) for discovering diversified versions for events. Unlike traditional clustering methods, we apply an iterative algorithm on a bipartite graph integrating co-occurrence and semantics to select the popular words and filter them to reduce the tight correlation between documents in a specific event. Hybrid link structures between words are utilized to find the hierarchical relationships. We employ a web communities discovery algorithm to construct virtual-documents which consist of a bag of words indicating one of the diversified versions. Under Rocchio Classification framework, we can classify the documents to diversified versions. With our novel evaluation method, empirical experiments on two real datasets show that DVD is effective and outperforms various related algorithms, including classic K-means and LDA.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16147 DVD: A model for event diversified versions discovery",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79955099395&partnerID=40&md5=d90514f097b9211122d290d54780d45e"
    },
    {
      "abstract": "Biological research papers are replete with speculative sentences. We present the BioExcom rule-based system, which detects speculations in biomedical literature. Furthermore, it enables to distinguish automatically between prior and new speculations in the analyzed paper. BioExcom is based on the Contextual Exploration processing (hierarchical research of linguistic surface markers with the EXCOM computational platform). To accomplish this task, BioExcom uses also specific linguistic resources established by concise semantic analysis performed by a biologist and a linguist. Our work shows that it is possible to detect and categorize speculative sentences without computational deep linguistic analyses. This work could be useful for biologists who are interested by finding new hypothesis in literature.  2011 Springer-Verlag.",
      "title": "16148 BioExcom: Detection and categorization of speculative sentences in biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79953202159&partnerID=40&md5=cd01a3fefa6f5f095973673f4759d371"
    },
    {
      "abstract": "Information retrieval systems traditionally rely on textual keywords to index and retrieve documents. Keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. Furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. Concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. In this article we introduce a new concept-based retrieval approach based on Explicit Semantic Analysis (ESA), a recently proposed method that augments keywordbased text representation with concept-based features, automatically extracted from massive human knowledge repositories such as Wikipedia. Our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. However, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. The resulting system is evaluated on several TREC datasets, showing superior performance over previous state-of-the-art results.  2011 ACM.",
      "title": "16151 Concept-based information retrieval using explicit semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051504927&partnerID=40&md5=6edab2cc8bc8b4308d25a144590b926e"
    },
    {
      "abstract": "Customers often present certain preferences relative to the same product, such as function, shape, color, and cost. The ideas in the mind of the customer can be represented by higher level concepts. However, the actual shape, color, and cost embodied in the product can only be viewed as lower-level features. In this paper, a model of preference elicitation from customers is proposed to bridge the gap between low-level features and high-level concepts. First, the attributes of customer preferences are classified using preference taxonomies that we develop. These taxonomies are represented using unstructured documents that are directly collected from customer descriptions. Second, the documents or catalogs of design requirements, containing some textual descriptions and survey reports, are then normalized by using an ontology-based semantic representation. Some semantic rules are developed to describe the low-level features of customer preferences to build an ontological knowledge base. Third, customer preferences are mapped to domain ontologies for driving high-level concept generation. A customer preference modeling framework is developed to construct a vector space model to measure the similarity between two preference concept ontologies. Finally, an empirical study is implemented, and five different customer groups are surveyed about the cell phone preferences. The query results are analyzed to deeply understand the validity of concept generation from the customer preferences.  2010 Elsevier Ltd. All rights reserved.",
      "title": "16152 Ontology-based customer preference modeling for concept generation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79954421021&partnerID=40&md5=5f0971bf2f4728c0c4a5ac65bf496469"
    },
    {
      "abstract": "Granular topic extraction and modeling are fundament tasks in text analysis. Hierarchical topic clustering algorithms and hierarchical topic models are usually employed for these purposes. However, it is difficult to make a clear distinguish between each pair of hierarchical topics from the semantic granularity point of view. STG (semantic topic granularity) is proposed to indicate the details degree of topic description, and aim at providing discrimination for topics from semantic aspect. A new model, mgMTM (multi-grain mixture topic model) based on STG is then proposed to model grain topics. DCT (discrete cosine transform) is employed to provide a mechanism for computing STG, extracting grain topics and learning mgMTM. Experiments on real world datasets show that the proposed model has lower perplexity score than that of LDA model and thus has better generalization performance in describing text. Experiments also show that the description of the extracted grain topics can be well explained with respect to a dataset including topics about recent global financial crisis.  2010 Elsevier Ltd. All rights reserved.",
      "title": "16155 Semantic multi-grain mixture topic model for text analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650700221&partnerID=40&md5=e7d02feb7718a6a160c7927bf49d2fbc"
    },
    {
      "abstract": "In recent years, with the massive growth of web, there is an explosion of information accessible to internet users. The search logic usually tries to recover this information by exploiting many text-matching techniques. Also, traditional search engines do not have the necessary infrastructure for exploiting relation-based information that belongs to the semantic annotations for a web page. In the semantic web, each page possesses semantic metadata that record additional details concerning the web page itself. Annotations are based on classes of concepts and relations among them. We propose that relations among concepts embedded into semantic annotations can be effectively exploited to define a ranking strategy for semantic based web search engines. This approach relies on the knowledge of the user query, the underlying ontology, the hypergraph based clustering, and the web pages to be ranked. Thus, it allows us to effectively manage the search space and to reduce the complexity associated with the ranking task.  2011 Springer-Verlag Berlin Heidelberg.",
      "title": "16156 Normalization of semantic based web search engines using page rank algorithm and hypergraph based clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952998008&partnerID=40&md5=a7f90a31d067a4e2f0d596c16d0b09be"
    },
    {
      "abstract": "As a sort of formalizing tool of knowledge representation, Description Logics have been successfully applied in Information System, Software Engineering and Natural Language processing and so on. Description Logics also play a key role in text representation, Natural Language semantic interpretation and language ontology description. Description Logics have been logical basis of OWL which is an ontology language that is recommended by W3C. This paper discusses the description logic basic ideas under vocabulary semantic, context meaning, domain knowledge and background knowledge.",
      "title": "16158 The applications of Description Logics in Natural Language processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952790639&partnerID=40&md5=a26b138818c01f155fdb37d989b2ab94"
    },
    {
      "abstract": "The ubiquity of the multimedia has raised a need for the system that can store, manage, structured the multimedia data in such a way that it can be retrieved intelligently. One of the current issues in media management or data mining research is ranking of retrieved documents. Ranking is one of the provocative problems for information retrieval systems. Given a user query comes up with the millions of relevant results but if the ranking function cannot rank it according to the relevancy than all results are just obsolete. However, the current ranking techniques are in the level of keyword matching. The ranking among the results is usually done by using the term frequency. This paper is concerned with ranking the document relying merely on the rich semantic inside the document instead of the contents. Our proposed ranking refinement strategy known as SemRank, rank the document based on the semantic intensity. Our approach has been applied on the open benchmark LabelMe dataset and compared against one of the well known ranking model i.e. Vector Space Model (VSM). The experimental results depicts that our approach has achieved significant improvement in retrieval performance over the state of the art ranking methods.  2010 Published by Elsevier Ltd.",
      "title": "16159 SemRank: Ranking refinement strategy by using the semantic intensity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952502885&partnerID=40&md5=834c67851bd116070b2c516bd93f525f"
    },
    {
      "abstract": "This paper investigates how to effectively do cross lingual text classification by leveraging a large scale and multilingual knowledge base, Wikipedia. Based on the observation that each Wikipedia concept is described by documents of different languages, we adapt existing topic modeling algorithms for mining multilingual topics from this knowledge base. The extracted topics have multiple types of representations, with each type corresponding to one language. In this work, we regard such topics extracted from Wikipedia documents as universal-topics, since each topic corresponds with same semantic information of different languages. Thus new documents of different languages can be represented in a space using a group of universal-topics. We use these universal-topics to do cross lingual text classification. Given the training data labeled for one language, we can train a text classifier to classify the documents of another language by mapping all documents of both languages into the universal-topic space. This approach does not require any additional linguistic resources, like bilingual dictionaries, machine translation tools, or labeling data for the target language. The evaluation results indicate that our topic modeling approach is effective for building cross lingual text classifier. Copyright 2011 ACM.",
      "title": "16160 Cross lingual text classification by mining multilingual topics from Wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952412572&partnerID=40&md5=9546c6f72a7efc7775107484a964c233"
    },
    {
      "abstract": "With the continuous digitisation of medical knowledge, information extraction tools become more and more important for practitioners of the medical domain. In this paper we tackle semantic relationships extraction from medical texts. We focus on the relations that may occur between diseases and treatments. We propose an approach relying on two different techniques to extract the target relations: (i) relation patterns based on human expertise and (ii) machine learning based on SVM classification. The presented approach takes advantage of the two techniques, relying more on manual patterns when few relation examples are available and more on feature values when a sufficient number of examples are available. Our approach obtains an overall 94.07% F-measure for the extraction of cure, prevent and side effect relations.  2011 Springer-Verlag.",
      "title": "16161 A hybrid approach for the extraction of semantic relations from MEDLINE abstracts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952260968&partnerID=40&md5=7de95e3eb05032999752e4ba3ba6e3e3"
    },
    {
      "abstract": "Although dependency parsers have become increasingly popular, little work has been done on how to associate dependency structures with deep semantic representations. In this paper, we propose a semantic calculus for dependency structures which can be used to construct deep semantic representations from joint syntactic and semantic dependency structures similar to those used in the ConLL 2008 Shared Task.  2011 Springer-Verlag.",
      "title": "16163 Deep semantics for dependency structures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952267103&partnerID=40&md5=2046d1f308f52d3ed7eb54399df654f8"
    },
    {
      "abstract": "In spite of their industrial success, the development of intelligent systems is still a complex and risky task. When building intelligent systems, we see that domain knowledge is often present at different levels of formalization-ranging from text documents to explicit rules. In this paper, we describe the knowledge formalization continuum as a metaphor to help domain specialists during the knowledge acquisition phase. To make use of the knowledge formalization continuum, the agile use of knowledge representations within a knowledge engineering project is proposed, as well as transitions between the different representations, when required. We show that a semantic wiki is a flexible tool for engineering knowledge on the knowledge formalization continuum. Case studies are taken from one industrial and one academic project, and they illustrate the applicability and benefits of semantic wikis in combination with the knowledge formalization continuum.",
      "title": "16164 Engineering intelligent systems on the knowledge formalization continuum",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79953224834&partnerID=40&md5=4c233771b829e72f7420005384348f09"
    },
    {
      "abstract": "Automatic keyword extraction has attracted much attention due to the many applications in information retrieval, text mining, information processing, etc. To extract desired keywords from documents automatically, other than lexical relations among words or sentences, the content-based measures are critical at the same time. From the perspective of information retrieval, we are to facilitate effective and intuitively-interpretable methods for automatic keyword extraction. In this paper, we propose multiple content-based measures for the weights, contributions to document classifications and coverage of words. We give the measure based on the Laplaces law to determine the weight of a word in documents, and give the measure based on the concept of average mutual information to determine the influence of a word on document classifications. Furthermore, we propose the concept of semantic coacervation degree as the measure for the coverage of words on documents, and give a branch and bound algorithm to find the minimal set of keywords that maximize the coverage. By addressing these content-based measures one by one, the desired keywords can be extracted automatically from documents. Experimental results show that our methods are not only efficient but also feasible.  2011 CRL Publishing Ltd.",
      "title": "16165 Automatic keyword extraction from documents based on multiple content-based measures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051740569&partnerID=40&md5=b92d1532832102a25d2940c99e986fef"
    },
    {
      "abstract": "Background: The OMIM database is a tool used daily by geneticists. Syndrome pages include a Clinical Synopsis section containing a list of known phenotypes comprising a clinical syndrome. The phenotypes are in free text and different phrases are often used to describe the same phenotype, the differences originating in spelling variations or typing errors, varying sentence structures and terminological variants.These variations hinder searching for syndromes or using the large amount of phenotypic information for research purposes. In addition, negation forms also create false positives when searching the textual description of phenotypes and induce noise in text mining applications. Description: Our method allows efficient and complete search of OMIM phenotypes as well as improved data-mining of the OMIM phenome. Applying natural language processing, each phrase is tagged with additional semantic information using UMLS and MESH. Using a grammar based method, annotated phrases are clustered into groups denoting similar phenotypes. These groups of synonymous expressions enable precise search, as query terms can be matched with the many variations that appear in OMIM, while avoiding over-matching expressions that include the query term in a negative context. On the basis of these clusters, we computed pair-wise similarity among syndromes in OMIM. Using this new similarity measure, we identified 79,770 new connections between syndromes, an average of 16 new connections per syndrome. Our project is Web-based and available at http://fohs.bgu.ac.il/s2g/csiomim. Conclusions: The resulting enhanced search functionality provides clinicians with an efficient tool for diagnosis. This search application is also used for finding similar syndromes for the candidate gene prioritization tool S2G.The enhanced OMIM database we produced can be further used for bioinformatics purposes such as linking phenotypes and genes based on syndrome similarities and the known genes in Morbidmap.  2011 Cohen et al",
      "title": "16166 CSI-OMIM - Clinical Synopsis Search in OMIM",
      "url": ""
    },
    {
      "abstract": "Due to the limitations of language-processing tools for the Thai language, pattern-based information extraction from Thai documents requires supplementary techniques. Based on sliding-window rule application and extraction filtering, we present a framework for extracting semantic information from medical-symptom phrases with unknown boundaries in Thai unstructured-text information entries. A supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand-tagged training symptom phrases. Two filtering components are introduced: one uses a classification model to predict rule application across a symptom-phrase boundary based on instantiation features of rule internal wildcards, the other uses weighted classification confidence to resolve conflicts arising from overlapping extractions. In our experimental study, we focus our attention on two basic types of symptom phrasal descriptions: one is concerned with abnormal characteristics of some observable entities and the other with human-body locations at which primitive symptoms appear. The experimental results show that the filtering components improve precision while preserving recall satisfactorily. 2011 The Institute of Electronics, Information and Communication Engineers.",
      "title": "16167 Extracting semantic frames from thai medical-symptom unstructured text with unknown target-phrase boundaries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952178437&partnerID=40&md5=e786935c69ce4f0ba1b6dc2b93934456"
    },
    {
      "abstract": "This paper presents sGRAPH - a domain ontology-driven semantic graph auto extraction system used to discover knowledge from text publications in traditional Chinese medicine. The traditional Chinese medicine language system (TCMLs), composed of an ontology schema and a knowledge base containing 153,692 words and 304,114 relations, is used as the domain ontology. The sGRAPH comprises two components: a user interface that interacts with users and the domain ontology-based semantic graph extraction algorithm. This algorithm is divided into five steps: text processing, semantic graph extraction, graph identification, keyword-based semantic graph search and the selectable enrichment to the knowledge base. When the knowledge base of TCMLs is used, the domain-specific words are extracted from sentences more accurately",
      "title": "16168 GRAPH: A domain ontology-driven semantic graph auto extraction system",
      "url": ""
    },
    {
      "abstract": "In the text summarization field, a table-of-contents is a type of indicative summary that is especially suited for locating information in a long document, or a set of documents. It is also a useful summary for a reader to quickly get an overview of the entire contents. The current models for generating a table-of-contents produced relatively low quality output with many meaningless titles, or titles that have no overlapping meaning with the corresponding contents. This problem may be due to the lack of semantic information and topic information in those models. In this research, we propose to integrate supportive knowledge into the learning models to improve the quality of titles in a generated tableof- contents. The supportive knowledge is derived from a hierarchical clustering of words, which is built from a large collection of raw text, and a topic model, which is directly estimated from the training data. The relatively good results of the experiments showed that the semantic and topic information supplied by supportive knowledge have good effects on title generation, and therefore, they help to improve the quality of the generated table-of-contents.  2011 The Institute of Electronics, Information and Communication Engineers.",
      "title": "16170 Learning to generate a table-of-contents with supportive knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952136170&partnerID=40&md5=ff8107d5df33fe7d05b1dd6a7a4acfee"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) is an effective feature extraction method which can capture the underlying latent semantic structure between words in documents. However, it is probably not the most appropriate for text categorization to use the method to select feature subspace, since the method orders extracted features according to their variance,not the classification power. We proposed a method based on support vector machine to extract features and select a Latent Semantic Indexing that be suited for classification. Experimental results indicate that the method improves classification performance with more compact representation.",
      "title": "16171 A method based on support vector machine for feature selection of Latent Semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79751502046&partnerID=40&md5=71771fffb44bd428467b043f5a1d9066"
    },
    {
      "abstract": "Proper understanding of textual data requires the exploitation and integration of unstructured and heterogeneous clinical sources, healthcare records or scientific literature, which are fundamental aspects in clinical and translational research. The determination of semantic similarity between word pairs is an important component of text understanding that enables the processing, classification and structuring of textual resources. In the past, several approaches for assessing word similarity by exploiting different knowledge sources (ontologies, thesauri, domain corpora, etc.) have been proposed. Some of these measures have been adapted to the biomedical field by incorporating domain information extracted from clinical data or from medical ontologies (such as MeSH or SNOMED CT). In this paper, these approaches are introduced and analyzed in order to determine their advantages and limitations with respect to the considered knowledge bases. After that, a new measure based on the exploitation of the taxonomical structure of a biomedical ontology is proposed. Using SNOMED CT as the input ontology, the accuracy of our proposal is evaluated and compared against other approaches according to a standard benchmark of manually ranked medical terms. The correlation between the results of the evaluated measures and the human experts ratings shows that our proposal outperforms most of the previous measures avoiding, at the same time, some of their limitations.  2010 Elsevier Inc.",
      "title": "16180 An ontology-based measure to compute semantic similarity in biomedicine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952022327&partnerID=40&md5=7f5fdc0cd930f38bdf726fce77e9c371"
    },
    {
      "abstract": "A major concern when incorporating large sets of diverse n-gram features for sentiment classification is the presence of noisy, irrelevant, and redundant attributes. These concerns can often make it difficult to harness the augmented discriminatory potential of extended feature sets. We propose a rule-based multivariate text feature selection method called Feature Relation Network (FRN) that considers semantic information and also leverages the syntactic relationships between n-gram features. FRN is intended to efficiently enable the inclusion of extended sets of heterogeneous n-gram features for enhanced sentiment classification. Experiments were conducted on three online review testbeds in comparison with methods used in prior sentiment classification research. FRN outperformed the comparison univariate, multivariate, and hybrid feature selection methods",
      "title": "16181 Selecting attributes for sentiment classification using feature relation networks",
      "url": ""
    },
    {
      "abstract": "Reciprocity is a pervasive concept that plays an important role in governing peoples behavior, judgments, and thus their social interactions. In this paper we present an analysis of the concept of reciprocity as expressed in English and a way to model it. At a larger structural level the reciprocity model will induce representations and clusters of relations between interpersonal verbs. In particular, we introduce an algorithm that semi-automatically discovers patterns encoding reciprocity based on a set of simple yet effective pronoun templates. Using the most frequently occurring patterns we queried the web and extracted 13,443 reciprocal instances, which represent a broad-coverage resource. Unsupervised clustering procedures are performed to generate meaningful semantic clusters of reciprocal instances. We also present several extensions (along with observations) to these models that incorporate meta-attributes like the verbs affective value, identify gender differences between participants, consider the textual context of the instances, and automatically discover verbs with certain presuppositions. The pattern discovery procedure yields an accuracy of 97 per cent, while the clustering procedures-clustering with pairwise membership and clustering with transitions-indicate accuracies of 91 per cent and 64 per cent, respectively. Our affective value clustering can predict an unknown verbs affective value (positive, negative, or neutral) with 51 per cent accuracy, while it can discriminate between positive and negative values with 68 per cent accuracy. The presupposition discovery procedure yields an accuracy of 97 per cent.  2011 Cambridge University Press.",
      "title": "16185 Modeling reciprocity in social interactions with probabilistic latent space models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957442444&partnerID=40&md5=0ff95d616034c29435a9e8db9c312a5f"
    },
    {
      "abstract": "BLOG has made it possible to find out the opinions and experiences of those in the vast pool of people that are neither our personal acquaintances nor well-known. How to assess semantic inclination of BLOG contents and reviews automatically has attracted a lot of researchers concern. Due to Chinese expression is more complex than English, it is more difficult to assess semantic inclination of Chinese BLOG and accuracy is not high. This paper presents a continuous prior polarity algorithm. Our method reflects subtle change of sentiment in contrast to the previous studies which expressed sentiment polarity discretely. Furthermore, we propose a method to assess modified polarity based on Chinese dependency grammar. According to different dependency grammar relation, we can accurately identify subjective words and its modified polarity, then aggregate subjective words to predict sentence sentiment polarity. The experimental result illustrates that our method has higher accuracy compared with machine learning methods SVM and NB.  2010 IEEE.",
      "title": "16186 Semantic inclination mining based on dependency grammar for Chinese BLOG",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650596191&partnerID=40&md5=e2ff7858badfc58fec96f203513d1fa8"
    },
    {
      "abstract": "We present a new ensemble method that uses Entropy Guided Transformation Learning (ETL) as the base learner. The proposed approach, ETL Committee, combines the main ideas of Bagging and Random Subspaces. We also propose a strategy to include redundancy in transformation-based models. To evaluate the effectiveness of the ensemble method, we apply it to three Natural Language Processing tasks: Text Chunking, Named Entity Recognition and Semantic Role Labeling. Our experimental findings indicate that ETL Committee significantly outperforms single ETL models, achieving state-of-the-art competitive results. Some positive characteristics of the proposed ensemble strategy areworth to mention. First, it improves the ETL effectiveness without any additional human effort. Second, it is particularly useful when dealing with very complex tasks that use large feature sets. And finally, the resulting training and classification processes are very easy to parallelize.  Springer-Verlag 2010.",
      "title": "16187 ETL ensembles for chunking, NER and SRL",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650569670&partnerID=40&md5=ab545b918e74326001de5a8438cbff68"
    },
    {
      "abstract": "This paper presents an automatically generated Intermediate Logic Form of WordNets glosses. Our proposed logic form includes neo-Davidsonian reification in a simple and flat syntax close to natural language. We offer a comparison with other semantic representations such as those provided by Hobbs and Extended WordNet. The Intermediate Logic Forms are straightforwardly obtained from the output of a pipeline consisting of a part-of-speech tagger, a dependency parser and our own Intermediate Logic Form generator (all freely available tools). We apply the pipeline to the glosses of WordNet 3.0 to obtain a lexical resource ready to be used as knowledge base or resource for a variety of tasks involving some kind of semantic inference. We present a qualitative evaluation of the resource and discuss its possible application in Natural Language Understanding.  Springer-Verlag 2010.",
      "title": "16188 On the automatic generation of intermediate logic forms for WordNet glosses",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650428889&partnerID=40&md5=a290caca1ad9b5621edaedf70cfe0b63"
    },
    {
      "abstract": "We present an ontology-based semantic interpreter that can be linked to a grammar through grammar rule constraints, providing access to meaning during parsing and generation. In this approach, the parser will take as input natural language utterances and will produce ontology-based semantic representations. We rely on a recently developed constraint-based grammar formalism, which balances expressiveness with practical learnability results.We show that even with a weak ontological model, the semantic interpreter at the grammar rule level can help remove erroneous parses obtained when we do not have access to meaning.  Springer-Verlag 2010.",
      "title": "16191 Ontology-based semantic interpretation as grammar rule constraints",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650470609&partnerID=40&md5=de0f02463ae09f3449350fc56bcf482a"
    },
    {
      "abstract": "Named Entity Recognition and Classification is being studied for last two decades. Since semantic features take huge amount of training time and are slow in inference, the existing tools apply features and rules mainly at the word level or use lexicons. Recent advances in distributional semantics allow us to efficiently create paradigmatic models that encode word order. We used Sahlgren et als permutation-based variant of the Random Indexing model to create a scalable and efficient system to simultaneously recognize multiple entity classes mentioned in natural language, which is validated on the GENIA corpus which has annotations for 46 biomedical entity classes and supports nested entities. Using distributional semantics features only, it achieves an overall micro-averaged Fmeasure of 67.3% based on fragment matching with performance ranging from 7.4% for DNA substructure to 80.7% for Bioentity.  Springer-Verlag 2010.",
      "title": "16192 A distributional semantics approach to simultaneous recognition of multiple classes of named entities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650456992&partnerID=40&md5=dec9f0431d1d0dbba550cb2c0848e7c9"
    },
    {
      "abstract": "In this paper we describe the process of Russian and Romanian WordNet-Affect creation. WordNet-Affect is a lexical resource created on the basis of the Princeton WordNet which contains information about the emotions that the words convey. It is organized in six basic emotions: anger, disgust, fear, joy, sadness, surprise. We translated the WordNet-Affect synsets into Russian and Romanian and created an aligned English - Romanian - Russian lexical resource. The resource is freely available for research purposes.  Springer-Verlag 2010.",
      "title": "16193 Emotions in words: Developing a multilingual wordnet-affect",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650424482&partnerID=40&md5=13f87eeedaecd41d7c497b7692155346"
    },
    {
      "abstract": "The paper proposes a Service-oriented Knowledge Discovery (SoKD) framework and a prototype implementation named Orange4WS. To provide the proposed framework with semantics, we are using the Knowledge Discovery Ontology which defines relationships among the ingredients of knowledge discovery scenarios. It enables to reason which algorithms can be used to produce the results required by a specified knowledge discovery task, and to query the results of the knowledge discovery tasks. In addition, the ontology can also be used for automatic annotation of manually created workflows facilitating their reuse. Thus, the proposed framework provides an approach to third generation data mining: integration of distributed, heterogeneous data and knowledge resources and software into a coherent and effective knowledge discovery process. The abilities of the prototype implementation have been demonstrated on a text mining use case featuring publicly available data repositories, specialized algorithms, and third-party data analysis tools.  2010 Springer-Verlag.",
      "title": "16196 Workflow construction for service-oriented knowledge discovery",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650301504&partnerID=40&md5=d5caf289c57d43066c434d659a5fc923"
    },
    {
      "abstract": "A novel text classification approach is proposed in this paper based on deep belief network. Deep belief network constructs a deep architecture to obtain the high level abstraction of input data, which can be used to model the semantic correlation among words of documents. After basic features are selected by statistical feature selection measures, a deep belief network with discriminative fine tuning strategy is built on basic features to learn high level deep features. A support vector machine is then trained on the learned deep features. The proposed method outperforms traditional classifier based on support vector machine. As a dimension reduction strategy, the deep belief network also outperforms the traditional latent semantic indexing method. Detailed experiments are also made to show the effect of different fine tuning strategies and network structures on the performance of deep belief network.  2010 Springer-Verlag.",
      "title": "16197 A novel text classification approach based on deep belief network",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650190925&partnerID=40&md5=b2dbbb039be8ff3e95d20c4ee8d40441"
    },
    {
      "abstract": "This paper addresses the process of the ontology extension for a selected domain of interest which is defined by keywords and a glossary of relevant terms with descriptions. A new methodology for semiautomatic ontology extension, aggregating the elements of text mining and user-dialog approaches for ontology extension, is proposed and evaluated. We conduct a set of ranking, tagging and illustrative question answering experiments using Cyc ontology and business news collection. We evaluate the importance of using the textual content and structure of the ontology concept in the process of ontology extension. The experiments show that the best results are obtained with giving more to weight to ontology concept content and less weight to ontology concept structure.",
      "title": "16198 Ontology extension towards analysis of business news",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650158584&partnerID=40&md5=39edc6bd5251ad16005318993022838d"
    },
    {
      "abstract": "Presented is a multiple, Intelligent Information Agent (I2A) architecture for real-time, adaptive, need-to-know, authorization of access to confidential/classified information. The multi-agent system is based on the ELYSE cognitive neural, intelligent agent framework and provides need-to-know context-based authorization of requests for access to confidential/classified information. Need-to-know authorization is that which grants access to confidential/classified information only if that information is necessary for the requestors task, hased on their roles and credentials. In this system, authorization is treated as a text classification problem utilizing fuzzy-neural, self organizing semantic maps which learn a learn decision criteria hased on label information and are capable of generalizing this learned behavior to other information with a zero, or near-zero, false alarm rate. Since need-to-know authorizations must he determined for multiple tasks, multiple users, and multiple collections of information, with quick turn-around from request to delivery, the authorization agents must he adaptive and capahle of learning new profiles rapidly and with little impact on the overall system performance. We define five different classification methods and provide an architectural framework for the agent system. Copyright  2010 by the American Institute of Aeronautics and Astronautics, Inc.",
      "title": "16200 Architectures for dynamic, real-time need-to-know authorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649990559&partnerID=40&md5=c4435d2e0191e2f9b0452fd0f0ec1cde"
    },
    {
      "abstract": "The recent turmoil in the financial markets has demonstrated the growing need for automated information monitoring tools that can help to identify the issues and patterns that matter and that can track and predict emerging events in business and economic processes. One of the techniques that can address this need is sentiment mining. Existing approaches enable the analysis of a large number of text documents, mainly based on their statistical properties and possibly combined with numeric data. Most approaches are limited to simple word counts and largely ignore semantic and structural aspects of content. Yet, argumentation plays an important role in expressing and promoting an opinion. Therefore, we propose a framework that allows the incorporation of information on argumentation structure in the models for economic sentiment discovery in text.  2010 Springer-Verlag.",
      "title": "16201 Mining economic sentiment using argumentation structures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649976053&partnerID=40&md5=2496e3b54553ac36bbeee51dcd1b7352"
    },
    {
      "abstract": "Keyphrase extraction is a fundamental research task in natural language processing and text mining. A limitation of previous keyphrase extraction methods based on semantic analysis is that the acquisition of the semantic features within phrases is restricted by the constructed thesaurus and language. An approach to the acquisition of the semantic features within phrases from a single document is proposed in this paper, which is used to extract document keyphrases. Semantic relatedness degrees between phrases are computed using word co-occurrence information in the document, and the document is represented as a relatedness graph. Keyphrases are extracted based on the semantic relatedness features acquired from the graph. Our experiments demonstrate that the proposed keyphrase extraction method always outperforms the baseline methods TFIDF and Kea. Furthermore, our approach is not domain-specific and the method generalizes well when it is trained on one domain (journal articles) and tested on another (news web pages).  2010 IEEE.",
      "title": "16203 Keyphrase extraction based on semantic relatedness",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649886023&partnerID=40&md5=2730213e416dfa061b08412d57ca8ce5"
    },
    {
      "abstract": "The essential abilities of text knowledge representation, such as automatic construction, carrying abundant semantics and flexible reasoning, should be held due to the rapid growth of web resources and the requirements of the reasoning-based web services. However, current text knowledge representation models either lose many textual semantics or cannot be constructed automatically. To solve the above issues, text knowledge representation model is proposed based on the concept algebra of human concept learning. Then, the degree-2 power series hypothesis is developed and the reasoning ability of text representation is proposed. Finally, the results compared with current knowledge representation models show that our model performs better than other models in representing text knowledge.  2010 IEEE.",
      "title": "16205 Text knowledge representation model based on human concept learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649805086&partnerID=40&md5=1c94159d0d4da979052e30efa121b512"
    },
    {
      "abstract": "Text mining is an effective means of acquiring potentially useful knowledge from text document. However, traditional text mining cannot achieve high accuracy, because it cannot effectively make use of the semantic information of the text. Ontology provides theoretical basis and technical support for semantic information representation and organization. This paper introduces and analyzes text mining based on domain ontology.  2010 IEEE.",
      "title": "16207 Text mining based on domain ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649641132&partnerID=40&md5=f802b2b224be373b707915a59a29ac4c"
    },
    {
      "abstract": "Firstly, based on the needs of supply-chain management in the environment of E-business, this paper analyzed problem existed in present search engine for apparel-goods and proposed a new kind of distributed search engine for apparel goods based on semantic web services, and then discussed its architecture. Secondly, this paper introduced apparel ontology design model and the representation method that based on Ontology Web Language (OWL). Thirdly, this paper analyzed basic functions of apparel search engine and Web Services (WS) synthesizing in distributed environment. At last, through the theoretical Analysis and the designed prototype, it showed that this kind of search engine, based on apparel semantic tree, was more efficient apparently than full-text search engine when searching with multi-keywords. (Abstract)  2010 IEEE.",
      "title": "16208 Design of a semantic search engine system for apparel",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649660547&partnerID=40&md5=f6da346db9268c2cedf0c3bb74ce6a0c"
    },
    {
      "abstract": "Semantic orientation estimating for Chinese vocabulary has been widely used in text classification, automatic summarization and text filtering, although it is difficult for estimating for Chinese vocabulary, as the rich semantic meaning of Chinese vocabulary. The HowNet is a elaborate semantic repository that is helpful for estimating for Chinese vocabulary. Ihis paper proposes using semantic orientation of sememe of the HowNet to estimating semantic orientation of vocabulary. The experiment proves that the method is effective, and it is superior to the method based on the paradigm vocabulary.  2010 IEEE.",
      "title": "16210 Sememe description based estimating for semantic orientation of Chinese vocabulary",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649595740&partnerID=40&md5=79b2d54f7836fff316f2e6e2e31e92d7"
    },
    {
      "abstract": "Feather selection is a process that extracts a number of feature subsets which are the most representative of the original meaning from original feature set. It greatly reduces the text processing time and increases the accuracy because of removing some data outliers. With the rapid development of Web 2.0 and the further evolution of the Internet, short text like micro-blog plays an important role in peoples daily life. However, existing feature selection methods cannot effectively extract these short text features, and greatly reduce the classification and clustering performance of short text. In this regard, we propose a novel feature selection method based on part-of-speech and HowNet. According to the composition of the text property, we choose the words with larger amount of information by different part-of-speech, and then expand the semantic features of these words based on HowNet, in this way the short text has more useful features. We use test data set collected from sina micro-blog and adopt the micro average and macro average of F1-Measure to evaluate the effects of short text classification. The results show that the short text feature selected by our method has a good amount of information, as well as good classification results. 2010 IEEE.",
      "title": "16215 Short text feature selection for micro-blog mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951596621&partnerID=40&md5=fc88ededefaeaad3d45172e6593ba836"
    },
    {
      "abstract": "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). Both of these variants start with a basic model of documents (PCA and PLSA). Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best.  2010 Association for Computational Linguistics.",
      "title": "16217 Translingual document representations from discriminative projections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053251993&partnerID=40&md5=29678537b54ab8a69aa16aa296281bba"
    },
    {
      "abstract": "Owing to extensive knowledge management activities are promoted in various enterprises. How to enhance the application of knowledge management systems (KMS) becomes a critical issue. One of the enhanced applications of KMS is the SOS system, which aims at sharing knowledge for solving emerging problems. While it takes time for experienced employees to share their knowledge via the SOS system, it is advantageous to introduce an Automatic Problem Answering (APA) mechanism. When an emerging problem is issued, the APA mechanism will find actively suitable knowledge from the Intellectual Asset Repository (IAR), which consists of various sources of knowledge. In this paper, a semantic segmentation method is proposed and is employed in building a semantic segmentation module (SSM) in APA. SSM will automatically extract the knowledge corpuses embedded in documents. The extracted knowledge corpuses will be reused in APA to solve emergent problems and thus the application of KMS is enhanced.",
      "title": "16218 A pilot study on enhancing the application of knowledge management systems using semantic segmentation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863740797&partnerID=40&md5=3d54f4e93185c506ac3b07e0926e08bf"
    },
    {
      "abstract": "This research is to propose an IR related technique, the incremental aspect model (ISM), which not only uncovers latent aspects from the collected documents but also adapts the aspect model on streaming documents chronologically. ISM includes two stages: in Stage I, probabilistic latent semantic indexing (PLSI) technique is used to build a primary aspect model",
      "title": "16219 Incremental learning of aspect model on streaming documents",
      "url": ""
    },
    {
      "abstract": "The passage from a read-only to a read-write Web gave people the possibility to freely interact, share and collaborate through social networks, online communities, blogs, wikis and other online collaborative media. The democracy of the Web is what made it so popular in the past decades but such a high degree of freedom of expression also gave birth to negative side effects The so called dark side of the Web. An example of this is trolling i.e. the exploitation of the anonymity of the Web to post inflammatory and outrageous messages directed to one specific person or community to provoke them into a desired emotional response. Online community masters usually warn users against trolls with messages such as DNFTT (Do Not Feed The Trolls) but so far this has not been enough to stop trolls trolling. The aim of this work is to use Sentic Computing, a new paradigm for the affective analysis of natural language text, to detect trolls and hence prevent web-users from being emotionally hurt by malicious posts.",
      "title": "16220 Do not feel the trolls",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890675752&partnerID=40&md5=3caf7aaa86cf17a9612af5eb7a2cd562"
    },
    {
      "abstract": "This paper describes an application for computing first-order semantic representations of English texts. It is based on a combination of hybrid shallow-deep components arranged within the middleware framework Heart of Gold. The shallowdeep semantic analysis employs Robust Minimal Recursion Semantics (RMRS) as a common semantic underspecification formalism for natural language processing components. In order to compute efficiently first-order representations of the input text, the intermediate RMRS results of the shallow-deep analysis are transformed into the dominance constraints formalism and resolved by the underspecification resolver UTool. First-order expressions can serve as a formal knowledge representation of natural text and thus can be utilized in knowledge engineering or textual reasoning. At the end of this paper, we describe their application for recognizing textual entailment.  2010 IEEE.",
      "title": "16221 Generation of first-order expressions from a broad coverage HPSG grammar",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79551549751&partnerID=40&md5=e555dc7e992f6a9bc6e846fdd570d452"
    },
    {
      "abstract": "We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.  2010 Association for Computational Linguistics.",
      "title": "16222 Bootstrapping semantic analyzers from non-contradictory texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859986522&partnerID=40&md5=bd22bb887dcbfcea2c7c5d5365eddf98"
    },
    {
      "abstract": "Systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. However, in the recent years, it becomes evident that one of the most important directions of improvement in natural language processing (NLP) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. While in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the Semantic Web made available a large amount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitutes a valuable source of semantics. However, web semantics cannot be easily plugged into machine learning systems. Therefore the objective of this paper is to define a reference methodology for combining semantic information available in the web under the form of logical theories, with statistical methods for NLP. The major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from Semantic Web sources with semantics in the text. In order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the standard dataset, how the injection of knowledge leads to the improvement of this task performance.  2010 Springer-Verlag.",
      "title": "16223 Supporting natural language processing with background knowledge: Coreference resolution case",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650920821&partnerID=40&md5=831bc6d5079b7807b16fc11ef6a06b82"
    },
    {
      "abstract": "Access to reliable data from electronic health records is of high importance in several key areas in patient care, biomedical research, and education. However, many of the clinical entities are negated in the patient record text. Detecting what is a negation and what is not is therefore a key to high quality text mining. In this study we used the NegEx system adapted for Swedish to investigate negated clinical entities. We applied the system to a subset of free-text entries under a heading containing the word assessment from the Stockholm EPR corpus, containing in total 23,171,559 tokens. Specifically, the explored entities were the SNOMED CT terms having the semantic categories finding or disorder. The study showed that the proportion of negated clinical entities was around 9%. The results thus support that negations are abundant in clinical text and hence negation detection is vital for high quality text mining in the medical domain.",
      "title": "16224 Retrieving disorders and findings: Results using SNOMED CT and negex adapted for swedish",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890865081&partnerID=40&md5=beb4075c258b563aee93f984a993117f"
    },
    {
      "abstract": "In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline.  2010 Association for Computational Linguistics.",
      "title": "16227 Automatic detection and classification of social events",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053241535&partnerID=40&md5=799aac5e7c9468abaddeba3ea64ec4ff"
    },
    {
      "abstract": "This paper analyzes and evaluates, in the context of Ontology learning, some techniques to identify and extract candidate terms to classes of a taxonomy. Besides, this work points out some inconsistencies that may be occurring in the preprocessing of text corpus, and proposes techniques to obtain good terms candidate to classes of a taxonomy.  2010 Springer-Verlag.",
      "title": "16229 Analysis and evaluation of techniques for the extraction of classes in the ontology learning process",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79551549015&partnerID=40&md5=d0cd77d22fdb3798f7c40cc09fca80dc"
    },
    {
      "abstract": "Bursty features in text streams are very useful in many text mining applications. Most existing studies detect bursty features based purely on term frequency changes without taking into account the semantic contexts of terms, and as a result the detected bursty features may not always be interesting or easy to interpret. In this paper we propose to model the contexts of bursty features using a language modeling approach. We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of a stream of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features.  2010 ACM.",
      "title": "16230 Context modeling for ranking and tagging bursty features in text streams",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651326328&partnerID=40&md5=274be767280a53669af7f53f7bbc15c5"
    },
    {
      "abstract": "In this paper, we investigate the text classification and regression problems: given a corpus of text documents as training, each of which has a response label, the task is to train a predictor for predicting its response of any given document. In previous work, many researchers decompose this task into two separate steps: they first use a generative latent topic model to learn low-dimensional semantic representations of documents",
      "title": "16231 Supervising latent topic model for maximum-margin text classification and regression",
      "url": "Conference Paper"
    },
    {
      "abstract": "In this research, we propose the similarity matrix based version of NTSO as the approach to the text clustering. For using one of traditional approaches to text clustering, documents should be encoded into numerical vectors",
      "title": "16232 Using semantic similarity matrix for defining operations involved in NTSO for clustering 20NewsGroups",
      "url": ""
    },
    {
      "abstract": "In traditional Vector Space Model (VSM) the TF*IDF method is widely used to adjust the weight of terms in text mining. However TF*IDF can not represent the semantic information of text by neglecting the semantic relevance between terms. In this paper, an improved ontology-based VSM is presented, in which the ontology-based term similarity is used to readjust the weight of semantically related terms. The experimental results show that the improved VSM can perform more accurately than the traditional VSM in the calculation of the term weights. 2010 IEEE.",
      "title": "16233 Research on ontology-based text representation of vector space model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651332812&partnerID=40&md5=1c38983fe727816a7a9df63066d6225f"
    },
    {
      "abstract": "Latent Dirichlet allocation (LDA) has been widely used for analyzing large text corpora. In this paper we propose the topic-weak-correlated LDA (TWC-LDA) for topic modeling, which constrains different topics to be weak-correlated. This is technically achieved by placing a special prior over the topic-word distributions. Reducing the overlapping between the topic-word distributions makes the learned topics more interpretable in the sense that each topic word-distribution can be clearly associated to a distinctive semantic meaning. Experimental results on both synthetic and real-world corpus show the superiority of the TWC-LDA over the basic LDA for semantically meaningful topic discovery and document classification. 2010 IEEE.",
      "title": "16235 Topic-weak-correlated latent dirichlet allocation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79851478468&partnerID=40&md5=cbe8f12229b57945a66d8c2fd46a1c12"
    },
    {
      "abstract": "It is well known that synonymous and polysemous terms often bring in some noises when calculating the similarity between documents. Existing ontology-based document representation methods are static, hence, the chosen semantic concept set for representing a document has a fixed resolution and it is not adaptable to the characteristics of a document collection and the text mining problem in hand. We propose an Adaptive Concept Resolution (ACR) model to overcome this issue. ACR can learn a concept border from an ontology taking into consideration of the characteristics of a particular document collection. Then this border can provide a tailor-made semantic concept representation for a document coming from the same domain. Another advantage of ACR is that it is applicable in both classification task where the groups are given in the training document set, and clustering task where no group information is available. Furthermore, the result of this model is not sensitive to the model parameter. The experimental results show that ACR outperforms an existing static method significantly.  2010 ACM.",
      "title": "16239 Learning ontology resolution for document representation and its applications in text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651284866&partnerID=40&md5=061c03f8598132c8ee81dea14fc67320"
    },
    {
      "abstract": "There is a substantial body of work on the extraction of relations from texts, most of which is based on pattern matching or on applying tree kernel functions to syntactic structures. Whereas pattern application is usually more efficient, tree kernels can be superior when assessed by the F-measure. In this paper, we introduce a hybrid approach to extracting meronymy relations, which is based on both patterns and kernel functions. In a first step, meronymy relation hypotheses are extracted from a text corpus by applying patterns. In a second step these relation hypotheses are validated by using several shallow features and a graph kernel approach. In contrast to other meronymy extraction and validation methods which are based on surface or syntactic representations we use a purely semantic approach based on semantic networks. This involves analyzing each sentence of the Wikipedia corpus by a deep syntactico-semantic parser and converting it into a semantic network. Meronymy relation hypotheses are extracted from the semantic networks by means of an automated theorem prover, which employs a set of logical axioms and patterns in the form of semantic networks. The meronymy candidates are then validated by means of a graph kernel approach based on common walks. The evaluation shows that this method achieves considerably higher accuracy, recall, and F-measure than a method using purely shallow validation.  2010 IEEE.",
      "title": "16240 Validating meronymy hypotheses with support vector machines and graph kernels",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952436079&partnerID=40&md5=cef8e55d8c1b2afdad61e8111c5bf904"
    },
    {
      "abstract": "While a significant amount of research has been devoted to textual entailment, automated entailment from conversational scripts has received less attention. To address this limitation, this paper investigates the problem of conversation entailment: automated inference of hypotheses from conversation scripts. We examine two levels of semantic representations: a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations.  2010 Association for Computational Linguistics.",
      "title": "16242 Towards conversation entailment: An empirical investigation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053260040&partnerID=40&md5=28ead2a7b73438284ec18169747b4d7d"
    },
    {
      "abstract": "Monitoring epidemic crises, caused by rapid spread of infectious animal diseases, can be facilitated by the plethora of information about disease-related events that is available online. Therefore, the ability to use this information to perform domain-specific entity recognition and event-related sentence classification, which in turn can support time and space visualization of automatically extracted events, is highly desirable. Towards this goal, we present a rule-based approach to the problem of extracting animal disease-related events from web documents. Our approach relies on the recognition of structured entity tuples, consisting of attributes, which describe events related to animal diseases. The event attributes that we consider include animal diseases, dates, species and geo-referenced locations. We perform disease names and species recognition using an automatically- constructed ontology, dates are extracted using regular expressions, while location are extracted using a conditional random fields tool. The extracted events are further classified as confirmed or suspected based on semantic features, obtained from the e.g., GoogleSets1 and WordNet2. Our preliminary results demonstrate the feasibility of the proposed approach.",
      "title": "16246 Animal disease event recognition and classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888376446&partnerID=40&md5=cf9637f704ab4bf43131da2ee1eff9cc"
    },
    {
      "abstract": "Today millions of web-users express their opinions about many topics through blogs, wikis, fora, chats and social networks. For sectors such as e-commerce and e-tourism, it is very useful to automatically analyze the huge amount of social information available on the Web, but the extremely unstructured nature of these contents makes it a difficult task. SenticNet is a publicly available resource for opinion mining built exploiting AI and Semantic Web techniques. It uses dimensionality reduction to infer the polarity of common sense concepts and hence provide a public resource for mining opinions from natural language text at a semantic, rather than just syntactic, level. Copyright  2010, Association for the Advancement of Artificial Intelligence. All rights reserved.",
      "title": "16247 SenticNet: A publicly available semantic resource for opinion mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960150095&partnerID=40&md5=45ba607d5189f25b05e90a0dc02bca23"
    },
    {
      "abstract": "Web 2.0 has brought a lot of good things for internet audience. Interactive user generated contents like blogs, become a popular means to voice ones opinion related to anything. The blogosphere contains a lot of users generated contents, a growing readerships and ever increasing thoughts followers. Identifying influential bloggers is a recently introduced phenomenon",
      "title": "16249 Identifying influential bloggers using blogs semantics",
      "url": "Conference Paper"
    },
    {
      "abstract": "A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using verb, noun, and verb prep lexico-syntactic patterns. Humanbased evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge.  2010 Association for Computational Linguistics.",
      "title": "16251 Learning arguments and supertypes of semantic relations using recursive patterns",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859968040&partnerID=40&md5=280b6fcaecd90724cb537165971495b1"
    },
    {
      "abstract": "Social networks are a hugely popular part of the Internet. Not only do they offer a convenient way for like-minded people to get in contact with each other, they also represent a valuable knowledge resource. For instance, people publish personal images with own descriptive attributes, or they annotate music while listening to it. These attributes can be of general or rather specific nature, assigned according to the knowledge of the tagging person. Due to the fact that many people annotate the same items with topically related words, often a common agreement regarding their semantic categorization is achieved. This approach is often referred to as building a folksonomy, a portmanteau of the words folk and taxonomy. Unlike this term suggests, a taxonomy cannot be explicitly obtained by simply analyzing the tags of a set of items. This paper presents a method to automatically extract taxonomies from social networks by analyzing co-occurring terms assigned to items and calculating their semantic hierarchy, more precisely by determining the super- and subclasses of terms from a specific tag set based on conditional relative frequencies. Furthermore, practical benefits derived from this method, e.g. to calculate similarities of maybe sparsely or completely differently annotated items, are discussed as well. Additionally, these techniques can also be used to detect inconsistencies among annotations of a specific item.",
      "title": "16254 Automatic taxonomy extraction through mining social networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84871367356&partnerID=40&md5=a73e5f2881e994c5654f53c451276d10"
    },
    {
      "abstract": "This paper presents a novel technique of document clustering based on frequent concepts. The proposed FCDC (Frequent Concepts based Document Clustering), a clustering algorithm works with frequent concepts rather than frequent itemsets used in traditional text mining techniques. Many well known clustering algorithms deal with documents as bag of words while they ignore the important relationship between words like synonym relationship. The proposed algorithm utilizes the semantic relationship between words to create concepts. It exploits the WordNet ontology in turn to create low dimensional feature vector which allows developing a more accurate clustering algorithm.  2010 IEEE.",
      "title": "16255 Text document clustering based on frequent concepts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79551559521&partnerID=40&md5=d531d4caadba9204c7a7cdca40a56890"
    },
    {
      "abstract": "We present Luminoso, a tool that helps researchers to visualize and understand a dimensionality-reduced semantic space based on textual information by exploring it interactively. It streamlines the process of creating such a space by taking input from a directory of text documents, and optionally including common-sense background information. This interface is useful for interactively discovering trends in a text corpus, such as free-text responses to a survey. We discuss a case study about restaurant reviews to show how Luminoso can be used for opinion mining.  2010 ACM.",
      "title": "16256 Visualizing common sense connections with Luminoso",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80051560898&partnerID=40&md5=bba0a1ffe5b4d8da9e56ab228077b32d"
    },
    {
      "abstract": "In this paper, we study the combination of compression and l1-norm regularization in a machine learning context: learning compressible models. By including a compression operation into the l1 regularization, the assumption on model sparsity is relaxed to compressibility: model coefficients are compressed before being penalized, and sparsity is achieved in a compressed domain rather than the original space. We focus on the design of different compression operations, by which we can encode various compressibility assumptions and inductive biases, e.g., piecewise local smoothness, compacted energy in the frequency domain, and semantic correlation. We show that use of a compression operation provides an opportunity to leverage auxiliary information from various sources, e.g., domain knowledge, coding theories, unlabeled data. We conduct extensive experiments on brain-computer interfacing, handwritten character recognition and text classification. Empirical results show clear improvements in prediction performance by including compression in l1 regularization. We also analyze the learned model coefficients under appropriate compressibility assumptions, which further demonstrate the advantages of learning compressible models instead of sparse models. Copyright  by SIAM.",
      "title": "16257 Learning compressible models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80054063086&partnerID=40&md5=ab4de5ae42c598be7857b7383d053c7f"
    },
    {
      "abstract": "With the fast growing development of the Web, the adoption of ontologies to improve the exploitation of information resources, is already heralded as a promising model of representation. However, the relevance of information that they contain requires regular updating, and specifically, the addition of new knowledge. Recently, new research approaches were defined in order to automatically enrich ontology. Usually they extensively use either statistical models or experts to provide the relevance and placement of new concepts. Unfortunately these approaches suffer the following drawback: The detection of new elements and position in ontology are not automatically established, and lack of use of semantic or syntactic information to extract relations between concepts. In this paper, we present an approach for ontology enrichment based on two steps where a novel effective filtering step is utilized. First we extract the correlation between terms of a learning dataset using the generation of association rules. Second we retain the relevant new concepts using an extracted semantic information. The suggested approach was tested on an ontology of mechanical industry competencies. Experiments were performed on real data, which show the usefulness of our approach.  2010 IEEE.",
      "title": "16258 Enhanced semantic automatic ontology enrichment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79851491074&partnerID=40&md5=513a1085e5009b8fb506ee1e0aa586d3"
    },
    {
      "abstract": "The use of domain knowledge is generally found to improve query efficiency in content filtering applications. In particular, tangible benefits have been achieved when using knowledge-based approaches within more specialized fields, such as medical free texts or legal documents. However, the problem is that sources of domain knowledge are time consuming to build and equally costly to maintain. As a potential remedy, recent studies on Wikipedia suggest that this large body of socially constructed knowledge can be effectively harnessed to provide not only facts but also accurate information about semantic concept-similarities. This paper describes a ramework for document filtering, where Wikipedias concept relatedness information is combined with a domain ontology to produce semantic content classifiers. The approach is evaluated using Reuters RCV1 corpus and TREC-11 filtering task definitions. In a comparative study, the approach shows robust performance and appears to outperform content classifiers based on Support Vector Machines (SVM) and C4.5 algorithm.  2010 IEEE.",
      "title": "16259 Semantic content filtering with wikipedia and ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951760669&partnerID=40&md5=ae2c3697718bd4e2e2d357a333f6ac71"
    },
    {
      "abstract": "In this paper we describe our approach to the CoNLL-2010 shared task on detecting speculative language in biomedical text. We treat the detection of sentences containing uncertain information (Task1) as a token classification task since the existence or absence of cues determines the sentence label. We distinguish words that have speculative and non-speculative meaning by employing syntactic features as a proxy for their semantic content. In order to identify the scope of each cue (Task2), we learn a classifier that predicts whether each token of a sentence belongs to the scope of a given cue. The features in the classifier are based on the syntactic dependency path between the cue and the token. In both tasks, we use a Bayesian logistic regression classifier incorporating a sparsity-enforcing Laplace prior. Overall, the performance achieved is 85.21% F-score and 44.11% F-score in Task1 and Task2, respectively.  2010 The Association for Computational Linguistics.",
      "title": "16261 Detecting speculative language using syntactic dependencies and logistic regression",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960748455&partnerID=40&md5=8013e67ddb37feb279bb5261436ae013"
    },
    {
      "abstract": "This paper evaluates CONSPECT, a service that analyses states in a learners conceptual development. It combines two technologies - Latent Semantic Analysis to analyse text and Network Analysis (NA) to provide visualisations - into a technique called Meaningful Interaction Analysis (MIA). CONSPECT was designed to help both online learners and their tutors monitor their conceptual development. This paper reports on the validation experiments undertaken to determine how well LSA matches first year medical students in clustering concepts and in annotating text. The validation used several techniques, including card sorting and Likert scales. CONSPECT produces almost peer quality results and what remains to be tested is whether it improves with more advanced learners. One of the experiments showed an average 0.7 correlation between humans and CONSPECT. Copyright  2010 The Authors.",
      "title": "16262 Monitoring conceptual development with text mining technologies: CONSPECT",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79957464979&partnerID=40&md5=2c1b53f46936eecc402fd6b8ce770a56"
    },
    {
      "abstract": "Many applications generate a large volume of parallel document collections. A parallel document collection consists of two sets of documents where the documents in each set correspond to each other and form semantic pairs (e.g., pairs of problem and solution descriptions in a help-desk setting). Although much work has been done on text mining, little previous work has attempted to mine such a novel kind of text data. In this paper, we propose a new probabilistic topic model, called Probabilistic Topic Mapping (PTM) model, to mine parallel document collections to simultaneously discover latent topics in both sets of documents as well as the mapping of topics in one set to those in the other. We evaluate the PTM model on a parallel document collection in IT service domain. We show that PTM can effectively discover meaningful topics, as well as their mappings, and its also useful for improving text matching and retrieval when theres a vocabulary gap.  2010 ACM.",
      "title": "16266 PTM: Probabilistic topic mapping model for mining parallel document collections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651344392&partnerID=40&md5=60db6cbcea714c9672108b14d51f4609"
    },
    {
      "abstract": "This study proposes an emotion detection engine for real time Internet chatting applications. We adopt a Web-scale text mining approach that automates the categorization of affection state of daily events. We first accumulated a huge collection of real-life entities from Web that would participate in events with a user in the chatting room. Based on the common actions between each entity and the type of the user in a chatting room session, such as boy, girl, old man and so on, each collected entity was automatically classified into different affective categories such as pleasant, provoking, grievous, and scary. During a chatting session, each sentence is first parsed using semantic roles labeling techniques to retrieve the verb and object of the event embedded in the sentence. Based on a set of manually authored emotion generation rule, the system then assigns the emotion based on the verb and the affective categories of the object. Primitive evaluations show that the precision rate of the emotion detection engine is rather satisfactory for applications that distinguish emotions of Happiness, Sadness, Anger, and Fear.  2010 IEEE.",
      "title": "16267 Emotion sensing for Internet chatting: A Web mining approach for affective categorization of events",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951663168&partnerID=40&md5=1e9880bcb7000f9792522dcef71c6f0d"
    },
    {
      "abstract": "In this work, we investigate sentiment mining of Arabic text at both the sentence level and the document level. Existing research in Arabic sentiment mining remains very limited. For sentence-level classification, we investigate two approaches. The first is a novel grammatical approach that employs the use of a general structure for the Arabic sentence. The second approach is based on the semantic orientation of words and their corresponding frequencies, to do this we built an interactive learning semantic dictionary which stores the polarities of the roots of different words and identifies new polarities based on these roots. For document-level classification, we use sentences of known classes to classify whole documents, using a novel approach whereby documents are divided dynamically into chunks and classification is based on the semantic contributions of different chunks in the document. This dynamic chunking approach can also be investigated for sentiment mining in other languages. Finally, we propose a hierarchical classification scheme that uses the results of the sentence-level classifier as input to the document-level classifier, an approach which has not been investigated previously for Arabic documents. We also pinpoint the various challenges that are faced by sentiment mining for Arabic texts and propose suggestions for its development. We demonstrate promising results with our sentence-level approach, and our document-level experiments show, with high accuracy, that it is feasible to extract the sentiment of an Arabic document based on the classes of its sentences.  2010 IEEE.",
      "title": "16275 Sentence-level and document-level sentiment mining for arabic texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951749383&partnerID=40&md5=11da31d0c598ac63143a2675f887b019"
    },
    {
      "abstract": "Question classification is crucial for the automatically question answering. And Random Walk is a promising approach for semi-supervised learning problems of learning from labeled and unlabeled data. Given a set of points, some of them are labeled, and the remaining points are unlabeled, the goal is to predict the labels of the unlabeled points. Since labeling often requires expensive human labor, whereas unlabelled data is easier to obtain, semi-supervised learning is very useful in many real-world problems, such as text classification. Here we proposed an approach for Chinese question Classification using Multilevel Random Walk (MRK), which is an improvement of random walk. In this paper, we selected four kinds of features (words, pos, named entity, semantic) to present Chinese questions, and carried out experiments to validate the method on a large-scale real-world dataset. 2010 IEEE.",
      "title": "16276 Chinese question classification using multilevel random walk",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651331398&partnerID=40&md5=b782c7384028804ee2731ede07d0b19a"
    },
    {
      "abstract": "We present a topic mixture language modeling approach making use of the soft classification notion of topic models. Given a text document set, we first perform document soft classification by applying a topic modeling process such as probabilistic latent semantic analyses (PLSA) or latent Dirichlet allocation (LDA) on the dataset. Then we can derive topic-specific n-gram counts from the classified texts. Finally we build topic-specific n-gram language models (LM) from the n-gram counts using traditional n-gram modeling approach. In decoding we perform topic inference from the processing context, and we use unsupervised topic adaptation approach to combine the topic-specific models. Experimental results show that the suggested method outperforms the state-of-the-art topic-model-based unsupervised adaptation approaches. 2010 IEEE.",
      "title": "16278 Building topic mixture language models using the document soft classification notion of topic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79851480448&partnerID=40&md5=d56f5f5700f4263596dce576480047fd"
    },
    {
      "abstract": "Web is one of major information sources. Failure in proper management of knowledge leads to incorrect results returned by search engines. Therefore, the web should have an effective information retrieval system to improve the correctness of retrieval results. This study provides a method to assign a new document to the fittest category out of predefined categories, where latent semantic analysis (LSA) is used to evaluate each term in documents, the similarity between terms and documents as well as the one between terms and categories. The objective of our method is to fuse evidential reasoning method with LSA which can assign a new document to a predefined category. The method provides better results in performance of classification comparing to the fusion of an evidential reasoning approach with term frequency inverse document frequency (TFIDF). 2010 IEEE.",
      "title": "16279 An evidential reasoning based LSA approach to document classification for knowledge acquisition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78751683031&partnerID=40&md5=2d266cbc0819efe34d3ec04b0588f48e"
    },
    {
      "abstract": "Advances in biomedical technology and research have resulted in a large number of research findings, which are primarily published in unstructured text such as journal articles. Text mining techniques have been thus employed to extract knowledge from such data. In this article we focus on the task of identifying and extracting relations between bio-entities such as green tea and breast cancer. Unlike previous work that employs heuristics such as co-occurrence patterns and handcrafted syntactic rules, we propose a verb-centric algorithm. This algorithm identifies and extracts the main verb(s) in a sentence",
      "title": "16281 A verb-centric approach for relationship extraction in biomedical text",
      "url": ""
    },
    {
      "abstract": "In text categorization, one well-known document representation is bag-of-words. Although it is simple and popular, it ignores semantics, underlying linguistic information, and word correlations. In this paper, a new representation for text data is proposed which is called Bag-Of-Queries (BOQ). First, a taxonomy of the terms in the local vocabulary is extracted. Extracting a taxonomy is performed by learning term dependencies using an information theoretic inclusion index. Next, the taxonomy is partitioned to generate a set of correlated terms or bag of queries. Since every two partitions belong to different concepts, they are considered semantically orthogonal queries. This provides a new space of orthogonal features, which is necessary for an efficient categorization. Finally, instead of using terms as features, we use them to build a set of queries. Documents are ranked in response to the queries using a similarity measure. The similarity indices are considered as new features in a vector space model representation. The proposed approach outperforms bag of word based clustering. It also extracts new non-redundant features and at the same time reduces dimensionality. 2010 IEEE.",
      "title": "16284 Query-relevant document representation for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650932707&partnerID=40&md5=150e986b6c8f42875690813b523ee79e"
    },
    {
      "abstract": "This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynamically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.  2010 Association for Computational Linguistics.",
      "title": "16286 Inducing domain-specific semantic class taggers from (almost) nothing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84859956127&partnerID=40&md5=ceb8a7fd3ef26389d056b930af1d2cd3"
    },
    {
      "abstract": "This paper describes steps that have been taken to construct a development dataset for the task of Technology Structure Mining. We have defined the proposed task as the process of mapping a scientific corpus into a labeled digraph named a Technology Structure Graph as described in the paper. The generated graph expresses the domain semantics in terms of interdependencies between pairs of technologies that are named (introduced) in the target scientific corpus. The dataset comprises a set of sentences extracted from the ACL Anthology Corpus. Each sentence is annotated with at least two technologies in the domain of Human Language Technology and the interdependence between them. The annotations - technology mark-up and their interdependencies - are expressed at two layers: lexical and termino-conceptual. Lexical representation of technologies comprises varying lexicalizations of a technology. However, at the termino-conceptual layer all these lexical variations refer to the same concept. We have adopted the same approach for representing Semantic Relations",
      "title": "16287 Developing a dataset for technology structure mining",
      "url": "Conference Paper"
    },
    {
      "abstract": "In this paper we mine over 80 million twitter microblogs in order to explore whether data from the social media initiative known as Twitter can be used to identify sentiment about red wines. We test to see whether models derived from Twitter data can corroborate industry sales figures and we employ text analysis software developed to assess emotional, cognitive, and structural components of text to analyze the twitter dataset to harvest knowledge about consumer sentiment on different wine varietals. A multi-knowledge based approach is proposed using, Self-Organizing Maps and domain expertise in order to establish view the social network conversation. We show that it is possible to both confirm previously known knowledge and And novel information through the proposed methodology.  2010 IEEE.",
      "title": "16290 Harvesting consumer opinion and wine knowledge off the social media grape vine utilizing artificial neural networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952170637&partnerID=40&md5=7d50d97398a4802b725af66da3cb45f4"
    },
    {
      "abstract": "How to map from texts to structured case representations and how to automatically generate representations have become the research hotspot in the fields of Textual Case-based Reasoning (TCBR). This paper presents methods that support automatically generation ontology-based representation for textual cases. We used the Ontology to describe the relationship between terms in application fields. First, we defined a new formal description method for Ontology. And then we propose a representations model for textual cases and describe the method of automatic generation method of this Model. Finally, based on the proposed model, a marketing case representation and management system is established. A realworld case of marketing is also applied in order to verify the feasibility of the proposed model. The verification results show that the system is efficient for keeping semantic information. As a whole, this research provides a knowledge representation and the automatic generation approach to facilitate knowledge management to efficiently and accurately describe the contents of text knowledge. The proposed model can be applied in TCBR to enhance reuse of domain knowledge.  2010 IEEE.",
      "title": "16294 Research on ontology-based representation method for textual cases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651412819&partnerID=40&md5=9783480688fc9a59888b87c8d09d5450"
    },
    {
      "abstract": "Topic modeling has been widely applied in a variety of text modeling tasks as well as in speech recognition systems for effectively capturing the semantic and statistic information in documents or speech utterances. Most topic models rely on the bag-of-words assumption that results in learned latent topics composed of lists of individual words. Unfortunately, these words may convey topical information but lack accurate semantic knowledge of the text. In this paper, we present the semantic associative topic model, where the concept of the semantic association terms is extended to topic modeling, which provides guidance on modeling the semantic associations that occur among single words by expressing a document as an association of multiple words. Further, the pointwise KL-divergence metric is used to measure the significance of the association. We also integrate original PLSA and SATM models, which have mixed feature representations. Experimental results on WSJ and AP datasets show that the proposed approaches achieved higher performance compared to other methods. 2010 IEEE.",
      "title": "16295 Exploiting semantic associative information in topic modeling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951788384&partnerID=40&md5=55eb0892c56eee6b94faf0aaa6b10bd8"
    },
    {
      "abstract": "In this paper we are interested in discovering collaborative writing patterns in student data collected from a system we designed to support student collaborative writing, and which has been used by over 1,000 students in the past year. A particular functionality that we are investigating is the extraction and display to learners and teachers of the process followed during the course of the writing. We used a heuristic to derive semantic interpretation of specific sequences of raw data and Markov models (MM) to derive the processes. We propose two models, a Heuristic MM and a Hidden MM for analysing students writing behavior. We also refined the semantic preprocessing by adding the notion of pauses between activities. We illustrate our approach and compare these models using real data from two groups of high and low performance level and highlight the different information they each provide.  2010 IEEE.",
      "title": "16298 Analysis of collaborative writing processes using hidden markov models and semantic heuristics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951735774&partnerID=40&md5=4f06e06c458ac2c1750547249de42b25"
    },
    {
      "abstract": "Most pharmacogenomics knowledge is contained in the text of published studies, and is thus not available for automated computation. Natural Language Processing (NLP) techniques for extracting relationships in specific domains often rely on hand-built rules and domain-specific ontologies to achieve good performance. In a new and evolving field such as pharmacogenomics (PGx), rules and ontologies may not be available. Recent progress in syntactic NLP parsing in the context of a large corpus of pharmacogenomics text provides new opportunities for automated relationship extraction. We describe an ontology of PGx relationships built starting from a lexicon of key pharmacogenomic entities and a syntactic parse of more than 87 million sentences from 17 million MEDLINE abstracts. We used the syntactic structure of PGx statements to systematically extract commonly occurring relationships and to map them to a common schema. Our extracted relationships have a 70-87.7% precision and involve not only key PGx entities such as genes, drugs, and phenotypes (e.g., VKORC1, warfarin, clotting disorder), but also critical entities that are frequently modified by these key entities (e.g., VKORC1 polymorphism, warfarin response, clotting disorder treatment). The result of our analysis is a network of 40,000 relationships between more than 200 entity types with clear semantics. This network is used to guide the curation of PGx knowledge and provide a computable resource for knowledge discovery.  2010 Elsevier Inc.",
      "title": "16301 Using text to build semantic networks for pharmacogenomics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149308924&partnerID=40&md5=62e2562fbe01ed03b396f0950e62bf8f"
    },
    {
      "abstract": "A Priori knowledge is the knowledge an agent has gained prior to experiences and learning. A priori knowledge acquisition along with relevant functional ontology building remain obstacles in the process of building knowledge-based agent-oriented systems. In this paper, we describe epistemic analysis techniques that we are exploring at Ctest Laboratories that arc used to automatically discover ontological artifacts within the transcripts of interrogative domains. These ontological artifacts are then used as the fundamental basis of a priori knowledge spaces for epistemic agent-oriented systems. We are using ROGUE (Real-time Ontology Generation Using Epistemic Agents) to perform the transcript and text mining. ROGUE is a multi-agent system under development at Ctest Labs.  2010 IEEE.",
      "title": "16302 Building a priori knowledge spaces from interrogative domains",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952051782&partnerID=40&md5=26a27bad7555dd4db04e0d6568b1f408"
    },
    {
      "abstract": "The analysis of sensible data requires a proper anonymization of values in order to preserve the privacy of individuals. Information loss should be minimized during the masking process in order to enable a proper exploitation of data. Even though several masking methods have been designed for numerical data, very few of them deal with categorical (textual) information. In this case, the quality of the anonymized dataset is closely related to the preservation of semantics, a dimension which is commonly neglected of shallowly considered in related words. In this paper, a new masking method for unbounded categorical attributes is proposed. It relies on the knowledge modeled in ontologies in order to semantically interpret the input data and perform data transformations aiming to minimize the loss of semantic content. On the contrary to exhaustive methods based on simple hierarchical structures, our approach relies on a set of heuristics in order to guide and optimize the masking process, ensuring its scalability when dealing with big and heterogenous datasets and wide ontologies. The evaluation performed over real textual data suggests that our method is able to produce anonymized datasets which significantly preserve data semantics in comparison to apporaches based on data distribution metrics.  Springer-Verlag 2010.",
      "title": "16303 Ontology-based anonymization of categorical values",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952262010&partnerID=40&md5=7bdd300816d33237f0fc81115781a232"
    },
    {
      "abstract": "We propose an event-enriched model to alleviate the semantic deficiency problem in the IR-style text processing and apply it to sentence ordering for multi-document news summarization. The ordering algorithm is built on event and entity coherence, both locally and globally. To accommodate the eventenriched model, a novel LSA-integrated two-layered clustering approach is adopted. The experimental result shows clear advantage of our model over event-agonistic models.",
      "title": "16306 Sentence ordering with event-enriched semantics and two-layered clustering for multi-document news summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053429115&partnerID=40&md5=017b433f8f134ab392ca0eaa58cdeb8b"
    },
    {
      "abstract": "In Latent Semantic Indexing (LSI), a collection of documents is often pre-processed to form a sparse term-document matrix, followed by a computation of a low-rank approximation to the data matrix. A multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term-document matrix representing the data. The main goal is to reduce the cost of the matrix approximation without sacrificing accuracy. Because coarsening by multilevel hypergraph techniques is a form of clustering, the proposed approach can be regarded as a hybrid of factorization-based LSI and clustering-based LSI. Experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost.  2010 ACM.",
      "title": "16308 Hypergraph-based multilevel matrix approximation for text information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651295428&partnerID=40&md5=e115483e704301d22d37968d26fdee6e"
    },
    {
      "abstract": "Most web text clustering is based on the space vector text representation model. This results in a high dimension in the terms",
      "title": "16309 Web text clustering based on concept lattice",
      "url": ""
    },
    {
      "abstract": "Unsupervised word sense discrimination relies on the idea that words that occur in similar contexts will have similar meanings. These techniques cluster multiple contexts in which an ambiguous word occurs, and the number of clusters discovered indicates the number of senses in which the ambiguous word is used. One important distinction among these methods is the underlying means of representing the contexts to be clustered. This paper compares the efficacy of first-order methods that directly represent the features that occur in a context with several second-order methods that use a more indirect representation. The experiments in this paper show that second order methods that use word by word co-occurrence matrices result in the highest accuracy and most robust word sense discrimination. These experiments were conducted on MedLine abstracts that contained pseudo - words created by conflating together pairs of MeSH preferred terms to create new ambiguous words. The experiments were carried out with SenseClusters, a freely available open source software package.  2010 ACM.",
      "title": "16311 The effect of different context representations on word sense discrimination in biomedical texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650949174&partnerID=40&md5=acb9a6b55a6032b19db8df35f27eddd5"
    },
    {
      "abstract": "The distributed topic maps often need be merged when they are used for knowledge representation, the similarity calculation of two topics is a critical factor which affects the quality of final topic maps directly. In this paper, we present a novel approach to calculate the similarity of topics and merge the distributed topic maps, the method not only implements the syntax comparison between the topics, but constructs a domain-specific dictionary to resolve the low precision of topic semantic similarity calculation using the common dictionary purely, the massive texts are gathered form Wikipedia and Google snippets as corpus, on which the similarity score of the specific terms is calculated and stored to dictionary by a semantic text comparison method. The experiment indicates the new method can resolve particularly the problems of the common dictionary lacking many technical terms.  2010 IEEE.",
      "title": "16312 Merging of topic maps based on corpus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952214179&partnerID=40&md5=c0e34d5436e7db63de4956228bb6f33d"
    },
    {
      "abstract": "Open-class semantic lexicon induction is of great interest for current knowledge harvesting algorithms. We propose a general framework that uses patterns in bootstrapping fashion to learn open-class semantic lexicons for different kinds of relations. These patterns require seeds. To estimate the goodness (the potential yield) of new seeds, we introduce a regression model that considers the connectivity behavior of the seed during bootstrapping. The generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when predicting the goodness of seeds.  2010 Association for Computational Linguistics.",
      "title": "16313 Not all seeds are equal: Measuring the quality of text mining seeds",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053271784&partnerID=40&md5=179c9f69b1c3d8976b5e61f9891d9cc6"
    },
    {
      "abstract": "Ontologies have been widely accepted as the most advanced knowledge representation model. This paper introduces PsychoNet, a new knowledgebase that forms the link between psycholinguistic taxonomy, existing in LIWC, and its semantic textual representation in the form of commonsense semantic ontology, represented by ConceptNet. The integration of LIWC and ConceptNet and the added functionalities facilitate employing ConceptNet in psycholinguistic studies. Furthermore, it simplifies utilization of the huge network of ConceptNet for a specific multimedia application based on key category(ies) from LIWC, such as visual or biological applications. PsychoNet adds a new layer of complementary psycholinguistic functions to the original semantic network. Moreover, learning, either clustering or classification, is more applicable in the developed ontology. The paper shows a sample application of text classification for mood prediction task. The result confirms the validity of the proposed network as PsychoNet outperforms LIWC in mood prediction.",
      "title": "16314 Psychonet - A psycholinguistc commonsense ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651424758&partnerID=40&md5=7f177c11e8779b689867b1fd11715bba"
    },
    {
      "abstract": "Using the superiority of the genetic algorithms solving nonlinear problems which is applied to feature extraction on Text Categorization is proposed in this paper. A synonym problem in text is fully considered by this method during the feature extraction processing, and is processed using the concept of fuzzy set. The membership of synonymous as the fitness function of Genetic Algorithm is carried out by evolutionary computation. Comparison test results show that this method not only can reduce the dimension of feature, but also can improve accuracy ration and recall ratio of classification, the overall performance of the classification system is improved finally, the system is achieved a higher level of automation and the strong portability.  2010 IEEE.",
      "title": "16316 Research on feature extraction based on genetic algorithm in text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951594360&partnerID=40&md5=b15241b1d1b26002682952f21fa79bbc"
    },
    {
      "abstract": "Opinion mining focuses on extracting customers opinions from the reviews and predicting their sentiment orientation. Reviewers usually praise a product in some aspects and bemoan it in other aspects. With the business globalization, it is very important for enterprises to extract the opinions toward different aspects and find out cross-lingual/cross-culture difference in opinions. Cross-lingual opinion mining is a very challenging task as amounts of opinions are written in different languages, and not well structured. Since people usually use different words to describe the same aspect in the reviews, product-feature (PF) categorization becomes very critical in cross-lingual opinion mining. Manual cross-lingual PF categorization is time consuming, and practically infeasible for the massive amount of data written in different languages. In order to effectively find out cross-lingual difference in opinions, we present an aspect-oriented opinion mining method with Cross-lingual Latent Semantic Association (CLaSA). We first construct CLaSA model to learn the cross-lingual latent semantic association among all the PFs from multi-dimension semantic clues in the review corpus. Then we employ CLaSA model to categorize all the multilingual PFs into semantic aspects, and summarize cross-lingual difference in opinions towards different aspects. Experimental results show that our method achieves better performance compared with the existing approaches. With CLaSA model, our text mining system OpinionIt can effectively discover cross-lingual difference in opinions.  2010 ACM.",
      "title": "16317 OpinionIt: A text mining system for cross-lingual opinion analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651345667&partnerID=40&md5=8292e2a9733f8b95453586d3a3038e68"
    },
    {
      "abstract": "Lifelog systems, inspired by Vannevar Bushs concept of MEMory EXtenders (MEMEX), are capable of storing a persons lifetime experience as a multimedia database. Despite such systems huge potential for improving peoples everyday life, there are major challenges that need to be addressed to make such systems practical. One of them is how to index the inherently large and heterogeneous lifelog data so that a person can efficiently retrieve the log segments that are of interest. In this paper, we present a novel approach to indexing lifelogs using activity language. By quantizing the heterogeneous high dimensional sensory data into text representation, we are able to apply statistical natural language processing techniques to index, recognize, segment, cluster, retrieve, and infer high-level semantic meanings of the collected lifelogs. Based on this indexing approach, our lifelog system supports easy retrieval of log segments representing past similar activities and generation of salient summaries serving as overviews of segments.  2010 ACM.",
      "title": "16318 A language-based approach to indexing heterogeneous multimedia lifelog",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650933202&partnerID=40&md5=1ce489b4561be7972af089c596b1bca4"
    },
    {
      "abstract": "A document search in PubMed is certainly one of the most exhaustive ways for finding information related to any biological or biomedical topic. However, a keyword search in this database that is not specific enough will provide a number of results that exceeds by far an amount of documents the user can read through one by one. In this work, we therefore present a new document clustering tool called Med- Clus for bioinformaticians in order to make a keyword search result from PubMed more concise by grouping such a set of documents into clusters. MedClus contains two modules. First, a pre-clustering module that creates the data matrix. This matrix contains term-document frequencies according to the TF*IDF method and optional weights. These weights are given by comparing the term list with the MeSH terms contained in the related MEDLINE abstracts. Second, it contains a clustering module, which is based on a Non-negative Matrix Factorization algorithm that finds an approximate factorization of the data matrix. This application was tested in different experiments evaluating its performance and reliability. Based on these results, a list of recommended ranges for crucial parameters such as the number of clusters was edited in order to constitute an user assistance for the application of Med- Clus. Finally, some results were analyzed by scientists from the field of medicine and biology, who evaluated the relevance of the terms and the existence of a relation between them. MedClus is a tool that is able to re-structure the result list of a keyword search for documents in PubMed. This is done by extracting terms before and finding latent semantics during the clustering process. Also, it optionally applies weights to terms that also appear as MeSH terms in at least one of the MEDLINE abstracts. Therefore, it helps users to refine a search result in PubMed via term-based clustering in order to economize time and efforts. At this development stage, the software is suitable for experienced users such as bioinformaticians, database administrators and developers. Also Web service for Semantic Toxicogenomics Knowledgebase, available at http://stkb2. labkm.net, has applied this technology to provide comprehensive and accurate relations between chemical and toxicological contexts.  The Korean BioChip Society and Springer 2010.",
      "title": "16320 Document clustering of MEDLINE abstracts based on non-negative matrix factorization using local confidence assessment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79959690425&partnerID=40&md5=e82bbccfcc9d840ad7925e5467c898b2"
    },
    {
      "abstract": "Near-synonyms are useful knowledge resources for many natural language applications such as query expansion for information retrieval (IR) and paraphrasing for text generation. However, near-synonyms are not necessarily interchangeable in contexts due to their specific usage and syntactic constraints. Accordingly, it is worth to develop algorithms to verify whether near-synonyms do match the given contexts. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the near-synonyms in the set. We also propose the use of discriminative training to improve classifiers by distinguishing positive and negative features for each nearsynonym. Experimental results show that the proposed method achieves higher accuracy than both pointwise mutual information (PMI) and n-gram-based methods that have been used in previous studies.",
      "title": "16321 Discriminative training for near-synonym substitution",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053415801&partnerID=40&md5=3d27f0139a301aed7e22e440d6761b86"
    },
    {
      "abstract": "The automatic clustering of textual data according to their semantic concepts is a challenging, yet important task. Choosing an appropriate method to apply when clustering text depends on the nature of the documents being analysed. For example, traditional clustering algorithms can struggle to correctly model collections of very short text due to their extremely sparse nature. In recent times, much attention has been directed to finding methods for adequately clustering short text. Many popular approaches employ large, external document repositories, such as Wikipedia or the Open Directory Project, to incorporate additional world knowledge into the clustering process. However the sheer size of many of these external collections can make these techniques difficult or time consuming to apply. This paper also employs external document collections to aid short text clustering performance. The external collections are referred to in this paper as Background Knowledge. In contrast to most previous literature a separate collection of Background Knowledge is obtained for each short text dataset. However, this Background Knowledge contains several orders of magnitude fewer documents than commonly used repositories like Wikipedia. A simple approach is described where the Background Knowledge is used to re-express short text in terms of a much richer feature space. A discussion of how best to cluster documents in this feature space is presented. A solution is proposed, and an experimental evaluation is performed that demonstrates significant improvement over clustering based on standard metrics with several publicly available datasets represented in the richer feature space.  2011, Australian Computer Society, Inc.",
      "title": "16322 Enhancing Short Text Clustering with Small External Repositories",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84870551266&partnerID=40&md5=a27eb24f6d2147e971d01e69b5b6490b"
    },
    {
      "abstract": "The task of analytic-synthetic process of text information cannot be solved without deep knowledge about the structure of language, without using a deep linguistic processor, processing morphology and syntax as well as semantics of language. On the stage of semantic analysis methods and algorithms of the theory of categories and predicate algebra are suggested to be used for defining connections between informational objects, which are in different superphrases unities, and referring them to description or consideration of one theme or one event. The linear logical operators were used for extracting a semantic meaning of a higher level language system from semantic meanings of the preceding level of a language system.  2010 IEEE.",
      "title": "16323 Use of predicate categories for modelling of operation of the semantic analyzer of the linguistic processor",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79955978090&partnerID=40&md5=5b0db82972d5fc834f700b265b9cab3e"
    },
    {
      "abstract": "In the present work we show our approach to generate hierarchies of concepts in the form of ontologies starting from free text. This approach relies on the statistical model of Correspondence Analysis to analyze term occurrences in text, identify the main concepts it refers to, and retrieve semantic relationships between them. We present a tool which is able to apply different methods for the generation of ontologies from text, namely hierarchy generation from hierarchical clustering representation, search for Hearst Patterns on the Web, and bootstrapping. Our evaluation shows that the precision in the generation of hierarchies of the tool is attested to be around 60% for the best automatic approach and around 90% for the best human-assisted approach.",
      "title": "16324 On the use of correspondence analysis to learn seed ontologies from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651432014&partnerID=40&md5=55d812dc1c992cafcbd756e16517329d"
    },
    {
      "abstract": "There exists a wide gap between the information that people and computers respectively can operate with online. Because most of the web is in plain text and the Semantic Web requires structured information (RDF), bridging the two worlds is an important current research topic. Here we propose a web service that uses a Random Indexing (RI) semantic space trained on the plain text of the one million most central Wikipedia concepts. The space provides us with vectors for each of the equivalent DBpedia concepts and vectors for any text or webpage. It can also provide a hashed version of the RI vector that works as unique handler like URIs do, but with the additional advantage that it represents text meaning. As a result, any page (previously readable only for humans) is now integrated with the Semantic Web graph using links to one of its most central parts, DBpedia.",
      "title": "16326 Random indexing spaces for bridging the human and data webs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84889020924&partnerID=40&md5=7ee698d1e30eece50c008c5872aa5199"
    },
    {
      "abstract": "How to organize and visualize big amount of text messages stored on ones mobile phone is a challenging problem, since they can hardly be organized by threads as we do for emails due to lack of necessary metadata such as subject and reply-to. In this paper, we propose an innovative approach based on clustering algorithms and natural language processing methods. We first cluster the text messages into candidate conversations based on their temporal attributes, and then do further analysis using a semantic model based on Latent Dirichlet Allocation (LDA). Considering that the text messages are usually short and sparse, we trained the model using a large scale external data collected from twitter-like web sites, and applied the model to text messages. In the end, the text messages are organized as conversations based on their topics. We evaluated our approach based on 122,359 text messages collected from 50 university students during 6 months.  2010 ACM.",
      "title": "16327 Topic detection and organization of mobile text messages",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651313316&partnerID=40&md5=c908623676a5754e07852a226fcb7e40"
    },
    {
      "abstract": "Through research on the calculation method of feature words weight in texts and semantic similarity between words, we proposed a calculation method of feature words weight based on concept weight for the semantic association phenomenon of text features and the prevalence of high-dimensional problem in a text vector space model. This method reduces the semantic loss of the feature set and the dimension of the text vector, and then makes the text vector space model better and improves the quality of text clustering. Experimental results show the feasibility of the method, and prove that concept-weight-based text clustering increased by 22 percentage points or so than non-concept-weight-based in the final evaluation of the FI index value.  2010 IEEE.",
      "title": "16330 Research on text clustering based on concept weight",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952564721&partnerID=40&md5=ab8563f638dee03c0f7e67844c5633e3"
    },
    {
      "abstract": "Many methods have been developed to recognize those progresses of technologies, and one of them is to analyze patent information. However, current analysis methods for patent documents have some drawbacks. For example, previous researchers mainly paid attention to the structured item in patent documents such as patent number, filing date other than the unstructured item such as claims, abstracts and so on. However, many methods were not full and lots of valuable information can not be gained. Therefore, we proposed a new method based on text mining to handle not only structured items but also unstructured items in this paper. With collected keywords of a target technology field, we represented patent documents in vector space model and cluster patent documents by the kohonens self-organising neural network algorithm. With the clustering results, we formed a semantic network of keywords without respect of filing dates. And a technical development trend graph was built up by rearranging each keyword node of the semantic network according to its earliest filing date and frequency in patent documents. Then we calculateed the value of keywords indices such as the relative growth and relative share and visualize the analysis results to get the technical hot points graph. Our approach contributes to considering both structured and unstructured items of a patent document. Besides, ours visualizes a clear overview of technology information in a more comprehensible way. And as a result of those contributions, it enables us to understand advances of emerging technologies and technical hot points at present, and it helps us to have an insight to the technology field, thereby to avoid unnecessary investments. 2010 IEEE.",
      "title": "16335 Research of technical development trend and hot points based on text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951666335&partnerID=40&md5=b19c9aea2b87fb21d3f137d10e28f6a0"
    },
    {
      "abstract": "It does not consider how similar words are distributed in the text that the traditional algorithm of the VSM characteristic weighs - TF-IDF. For solving the problem, from the semantic view and combined optimization techniques, a improved IF-IDF Weighting algorithm is proposed. This algorithm can effectually reduce the subjective factors of faceted classification, and further improve the effect of current most text clustering algorithm that based on Vector Space Model (VSM). By experiments, the algorithm is feasible and effective, and to some extent, the precision ratio and recall ratio of text clustering is enhanced.  2010 IEEE.",
      "title": "16337 Research of improved IF-IDF Weighting algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951983979&partnerID=40&md5=23dc931c23e5b68f2b5242566cb390cc"
    },
    {
      "abstract": "Along with the rapid popularity of the Internet, crime information on the web is becoming increasingly rampant, and the majority of them are in the form of text. Because a lot of crime information in documents is described through events, event-based semantic technology can be used to study the patterns and trends of web-oriented crimes. In our research project on cyber crime mining, we construct event ontology to extract the attributes and relations in web pages and reconstruct the scenario for crime mining. This article discusses the methods of analyzing and reconstructing the features of Chinese web pages. Event ontology and Support Vector Machine (SVM) classification are used to validate the proposed methods. As an example, a prototype system is implemented for Chinese text classification. The experimental results show that event ontology has many advantages in web crime mining application. 2010 IEEE.",
      "title": "16339 An event ontology construction approach to web crime mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649299874&partnerID=40&md5=71005a5e5a1a89ed09905e1d480b9059"
    },
    {
      "abstract": "In this paper, on the foundation of summarizing several common used text representation models, such as Boolean model, probability model, vector space model and so on, mainly according to the defects of the vector space model, the word semantic space is proposed. And in the word semantic space, a graph-based text representation model is designed. Some properties of this type of text representation model have been given, and this model can describe the words semantic constraints in the text. At the same time, this model can also solve the defects of vector space model, such as the order or words, the boundary between sentences and phrases, etc. And at last the method of computing the text similarity is put forward. 2010 IEEE.",
      "title": "16340 Graph-based text representation model and its realization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649307179&partnerID=40&md5=5ed0505f3a1e99865626933b14915482"
    },
    {
      "abstract": "Feature selection for text classification is a well-studied problem and the goals are improving classification effectiveness, computational efficiency, or both. In this paper, we propose a two-stage feature selection algorithm based on a kind of feature selection method and latent semantic indexing. Traditional wordmatching based text categorization system uses vector space model to represent the document. However, it needs a high dimensional space to represent the document, and does not take into account the semantic relationship between terms, which can also lead to poor classification accuracy. Latent semantic indexing can overcome the problems caused by using statistically derived conceptual indices instead of individual words. It constructs a conceptual vector space in which each term or document is represented as a vector in the space. It not only greatly reduces the dimensionality but also discovers the important associative relationship between terms. Because of the too much calculation time of constructing a new semantic space, in this algorithm, firstly we apply a kind of feature selection method to reduce the term dimensions. Secondly, we construct a new reduced semantic space between terms based on latent semantic indexing method. Through some applications involving spam database categorization, we find that our two-stage feature selection method performs better. 2010 IEEE.",
      "title": "16342 A two-stage feature selection method for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649241327&partnerID=40&md5=ff3deed153bcc079fa804eda9a6f3425"
    },
    {
      "abstract": "Development of Blog texts information on the internet has brought new challenge to Chinese text classification. Aim to solving the semantics deficiency problem in traditional methods for Chinese text classification, this paper implements a text classification method on classifying a blog as joy, angry, sad or fear using a simple unsupervised learning algorithm. The classification of a blog text is predicted by the max semantic orientation (SO) of the phrases in the blog text that contains adjectives or adverbs. In this paper, the SO of a phrase is calculated as the mutual information between the given phrase and the polar words. Then the SO of the given blog text is determined by the max mutual information value. A blog text is classified as joy if the SO of its phrases is joy. Two different corpora are adopted to test our method, one is the Blog corpus collected by Monitor and Research Center for National Language Resource Network Multimedia Sub-branch Center, and the other is Chinese dataset provided by COAE2008 task. Based on the two datasets, the method respectively achieves a high improvement compared to the traditional methods. 2010 IEEE.",
      "title": "16343 Research on sentiment classification of blog based on PMI-IR",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649257647&partnerID=40&md5=e3eba32ce5922db9260bd54855a336c9"
    },
    {
      "abstract": "A comprehensive theoretical framework for the development of a Semantic Web of a new generation, or of a Multilingual Semantic Web, is outlined. Firstly, the paper grounds the possibility of using a mathematical model being the kernel of the theory of K-representations and describing a system of 10 partial operations on conceptual structures for building semantic representations (or text meaning representations) of, likely, arbitrary sentences and discourses in English, Russian, French, German, and other languages. The possibilities of using SK-languages defined by the theory of K-representations for building semantic annotations of informational sources and for constructing semantic representations of discourses pertaining to biology and medicine are illustrated. Secondly, an original strategy of transforming the existing Web into a Semantic Web of a new generation with the well-developed mechanisms of understanding natural language texts is described. The third subject of this paper is a description of the correspondence between the inputs and outputs of the elaborated algorithm of semantic-syntactic analysis and of its advantages",
      "title": "16344 Theory of K-representations as a comprehensive formal framework for developing a multilingual semantic web",
      "url": ""
    },
    {
      "abstract": "In this paper we propose a method for semantic text representation and term weighting. It is based on a semantic resource, WordNet, that provides meaning information and relations between the terms of a document. The heart of the proposed method is the way the concepts (terms) of documents are clustered and weighted. More precisely, we introduce two notions: the centrality of a term and its specificity. The centrality of a term is given by the number of terms of the document that are directly related to it in the same conceptual cluster. The specificity represents the depth of a concept in WordNet. These parameters are different from the usual term frequency tf and inverse term frequency idf used in classical information retrieval. This method is based on two steps: 1) matching document terms with concepts of WordNet in order to obtain the most appropriate ones 2) for each concept calculating its centrality using existing semantic WordNet relations, and its specificity. The preliminary experiments undertaken on TREC collections show the effective interest of these parameters.",
      "title": "16345 A new factor for computing the relevance of a document to a query",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78549255930&partnerID=40&md5=1b5fdbaff775e10b16ead9be0f2fe92a"
    },
    {
      "abstract": "In this paper, we present a new approach that incorporates semantic information from a document, in the form of Hierarchical Document Signature (HDS), to measure semantic similarity between sentences. Due to variability of expressions of natural language, it is very essential to exploit the semantic properties of a document to accurately identify semantically similar sentences since sentences conveying the same fact or concept may be composed lexically and syntactically different. Inversely, sentences which are lexically common may not necessarily convey the same meaning. This poses a significant impact on many text mining applications performance where sentence-level judgment is involved. Our HDS uses the natural hierarchy of the document and represents it in a modularized form of document level to sentence level, sentence to word level",
      "title": "16346 Semantic Hierarchical Document Signature for determining sentence similarity",
      "url": ""
    },
    {
      "abstract": "Measuring similarity has a wide range of application in information retrieval, machine translation or other related fields. In this paper, we proposed a text similarity computation based on improved optimal assignment model, which combine the improved Hungarian algorithm with the semantic similarity of terms to obtain the maximum semantic similarity between two documents or between a query and a document. Experiment shows that the algorithm has a significant improvement for semantic similarity comparing to traditional models of similarity measure. the method can be applied to document clustering, which will enchance the accuracy of result.  2010 IEEE.",
      "title": "16350 Similarity measure based on improved optimal assignment model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78449233825&partnerID=40&md5=e47c2bf8a0be24c81a7382bb5af5a49b"
    },
    {
      "abstract": "In the research of modern linguistics, word frame information, which is significant in the study of Chinese information processing, draws more researchers attention. Its distinction between word argument and auxiliaries plays an important part in the precision of syntactic analysis, elimination of semantic ambiguities and semantic role labeling. Therefore, the study of categorization frame information became a hot issue in the recent years. With the constant development of machine learning technology, an increasing number of computational methods have been applied to many fields, including text classification, language processing and semantic analysis, etc. This approach is the supplement and breakthrough to the traditional methods of linguistic study. In this paper, Pearsons correlation coefficient, which can reflect the correlative information between two variables, is adopted to analyze the correlation between the frequency and functions of Chinese visual verbs. The result is that word frequency takes on positive correlations with the main functions of the word, though with certain differences in the degree of the correlation.  2010 IEEE.",
      "title": "16353 Correlation analysis of visual verbs subcategorization based on Pearsons correlation coefficient",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149372680&partnerID=40&md5=31941d7efb1c1ba11d4700754cc8ab12"
    },
    {
      "abstract": "Most of text association pattern mining techniques transform texts into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. So the depth and accuracy of mining are not satisfying. In order to solve this problem, a novel ontology-based semantic association pattern mining model is proposed. The suggested model applies semantic role labeling to semantic analysis so that the semantic relations are able to be extracted precisely. For the defects of traditional association pattern mining algorithms that could not effectively deal with semantic metabase, a novel semantic-based association pattern mining algorithm is designed for the acquisition of the deep semantic association patterns from semantic metabase. Experimental results reveal that the new model can acquire deep semantic knowledge easily from text database and the algorithm mentioned above has strong adaptability and scalability.  2010 IEEE.",
      "title": "16354 Research on semantic association pattern mining model based on ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149335605&partnerID=40&md5=6adff5aa284064e0bdb3cb41922c711d"
    },
    {
      "abstract": "In case of resolved software change requests (SCRs), the names of impacted source files are known. In this paper, we tackle the question whether it is possible to use this information in order to predict the files that have to be changed whenever a new SCR is received. In order to provide a solution, we present two different approaches, which are based on automatic text classification of SCRs. First, we use Latent Semantic Indexing (LSI) to index the key terms of SCRs. Then, for classification we use two different approaches of machine learning i.e., single and multi label classification. We applied our approaches on the SCRs data of Gnome, Mozilla and Eclipse OSS projects. Our initial experimental results are promising, the obtained maximum precision values for single and multi label classification are 58.2% and 47.1% respectively. Furthermore, in case of single and multilabel classification, the maximum attained precision values for any individual label are 86.5% and 92% respectively.  2010 ACM.",
      "title": "16356 Impact analysis of SCRs using single and multi-label machine learning classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149272918&partnerID=40&md5=2ce9932dfa418d550756a107139840f7"
    },
    {
      "abstract": "Event extraction is a significant task in information extraction. This importance increases more and more with the explosion of textual data available on the Web, the appearance of Web 2.0 and the tendency towards the Semantic Web. Thus, we propose a generic approach to extract events from text and to analyze them. We propose an event extraction algorithm with a polynomial complexity O(n5), and a new similarity measurement between events. We use this measurement to gather similar events. We also present a semantic map of events, and we validate the first component of our approach by the development of the EventEC system.",
      "title": "16359 Event extraction approach for web 2.0",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78049456135&partnerID=40&md5=a35d262ca254f1d93d2066142ceddabc"
    },
    {
      "abstract": "Bio-relation extraction (bRE), an important goal in bio-text mining, involves subtasks identifying relationships between bio-entities in text at multiple levels, e.g., at the article, sentence or relation level. A key limitation of current bRE systems is that they are restricted by the availability of annotated corpora. In this work we introduce a semi-supervised approach that can tackle multi-level bRE via string comparisons with mismatches in the string kernel framework. Our string kernel implements an abstraction step, which groups similar words to generate more abstract entities, which can be learnt with unlabeled data. Specifically, two unsupervised models are proposed to capture contextual (local or global) semantic similarities between words from a large unannotated corpus. This Abstraction-augmented String Kernel (ASK) allows for better generalization of patterns learned from annotated data and provides a unified framework for solving bRE with multiple degrees of detail. ASK shows effective improvements over classic string kernels on four datasets and achieves state-of-the-art bRE performance without the need for complex linguistic features.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16361 Semi-supervised abstraction-augmented string kernel for multi-level bio-relation extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78049354809&partnerID=40&md5=d27ea98bcefe219f9265d610e2b21476"
    },
    {
      "abstract": "Identifying duplicate texts is important in many areas like plagiarism detection, information retrieval, text summarization, and question answering. Current approaches are mostly surface-oriented (or use only shallow syntactic representations) and see each text only as a token list. In this work however, we describe a deep, semantically oriented method based on semantic networks which are derived by a syntactico-semantic parser. Semantically identical or similar semantic networks for each sentence of a given base text are efficiently retrieved by using a specialized index. In order to detect many kinds of paraphrases the semantic networks of a candidate text are varied by applying inferences: lexico-semantic relations, relation axioms, and meaning postulates. Important phenomena occurring in difficult duplicates are discussed. The deep approach profits from background knowledge, whose acquisition from corpora is explained briefly. The deep duplicate recognizer is combined with two shallow duplicate recognizers in order to guarantee a high recall for texts which are not fully parsable. The evaluation shows that the combined approach preserves recall and increases precision considerably in comparison to traditional shallow methods.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16362 Semantic duplicate identification with parsing and machine learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78049252783&partnerID=40&md5=eca6f53933bd35d3aa79fb7d492c3e7e"
    },
    {
      "abstract": "In this paper, a method to extract knowledge from a set of surveyors reports is proposed. Many surveyors from shipping companies are working in shipyards to improve the quality and to meet requirements of the ship owner. At present, the reports from surveyors are used only for managing the status of problems of the product. The proposed method here has three steps. First, the description field of the reports which contains information about some trouble is summarized in a pair of a component name and a trouble name in the failure database",
      "title": "16364 Ontology based knowledge extraction for shipyard fabrication workshop reports",
      "url": ""
    },
    {
      "abstract": "With the rapid growth of text documents, document clustering has become one of the main techniques for organizing large amount of documents into a small number of meaningful clusters. However, there still exist several challenges for document clustering, such as high dimensionality, scalability, accuracy, meaningful cluster labels, overlapping clusters, and extracting semantics from texts. In order to improve the quality of document clustering results, we propose an effective Fuzzy-based Multi-label Document Clustering (FMDC) approach that integrates fuzzy association rule mining with an existing ontology WordNet to alleviate these problems. In our approach, the key terms will be extracted from the document set, and the initial representation of all documents is further enriched by using hypernyms of WordNet in order to exploit the semantic relations between terms. Then, a fuzzy association rule mining algorithm for texts is employed to discover a set of highly-related fuzzy frequent itemsets, which contain key terms to be regarded as the labels of the candidate clusters. Finally, each document is dispatched into more than one target cluster by referring to these candidate clusters, and then the highly similar target clusters are merged. We conducted experiments to evaluate the performance based on Classic, Re0, R8, and WebKB datasets. The experimental results proved that our approach outperforms the influential document clustering methods with higher accuracy. Therefore, our approach not only provides more general and meaningful labels for documents, but also effectively generates overlapping clusters.  2010 Elsevier B.V. All rights reserved.",
      "title": "16365 An integration of WordNet and fuzzy association rule mining for multi-label document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77957752478&partnerID=40&md5=b39c18739cb66f06064f3e508dce99a9"
    },
    {
      "abstract": "Dimensionality reduction is an essential task for many large-scale information processing problems such as classifying document sets, searching over Web data sets, etc. It can be used to improve both the efficiency and the effectiveness of classifiers. In this paper, a comparative study is conducted of five Dimension Reduction Techniques in the context of the Arabic text classification problem using an inhouse Arabic dataset. We evaluated and compared Stemming, Light-Stemming, Document Frequency (DF), TFIDF and Latent Semantic Indexing (LSI) methods to reduce the feature space into an input space of much lower dimension for the neural network classifier. The results showed that the proposed model was able to achieve high categorization effectiveness as measured by Macro-Average F1 measure. Experiments on Arabic data sets indicate that the DF, TFIDF and LSI techniques are favorable in terms of its effectiveness and efficiency when compared with the two other methods.  2010 IEEE.",
      "title": "16366 Comparing dimension reduction techniques for Arabic text classification using BPNN algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958522399&partnerID=40&md5=6e87d3c6e4eea59806b89b6dfceb8c64"
    },
    {
      "abstract": "The lack of semantic information is a critical problem of context tree kernel in text representation. A context tree kernel method based on latent topics is proposed. First, words are mapped to latent topic space through Latent Dirichlet Allocation (LDA). Then, context tree models are built using latent topics. Finally, context tree kernel for text is defined through mutual information between the models. In this approach, document generative models are defined using semantic class instead of words, and the issue of statistic data sparse is solved. The clustering experiment results on text data set show, the proposed context tree kernel is a better measure of topic similarity between documents, and the performance of text clustering is greatly improved.",
      "title": "16367 A context tree kernel based on latent semantic topic",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649998484&partnerID=40&md5=10da730c8f106d5af510695a488e0f9b"
    },
    {
      "abstract": "Most of the existing methods for community discovery only deal with social network with a fixed structure, so they can not effectively deal with dynamic social network. This paper proposes a Multi-agent system method which is applied to real-time dynamic web texts for community discovery. This method combines the multi-agent system control mechanisms with community discovery algorithm. The approach not only divides community for the published web text, but also changes the community division according to the newly updated contents of web texts. Meanwhile, the similarity of two web texts is evaluated through similarity algorithm which compares content similarity and semantic similarity. The effectiveness of this algorithm has been tested on real web texts networks and experiment 3 illustrated the scalability of our algorithm for dynamic community discovery of web texts.  2010 Binary Information Press.",
      "title": "16368 A community mining algorithm for web texts based on Multi-agent system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649948292&partnerID=40&md5=26375c399f983f0d0c049e4b25ef22e7"
    },
    {
      "abstract": "We propose a system that bridges the gap between the two major approaches toward natural language processing: robust shallow text processing and domain-specific (of ten linguistically-based) deep understanding. We propose to use an existing linguistically motivated deep understanding system as the core and to leverage statistical techniques and external resources such as world knowledge to broaden coverage and increase robustness. We will also develop a semantic representation framework, which supports underspecification, granularity and incrementality, the critical factors of robustness in representing natural language semantics. Copyright  2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "16370 Towards a robust deep language understanding system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958595433&partnerID=40&md5=807d070838eafb38deb9af3e5c6737c1"
    },
    {
      "abstract": "The objective of this work is to provide, as a search engine, latent semantic indexing (LSI), which is a classical method to produce optimal approximations of a term-document matrix and has been used for textual information mining. The use of this technique is examining mine content which based web document, using keyword features of documents. Experimental results show that together with both textual and latent features LSI can extract the underlying semantic structure of web documents, thus improve the search engine performance significantly.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16372 Wise search engine based on LSI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958450887&partnerID=40&md5=ccb6087503b91bf6491bba505028120d"
    },
    {
      "abstract": "Alert monitoring of psychological health for web text is to evaluate and forecast the unhealthy psychological status of web users by the technology of web text mining. Based on the idea of Service Distributed and Management Centralized, this paper proposes a cloud architecture for psychological health analysis and describes the service component representation, workflow customization and workflow execution. With the excellent semantic representation ability of resource description frame (RDF), a service component metadata representation method is presented. Through querying the metadata of service component, end-users could choose the service components and customize the logic workflow on demand. In the workflow execution, the workflow case database is constructed. Through analyzing the similarity between history case and user workflow, a series of workflow reuse strategies are proposed. To validate the performance of the cloud architecture for psychological health analysis, a set of experiments are prepared to test the viability, the results demonstrate that the cloud architecture has higher precision, reuse and scalability for psychological health analysis of web text.  2010 IEEE.",
      "title": "16375 Alert monitoring cloud: Psychological health analysis of web text in cloud",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958174712&partnerID=40&md5=ba3aa70f6280b2f2cdcab1ceba02bee6"
    },
    {
      "abstract": "With a rapid growth of the internet communication, many types of text are produced. They can convey the meanings that can contribute to text categorization. Emotion classification also becomes more interesting, but emotion classification in Thai text is still not able to be correctly classified. Thus, this paper proposes a novel approach that takes advantage of bi-words occurrence to classify emotion hidden in a short sentence. In this paper, we classify Thai text into six basic universal emotions including anger, disgust, fear, happiness, sadness, and surprise based on latent semantic analysis approach. We compared the results between two models which construct features from the sentences and applied to three classification methods, i.e. Naive Bayes, SVM, and Decision Tree. The first feature model uses only single word occurrence in the classification. The second model uses single word combined with bi-words occurrence in the classification. The results show that the second model can yield higher accuracy than the first model based on the Naive Bayes classification method.  2010 IEEE.",
      "title": "16379 Applying latent semantic analysis to classify emotions in Thai text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958061195&partnerID=40&md5=996e4ee548a00912f31969448ab85bb1"
    },
    {
      "abstract": "Blogs-Online digital diary like application on web 2.0 has opened new and easy way to voice opinion, thoughts, and like-dislike of every Internet user to the World. Blogosphere has no doubt the largest user-generated content repository full of knowledge. The potential of this knowledge is still to be explored. Knowledge discovery from this new genre is quite difficult and challenging as it is totally different from other popular genre of web-applications like World Wide Web (WWW). Blog-posts unlike web documents are small in size, thus lack in context and contain relaxed grammatical structures. Hence, standard text similarity measure fails to provide good results. In this paper, specialized requirements for comparing a pair of blog-posts is thoroughly investigated. Based on this we proposed a novel algorithm for sentence oriented semantic similarity measure of a pair of blog-posts. We applied this algorithm on a subset of political blogosphere of Pakistan, to cluster the blogs on different issues of political realm and to identify the influential bloggers.",
      "title": "16380 Sentence based semantic similarity measure for blog-posts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77958044253&partnerID=40&md5=420f4b541b1d16d96936be0e828588d4"
    },
    {
      "abstract": "Event extraction is a significant task in information extraction. This importance increases more and more with the explosion of textual data available on the Web, the appearance of Web 2.0 and the tendency towards the Semantic Web. Thus, we propose a generic approach to extract events from text and to analyze them. We propose an event extraction algorithm with a polynomial complexity O(n5), and a new similarity measurement between events. We use this measurement to gather similar events. We also present a semantic map of events, and we validate the first component of our approach by the development of the EventEC system. Copyright  2010, American Association for Artificial Intelligence (www.aaai.org ). All rights reserved.",
      "title": "16382 French-written event extraction based on contextual exploration",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77957862819&partnerID=40&md5=58f779038e71e89a0f4c1218af5269ff"
    },
    {
      "abstract": "Emotions are a key semantic component of human communication. This study focuses on automatic emotion detection in descriptive sentences and how this can be used to tune facial expression parameters for 3-D character generation. A comparison of manual and automatic word feature selection approaches is performed to determine the influence of word features on classification accuracy using support vector machines (SVM). The automatic emotion feature selection algorithm presented here builds on the framework used by mutual information for feature selection. Results of the study indicate that the set of automatically selected features was as good as the set of manually selected features. The proposed automatic feature selection algorithm implemented in this study helped to detect new words from the training corpus which were relevant to the classification task but were not considered by the researchers. An example of potential outcomes from facial expression tuning is also presented. The analysis includes initial results for dealing with the class imbalance challenge present in the data.  2010 IEEE.",
      "title": "16385 Emotion recognition in text for 3-D facial expression rendering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956758218&partnerID=40&md5=ef6e20318b162ef15a8cde1681e6021a"
    },
    {
      "abstract": "A non-linear semantic mapping procedure is implemented for cross-language text matching at the sentence level. The method relies on a non-linear space reduction technique which is used for constructing semantic embeddings of multilingual sentence collections. In the proposed method, an independent embedding is constructed for each language in the multilingual collection and the similarities among the resulting semantic representations are used for cross-language matching. It is shown that the proposed method outperforms other conventional cross-language information retrieval methods.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16387 A non-linear semantic mapping technique for cross-language sentence matching",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956594250&partnerID=40&md5=31442207941279cc24b6beaab12794ed"
    },
    {
      "abstract": "Business analytics (BA) involves defining organizations capabilities and requirements in order to understand how it functions to accomplish its purposes. BA is typically performed to define and validate solutions that are designed to meet organizations business needs, goals or objectives. To pursue this mission, analysts have to collect, analyse and synthesise huge amount of information. In this paper we demonstrate how constructing semantic ontologies can increase the level of automation of BA methods and techniques. This can be achieved by sophisticated semantic data analysis that can help us structure information and obtain birds-eye overview of a given domain. For the case study we take a set of abstracts from the articles presented on CompSysTech09 conference and use OntoGen tool to semi-automatically construct ontology for the domain. The obtained results indicate that the proposed approach can be effectively used to digest textual information and present it in a more operational form of topic ontology. We argue that such approach can be useful also to generate domain knowledge in the field of BA. Copyright  2010 ACM.",
      "title": "16390 Constructing semantic ontologies for business analytics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956271598&partnerID=40&md5=d1490eda0f1f5b138df76cb6e7ead5a0"
    },
    {
      "abstract": "Ontology evolution is the process of incrementally and consistently adapting an existing ontology to changes in the relevant domain. Even though ontology management and versioning tools are now available, they are of limited use for ontology evolution unless the desired changes are known beforehand. Ontology learning toolsets are often employed, but they require large document sets and do not take the existing structures into account. Semantic drift refers to how concepts intentions gradually change as the domain evolves. When a semantic drift is detected, it means that a concept is gradually understood in a different way or its relationships with other concepts are undergoing some changes. A semantic drift captures small domain changes that are hard to detect with traditional ontology engineering approaches. This paper discusses a new approach to detecting and assessing semantic drift in ontologies. The method makes use of concept signatures that are constructed on the basis of how concepts are used and described. Comparing how signatures change over time, we see how concepts semantic content evolves and how their relationships to other concepts gradually reflect these changes. An experiment with the DNVs business sector ontology from 2004 and 2008 demonstrates the value of this approach to ontology evolution.",
      "title": "16392 Semantic drift in ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956272836&partnerID=40&md5=a289925d1d9026fa9830a912f1addfa3"
    },
    {
      "abstract": "An important step for understanding the semantic content of text is the extraction of semantic relations between entities in natural language documents. Automatic extraction techniques have to be able to identify different versions of the same relation which usually may be expressed in a great variety of ways. Therefore these techniques benefit from taking into account many syntactic and semantic features, especially parse trees generated by automatic sentence parsers. Typed dependency parse trees are edge and node labeled parse trees whose labels and topology contains valuable semantic clues. This information can be exploited for relation extraction by the use of kernels over structured data for classification. In this paper we present new tree kernels for relation extraction over typed dependency parse trees. On a public benchmark data set we are able to demonstrate a significant improvement in terms of relation extraction quality of our new kernels over other state-of-the-art kernels.  2010 ACM.",
      "title": "16395 Semantic relation extraction with kernels over typed dependency trees",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956206671&partnerID=40&md5=d2ddc656f57601b0b3576700f6291296"
    },
    {
      "abstract": "In this paper, we detail an approach to a very specific task of information extraction namely, extracting biomarker information in biomedical literature. Starting with the abstract of a given publication, we first identify the evaluative sentence(s) among other sentences by recognizing words and phrases in the text belonging to semantic categories of interest to bio-medical entities (i.e., semantic category recognition). For the entities like, protein, gene and disease, we determine whether the statement refers to biomarker relationship (i.e., assertion classification). Finally, we identify the biomarker relationship among the biomedical entities (i.e., semantic relationship classification). The system, Biomarker Information Extraction Tool (BIET) implements Machine Learning-based biomarker extraction using support vector machines (SVM). The system is trained and tested on a corpus of oncology related PubMed/MEDLINE literatures hand-annotated with biomarker information. We investigate the effectiveness of different features for this task and examine the amount of training data needed to learn the biomarker relationship with the entities. Our system achieved an average Fscore of 86% for the task of biomarker information extraction comparing to the human annotated dataset (i.e. gold standard) scores.  2010 IEEE.",
      "title": "16398 Extracting biomarker information applying natural language processing and machine learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956156444&partnerID=40&md5=c19ab670652424251713295572d4af85"
    },
    {
      "abstract": "The amount of textual data that is available for researchers and businesses to analyze is increasing at a dramatic rate. This reality has led IS researchers to investigate various text mining techniques. This essay examines four text mining methods that are frequently used in order to identify their characteristics and limitations. The four methods that we examine are (1) latent semantic analysis, (2) probabilistic latent semantic analysis, (3) latent Dirichlet allocation, and (4) correlated topic model. We review these four methods and compare them with topic detection and spam filtering to reveal their peculiarity. Our paper sheds light on the theory that underlies text mining methods and provides guidance for researchers who seek to apply these methods.",
      "title": "16399 An empirical comparison of four text mining methods",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78649819232&partnerID=40&md5=35cc12d9b89778bfc2880840f41aebfa"
    },
    {
      "abstract": "This paper deals with computational detection of humor. It assumes that computational humor is an useful task for any number of reasons and in many applications. Besides these applications, it also shows that recognition of humor is a perfect test platform for an advanced level of language understanding by a computer. It discusses the computational linguistic/semantic preconditions for computational humor and an ontological semantic approach to the task of humor detection, based on direct and comprehensive access to meaning rather than on trying to guess it with statistical-cum-syntactical keyword methods. The paper is informed by the experience of designing and implementing a humor detection model, whose decent success rate confirmed some of the assumptions while its flaws made other ideas prominent, including the necessity of full text comprehension. The bulk of the paper explains how the comprehensive meaning access technology makes it possible for unstructured natural language text to be automatically translated into the ontologically defined text meaning representations that can be used then to detect humor in them, if any, automatically. This part is informed by the experience, subsequent to humor detection, of designing, implementing, and testing an ontological semantic text analyzer that takes an English sentence as input and outputs its text meaning representation (TMR). Every procedure mentioned in the paper has either been implemented or proven to be implementable within the approach.  Springer-Verlag 2010.",
      "title": "16401 Ontology-based view of natural language meaning: The case of humor detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79952537678&partnerID=40&md5=e455b78905b6bc2316d654bd78a7245f"
    },
    {
      "abstract": "Most of the common techniques in text mining are based on the statistical analysis of a term, either word or phrase. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying text mining model should indicate terms that capture the semantics of text. In this case, the mining model can capture terms that present the concepts of the sentence, which leads to discovery of the topic of the document. A new concept-based mining model that analyzes terms on the sentence, document, and corpus levels is introduced. The concept-based mining model can effectively discriminate between nonimportant terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed mining model consists of sentence-based concept analysis, document-based concept analysis, corpus-based concept-analysis, and concept-based similarity measure. The term which contributes to the sentence semantics is analyzed on the sentence, document, and corpus levels rather than the traditional analysis of the document only. The proposed model can efficiently find significant matching concepts between documents, according to the semantics of their sentences. The similarity between documents is calculated based on a new concept-based similarity measure. The proposed similarity measure takes full advantage of using the concept analysis measures on the sentence, document, and corpus levels in calculating the similarity between documents. Large sets of experiments using the proposed concept-based mining model on different data sets in text clustering are conducted. The experiments demonstrate extensive comparison between the concept-based analysis and the traditional analysis. Experimental results demonstrate the substantial enhancement of the clustering quality using the sentence-based, document-based, corpus-based, and combined approach concept analysis.  2006 IEEE.",
      "title": "16402 An efficient concept-based mining model for enhancing text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956016296&partnerID=40&md5=7b28a36ad4e8a1543fa31d6dacd9848c"
    },
    {
      "abstract": "Multiword Expressions (MWEs) appear frequently and ungrammatically in natural languages. Identifying MWEs in free texts is a very challenging problem. This paper proposes a knowledge-free, unsupervised, and language-independent Multiword Expression Distance (MED). The new metric is derived from an accepted physical principle, measures the distance from an n-gram to its semantics, and outperforms other state-of-the-art methods on MWEs in two applications: question answering and named entity extraction.  2011 Springer Science+Business Media, LLC & Science Press, China.",
      "title": "16403 A new multiword expression metric and its applications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79951488035&partnerID=40&md5=91388fd5e5d397a99099b7e752177627"
    },
    {
      "abstract": "Single term based document representations, e.g. bag- of-words, have been widely accepted in the machine learning, information retrieval and text mining community. One notable limitation of such methods is that they do not consider the rich information resident in the semantic relations among terms. This paper reports our approach of concepts handling in document representation and its effect on the performance of text categorization. We introduce a Frequent word Sequence algorithm that generates concept-centered phrases to render domain knowledge concepts. Our experimental study based on a domain centered corpus shows that a consistent performance improvement can be achieved when concept-centered phrases are included in addition to the single term based features in document representations. We also observed that a universally suitable support threshold does not exist and the removal of concept irrelevant sequences can possibly further improve the performance at a lower support level.  2010 IEEE.",
      "title": "16404 Domain concept handling in automated text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956050258&partnerID=40&md5=6ae2edff1d5d72514650bb54472edd2b"
    },
    {
      "abstract": "Domain knowledge is essential resource in Information Extraction (IE) from free text since it supports the decisions about structuring the extracted text objects into domain statements. Thus manually-created conceptual structures enable the semantic representation of textual information. This paper discusses the role of domain knowledge in information extraction of structured data from patient-related texts. The article shows that domain knowledge is encoded not only in the conceptual structures, which provide the ontological framework for the IE task, but also in the IE templates that are designed to capture domain semantics. A prototype system and IE examples of domain knowledge usage are considered together with results of the current prototype evaluation.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16405 Use of domain knowledge in the automatic extraction of structured representations from patient-related texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955911832&partnerID=40&md5=a3811594ff50e34b1641fb1f8476675c"
    },
    {
      "abstract": "We present a method for automatically creating large-scale semantic networks from natural language text, based on deep semantic analysis. We provide a robust and scalable implementation, and sketch various ways in which the representation may be deployed for conceptual knowledge acquisition. A translation to RDF establishes interoperability with a wide range of standardised tools, and bridges the gap to the field of semantic technologies.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16406 Conceptual knowledge acquisition using automatically generated large-scale semantic networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955889455&partnerID=40&md5=be50ed20286c07c62aa9d8ddb5570a5c"
    },
    {
      "abstract": "In this paper, we reduce the rescoring problem in a spoken dialogue understanding task to a classification problem, by using the semantic error rate as the reranking target value. The classifiers we consider here are trained with linguistically motivated features. We present comparative experimental evaluation results of four supervised machine learning methods: Support Vector Machines, Weighted K-Nearest Neighbors, Naive Bayes and Conditional Inference Trees. We provide a quantitative evaluation of learning and generalization during the classification supervised training, using cross validation and ROC analysis procedures. The reranking is derived using the posterior knowledge given by the classification algorithms.  2010 Springer-Verlag.",
      "title": "16407 Spoken language understanding via supervised learning and linguistically motivated features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955433943&partnerID=40&md5=257bfde52efa4066f8e3ef4178d15be1"
    },
    {
      "abstract": "We present a lightweight, user-centred approach for document navigation and analysis that is based on an ontology of text mining results. This allows us to bring the result of existing text mining pipelines directly to end users. Our approach is domain-independent and relies on existing NLP analysis tasks such as automatic multi-document summarization, clustering, question-answering, and opinion mining. Users can interactively trigger semantic processing services for tasks such as analyzing product reviews, daily news, or other document sets.  2010 Springer-Verlag.",
      "title": "16408 Semantic content access using domain-independent NLP ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955447565&partnerID=40&md5=9f6b82145cf42a4b17363940d9d70d5c"
    },
    {
      "abstract": "Clustering was usually applied on numerical and categorical information. However, textual information is acquiring an increasing importance with the appearance of methods for textual data mining. This paper proposes the use of classical clustering algorithms with a mixed function that combines numerical, categorical and semantic features. The content of the semantic features is extracted from textual data. This paper analyses and compares the behavior of different existing semantic similarity functions that use WordNet as background ontology. The different partitions obtained with the clustering algorithm are compared to human classifications in order to see which one approximates better the human reasoning. Moreover, the interpretability of the obtained clusters is discussed. The results show that those similarity measures that provide better results when compared using a standard benchmark also provide better and more interpretable partitions.  2010 Springer-Verlag.",
      "title": "16409 Performance of ontology-based semantic similarities in clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955359974&partnerID=40&md5=196a4a2737d8be6dc08afe3b3022711b"
    },
    {
      "abstract": "This paper proposes a verb-driven approach to extract 5W1H (Who, What, Whom, When, Where and How) event semantic information from Chinese online news. The main contributions of our work are two-fold: First, given the usual structure of a news story, we propose a novel algorithm to extract topic sentences by stressing the importance of news headline",
      "title": "16411 Extracting 5W1H event semantic elements from Chinese online news",
      "url": "11th International Conference on Web-Age Information Management - WAIM 2010 - Jiuzhaigou - China"
    },
    {
      "abstract": "Existing research efforts in sentiment analysis of online user reviews mainly focus on extracting features (such as quality and price) of products/services and classifying users sentiments into semantic orientations (such as positive, negative or neutral). However, few of them take the strength of user sentiments into consideration, which is particularly important in measuring the overall quality of products/services. Intuitively, different reviews for the same feature should have quite different sentiment strength, even though they may express the same polarity of sentiment. This paper presents an approach to estimating the sentiment strength of user reviews according to the strength of adverbs and adjectives expressed by users in their opinion phrases. Experimental result on a hotel review dataset in Chinese shows that the proposed approach is effective in the task of sentiment classification and achieves a good performance on a multi-scale evaluation.  2010 Springer-Verlag.",
      "title": "16412 Exploring the sentiment strength of user reviews",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955045281&partnerID=40&md5=0f022d17681e698f123ff376303d8437"
    },
    {
      "abstract": "This paper addresses the issue of adapting cases represented by plain text with the help of formal concept analysis and natural language processing technologies. The actual cases represent recipes in which we classify ingredients according to culinary techniques applied to them. The complex nature of linguistic anaphoras in recipe texts make usual text mining techniques inefficient so a stronger approach, using syntactic and dynamic semantic analysis to build a formal representation of a recipe, had to be used. This representation is useful for various applications but, in this paper, we show how one can extract ingredient-action relations from it in order to use formal concept analysis and select an appropriate replacement sequence of culinary actions to use in adapting the recipe text.  2010 Springer-Verlag.",
      "title": "16413 Text adaptation using formal concept analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954991609&partnerID=40&md5=8c1a3bc17fe278c1a1a9b1abe2585017"
    },
    {
      "abstract": "This paper proposes a first attempt to include real semantic information into the process of handwriting recognition. We take advantage of the fact that the main topic of handwritten notes is often known beforehand like in annotation or reviewing tasks. Using state-of-the-art technologies from the knowledge management research area it is possible to store a semantic representation of the users knowledge in a Personal Information Model (PIMO). This PIMO stores the relations between semantic concepts and documents on the computer. In this paper we extract texts from related documents and concepts of the PIMO. The vocabulary of these texts is then used to aid the recognizer. In our multi-writer experiments, a significant improvement of the recognition accuracy by 8% on the text line level has been achieved. Copyright 2010 ACM.",
      "title": "16414 Improving handwriting recognition by the use of semantic information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954982378&partnerID=40&md5=1bcfbd91ed864f7fd92225930ba1bb62"
    },
    {
      "abstract": "Phishing email fraud has been considered as one of the main cyber-threats over the last years. Its development has been closely related to social engineering techniques, where different fraud strategies are used to deceit a naive email user. In this work, a latent semantic analysis and text mining methodology is proposed for the characterisation of such strategies, and further classification using supervised learning algorithms. Results obtained showed that the feature set obtained in this work is competitive against previous phishing feature extraction methodologies, achieving promising results over different benchmark machine learning classification techniques.  2010 IEEE.",
      "title": "16420 Latent semantic analysis and keyword extraction for phishing classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954766719&partnerID=40&md5=7785056eea705eeb59b420ccbe76ed14"
    },
    {
      "abstract": "Automated extraction of named entities is an important text analysis task. In addition to recognizing the occurrence of entity names, it is important to be able to label those names by type. Most entity extraction techniques categorize extracted entities into a few basic types, such as PERSON, ORGANIZATION, and LOCATION. This paper presents an approach for generating more fine-grained subdivisions of entity type. The technique of latent semantic indexing (LSI) is used to provide semantic context as an indicator of likely entity subtype. Tests were carried out on a collection of 5.5 million English-language news articles. At modest levels of recall, the accuracy of sub-type assignment was comparable to the accuracy with which the gross type was assigned by a state-of-the-art commercial entity extraction software package.  2010 IEEE.",
      "title": "16421 Entity refinement using latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954789300&partnerID=40&md5=c70746e1484726dd69e31550260434dd"
    },
    {
      "abstract": "Classification of samples in expression microarray experiments represents a crucial task in bioinformatics and biomedicine. In this paper this scenario is addressed by employing a particular class of statistical approaches, called Topic Models. These models, firstly introduced in the text mining community, permit to extract from a set of objects (typically documents) an interpretable and rich description, based on an intermediate representation called topics (or processes). In this paper the expression microarray classification task is cast into this probabilistic context, providing a parallelism with the text mining domain and an interpretation. Two different topic models are investigated, namely the Probabilistic Latent Semantic Analysis (PLSA) and the Latent Dirichlet Allocation (LDA). An experimental evaluation of the proposed methodologies on three standard datasets confirms their effectiveness, also in comparison with other classification methodologies.  2010 ACM.",
      "title": "16423 Expression microarray classification using topic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954711241&partnerID=40&md5=fbb90becdd158eaf44227509aab54b22"
    },
    {
      "abstract": "Accelerated by the technological advances in the domain, the size of the biomedical literature has been growing rapidly. As a result, it is not feasible for individual researchers to comprehend and synthesize all the information related to their interests. Therefore, it is conceivable to discover hidden knowledge, or hypotheses, by linking fragments of information independently described in the literature. In fact, such hypotheses have been reported in the literature mining community",
      "title": "16424 Hypothesis generation and ranking based on event similarities",
      "url": ""
    },
    {
      "abstract": "Recent work in Ontology learning and Text mining has mainly focused on engineering methods to solve practical problem. In this thesis, we investigate methods that can substantially improve a wide range of existing approaches by minimizing the underlying problem: The Semantic Gap between formalized meaning and human cognition. We deploy OWL as a Meaning Representation Language and create a unified model, which combines existing NLP methods with Linguistic knowledge and aggregates disambiguated background knowledge from the Web of Data. The presented methodology here allows to study and evaluate the capabilities of such aggregated knowledge to improve the efficiency of methods in NLP and Ontology learning.  2010 Springer-Verlag.",
      "title": "16428 The semantic gap of formalized meaning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954420020&partnerID=40&md5=6df162a81d4d488cd73dd1fc0b1c6669"
    },
    {
      "abstract": "Effective feature selection is essential to make the learning task efficient and more accurate. In this paper, a novel Chinese text feature selection algorithm based on PLSA was presented for text classification, and it was compared with other effective feature selection methods on a benchmark of 8 text classification problem instances that were gathered from Sougou Labs corpus. The results show that this method could make SVM classifier have the best classification performance and more robust than other methods.  2010 Springer-Verlag.",
      "title": "16429 A novel Chinese text feature selection method based on probability latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954393638&partnerID=40&md5=e38e93c394d26f127995ccb7613a15a2"
    },
    {
      "abstract": "The novel mobile application csxPOI (short for: collaborative, semantic, and context-aware points-of-interest) enables its users to collaboratively create, share, and modify semantic points of interest (POI). Semantic POIs describe geographic places with explicit semantic properties of a collaboratively created ontology. As the ontology includes multiple subclassifications and instantiations and as it links to DBpedia, the richness of annotation goes far beyond mere textual annotations such as tags. Users can search for POIs through the subclass hierarchy of the collaboratively created ontology. For example, a POI annotated as bakery can be found through the search string shop as it is a superclass of bakery. Data mining techniques are employed to cluster and thus improve the quality of the collaboratively created POIs.  2010 Springer-Verlag.",
      "title": "16431 Collaborative semantic points of interests",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954418302&partnerID=40&md5=e200ddcfb19f04b326c3dd64dfb48fe5"
    },
    {
      "abstract": "Semantic enrichment of textual data is the operation of linking mentions with the entities they refer to, and the subsequent enrichment of such entities with the background knowledge about them available in one or more knowledge bases (or in the entire web). Information about the context in which a mention occurs, (e.g., information about the time, the topic, and the space, which the text is relative to) constitutes a critical resource for a correct semantic enrichment for two reasons. First, without context, mentions are too little text to unambiguously refer to a single entity. Second, knowledge about entities is also context dependent (e.g., speaking about political life of Illinois during 1996, Obama is a Senator, while since 2009, Obama is the US president). In this paper, we describe a concrete approach to context-driven semantic enrichment, built upon four core sub-tasks: detection of mentions in text (i.e., finding references to people, locations and organizations)",
      "title": "16433 Context-driven semantic enrichment of Italian news archive",
      "url": "7th Extended Semantic Web Conference - ESWC 2010 - Heraklion - Crete - Greece"
    },
    {
      "abstract": "Capitulo 1",
      "title": "16436 Knowledge-Based Bioinformatics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885510903&partnerID=40&md5=0b394c5a0d373135796ccc6da34ca236"
    },
    {
      "abstract": "We explore the use of independent component analysis (ICA) for the automatic extraction of linguistic roles or features of words. The extraction is based on the unsupervised analysis of text corpora. We contrast ICA with singular value decomposition (SVD), widely used in statistical text analysis, in general, and specifically in latent semantic analysis (LSA). However, the representations found using the SVD analysis cannot easily be interpreted by humans. In contrast, ICA applied on word context data gives distinct features which reflect linguistic categories. In this paper, we provide justification for our approach called WordICA, present the WordICA method in detail, compare the obtained results with traditional linguistic categories and with the results achieved using an SVD-based method, and discuss the use of the method in practical natural language engineering solutions such as machine translation systems. As the WordICA method is based on unsupervised learning and thus provides a general means for efficient knowledge acquisition, we foresee that the approach has a clear potential for practical applications.  Cambridge University Press 2010.",
      "title": "16439 WordICA-emergence of linguistic representations for words by independent component analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650052132&partnerID=40&md5=f8d4393e8a2982e6b2f3764686b523e5"
    },
    {
      "abstract": "A new method to compute Chinese text concept is proposed in this paper. In this method, we construct sentence vectors from the text by extracting and quantifying some syntax and semantic features such as concept elements, dependent relations and correlative relations. Then, we combine these sentence vectors to the text vector to represent the text concept. Experimental results show that, in the application of text classification, the precision and the recall of this method achieve 91.5% and 91.3%. Contrast with TF-IDF and LSA, the proposed method is more accurate, less affected by text class and more stable on text concept approaching.  2010 IEEE.",
      "title": "16445 A new method to compute Chinese text concept",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77953217826&partnerID=40&md5=5abe3800a9e6d2964407f19d8873ceed"
    },
    {
      "abstract": "Information extraction (IE) aims to retrieve certain types of information from natural language text by processing them automatically. For example, an IE system might retrieve information about geopolitical indicators of countries from a set of web pages while ignoring other types of information. Ontology-based information extraction (OBIE) has recently emerged as a subfield of information extraction. Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the IE process. Because of the use of ontologies, this field is related to knowledge representation and has the potential to assist the development of the Semantic Web. In this paper, we provide an introduction to ontology-based information extraction and review the details of different OBIE systems developed so far. We attempt to identify a common architecture among these systems and classify them based on different factors, which leads to a better understanding on their operation. We also discuss the implementation details of these systems including the tools used by them and the metrics used to measure their performance. In addition, we attempt to identify the possible future directions for this field.  The Author(s), 2010.",
      "title": "16448 Ontology-based information extraction: An introduction and a survey of current approaches",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952984528&partnerID=40&md5=bfb644d83c22b4b66da137324929aea1"
    },
    {
      "abstract": "The considerable improvement in the biotechnogical field and the adoption of screen techniques such as high throughput arrays produced a spread of biological and genetical data and scientific papers, mostly diffused on the Web. Even if the huge amount of available information represents a major step forward for the biomedical research field, the main effort for a scientist is to evaluate the correlation among the concepts both in qualitative and in quantitative terms. In the presented work, exploiting the richness of the UMLS Metathesaurus in combination with the novelty of the literature provided by PubMed, an automatic flow aimed at the scoring the semantic correlation among biomedical terms (e.g. biomolecules and biological processes) is proposed. The experiments show that obtained correlations are fully coherent with the information coming from the biological literature. The accuracy of the results remarks the importance of combining text mining techniques with complete and well structured thesaurus, such as UMLS.  2010 IEEE.",
      "title": "16452 An automated tool for scoring biomedical terms correlation based on semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952720010&partnerID=40&md5=18029d1e30a97eb1c03253d34fac3948"
    },
    {
      "abstract": "With the increasing amount of unstructured content available electronically on the web, content categorization becomes very important for efficient information retrieval. The basic approaches for information retrieval in text documents are searching using keywords, categorization of the documents and filtering out the stream. To extract information from raw data, its complexity needed to be first reduced. Clustering methods and Projection methods are aimed at reducing the amount of data and dimensionality of data respectively. SOM is a special case in that it can be used at the same time for both clustering and projection. It projects onto a 2D-grid. Various methods were developed for the automatic clustering of worldwide web documents according to the user requirements. The objective of this paper is to reduce the time and effort the user has to find the information sought after. The method termed topologica organization of content can generate classified topics from a se of unstructured documents. The TOC is a set of hierarchicall organized 1D-growing SOMs. In TOC, vector space model is used for indexing of 1D-SOM. In the proposed approach, latent semantic indexing of 1D-SOM can be used to enhance the association between terms. Latent semantic analysis is a technique that projects the original high dimensional document vector into a space with latent semantic dimensions. A term-bydocument matrix is constructed for the information retrieval. A brief review is given on existing methods for documents clustering and organization. The proposed method which can use LSI will be efficient in terms of computational cost, accuracy and visualization. It can be easily adapted for large data set. The proposed method will provide feature for retrieving meaningful related topics.",
      "title": "16460 Self-organising map for document categorization using latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952383395&partnerID=40&md5=c4d2e847a5e9bea73fc7e6f6cfedfc08"
    },
    {
      "abstract": "The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.  2010 AI Access Foundation.",
      "title": "16465 Cause identification from aviation safety incident reports via weakly supervised semantic lexicon construction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77957878646&partnerID=40&md5=07ceeaa566b0a34027aa88c6013ab61e"
    },
    {
      "abstract": "Obtaining or approximating a geographic location for search results often motivates users to include place names and other geography-related terms in their queries. Previous work shows that queries that include geography-related terms correspond to a significant share of the users demand. Therefore, it is important to recognize the association of documents to places in order to adequately respond to such queries. This paper describes strategies for text classification into geography-related categories, using evidence extracted from Wikipedia. We use terms that correspond to entry titles and the connections between entries in Wikipedias graph to establish a semantic network from which classification features are generated. Results of experiments using a news data-set, classified over Brazilian states, show that such terms constitute valid evidence for the geographical classification of documents, and demonstrate the potential of this technique for text classification. Copyright  2010 ACM.",
      "title": "16466 Geographical classification of documents using evidence from Wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951466127&partnerID=40&md5=58b317f611cabd605e68b1af8bb61e66"
    },
    {
      "abstract": "The discovery of implicit connections between terms that do not occur together in any scientific document underlies the model of literature-based knowledge discovery first proposed by Swanson. Corpus-derived statistical models of semantic distance such as Latent Semantic Analysis (LSA) have been evaluated previously as methods for the discovery of such implicit connections. However, LSA in particular is dependent on a computationally demanding method of dimension reduction as a means to obtain meaningful indirect inference, limiting its ability to scale to large text corpora. In this paper, we evaluate the ability of Random Indexing (RI), a scalable distributional model of word associations, to draw meaningful implicit relationships between terms in general and biomedical language. Proponents of this method have achieved comparable performance to LSA on several cognitive tasks while using a simpler and less computationally demanding method of dimension reduction than LSA employs. In this paper, we demonstrate that the original implementation of RI is ineffective at inferring meaningful indirect connections, and evaluate Reflective Random Indexing (RRI), an iterative variant of the method that is better able to perform indirect inference. RRI is shown to lead to more clearly related indirect connections and to outperform existing RI implementations in the prediction of future direct co-occurrence in the MEDLINE corpus.  2009 Elsevier Inc. All rights reserved.",
      "title": "16469 Reflective Random Indexing and indirect inference: A scalable method for discovery of implicit connections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949262334&partnerID=40&md5=de75ea125ce65d7cdf3080227bcff330"
    },
    {
      "abstract": "Massive increases in electronically available text have spurred a variety of natural language processing methods to automatically identify relationships from text",
      "title": "16472 Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles",
      "url": ""
    },
    {
      "abstract": "In order to compute intelligent answers to complex questions, using the vast amounts of information existing in the Web, computers have (1) to translate such knowledge, typically from text documents, into a data structure suitable for automatic exploitation",
      "title": "16474 Knowledge accumulation through automatic merging of ontologies",
      "url": "Article"
    },
    {
      "abstract": "Ontologies and rules are usually loosely coupled in knowledge representation formalisms. In fact, ontologies use open-world reasoning while the leading semantics for rules use non-monotonic, closed-world reasoning. One exception is the tightly-coupled framework of Minimal Knowledge and Negation as Failure (MKNF), which allows statements about individuals to be jointly derived via entailment from an ontology and inferences from rules. Nonetheless, the practical usefulness of MKNF has not always been clear, although recent work has formalized a general resolution-based method for querying MKNF when rules are taken to have the well-founded semantics, and the ontology is modeled by a general Oracle. That work leaves open what algorithms should be used to relate the entailments of the ontology and the inferences of rules. In this paper we provide such algorithms, and describe the implementation of a query-driven system, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic) rules under the well-founded semantics and a (monotonic) ontology, represented by a CDF (ALCQ) theory.  2010 Springer-Verlag.",
      "title": "16475 Implementing query answering for hybrid MKNF knowledge bases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77749271121&partnerID=40&md5=a6765ca1e56adad038eb0dad6cd465fc"
    },
    {
      "abstract": "This paper proposes a system, named DefExplorer, which analyzes the type of given Chinese terms, extracts term definitions from the Web, and selects answers from noisy Web pages. DefExplorer filters out invalid data with a semantic approach. Two types of candidate sets, common and domain specific, are employed to cluster similar candidates into groups. Different approaches are also deployed to evaluate candidates importance which is the key factor for selecting the best answers from retrieved candidates. Experimental results show that DefExplorer can effectively extract term definitions from the Web, especially for the definitions of out-of-vocabulary terms.",
      "title": "16476 An automated term definition extraction system using the web corpus in the Chinese language",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77950456930&partnerID=40&md5=eee96eca3245fb9a3f4ff95812fb01b7"
    },
    {
      "abstract": "Most document clustering algorithms operate in a high dimensional bag-of-words space. The inherent presence of noise in such representation obviously degrades the performance of most of these approaches. In this paper we investigate an unsupervised dimensionality reduction technique for document clustering. This technique is based upon the assumption that terms co-occurring in the same context with the same frequencies are semantically related. On the basis of this assumption we first find term clusters using a classification version of the EM algorithm. Documents are then represented in the space of these term clusters and a multinomial mixture model (MM) is used to build document clusters. We empirically show on four document collections, Reuters-21578, Reuters RCV2-French, 20Newsgroups and WebKB, that this new text representation noticeably increases the performance of the MM model. By relating the proposed approach to the Probabilistic Latent Semantic Analysis (PLSA) model we further propose an extension of the latter in which an extra latent variable allows the model to co-cluster documents and terms simultaneously. We show on these four datasets that the proposed extended version of the PLSA model produces statistically significant improvements with respect to two clustering measures over all variants of the original PLSA and the MM models.  2009 Elsevier Ltd. All rights reserved.",
      "title": "16477 Improving document clustering in a learned concept space",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76049124409&partnerID=40&md5=2a01ddaed9d69b56539bd6018691652e"
    },
    {
      "abstract": "Text categorization is an important research area of text mining. The original purpose of text categorization is to recognize, understand and organize different types of texts or documents. The general categorization approaches are treated as supervised learning, which infers similarity among a collection of categorized texts for training purposes. The existing categorization approaches are obviously not content-oriented and constrained at single word level. This paper introduces an innovative content-oriented text categorization approach named as CogCate. Inspired by cognitive situation models, CogCate exploits a human cognitive procedure in categorizing texts. In addition to traditional statistical analysis at word level, CogCate also applies lexical/semantical analysis, which ensures the accuracy of categorization. The evaluation experiments have testified the performance of CogCate. Meanwhile, CogCate remarkably reduces the time and effort spent on software training and maintenance of text collections. Our research work attests that interdisciplinary research efforts benefit text categorization.  2009 Elsevier Inc. All rights reserved.",
      "title": "16478 Automatic text categorization based on content analysis with cognitive situation models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-71849096323&partnerID=40&md5=c74506302c60639c0aae6851503b38eb"
    },
    {
      "abstract": "The SPECTRa-T project has developed text-mining tools to extract named chemical entities (NCEs), such as chemical names and terms, and chemical objects (COs), e.g., experimental spectral assignments and physical chemistry properties, from electronic theses (e-theses). Although NCEs were readily identified within the two major document formats studied, only the use of structured documents enabled identification of chemical objects and their association with the relevant chemical entity (e.g., systematic chemical name). A corpus of theses was analyzed and it is shown that a high degree of semantic information can be extracted from structured documents. This integrated information has been deposited in a persistent Resource Description Framework (RDF) triple-store that allows users to conduct semantic searches. The strength and weaknesses of several document formats are reviewed.  2010 American Chemical Society.",
      "title": "16481 SPECTRa-T: Machine-based data extraction and semantic searching of chemistry e-theses",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77649224313&partnerID=40&md5=14117b582180ae25a1031e6376ff3d84"
    },
    {
      "abstract": "Automatic text classification (TC) is essential for the management of information. To properly classify a document d, it is essential to identify the semantics of each term f in d, while the semantics heavily depend on context (neighboring terms) of f in d. Therefore, we present a technique CTFA (Context-based Term Frequency Assessment) that improves text classifiers by considering term contexts in test documents. The results of the term context recognition are used to assess term frequencies of terms, and hence CTFA may easily work with various kinds of text classifiers that base their TC decisions on term frequencies, without needing to modify the classifiers. Moreover, CTFA is efficient, and neither huge memory nor domain specific knowledge is required. Empirical results show that CTFA successfully enhances performance of several kinds of text classifiers on different experimental data.  2009 ASIS&T.",
      "title": "16484 Context-based term frequency assessment for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76649137666&partnerID=40&md5=c6e84539ece060e6afa7dbd3af34e867"
    },
    {
      "abstract": "Graphical models have become the basic framework for topic based probabilistic modeling. Especially models with latent variables have proved to be effective in capturing hidden structures in the data. In this paper, we survey an important subclass Directed Probabilistic Topic Models (DPTMs) with soft clustering abilities and their applications for knowledge discovery in text corpora. From an unsupervised learning perspective, topics are semantically related probabilistic clusters of words in text corpora",
      "title": "16486 Knowledge discovery through directed probabilistic topic models: A survey",
      "url": ""
    },
    {
      "abstract": "Many tasks in library and information science(e.g., indexing, abstracting, classification, and text analysis techniques such as discourse and content analysis) require text meaning interpretation, and, therefore, any individual differences in interpretation are relevant and should be considered, especially for applications in which these tasks are done automatically. This article investigates individual differences in the interpretation of one aspect of text meaning that is commonly used in such automatic applications: lexical cohesion and lexical semantic relations. Experiments with 26 participants indicate an approximately 40% difference in interpretation. In total, 79, 83, and 89 lexical chains (groups of semantically related words) were analyzed in 3 texts, respectively. A major implication of this result is the possibility of modeling individual differences for individual users. Further research is suggested for different types of texts and readers than those used here, as well as similar research for different aspects of text meaning.",
      "title": "16488 Individual differences in the interpretation of text: Implications for information science",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72849137879&partnerID=40&md5=008e595c6508c87b15e2fda2a848e5ec"
    },
    {
      "abstract": "This paper presents a new knowledge-based vector space model (VSM) for text clustering. In the new model, semantic relationships between terms (e.g., words or concepts) are included in representing text documents as a set of vectors. The idea is to calculate the dissimilarity between two documents more effectively so that text clustering results can be enhanced. In this paper, the semantic relationship between two terms is defined by the similarity of the two terms. Such similarity is used to re-weight term frequency in the VSM. We consider and study two different similarity measures for computing the semantic relationship between two terms based on two different approaches. The first approach is based on the existing ontologies like WordNet and MeSH. We define a new similarity measure that combines the edge-counting technique, the average distance and the position weighting method to compute the similarity of two terms from an ontology hierarchy. The second approach is to make use of text corpora to construct the relationships between terms and then calculate their semantic similarities. Three clustering algorithms, bisecting k-means, feature weighting k-means and a hierarchical clustering algorithm, have been used to cluster real-world text data represented in the new knowledge-based VSM. The experimental results show that the clustering performance based on the new model was much better than that based on the traditional term-based VSM.  2009 Springer-Verlag London Limited.",
      "title": "16489 Knowledge-based vector space model for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77957556167&partnerID=40&md5=7319c786467fa9c36170b72af028d098"
    },
    {
      "abstract": "The World Wide Web (WWW) offers an enormous wealth of information and data, and assembles a tremendous amount of knowledge. Much of this knowledge, however, comprises either non-structured data or semistructured data. To make use of these unexploited or underexploited resources more efficiently, the management of information and data gathering has become an essential task for research and development. In this paper, the author examines the task of researching a hostel or homestay using the Google search web service as a base search engine. From the search results, mining, retrieving and sorting out location and semantic data were carried out by combining the Chinese Word Segmentation System with text mining technology to find geographic information gleaned from web pages. The results obtained from this particular searching method allowed users to get closer to the answers they sought and achieve greater accuracy, as the results included graphics and textual geographic information. In the future, this method may be suitable for and applicable to various types of queries, analyses, geographic data collection, and in managing spatial knowledge related to different keywords within a document Mining, Web Mining Copyright  2010, IGI Global.",
      "title": "16490 Geographic information retrieval and text mining on Chinese tourism web pages",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954895860&partnerID=40&md5=1f1b22deb3eb655aebb663da5170370a"
    },
    {
      "abstract": "Domain ontologies play an important role in supporting knowledge-based applications in the Semantic Web. To facilitate the building of ontologies, text mining techniques have been used to perform ontology learning from texts. However, traditional systems employ shallow natural language processing techniques and focus only on concept and taxonomic relation extraction. In this paper we present a system, known as Concept-Relation-Concept Tuple-based Ontology Learning (CRCTOL), for mining ontologies automatically from domain-specific documents. Specifically, CRCTOL adopts a full text parsing technique and employs a combination of statistical and lexico-syntactic methods, including a statistical algorithm that extracts key concepts from a document collection, a word sense disambiguation algorithm that disambiguates words in the key concepts, a rule-based algorithm that extracts relations between the key concepts, and a modified generalized association rule mining algorithm that prunes unimportant relations for ontology learning. As a result, the ontologies learned by CRCTOL are more concise and contain a richer semantics in terms of the range and number of semantic relations compared with alternative systems. We present two case studies where CRCTOL is used to build a terrorism domain ontology and a sport event domain ontology. At the component level, quantitative evaluation by comparing with Text-To-Onto and its successor Text2Onto has shown that CRCTOL is able to extract concepts and semantic relations with a significantly higher level of accuracy. At the ontology level, the quality of the learned ontologies is evaluated by either employing a set of quantitative and qualitative methods including analyzing the graph structural property, comparison to WordNet, and expert rating, or directly comparing with a human-edited benchmark ontology, demonstrating the high quality of the ontologies learned.",
      "title": "16491 CRCTOL: A semantic-based domain ontology learning system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72849148903&partnerID=40&md5=2ccd646408bb0de5df60d1581ee2f794"
    },
    {
      "abstract": "The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy",
      "title": "16493 Text relatedness based on aword thesaurus",
      "url": ""
    },
    {
      "abstract": "The goal of identifying textual entailment ? whether one piece of text can be plausibly inferred from another ? has emerged in recent years as a generic core problem in natural language understanding. Work in this area has been largely driven by the PASCAL Recognizing Textual Entailment (RTE) challenges, which are a series of annual competitive meetings. The current work exhibits strong ties to some earlier lines of research, particularly automatic acquisition of paraphrases and lexical semantic relationships and unsupervised inference in applications such as question answering, information extraction and summarization. It has also opened the way to newer lines of research on more involved inference methods, on knowledge representations needed to support this natural language understanding challenge and on the use of learning methods in this context. RTE has fostered an active and growing community of researchers focused on the problem of applied entailment. This special issue of the JNLE provides an opportunity to showcase some of the most important work in this emerging area. Copyright  Cambridge University Press 2010.",
      "title": "16494 Recognizing textual entailment: Rational, evaluation and approaches",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77953681584&partnerID=40&md5=14e866402bf6bebf2588700999898937"
    },
    {
      "abstract": "Semantic relations are an important component of ontologies that can support many applications e.g. text mining, question answering, and information extraction. Automatic semantic relation extraction system is a crucial tool that can reduce the bottleneck of knowledge acquisition in the ontologies construction. In this paper, we present a statistical approach for learning the semantic relations between concepts of an ontology in the agricultural domain. The semantic relations are acquired by using verbs to indicate the relations between ontology concepts. The co-occurrences of domain verbs with their components, which are annotated the concepts, are analyzed by using several statistical methodologies. Moreover, we expand the sets of verb expressing the same semantic relation by using the extracted patterns of concept pairs of the seed verbs component. Our experiment has been done on a collection of Thai shallow parsed texts in the domain of agriculture. The precision and recall of the presented system is 65% and 82%, respectively.  2009 IEEE.",
      "title": "16496 A statistical approach for semantic relation extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72449174218&partnerID=40&md5=b5394a3d732131e8ce063f2624f6cc78"
    },
    {
      "abstract": "The graphical text representation method such as Conceptual Graphs (CGs) attempts to capture the structure and semantics of documents. As such, they are the preferred text representation approach for a wide range of problems namely in natural language processing, information retrieval and text mining. In a number of these applications, it is necessary to measure the dissimilarity (or similarity) between knowledge represented in the CGs. In this paper, we would like to present a dissimilarity algorithm to detect outliers from a collection of text represented with Conceptual Graph Interchange Format (CGIF). In order to avoid the NP-complete problem of graph matching algorithm, we introduce the use of a standard CG in the dissimilarity computation. We evaluate our method in the context of analyzing real world financial statements for identifying outlying performance indicators. For evaluation purposes, we compare the proposed dissimilarity function with a dice-coefficient similarity function used in a related previous work. Experimental results indicate that our method outperforms the existing method and correlates better to human judgements. In Comparison to other text outlier detection method, this approach managed to capture the semantics of documents through the use of CGs and is convenient to detect outliers through a simple dissimilarity function. Furthermore, our proposed algorithm retains a linear complexity with the increasing number of CGs.  2009 IEEE.",
      "title": "16499 Dissimilarity algorithm on conceptual graphs to mine text outliers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72449172889&partnerID=40&md5=276cdf6cb7fbd72c8c8519945c7b6331"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) is an effective method in the way of feature extraction, which has been applied to many text learning tasks, such as text clustering and information retrieval. This paper thoroughly analyses the influence of term co-occurrences on the mapping of Latent Semantic Indexing and brings forward a method named pseudo document which strengthens the beneficial term co-occurrences by adding heuristic knowledge to text collection so as to make the mapping of Latent Semantic Indexing more reasonable. The experimental results show that the method named pseudo document can effectively improve the performance of patent retrieval. 2009 IEEE.",
      "title": "16500 A weakly supervised optimize method in latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72249087317&partnerID=40&md5=b056a7aa1972722c89f569fa665f0774"
    },
    {
      "abstract": "In this paper, the purpose is to arrange information to understand at one view. The proposed summarization frame technology is a system to hierarchically arrange and classify information by targeting content and level of importance in sentences. Moreover, the technique in which the Concept Base, the Degree of Association Algorithm, the Time Judgment system and the Place judgment system are used to understand content of sentences is proposed. The Concept Base generates semantics from a certain word, and the Degree of Association Algorithm uses the results of the semantics expansion to express the relationship between one word and another as a numeric value. Only needed information like the number of strokes limitation etc. can be easily extracted by hierarchically arranging information in the document summary. Moreover, the speed-up of the retrieval can be expected by narrowing the retrieval object in information retrieval. An answer matched to TPO can be expected to be achieved in a QA system. Sentences are classified according to the content. Each classification is classified into a more detailed field. Important keywords are extracted from the sentences classified into the field. Moreover, the extracted keywords are classified into common and peculiar word for the sentences in the field. In addition, sentences of each field hierarchize sentences to three stages according to the importance of the content. In addition, the sentences of each field are hierarchized at three levels according to the importance of the content. 2009 IEEE.",
      "title": "16501 An information arrangement technique for a text classification and summarization based on a summarization frame",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72249105273&partnerID=40&md5=e69b8e324b30ba818b68d0167ca5443c"
    },
    {
      "abstract": "Word-based representation is widely used in text categorization. However, performance of this approach is affected by the problems derived from language variation. In this paper, we investigate a document representation combining words and concepts to integrate the advantages of two types of representations. The approach takes the part of speech as the concept for the word which is error-prone in word sense disambiguation to reduce the disambiguation mistakes. The approach employs three ways to measure the contributions of different representation forms to classification and selects the most productive one as the feature to drop the concepts not suitable for representation while not losing the lexical semantic information. We conduct experiments to compare the performance of different types of representations on Chinese text categorization corpus of Fudan University. And the results confirm the validity of our combination representation.",
      "title": "16502 Document representation combining concepts and words in Chinese text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72249106466&partnerID=40&md5=90d86fbf448069afaa0116eb56e50742"
    },
    {
      "abstract": "This paper presents a new method for ontology-based Question Answering (QA) with the use of textual entailment. In this method, a set of question patterns, called hypothesis questions, was automatically produced from a domain ontology, along with their corresponding SPARQL query templates for answer retrieval. Then the QA task was reduced to the problem of looking for the hypothesis question that was entailed by a user question and taking its corresponding query template to produce a complete query for retrieving the answers from underlying knowledge bases. An entailment engine was used to discover the entailed hypothesis questions with the help of question classification. An evaluation was carried out to assess the accuracy of the QA method, and the results revealed that most of the user questions (65%) can be correctly answered with a semantic entailment engine enhanced by the domain ontology.",
      "title": "16504 An ontology-based question answering method with the use of textual entailment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72249098728&partnerID=40&md5=9d37ff267519fcd2e13bfb259aa1dcd4"
    },
    {
      "abstract": "The textual content of company annual reports has proven to contain predictive indicators for the company future performance. This paper addresses the general research question of evaluating the effectiveness of applying machine learning and text mining techniques to building predictive models with annual reports. More specifically, we focus on these two questions: 1) can the advantages of the ranking algorithm help achieve better predictive performance with annual reports? and 2) can we integrate meta semantic features to help support our prediction? We compare models built with different ranking algorithms and document models. We evaluate our models with a simulated investment portfolio. Our results show significantly positive average returns over 5 years with a power law trend as we increase the ranking threshold. Adding meta features to document model has shown to improve ranking performance. The SVR & Meta-augemented model outperforms the others and provides potential for explaining the textual factors behind the prediction.  2009 IEEE.",
      "title": "16505 Learning to rank firms with annual reports",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76249121453&partnerID=40&md5=0ba1bc08b9aa04a9007ee688dcfd8a72"
    },
    {
      "abstract": "This paper presents a Collaborative Ontology Learning Approach for the implementation of an Ontology-based Web Content Management System (OWCMS). The proposal system integrates two supervised learning approach - Content-based Learning and User-based Learning Approach. The Content-based Learning Approach applies text mining methods to extract ontology concepts, and to build an Ontology Graph (OG) through the automatic learning of web documents. The User-based Learning Approach applies features analysis methods to extract the subset of the Ontology Graphs, in order to build a personalized ontology by using intelligent agent approach to capture user reading habit and preference through their semantic navigation and search over the ontology-based web content. This system combines the two methods to create collaborative ontology learning through an ontology matching and refinement process on the ontology created from content-based learning and user-based learning. The proposed method improves the validness of the classical ontology learning outcome by user-based learning refinement and validation. 2009 IEEE.",
      "title": "16507 Collaborative content and user-based web ontology learning system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-71249121006&partnerID=40&md5=f7a311cac2943ada5ebd6f88b1643b7f"
    },
    {
      "abstract": "Analyzing failure trends and establishing effective coping processes for complex problems in advance is essential in telecommunication services. We propose a method for semantically analyzing and classifying customer enquiries efficiently and precisely. Our method can also construct semantic content efficiently by extracting related terms through analysis and classification. This method is based on a dependency parsing and co-occurrence technique to enable classification of a large amount of unstructured data into patterns because customer enquiries are generally stored as unstructured textual data.  2009 Springer Berlin Heidelberg.",
      "title": "16508 Text mining for customer enquiries in telecommunication services",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70849119446&partnerID=40&md5=21f655c793b805448b171489684ceb06"
    },
    {
      "abstract": "A bug triage system is used for validation and allocation of bug reports to the most appropriate developers. An automatic bug triage system may reduce the software maintenance time and improve its quality by correct and timely assignment of new bug reports to the appropriate developers. In this paper, we present the techniques behind an automatic bug triage system, which is based on the categorization of bug reports. In order to obtain an automatic bug triage system we used these techniques and performed comparative experiments. We downloaded 1,983 resolved bug reports along with the developer activity data from the Mozilla open source project. We extracted the relevant features like report title, report summary etc., from each bug report, and extracted developers name who resolved the bug reports from the developers activity data. We processed the extracted textual data, and obtained the term-to-document matrix using parsing, filtering and term weighting methods. For term weighting methods we used simple term frequency and TFIDF (term frequency inverse document frequency) methods. Furthermore, we reduced the dimensionality of the obtained term-to-document matrix by applying feature selection and latent semantic indexing methods. Finally we used seven different machine learning methods for the classification of bug reports. The best obtained bug triage system is based on latent semantic indexing and support vector machine having 44.4% classification accuracy. The average precision and recall values are 30% and 28%, respectively.  2009 IEEE.",
      "title": "16510 Automatic software bug triage system (BTS) based on latent semantic indexing and support vector machine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70749088840&partnerID=40&md5=8eec7e7f596233ccc0fd2069a325df32"
    },
    {
      "abstract": "This paper proposes a Fully Automatic Categorization approach for Text (FACT) by exploiting the semantic features from WordNet and document clustering. In FACT, the training data is constructed automatically by using the knowledge of the category name. With the support of WordNet, it first uses the category name to generate a set of features for the corresponding category. Then, a set of documents is labeled according to such features. To reduce the possible bias originating from the category name and generated features, document clustering is used to refine the quality of initial labeling. The training data are subsequently constructed to train the discriminative classifier. The empirical experiments show that the best performance of FACT can achieve more than 90% of the baseline SVM classifiers in F1 measure, which demonstrates the effectiveness of the proposed approach.  2009 Springer Berlin Heidelberg.",
      "title": "16511 Fully automatic text categorization by exploiting wordnet",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70649105434&partnerID=40&md5=80b2c11b62119ec0f435f02a9e9ce5fe"
    },
    {
      "abstract": "Web document could be seen to be composed of textual content as well as social metadata of various forms (e.g., anchor text, search query and social annotation), both of which are valuable to indicate the semantic content of the document. However, due to the free nature of the web, the two streams of web data suffer from the serious problems of noise and sparseness, which have actually become the major challenges to the success of many web mining applications. Previous work has shown that it could enhance the content of web document by integrating anchor text and search query. In this paper, we study the problem of exploring emergent social annotation for document enhancement and propose a novel reinforcement framework to generate social representation of document. Distinguishing from prior work, textual content and social annotation are enhanced simultaneously in our framework, which is achieved by exploiting a kind of mutual reinforcement relationship behind them. Two convergent models, social content model and social annotation model, are symmetrically derived from the framework to represent enhanced textual content and enhanced social annotation respectively. The enhanced document is referred to as Social Document or sDoc in that it could embed complementary viewpoints from many web authors and many web visitors. In this sense, the document semantics is enhanced exactly by exploring social wisdom. We build the framework on a large Del.icio.us data and evaluate it through three typical web mining applications: annotation, classification and retrieval. Experimental results demonstrate that social representation of web document could boost the performance of these applications significantly. Copyright 2009 ACM.",
      "title": "16512 sDoc: Exploring social wisdom for document enhancement in web mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74549196222&partnerID=40&md5=55c1f2146d4fd033491f157113cba29c"
    },
    {
      "abstract": "In this paper we introduce a novel approach for the thematic organization of bibliographic records that builds upon a semantic relatedness measure we have implemented for this task. In particular, we introduce the Omiotis measure, which captures the semantic relatedness between text segments and enables the thematic organization of the bibliographic data stored in online databases. Experimental evaluation demonstrates that Omiotis can significantly improve the performance of several data mining tasks, such as publications classification and clustering, compared to existing approaches",
      "title": "16516 Semantic relatedness hits bibliographic data",
      "url": ""
    },
    {
      "abstract": "A model of episodic memory is derived to propose algorithms of text categorization with semantic space models. Performances of two algorithms are contrasted using textual material of the text-mining context DEFT09. Results confirm that the episodic memory metaphor provides a convenient framework to propose efficient algorithm for text categorization. One algorithm has already been tested with LSA. The present paper extends these algorithms to another model of Word Vector named Random Indexing. 2009 IEEE.",
      "title": "16517 Random indexing and the episodic memory metaphor. Application to text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74549174773&partnerID=40&md5=9e80ef4bb0c89e2eee7d912756f99e11"
    },
    {
      "abstract": "Scientific articles published in electronic format are knowledge bases, especially in Medicine. An obstacle to semantic processing of this knowledge by computers is that in spite of their digital format, articles are in text format for human reading and processing. A model is proposed for electronic publishing scientific articles both in textual format and in machine  understandable format, in ontology format. Software agents can process the content of an article published according to the model, thus enabling semantic retrieval, consistence checking and the identification of new discoveries. The model is described and initial steps toward the development of an authoring/publishing system which implements the model proposed are related.",
      "title": "16525 A publishing system to extract and represent the knowledge content of scientific articles on health science in machine-processable format",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84869145190&partnerID=40&md5=9484bd2093dfc0b7bb6777c57d7ba043"
    },
    {
      "abstract": "We argue that web service discovery technology should help the user navigate a complex problem space by providing suggestions for services which they may not be able to formulate themselves as (s)he lacks the epistemic resources to do so. Free text documents in service environments provide an untapped source of information for augmenting the epistemic state of the user and hence their ability to search effectively for services. A quantitative approach to semantic knowledge representation is adopted in the form of semantic space models computed from these free text documents. Knowledge of the users agenda is promoted by associational inferences computed from the semantic space. The inferences are suggestive and aim to promote human abductive reasoning to guide the user from fuzzy search goals into a better understanding of the problem space surrounding the given agenda. Experimental results are discussed based on a complex and realistic planning activity.  2009 IEEE.",
      "title": "16528 Augmenting web service discovery by cognitive semantics and abduction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856944878&partnerID=40&md5=f520c3771d2a7a53acc0aa03d6cfa42e"
    },
    {
      "abstract": "Traditional statistics based text classification methods almost construct their characteristic vectors with some key terms, and they consider terms are independent of each other and there are no semantic relations among them. However, in the real world, words used to have semantic relationships, such as synonym, hyponymy and so on. Therefore, classification methods based on statistics do not conform to the fact and the classification results also do not satisfying. To draw this problem, there is a need to obtain characteristic semantic information by taking advantage of ontology. With the help of the features of ontology - class hierarchical structure and property constraint, one can match the terms with domain ontology concepts and build up the concept vector space model. Using ontology method for text classification alone will lack scientific and stringency of the statistics. Taking all the above into consideration, this paper takes a combination of the two classification methods. Firstly, we choose the characteristics with statistics method and based on this, add in the ontology and form the concept vector space. Besides, we improve the KNN algorithm from two aspects. Finally, we implement a module for text classification of telecom domain. In the end, we make an analysis and comparison of the results of both statistics-only based (without improving the KNN algorithm) and the combination of two classification methods (with improved KNN). 2009 IEEE.",
      "title": "16529 Research on text classification algorithm by combining statistical and ontology methods",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949694752&partnerID=40&md5=ccdb99fd19d587ef1b1168059041390a"
    },
    {
      "abstract": "In this paper we describe our work on developing a novel technique for discovery of implicit knowledge about patents from multilingual patent information sources. In this work we developed a system platform to support locating similar and relevant multilingual patent documents. The platform was implemented using a multilingual vector space based on the latent semantic indexing (LSI) model, and utilizing collected professional Chinese-English parallel corpora for training the system model. These multilingual patent documents could then be mapped into the semantic vector space for evaluating their similarity by means of text clustering techniques. The preliminary results show that our platform framework has potential for retrieval and relatedness evaluation of multilingual patent documents. 2009 IEEE.",
      "title": "16530 Development of a multilingual text mining approach for knowledge discovery in patents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74849123503&partnerID=40&md5=e261fe5a8e6644062b3928002ca70908"
    },
    {
      "abstract": "Knowledge extraction provides the potential for producing high quality representation of the document and can help storing, retrieving, sharing, and management of explicit biomedical knowledge. This article addresses the task of mining binary relationships between concepts from biomedical literature for scientific discovery from medical literature. The UMLS has defined the domain entities and the internal relation network between them. The expert is then guided in construct the relation template by define the syntactic and semantic constraints with the help of the pre-established domain ontology. We present the technologies in finding the possible knowledge from plain text based on some grammar analysis. Medical concepts with the similar role are grouped as a union to improve the recall and decrease the useless calculate. The medical relations are extracted according the syntactic and semantic mapping. Experiment show that the biomedical extraction system get better performance. 2009 IEEE.",
      "title": "16534 Extract medical interpretation based on shallow syntactic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949857945&partnerID=40&md5=c4b7eef041c51288b06871d2e5ab9781"
    },
    {
      "abstract": "This paper discusses some language technologies applied for the automatic processing of Electronic Health Records in Bulgarian, in order to extract multi-layer conceptual chunks from medical texts. We consider an Information Extraction view to text processing, where semantic information is extracted using predefined templates. At the first step the templates are filled in with information about the patient status. Afterwards the system excerpts or infers temporal relations between the events, described in the EHR text. Then cause-effect relations are explicated and at last, implicit knowledge is derived from the medical records after reasoning. Thus we propose a cascade approach for the extraction of multi-layer knowledge representation statements because the subject is too complex. In this article we present laboratory prototypes for the first two tasks and discuss typical examples of conceptual structures, which cover the most challenging tasks in the extraction scenario - The recognition of cause-effect relations and temporal structures. The present work in progress is part of the research project EVTIMA (2009-2011) that aims at the design and implementation of technologies for efficient search of conceptual patterns in medical information.",
      "title": "16539 Context related extraction of conceptual information from electronic health records",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887227553&partnerID=40&md5=e33c0ed24ae7a601a1d4ae452862b75e"
    },
    {
      "abstract": "Experimenting with different mathematical objects for text representation is an important step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in index terms. We introduce an algorithm for sense-based semantic ordering of index terms which approximates Cruses description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among index terms while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness.  2009 Association for Computational Linguistics.",
      "title": "16540 Improving text classification by a sense spectrum approach to term expansion",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955447630&partnerID=40&md5=d4551120b7732b220af9ed2e0f793417"
    },
    {
      "abstract": "Identification of sentiment orientation in Chinese words is essential for getting sentiment comprehension of Chinese text, and building a basic semantic lexicon with Chinese emotional words will provide a core subset for identifying emotional words in a special area. It can not only help to identify and enlarge semantic lexicon in corpus effectively but also improve classification efficiency. On the basis of the similarity of Chinese words, the paper has proposed a method of calculating sentiment weight of Chinese emotional words. In addition, a dictionary with basic Chinese emotional words has been constructed based on the HowNet semantic lexicon. By utilizing the dictionary together with TF-IDF, we have done experiments to identify sentiment orientation in Chinese text and have got satisfying classification result. 2009 IEEE.",
      "title": "16541 A method of building Chinese basic semantic lexicon based on word similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74549178076&partnerID=40&md5=0b3d9a1ab477cd2ada25b379d30dbb4a"
    },
    {
      "abstract": "The process of term extraction and weighting affects the performance of information retrieval, search engines and text mining systems. A text document is abstracted as a vector of terms, and the weight for each term is usually given by using popular TF-IDF method. In the TF-IDF method, the weight of a term is a function of its frequency in the document and in overall document collection. The similarity computation by cosine similarity method is influenced by common terms (and their weight) between two document vectors and ignores the semantic relation between terms. We can use the generalization property of hierarchical knowledge repositories to establish that the terms correspond to specific instances of some generalized term. These generalized terms can be used to enrich the document vector, by enriching and weighting we intend to obtain better similarity values between two documents. In this paper, we have proposed an improved term extraction and weighting method by exploiting the contextual/semantic relationship between terms using knowledge repositories such as open web directories. The experiment results show that the proposed approach improves clustering performance over other term extraction and weighting approaches.",
      "title": "16544 Improving text document clustering by exploiting open web directory",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78149304383&partnerID=40&md5=4e24bf1ef0fca1e9ff56f124df970b51"
    },
    {
      "abstract": "Keyphrases provide semantic metadata that summarizes the documents and enable the reader to quickly determine whether the given article is in the readers fields of interest. This paper presents an automatic keyphrase extraction method based on the naive Bayesian learning that exploits a number of domain-specific features to boost up the keyphrase extraction performance in medical domain. The proposed method has been compared to a popular keyphrase extraction algorithm, called Kea.  2009 Springer-Verlag Berlin Heidelberg.",
      "title": "16545 Automatic keyphrase extraction from medical documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76249093113&partnerID=40&md5=186e86b14d6694eeb197c1c5258c90bc"
    },
    {
      "abstract": "Data dimension reduction plays an important role in the field of text representation. An effective dimension reduction method can not only reduce computation complexity, but help to improve the accuracy of text classification. This paper presents a new method of dimension reduction which is based on words semantic similarities. Being different with traditional methods which usually use the statistical information of words, natural language processing knowledge is used in our method which considers semantic information and POS information of feature terms. The experimental results show that our method is effective in dimensionality reduction of text representation and achieves a higher accuracy of text classification. The semantic similarity based method is a suitable method for text representation.  2009 IEEE.",
      "title": "16548 A HowNet-based feature selection method for chinese text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76349111359&partnerID=40&md5=b9c177a64b472e892bb42359bbecd28b"
    },
    {
      "abstract": "Responding correctly to a question given a large collection of textual data is not an easy task. There is a need to perceive and recognize the question at a level that permits to detect some constraints that the question imposes on possible answers. The question classification task is used in Question Answering systems. This deduces the type of expected answer, to perform a semantic classification to the target answer. The purpose is to provide additional information to reduce the gap between answer and question to match them. An approach to ameliorate the effectiveness of classifiers focusing on the linguistic analysis (semantic, syntactic and morphological) and statistical approaches guided by a layered semantic hierarchy of fine grained questions types. This work also proposes two methods of questions expansion. The first finds for each word synonyms matching its contextual sense. The second one adds a high representation hypernym for the noun. Various representation features of documents, term weighting and diverse machine learning algorithms are studied. Experiments conducted on actual data are presented. Of interest is the improvement in the precision of the classification of questions.  2009 Springer-Verlag Berlin Heidelberg.",
      "title": "16549 Semantically expanding questions for supervised automatic classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70549111704&partnerID=40&md5=d3760bdcec092fe36de4542f2a4377be"
    },
    {
      "abstract": "This paper presents a novel approach for automatic text categorization. The mainstream of the research on rule-based classifier regards document as a container of term, and generates rules by using the term distribution in documents. General speaking, there must be existed some kind of semantic relevance between term and paragraph in a document. We call it Meaningful Inner Link Objects-MILO which must be varied with different semantics of a document itself. While this paper concentrates on using these MILOs that associate with semantic relevance for text categorization, hence we focus on two problems: (1) finding the best MILOs which associate with semantic relevance",
      "title": "16550 Meaningful inner link objects for automatic text categorization",
      "url": ""
    },
    {
      "abstract": "Feature-based opinion mining from product reviews is a difficult task, both due to the high semantic variability of opinion expression, as well as because of the diversity of characteristics and sub-characteristics describing the products and the multitude of opinion words used to depict them. Further on, this task supposes not only the discovery of directly expressed opinions, but also the extraction of phrases that indirectly or implicitly value objects and their characteristics, by means of emotions or attitudes. Last, but not least, evaluation of results is difficult, because there is no standard corpus available that is annotated at such a fine-grained level and no annotation scheme defined for this purpose. This article presents our contributions to this task, given by the definition and application of an opinion annotation scheme, the testing of different methodologies to detect phrases related to different characteristics and the employment of Textual Entailment recognition for opinion mining. Finally, we test our approaches both on the built corpus, as well as on an ad-hoc built collection of reviews that we evaluate on the basis of the stars given. We prove that our approaches are appropriate and give high precision results.  2010 Springer-Verlag Berlin Heidelberg.",
      "title": "16551 Semantic approaches to fine and coarse-grained feature-based opinion mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651259503&partnerID=40&md5=770a010adb4a5c3f9aaadcb8470683be"
    },
    {
      "abstract": "Most text mining methods are based on representing documents using a vector space model, commonly known as a bag of word model, where each document is modeled as a linear vector representing the occurrence of independent words in the text corpus. It is well known that using this vector-based representation, important information, such as semantic relationship among concepts, is lost. This paper proposes a novel text representation model called ConceptLink graph. The ConceptLink graph does not only represent the content of the document, but also captures some of its underlying semantic structure in terms of the relationships among concepts. The ConceptLink graph is constructed in two main stages. First, we find a set of concepts by clustering conceptually related terms using the self-organizing map method. Secondly, by mapping each documents content to concept, we generate a graph of concepts based on the occurrences of concepts using a singular value decomposition technique. The ConceptLink graph will overcome the keyword independence limitation in the vector space model to take advantage of the implicit concept relationships exhibit in all natural language texts. As an information-rich text representation model, the ConceptLink graph will advance text mining technology beyond feature-based to structure-based knowledge discovery. We will illustrate the ConceptLink graph method using samples generated from benchmark text mining dataset.  2009, Australian Computer Society, Inc.",
      "title": "16552 A conceptlink graph for text structure mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873493670&partnerID=40&md5=1560c3bf0541e52052b1b8dd255ca550"
    },
    {
      "abstract": "In this paper, we present a relation mining and visualization framework to identify important semi-structured information components using semantic and linguistic analysis of text documents. The novelty of the paper lies in identifying key snippet from text to validate the interaction between a pair of entities. The extracted information components are exploited to generate semantic network which provides distinct user perspectives and allows navigation over documents with similar information components. The efficacy of the proposed framework is established through experiments carried out on biomedical text documents extracted through PubMed search engine.  2009 Springer-Verlag Berlin Heidelberg.",
      "title": "16557 A relation mining and visualization framework for automated text summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76249120066&partnerID=40&md5=4b10750857701bcc7d2c4c462386ec55"
    },
    {
      "abstract": "Exploration of explicit and, especially, implicit information of customers is a vital task for industrial enterprises and organizations. Text and data mining are two indispensable means to accomplish above task. This paper presents a hybrid mining system, EIREx, which processes unstructured (text documents) and structured (data tables from databases) information as a whole source. The construction of EIREx is inspired by a cognitive situation model, Event-Indexing Model. EIREx monitors customer information in multidimensional vectors to simulate cognitive understanding processes of human beings. To ensure accuracy, EIREx implements syntactic parsing, lexical reference and semantic analysis in information extraction procedures from mining sources. In the procedures of information integration and cross referencing, EIREx delivers a structural description to reveal implicit connections among existing customers, which are valuable for further analysis of customer orders and purchase motivations or intentions, and dealing with customers complaints and suggestions.  2009 IEEE.",
      "title": "16558 Implicit customer relations mining with the Event-Indexing Model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-76949108386&partnerID=40&md5=d7c1fba874aa40360fd70d602a817299"
    },
    {
      "abstract": "Question-answering systems that provide precise answers to questions, by combining techniques for information retrieval, information extraction, and natural language processing, are seen as the next-generation search engines. Due to the growth and real-world impact of biomedical information, the need for question-answering systems that can aid medical researchers and health care professionals in their information search is acutely felt. In order to provide users with accurate answers, such systems need to go beyond lexico-syntactic analysis to semantic analysis and processing of texts and knowledge resources. Moreover, question-answering systems equipped with reasoning capabilities can derive more adequate answers by using inference. Research on question answering in the medical and health care domain is still in its inception stage. While several recent approaches to medical question answering have explored use of semantic knowledge, few approaches have exploited the utility of logic formalisms and of inference mechanisms. In this paper, we present a framework for a logic-based question-answering system for the medical domain, which uses Description Logic as the formalism for knowledge representation and reasoning. As a first step toward building the proposed system, we present semantic analysis and classification of medical questions. Copyright 2009 ACM.",
      "title": "16559 A framework of a logic-based question-answering system for the medical domain (LOQAS-Med)",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72949093754&partnerID=40&md5=ce7de5709e9a703d7c5f1690a8443bab"
    },
    {
      "abstract": "Word Sense Disambiguation (WSD) is a traditional AI-hard problem. An improvement of WSD would have a significant impact on applications such as knowledge acquisition, text mining, information extraction, etc. Lexical chain holds a set of semantically related words of a text and provides an effective way for WSD, but existing lexical chain systems have inaccuracies in WSD for lacking a weighting scheme to measure the weights of word senses. In this paper, we propose a new unsupervised WSD method based on lexical stability and improved lexical chain. This method can disambiguate all words with a high accuracy. We evaluate the performance of our algorithm on SemCor corpus which is widely used for evaluating the accuracy of the WSD algorithm. Empirical results show that the algorithm can achieve a significant higher accuracy than state-of-the-art result.  2009 IEEE.",
      "title": "16560 Combining lexical stability and improved lexical chain for unsupervised word sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951452730&partnerID=40&md5=9a36c2123d245c789b80c75f49b8218c"
    },
    {
      "abstract": "Text document classification based on the semantic level is a hot issue in text processing presently. In this paper, a method based on NMF for Chinese text classification is presented. According to NMF, the term-document matrix is decomposed to capture the relation between terms. This method settled effectively the problems of synonym and polysemy. It experimentally shows that, compared with LSI based on SVD, this method has advantages of faster computing speed, less memory occupancy and improvement of classification precision when the dimension reduces markedly.  2009 IEEE.",
      "title": "16561 Chinese text classification method based on NMF",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951198581&partnerID=40&md5=c89c2570680918e39f40163c69ea6f37"
    },
    {
      "abstract": "Due to increased competition in the IT Services business, improving quality, reducing costs and shortening schedules has become extremely important. A key strategy being adopted for achieving these goals is the use of an asset-based approach to service delivery, where standard reusable components developed by domain experts are minimally modified for each customer instead of creating custom solutions. One example of this approach is the use of contract templates, one for each type of service offered. A compliance checking system that measures how well actual contracts adhere to standard templates is critical for ensuring the success of such an approach. This paper describes the use of document similarity measures - Cosine similarity and Latent Semantic Indexing - to identify the top candidate templates on which a more detailed (and expensive) compliance analysis can be performed. Comparison of results of using the different methods are presented. Copyright 2009 ACM.",
      "title": "16562 Characteristics of document similarity measures for compliance analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74549211334&partnerID=40&md5=b11c65a81fdb0f21d6514469342600a7"
    },
    {
      "abstract": "An increasing number of recent information retrieval systems makes use of ontologies to help the users to detail queries and to come up with semantic representations of documents. A particular concern here is user-friendliness (usability) and scalability of those approaches for Web search purposes. In this paper, we present an approach where entities in an ontology are associated with domain terminology by feature vectors (FV). A FV reflects the semantic and linguistic neighbourhoods of a particular entity. The semantic neighbourhood is derived from an ontology and is based on related entities and specified properties, while linguistic neighbourhood is based on co-location of terms in a text corpus. Later, during the search process the FVs are used to filter and rerank the search results of the underlying search engine and thereby increasing the precision of the result. We elaborate on the approach and describe how the FVs are constructed. Then we report on a conducted evaluation where we analyse the sensitivity of the approach w.r.t. ontology quality and search tasks. Results indicate that the proposed approach and implemented prototype are able to improve the search results of a standard Web search engine. Furthermore, the analysis of the experiment data shows that the level of ontology specification is important for the quality of the FVs.  2010 ACM.",
      "title": "16564 An ontology-driven approach to web search: Analysis of its sensitivity to ontology quality and search tasks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954575265&partnerID=40&md5=e0132887fd41cae973f7e97ba643b781"
    },
    {
      "abstract": "A model of episodic memory is derived to propose algorithms of text categorization with semantic space models. Performances of two algorithms named Target vector and Sub-target vector are contrasted using textual material of the text-mining context DEFT09. The experience reported here have been realized on the english corpus which is composed of articles of the economic newspaper The Financial Times. The aim of the task was to categorize texts in function of the factuality or subjectivity they expressed. Results confirm (i) that the episodic memory metaphor provides a convenient framework to propose efficient algorithm for text categorization, and (ii) that Sub-target vector algorithm outperforms the Target vector algorithm.",
      "title": "16565 Detection of opinions and facts. A cognitive approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866878713&partnerID=40&md5=caf45f42f42ca8e4dad24db074c7e4f9"
    },
    {
      "abstract": "The maintenance of the domain ontology or a knowledge model after the appearance of changes in the studied domain is an essential stage. Several studies provide methodologies for the maintenance of an ontology but only some of them deal with ontologies that are created from texts. Text mining techniques provide good results when the processing of texts is done for the purpose of modelling or classification. The objective of our work is the use of text mining techniques for the maintenance of an ontology representing a dynamic domain, according to an analysis of textual changes and their effects on the corresponding ontology.",
      "title": "16566 The management of the knowledge evolution by using text mining techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884840659&partnerID=40&md5=bfcee6742b55d5001ce447f98b86f789"
    },
    {
      "abstract": "Question classification plays a crucial important role in the question answering system. Recent research on question classification for open-domain mostly concentrates on using machine learning methods to resolve the special kind of text classification. This paper presents our research about Chinese question classification using machine learning method and gives our approach based on SVM and semantic gram extraction. SVM has been widely used for question classification and got good performances. We use SVM as the classifier and propose a new feature extraction method of Chinese questions which is called semantic gram extraction. The method is proposed based on the word semantics and N-gram. The experiment results show that the feature extraction can perform well with SVM and our approach can reach high classification accuracy.  2009 IEEE.",
      "title": "16570 Chinese question classification based on semantic gram and SVM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949787817&partnerID=40&md5=90038c39487edb415af5de3f3601d3a8"
    },
    {
      "abstract": "Detecting and tracking of temporal data is an important task in multiple applications. In this paper we study temporal text mining methods for Music Information Retrieval. We compare two ways of detecting the temporal latent semantics of a corpus extracted from Wikipedia, using a stepwise Probabilistic Latent Semantic Analysis (PLSA) approach and a global multiway PLSA method. The analysis indicates that the global analysis method is able to identify relevant trends which are difficult to get using a step-by-step approach. Furthermore we show that inspection of PLSA models with different number of factors may reveal the stability of temporal clusters making it possible to choose the relevant number of factors.  2009 IEEE.",
      "title": "16572 Temporal analysis of text data using latent variable models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77950957833&partnerID=40&md5=7829fc9cc584fc152eabdc935b9786c6"
    },
    {
      "abstract": "A mind map is a diagram used to represent words, ideas, or other items linked to and arranged around a central keyword or idea. Mind maps are used to generate, visualize, structure, and classify ideas, and as an aid in organization, study, project management, problem solving, decision making, and writing. It has been long used in brainstorming and as an effective educational tool. There are numerous tools in the market, either as freeware or as proprietary software, that help users generate mind maps.. However, these tools are more of mind map editing tools to help users project their ideas from their minds into the tool mapping space. These tools also provide a comprehensive library of images that suits the most popular mind map types. The tools act as the media into which users projects the maps that has already more-or-less matured in their minds. In this work, we present a software tool that automatically generates mind maps directly from text. This tool provides a prospect to transform many literatures automatically into mind maps. One significant application of this tool is education. Many students finds it easier to follow and remember information presented in the mid map form rather than pure text. 2009 IEEE.",
      "title": "16573 Direct automatic generation of mind maps from text with M2Gen",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952726354&partnerID=40&md5=1da1bd65a0a8ef45475958c8ddd7ed2a"
    },
    {
      "abstract": "The extraction of verb synonyms is a key technology to build a verb dictionary as a language resource. This paper presents a coclustering-based verb synonym extraction approach that increases the number of extracted meanings of polysemous verbs from a large text corpus. For verb synonym extraction with a clustering approach dealing with polysemous verbs can be one problem issue because each polysemous verb should be categorized into different clusters depending on each meaning",
      "title": "16574 Co-clustering with recursive elimination for verb synonym extraction from large text corpus",
      "url": ""
    },
    {
      "abstract": "A new text categorization method based on Singular Value Decomposition (SVD) and Cascade Correlation (CC) algorithm is proposed. Most traditional classification systems represent the contents of documents with Vector Space Model (VSM) which represents documents with a set of index terms. However, this model needs a high dimensional space to represent the documents and it does not take into account the semantic relationship between terms, which could lead to poor classification performance. In contrast, SVD can represent relations among very large number of words and very large number of natural text passages in which they occur. It can not only greatly reduce the dimensionality but also discover the important relationships between terms. Based on this idea, we use singular value decomposition (SVD) to represent our documents in this paper. Then we use neural network constructed by cascade correlation (CC) algorithm to classify these represented documents. The experiments show that our method helps to accelerate the training speed and improves the classification accuracy as well.  2009 IEEE.",
      "title": "16577 A new text categorization method based on SVD and cascade correlation algorithm",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949310839&partnerID=40&md5=cd7cc6ff107157683ce07cac45d21471"
    },
    {
      "abstract": "Textual attributes in relational databases contain useful information that sometimes is not handled in a proper way. Syntactic querying of textual fields provides only superficial information, avoiding issues related to the semantics of data. In order to extract relevant information from these fields, a text mining process is conducted. The AP-Sets mining structure allows the building of an intermediate representation for texts based on the Apriori property. This intermediate form has the shape of a lattice of text itemsets, and can be translated into a basic ontology using semantic relations extracted from WordNet lexical database. The ontology generated can be seen as a summarized structured description of the contents of the textual attribute. Furthermore, it adds semantic capabilities to the query process on textual attributes.  2009 Springer-Verlag Berlin Heidelberg.",
      "title": "16579 Semantic enrichment of database textual attributes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70549093165&partnerID=40&md5=698194386aa4533e3d9ba13bb07dbc50"
    },
    {
      "abstract": "As a result of advance in Internet technology, automatic text sentiment classification for a large amount of on-line documents in the form of surveys or called reviews becomes attractive. The task of sentiment classification is to construct an effective classifier with the knowledge data of vocabularies semantic meaning and the relationships between the vocabularies to determine the sentiment orientation of a document. In this paper, one method combining HowNet knowledge base with a robust supervised sentiment classifier is proposed. It computes semantic similarity of characteristic words and phrases by using HowNet. Sentiment features of text are divided into characteristic words and phrases, and they adopt the positive and negative terms as features of sentiment classifier. Finally in the experiment, evaluation results show the effectiveness of our method.  2009 ISSN.",
      "title": "16580 Learning to classify semantic orientation on on-line document",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74349124357&partnerID=40&md5=a6e06ef467a07b521206473acf40cdc6"
    },
    {
      "abstract": "Introduction: This paper explores the benefits of using n-grams and semantic features for the classification of disease outbreak reports, in the context of the BioCaster disease outbreak report text mining system. A novel feature of this work is the use of a general purpose semantic tagger - the USAS tagger - to generate features. Background: We outline the application context for this work (the BioCaster epidemiological text mining system), before going on to describe the experimental data used in our classification experiments (the 1000 document BioCaster corpus). Feature sets: Three broad groups of features are used in this work: Named Entity based features, n-gram features, and features derived from the USAS semantic tagger. Methodology: Three standard machine learning algorithms - Naive Bayes, the Support Vector Machine algorithm, and the C4.5 decision tree algorithm - were used for classifying experimental data (that is, the BioCaster corpus). Feature selection was performed using the Chi2 feature selection algorithm. Standard text classification performance metrics - Accuracy, Precision, Recall, Specificity and F-score - are reported. Results: A feature representation composed of unigrams, bigrams, trigrams and features derived from a semantic tagger, in conjunction with the Naive Bayes algorithm and feature selection yielded the highest classification accuracy (and F-score). This result was statistically significant compared to a baseline unigram representation and to previous work on the same task. However, it was feature selection rather than semantic tagging that contributed most to the improved performance. Conclusion: This study has shown that for the classification of disease outbreak reports, a combination of bag-of-words, n-grams and semantic features, in conjunction with feature selection, increases classification accuracy at a statistically significant level compared to previous work in this domain.  2009 Elsevier Ireland Ltd. All rights reserved.",
      "title": "16581 Classifying disease outbreak reports using n-grams and semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-71549152231&partnerID=40&md5=e05b3d7ac52d21cd9f721202269de97e"
    },
    {
      "abstract": "In this paper, a novel document representation model the Phrases Semantic Similarity Based Model (PH-SSBM), is proposed. This model combines phrases analysis as well as words analysis with the use of WordNet as background knowledge to explore better ways of documents representation for clustering. The PH-SSBM assigns semantic weights to both document words and phrases. The new weights reflect the semantic relatedness between documents terms and capture the semantic information in the documents. The PH-SSBM finds similarity between documents based on matching terms (phrases and words) and their semantic weights. Experimental results show that the phrases semantic similarity based model (PH-SSBM) in conjunction with WordNet has a promising performance improvement for text clustering.  2009 IEEE.",
      "title": "16582 PH-SSBM: Phrase semantic similarity based model for document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951152359&partnerID=40&md5=7fb7ca026f019f85c28493b8dd984980"
    },
    {
      "abstract": "In this paper, we proposed a new generalized Multivariate Probalistic Modeling (MPM) to automatically extract topics from text collection and attach them with existing ontology. Specially, we first make use of KeyConcept which is a classification system classify documents into a set of predefined concepts.Then, by modeling documents cluster based MPM, we extract latent concepts and corresponding sub-clusters from document collection. We compare our MPM with Probabilistic Latent Semantic Indexing (PLSI) and other clustering algorithm on Citeseerx data sets. Experiment results show that MPM outperforms PLSI in terms of time efficiency and provides better topics representation. Clustering analysis also prove the advantages of our MPM over other clustering technique in precision.  2009 IEEE.",
      "title": "16585 Mining hidden concepts for ontology extension using Multivariate Probabilistic Modeling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84856823042&partnerID=40&md5=4cc6ddd3619a94668408d51d6f1ae11c"
    },
    {
      "abstract": "The field of information retrieval and text manipulation (classification, clustering) still strives for models allowing semantic information to be folded in to improve performance with respect to standard bag-of-word based models. Many approaches aim at a concept-based retrieval, but differ in the nature of the concepts, which range from linguistic concepts as defined in lexical resources such as WordNet, latent topics derived from the data itself - as in Latent Semantic Indexing (LSI) or (Latent Dirichlet Allocation (LDA) - to Wikipedia articles as proxies for concepts, as in the recently proposed Explicit Semantic Analysis (ESA) model. A crucial question which has not been answered so far is whether models based on explicitly given concepts (as in the ESA model for instance) perform inherently better than retrieval models based on latent concepts (as in LSI and/or LDA). In this paper we investigate this question closer in the context of a cross-language setting, which inherently requires concept-based retrieval bridging between different languages. In particular, we compare the recently proposed ESA model with two latent models (LSI and LDA) showing that the former is clearly superior to the both. From a general perspective, our results contribute to clarifying the role of explicit vs. implicitly derived or latent concepts in (crosslanguage) information retrieval research.",
      "title": "16586 Explicit versus latent concept models for cross-language information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77953999264&partnerID=40&md5=34e8fac24effeea26140a944c2284b02"
    },
    {
      "abstract": "Technology of automatic text summarization plays an important role in information retrieval and text classification, and may provide a solution to the information overload problem. Text summarization is a process of reducing the size of a text while preserving its information content. This paper proposes a sentences clustering based summarization approach. The proposed approach consists of three steps: first clusters the sentences based on the semantic distance among sentences in the document, and then on each cluster calculates the accumulative sentence similarity based on the multifeatures combination method, at last chooses the topic sentences by some extraction rules. The purpose of present paper is to show that summarization result is not only depends the sentence features, but also depends on the sentence similarity measure. The experimental result on the DUC 2003 dataset show that our proposed approach can improve the performance compared to other summarization methods.  2009 IEEE.",
      "title": "16588 Automatic text summarization based on sentences clustering and extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954223824&partnerID=40&md5=013c79475085d7eacdbff1d25f5ae273"
    },
    {
      "abstract": "Traditional vector-space-based text-classification models are established by calculating the weights of feature words on the lexical level. In such models, words are independent on one another and their semantic relations are unrevealed. This paper proposes a vector-space-based text analyzer by introducing conceptual semantic similarity into traditional vector-space-based models. Naive Bayes classification technology is also adopted into this new analyzer. Experiment results indicate that the new analyzer can improve text classification.  2009 IEEE.",
      "title": "16589 Study on the classification of mixed text based on conceptual vector space model and Bayes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77950869935&partnerID=40&md5=7630907eb980b368d4618380e72a3a6e"
    },
    {
      "abstract": "Getting a quick impression of the authors intention of a text is an task often performed. An authors intention plays a major role in successfully understanding a text. For supporting readers in this task, we present an intentional approach to visual text analysis, making use of tag clouds. The objective of tag clouds is presenting meta-information in a visually appealing way. How ever there is also much uncertainty associated with tag clouds, such as giving the wrong impression. It is not clear whether the authors intent can be grasped clearly while looking at a corresponding tag cloud. Therefore it is interesting to ask to what extent, with tag clouds, it is possible to support the user in understanding intentions expressed. In order to answer this question, we construct an intentional perspective on textual content. Based on an existing algorithm for extracting intent annotations from textual content we present a prototypical implementation to produce intent tag clouds, and describe a formative testing, illustrating how intent visualizations may support readers in understanding a text successfully. With the initial prototype, we conducted user studies of our intentional tag cloud visualization and a comparison with a traditional one that visualizes frequent terms. The evaluations results indicate, that intent tag clouds have a positive effect on supporting users in grasping an authors intent.",
      "title": "16591 Intent tag clouds: An intentional approach to visual text analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888396201&partnerID=40&md5=e5a0e8632a07aa6785760dfd9225623e"
    },
    {
      "abstract": "In this paper we present a question answering system supported by semantic graphs. Aside from providing answers to natural language questions, the system offers explanations for these answers via a visual representation of documents, their associated list of facts described by subject - verb - object triplets, and their summaries. The triplets, automatically extracted from the Penn Treebank parse tree obtained for each sentence in the document collection, can be searched, and we have implemented a question answering system to serve as a natural language interface to this search. The vocabulary of questions is general because it is not limited to a specific domain, however the questionss grammatical structure is restricted to a predetermined template because our system can understand only a limited number of question types. The answers are retrieved from the set of facts, and they are supported by sentences and their corresponding document. The document overview, comprising the semantic representation of the document generated in the form of a semantic graph, the list of facts it contains and its automatically derived summary, offers an explanation to each answer. The extracted triplets are further refined by assigning the corresponding co referenced named entity, by resolving pronominal anaphors, as well as attaching the associated WordNet synset. The semantic graph belonging to the document is developed based on the enhanced triplets while the document summary is automatically generated from the semantic description of the document and the extracted facts.",
      "title": "16592 Question answering based on semantic graphs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84887442475&partnerID=40&md5=bc7df2ad87ad767e2f65f78bcf531fe8"
    },
    {
      "abstract": "One major problem in text mining and semantic retrieval is that detected entity mentions have to be assigned to the true underlying entity. The ambiguity of a name results from both the polysemy and synonymy problem, as the name of a unique entity may be written in variant ways and different unique entities may have the same name. The term bush for instance may refer to a woody plant, a mechanical fixing, a nocturnal primate, 52 persons and 8 places covered in Wikipedia and thousands of other persons. For the first time, according to our knowledge we apply a kernel entity resolution approach to the German Wikipedia as reference for named entities. We describe the context of named entities in Wikipedia and the context of a detected name phrase in a new document by a context vector of relevant features. These are designed from automatically extracted topic indicators generated by an LDA topic model. We use kernel classifiers, e.g. rank classifiers, to determine the right matching entity but also to detect uncovered entities. In comparison to a baseline approach using only text similarity the addition of topics approach gives a much higher f-value, which is comparable to the results published for English. It turns out that the procedure also is able to detect with high reliability if a person is not covered by the Wikipedia.",
      "title": "16593 Named entity resolution using automatically extracted semantic information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874177050&partnerID=40&md5=f66b8dd19e9227ab3c031a282d36b2d7"
    },
    {
      "abstract": "Lexical databases are invaluable sources of knowledge about words and their meanings, with numerous applications in areas like NLP, IR, and AI. We propose a methodology for the automatic construction of a large-scale multilingual lexical database where words of many languages are hierarchically organized in terms of their meanings and their semantic relations to other words. This resource is bootstrapped from WordNet, a well-known English-language resource. Our approach extends WordNet with around 1.5 million meaning links for 800,000 words in over 200 languages, drawing on evidence extracted from a variety of resources including existing (monolingual) wordnets, (mostly bilingual) translation dictionaries, and parallel corpora. Graph-based scoring functions and statistical learning techniques are used to iteratively integrate this information and build an output graph. Experiments show that this wordnet has a high level of precision and coverage, and that it can be useful in applied tasks such as cross-lingual text classification. Copyright 2009 ACM.",
      "title": "16594 Towards a universal wordnet by learning from combined evidence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74749100162&partnerID=40&md5=02934c737a918f076affee1ad3db0a00"
    },
    {
      "abstract": "The problem of semantic clustering of Natural Language texts is posed. The use of syntactic contexts of nouns as the base structure for automatic formation of the set of text attributes is discussed. The principle of revealing splintered values in the aforementioned contexts is described using Russian language material.  2009 Pleiades Publishing, Ltd.",
      "title": "16602 Formation and clustering of noun contexts within the framework of splintered values",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952794478&partnerID=40&md5=bca830ad20aba8129e84c6a0891d8159"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) is an effective technique for feature extraction in text mining, and supervised LSI (SLSI) algorithms have been proposed to exploit the class labels of training data. In this paper, we propose an iterative SLSI framework based on class selection. We show that a previous iterative SLSI algorithm is an instance of the framework. We also propose a method under our framework, which selects a class at each iteration using a simple classifier and computes the main bias vector of one class only. Our experiments demonstrate that the proposed method both improves the classification accuracy and reduces the computation cost. 2009 IEEE.",
      "title": "16603 Class selection based iterative supervised latent semantic indexing for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77949623859&partnerID=40&md5=ad3288e85f70fa05157c8dbe13950eab"
    },
    {
      "abstract": "In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization. In an empirical study, we construct 190 different text classification tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the specific task to be enhanced, and the prediction model used.",
      "title": "16604 Learning the semantic correlation: An alternative way to gain from unlabeled text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863390414&partnerID=40&md5=d14471786ad2795e19ab33a19b5b2384"
    },
    {
      "abstract": "Most Chinese text classification methods are based on Chinese word segmentation and Bag of Words (BOW). The classification performance largely relies on the accuracy of segmentation. Unfortunately, perfect precision and disambiguation of segmentation cannot be reached. In order to solve this problem, a novel Chinese text classification method using string kernel is presented. String kernel computes the similarity of a pair of documents by comparing common substrings they have. Experimental results show that our method greatly enhances the classification on small training data sets. Although the performance of traditional string kernel is comparable to that of BOW methods on larger data set, the dimension of feature space is so high that the calculation process is very time-consuming. Our proposed key characters string kernel technique solves the efficiency and effectiveness problems. Experiments on larger data set show that SVM with Key Characters String Kernel can achieve superior performance.  2009 IEEE.",
      "title": "16605 Chinese text classification using key characters string kernel",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77649334260&partnerID=40&md5=7408a26487d3c373d496fb5ce62d1fd0"
    },
    {
      "abstract": "Citations are ubiquitous in scientific articles and play important roles for representing the semantic content of a full-text biomedical article. In this work, we manually examined full-text biomedical articles to analyze the semantic content of citations in full-text biomedical articles. After developing a citation relation schema and annotation guideline, our pilot annotation results show an overall agreement of 0.71, and here we report on the research challenges and the lessons weve learned while trying to overcome them. Our work is a first step toward automatic citation classification in full-text biomedical articles, which may contribute to many text mining tasks, including information retrieval, extraction, summarization, and question answering. 2009 IEEE.",
      "title": "16608 Investigating and annotating the role of citation in biomedical full-text articles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72849132337&partnerID=40&md5=f285b5b51529ea30e98a51300e432235"
    },
    {
      "abstract": "A large amount of research results have shown that events exist in many texts. Understanding texts from semantics, texts are composed of events, and events are the basic semantic units for texts. We present a novel approach for computing text similarity, which selects events as the features for documents and computes text similarity from two points of view: event class and event instance. The number of events is usually much fewer than the number of key words in documents. From this side, extracting event characters from documents is a good attempt to solve the high dimension of documents. 2009 IEEE.",
      "title": "16611 Event-based text similarity computing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-73849090537&partnerID=40&md5=d0800fee9b959438be701df01f31b853"
    },
    {
      "abstract": "Successful text classification is highly dependent on the representations used. Currently, most approaches to text classification adopt the bag-of-words document representation approach, where the frequency of occurrence of each word is considered as the most important feature, but this method ignores important semantic relationships between key terms. In this paper, we proposed a system that uses ontologies and Natural Language Processing techniques to index texts. Traditional BOW matrix is replaced by Bag of Concepts (BOC). For this purpose, we developed fully automated methods for mapping keywords to their corresponding ontology concepts. Support Vector Machine a successful machine learning technique is used for classification. Experimental results shows that our proposed method dose improve text classification performance significantly.  2009 IEEE.",
      "title": "16615 Improving documents classification with semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74049146769&partnerID=40&md5=462162cacc9e5e097412fe51e5477790"
    },
    {
      "abstract": "To understand text contents better, many research efforts have been made exploring detection and classification of the semantic relation between a concept pair. As described herein, we present our study of a semantic relation classification task as a graph-based multi-view learning task: each intra-view graph is constructed with instances in the view",
      "title": "16617 Graph based multi-view learning for CDL relation classification",
      "url": ""
    },
    {
      "abstract": "Incorporating background knowledge into data mining algorithms is an important but challenging problem. Current approaches in semi-supervised learning require explicit knowledge provided by domain experts, knowledge specific to the particular data set. In this study, we propose an ensemble model that couples two sources of information: statistics information that is derived from the data set, and sense information retrieved from WordNet that is used to build a semantic binary model. We evaluated the efficacy of using our combined ensemble model on the Reuters-21578 and 20newsgroups data sets. Copyright 2009 ACM.",
      "title": "16618 Combining statistics and semantics via ensemble model for document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72949118503&partnerID=40&md5=919435af91334d45925cff5873d756d5"
    },
    {
      "abstract": "Breast cancer is the leading cause of cancer mortality in women between the ages of 15 and 54. During mammography screening, radiologists use a strict lexicon (BI-RADS) to describe and report their findings. Mammography records are then stored in a well-defined database format (NMD). Lately, researchers have applied data mining and machine learning techniques to these databases. They successfully built breast cancer classifiers that can help in early detection of malignancy. However, the validity of these models depends on the quality of the underlying databases. Unfortunately, most databases suffer from inconsistencies, missing data, inter-observer variability and inappropriate term usage. In addition, many databases are not compliant with the NMD format and/or solely consist of text reports. BI-RADS feature extraction from free text and consistency checks between recorded predictive variables and text reports are crucial to addressing this problem. We describe a general scheme for concept information retrieval from free text given a lexicon, and present a BI-RADS features extraction algorithm for clinical data mining. It consists of a syntax analyzer, a concept finder and a negation detector. The syntax analyzer preprocesses the input into individual sentences. The concept finder uses a semantic grammar based on the BI-RADS lexicon and the experts input. It parses sentences detecting BI-RADS concepts. Once a concept is located, a lexical scanner checks for negation. Our method can handle multiple latent concepts within the text, filtering out ultrasound concepts. On our dataset, our algorithm achieves 97.7% precision, 95.5% recall and an F1-score of 0.97. It outperforms manual feature extraction at the 5% statistical significance level.  2009 IEEE.",
      "title": "16621 Information extraction for clinical data mining: A mammography case study",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951170652&partnerID=40&md5=bd9f500f59bddb47c1963b1f1737fdd9"
    },
    {
      "abstract": "Ambiguities, which are inherently present in natural languages represent a challenge of determining the actual identities of entities mentioned in a document (e.g., Paris can refer to a city in France but it can also refer to a small city in Texas, USA or to a 1984 film directed by Wim Wenders having title Paris, Texas). Disambiguation is a problem that can be successfully solved by entity resolution methods. This paper studies various methods for estimating relatedness between entities, used in collective entity resolution. We define a unified entity resolution approach, capable of using implicit as well as explicit relatedness for collectively identifying in-text entities. As a relatedness measure, we propose a method, which expresses relatedness using the heterogeneous relations of a domain ontology. We also experiment with other relatedness measures, such as using statistical learning of co-occurrences of two entities or using content similarity between them. Evaluation on real data shows that the new methods for relatedness estimation give good results.  2009 Springer-Verlag Berlin Heidelberg.",
      "title": "16623 Entity resolution in texts using statistical learning and ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77149158816&partnerID=40&md5=955c587d07641bbdef874a027c1e2e24"
    },
    {
      "abstract": "In this paper, we introduced a new matching method based on logic-centered paragraphs. This method is built on the concept dictionary, in this method, the paragraphs which have the same meaning will be clustered by analyzing the logical concept of the text to be classified, and establish the logical paragraphs on the basis of the division method of logical levels. Then put the text to be classified in the right classification, which considered the contribution to the theme of different paragraphs in the text. At the same time, in order to solve problem of synonyms and polysemy in the texts to be classified, we introduced the expansion of the synonyms concept and related words. Experimental results show that this method can improve the effectiveness of classification, and a higher accuracy rate can be obtained in content flitting 2009 IEEE.",
      "title": "16625 A logical paragraph division based on semantic characteristics and its application",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77950147981&partnerID=40&md5=656e365ffde1e594029c4e86128ed616"
    },
    {
      "abstract": "In this article, we present a semi-automated approach for identifying candidate early aspects in requirements specifications. This approach aims at improving the precision of the aspect identification process in use cases, and also solving some problems of existing aspect mining techniques caused by the vagueness and ambiguity of text in natural language. To do so, we apply a combination of text analysis techniques such as: natural language processing (NLP) and word sense disambiguation (WSD). As a result, our approach is able to generate a graph of candidate concerns that crosscut the use cases, as well as a ranking of these concerns according to their importance. The developer then selects which concerns are relevant for his/her domain. Although there are still some challenges, we argue that this approach can be easily integrated into a UML development methodology, leading to improved requirements elicitation. Copyright 2009 ACM.",
      "title": "16627 Early aspect identification from use cases using NLP and WSD techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954019099&partnerID=40&md5=260ff52e1efe81ac65147757ab8af858"
    },
    {
      "abstract": "Public relations is an important management staff function in this computer age, and accurate media information retrieval and appropriate measurement for publicity tracking in a campaign management system is of utmost importance. This paper proposes an intelligent campaign management system, called KnowleTracker, based on a semantic service-oriented approach. KnowleTracker has powerful deep mining functions to pull out news and other information that may lie several layers below the front page based on a semantic search. The returned results are presented at different levels of detail and viewed from different angles, and show customers, detailed information of where their news releases are distributed. We demonstrate the utility of KnowleTracker in a real-world scenario through a case study on A*STAR Event Campaign Management, and argue that our system is an effective tool to tackle the complex task of campaign management and tracking.  2010 ACM.",
      "title": "16631 KnowleTracker: A semantic service-oriented system for enterprise campaign management",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954574394&partnerID=40&md5=641de53cb0f312ed814fb896d867d2ba"
    },
    {
      "abstract": "With the rapid development of information technology, huge data is accumulated. A vast amount of such data appears as short messages such as emails in companies or conversations in open chatting rooms. It is useful to find the themes or exceptional information from the messages by clustering the short documents based on density. However, traditional vector space model based text clustering algorithms can not get acceptable accuracy because the key words appear at low frequency. On the other hand, traditional text clustering algorithms become very inefficient or even unavailable when processing massive data at TB level. In this paper a densitybased short message clustering algorithm using domain ontology based is presented. This algorithm uses domain ontology to calculate the semantic similarity between short messages which improves the Parallel method is also used to get better scalability.  2009 IEEE.",
      "title": "16633 Density-based clustering of massive short messages using domain ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70450194232&partnerID=40&md5=e9946b06728edf9b56e0dd7a1942a7cb"
    },
    {
      "abstract": "This paper focuses on information retrieval aspects of a new application in domain of collaborative systems based on utilization of semantic principles for representation of different types of knowledge, collaborative objects and relations between them. Proposed collaborative system (within European IST project called KP-Lab) uses ontologies as common communication framework and exchange format for different types of end-user tools. Theoretical background is provided by innovative theoretical approach called Trialogical learning. Information retrieval in KP-Lab System is supported by designed and implemented text-mining and search services. These two sets of functionalities provide features for management of shared objects, described with content in textual format, as well as with semantic metadata.  2009 Springer Berlin Heidelberg.",
      "title": "16634 Use of semantic principles in a collaborative system in order to support effective information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70450160454&partnerID=40&md5=362934cd3f9899997e1a7fe3c2c2990e"
    },
    {
      "abstract": "In this contribution we introduce SWeMoF, a semantic framework to discover patterns in learning networks and the blogosphere. Based on a description of the state of the art in data mining, text mining and blog mining we discuss the architecture of the Semantic Weblog Monitoring Framework (SWeMoF) and provide an outlook and an evaluation perspective for future research and development.  2009 Springer Berlin Heidelberg.",
      "title": "16635 SWeMoF: A semantic framework to discover patterns in learning networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70450213773&partnerID=40&md5=0fb8b3cb7adc9b9c62b80edb2d4a6727"
    },
    {
      "abstract": "This paper presents a Biomedical Semantic-based Association Rule method that significantly reduces irrelevant connections through semantic filtering. The experiment result shows that compared to traditional association rule-based approach, our approach generates much fewer rules and a lot of these rules represent relevant connections among biological concepts.",
      "title": "16636 A text mining method for discovering hidden links",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70449931047&partnerID=40&md5=2a6cf774f94cd438274684d4887e79d5"
    },
    {
      "abstract": "The purpose of the present work was to create a computational framework for radiological registry and diagnosis, which, by means of a nonintrusive approach, allows the users freedom annotation while storing related information in a structured and standard format. To get this goal, this work started with the investigation of lexical and semantic domain of radiological texts, followed by the design and implementation of an ontology, called RadOn, and the modeling of a database based on it. The next step was the development of a set of integrated software components, called E-Rad, based on text mining techniques, which identifies and extracts words, terms and expressions from free texts, adjusts them to the ontology structure and stores them. Finally, a query engine that works on the E-Rad structure, having a user interface, was implemented. The outcome is a computational environment that, by means of the implemented ontology and related tools, makes the data semantic explicit and allows the use of the context information in an optimal and standard way.  2009 IEEE.",
      "title": "16637 An ontology-based framework to support nonintrusive storage and analysis of radiological diagnosis data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70449637868&partnerID=40&md5=13cbb8e02f49f9cdf6c2be1fa5eb3f74"
    },
    {
      "abstract": "Text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among index terms while computing similarity. Adding expansion terms to the vector representation can also improve effectiveness. However, existing semantic smoothing kernels do not employ term expansion. This paper proposes a new nonlinear kernel for text classification to exploit semantic relations between terms to add weighted expansion terms.  2009 IEEE.",
      "title": "16638 A kernel-based feature weighting for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70449457204&partnerID=40&md5=9b1ab874b003b504a93bbc0a864a3dd4"
    },
    {
      "abstract": "In the field of information processing, most of the existing text clustering algorithm is based on Vector Space Model (VSM). However, VSM can not effectively express the structure of the text so that it can not fully express the semantic information of the text. In order to improve the ability of expression in the semantic information, this paper presents a new text structure graph model. With the weighted graph, this model expresses the characteristics term of the text and its associated location information. On this basis of spectral graph seriation, a spectral clustering algorithm is put forward. This algorithm replace solving common subgraph with matrix computation, then reduce the computational complexity of graph clustering. There are also algorithm analysis and experiment in the paper. The results of the study show that the text clustering algorithm based on spectral graph seriation is effective and feasible.  2009 IEEE.",
      "title": "16639 Text clustering algorithm based on spectral graph seriation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70449449798&partnerID=40&md5=234de5b28d0dbefc387c53e770170282"
    },
    {
      "abstract": "The content of a text is mainly defined by keywords and named entities occurring in it. In particular for news articles, named entities are usually important to define their semantics. However, named entities have ontological features, namely, their aliases, types, and identifiers, which are hidden from their textual appearance. In this paper, we explore weighted combinations of those latent named entity features with keywords for text clustering. To that end, the traditional Vector Space Model is adapted with multiple vectors defined over spaces of entity names, types, name-type pairs, identifiers, and keywords. Clustering quality is evaluated by both of the self purity-separation type and the relative comparison type of measures. Hard and fuzzy clustering experiments of the proposed model on selected data subsets of Reuters-21578 are conducted and evaluated.  2009 IEEE.",
      "title": "16640 Latent ontological feature discovery for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-71049185892&partnerID=40&md5=df08c11951b33c8cbbf41095c5f18a4d"
    },
    {
      "abstract": "There have been many researches on how to recommend tags for weblogs. In this paper, we propose a novel automatic tag recommendation algorithm, which can be used in the large-scale and real-time data process effectively and efficiently. Most existing researches on tag suggestion focus on firstly mining the relationship between testing and training data and then assigning the top ranked tags of the most related training data to the testing object. However, they ignore the internal relationship between tags and weblogs. According to our research, more than 43% tags, which have been labeled by weblog users, have actually been used in the body of the text. At the meanwhile, the term frequency distribution, the paragraph frequency distribution and the first occurrence position of tags are very different from the ones of non-tags in the text. In this paper, the tags of a weblog are assigned in two steps. First of all, some probability distributions of the word attributes are trained by the labeled training weblogs, and some keywords of a testing weblog are extracted as one part of the tags based on the probability distributions. Then the other part of the tags are obtained from the first part ones with the help of Latent Semantic Indexing (LSI) model. Experiments on a large-scale tagging dataset of weblogs 12 show that the average tagging time for a new weblog is less than 0.02 seconds, and over 74% testing weblogs are correctly labeled with the top 15 tags.  2009 IEEE.",
      "title": "16642 Automatic tag recommendation for weblogs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-71049177787&partnerID=40&md5=09bd4c96d83829c67851b5e39c154a1d"
    },
    {
      "abstract": "This paper proposes a weighted k-means clustering algorithm based on k-means (MacQueen, 1967",
      "title": "16643 Weighted k-means algorithm based text clustering",
      "url": "Conference Paper"
    },
    {
      "abstract": "In order to solve such problems as lack of innovative knowledge in the process of product creation design and the resulting exhaustion of creative thinking sources, this thesis propose a method to obtain from the Internet the knowledge which has auxiliary effects on creative thinking, by taking into consideration the knowledge categories and its characteristics in the product innovation design. First, the author puts forward and analyses the thought and reason which take knowledge texts of expert as the description carrier of his thinking and cognition condition, and expands the vocabulary of public features in knowledge texts on the basis of domain ontology. Based on the above, knowledge which conforms tothe thinking and cognition condition of experts is acquired from the Internet through search and filtration. Then, the knowledge base of product creation design to help innovative thinking is constructed by adopting the technology of knowledge element extraction and the knowledge structural representation is effected in the form of semantic network of knowledge elements. In the end of this paper, the author cites a concrete example to demonstrate the effectiveness and validity of the proposed method in stimulating innovative thinking.  2009 IEEE.",
      "title": "16646 A study of internet-knowledge acquiring method to aid thinking in product design",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70449120055&partnerID=40&md5=8e3cbfdf9befd3bad5ccb6fd85c4d459"
    },
    {
      "abstract": "Extension of ontology instance is the important part of ontology maintenance. In this paper, a novel and effective method is proposed to extending ontology instances from Chinese free text, which is achieved with classification using support vector machine (SVM). Firstly, classification features are extracted in terms of syntax and semantics from the training texts and the new texts based on the existed Chinese ontology. Then the ontology is turned into tree hierarchical structure which is used as the training and learning strategy of SVM classifier. Finally new ontology instances are extracted from the new texts according to the training results. The advantage of this method is that the semantic of ontology elements in texts is made full use of, and instances extraction and classification are completed in the identical procedure at same time. Experimental results show that the average accuracy of instances extraction and classification reached 86.6%, which is satisfactory.  2009 IEEE.",
      "title": "16647 Research on chinese ontology instance extension based on SVM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350707823&partnerID=40&md5=4aafd99bab5ef2a3c420f6188aa2ecfb"
    },
    {
      "abstract": "In traditional text clustering methods, documents are represented as bags of words without considering the semantic information of each document. For instance, if two documents use different collections of core words to represent the same topic, they may be falsely assigned to different clusters due to the lack of shared core words, although the core words they use are probably synonyms or semantically associated in other forms. The most common way to solve this problem is to enrich document representation with the background knowledge in an ontology. There are two major issues for this approach: (1) the coverage of the ontology is limited, even for WordNet or Mesh, (2) using ontology terms as replacement or additional features may cause information loss, or introduce noise. In this paper, we present a novel text clustering method to address these two issues by enriching document representation with Wikipedia concept and category information. We develop two approaches, exact match and relatedness-match, to map text documents to Wikipedia concepts, and further to Wikipedia categories. Then the text documents are clustered based on a similarity metric which combines document content information, concept information as well as category information. The experimental results using the proposed clustering framework on three datasets (20-newsgroup, TDT2, and LA Times) show that clustering performance improves significantly by enriching document representation with Wikipedia concepts and categories. Copyright 2009 ACM.",
      "title": "16648 Exploiting wikipedia as external knowledge for document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350637781&partnerID=40&md5=f0e3cc66e042a132d1f6390b0fbf3011"
    },
    {
      "abstract": "In this paper, we present a technique for visual analysis of documents based on the semantic representation of text in the form of a directed graph, referred to as semantic graph. This approach can aid data mining tasks, such as exploratory data analysis, data description and summarization. In order to derive the semantic graph, we take advantage of natural language processing, and carry out a series of operations comprising a pipeline, as follows. Firstly, named entities are identified and coreference resolution is performed",
      "title": "16649 Visual analysis of documents with semantic graphs",
      "url": ""
    },
    {
      "abstract": "Sentiment analysis of weblogs is a challenging problem. Most previous work utilized semantic orientations of words or phrases to classify sentiments of weblogs. The problem with this approach is that semantic orientations of words or phrases are investigated without considering the domain of weblogs. Weblogs contain the authors various opinions about multifaceted topics. Therefore, we have to treat a semantic orientation domain-dependently. In this paper, we present an unsupervised learning model based on aspect model to classify sentiments of weblogs. Our model utilizes domain-dependent semantic orientations of latent variables instead of words or phrases, and uses them to classify sentiments of weblogs. Experiments on several domains confirm that our model assigns domain-dependent semantic orientations to latent variables correctly, and classifies sentiments of weblogs effectively.  2009 Springer Berlin Heidelberg.",
      "title": "16650 Extracting domain-dependent semantic orientations of latent variables for sentiment classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350657185&partnerID=40&md5=28b9c2535b4dff56f9ae133904e0123d"
    },
    {
      "abstract": "In this article we present a model of human written text based on statistical mechanics consideration. The empirical derivation of the potential energy for the parts of the text and the calculation of the thermodynamic parameters of the system, show that the specific heat corresponds to the semantic classification of the words in the text, separating keywords, function words and common words. This can give advantages when the model is used in text searching mechanisms.  2009 Springer Berlin Heidelberg.",
      "title": "16651 Written texts as statistical mechanical problem",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350597562&partnerID=40&md5=403735381e8aa3ae4207bacdfa40dc27"
    },
    {
      "abstract": "In this paper, a hybrid music recommendation system is proposed, which combines collaborative filtering and content-base recommendation. Neither of these two parts can make full use of all the information. Our method integrates both user rating and music content information using an expansion method of LSA (Latent Semantic Analysis) called M-LSA. We use a text representation for music content information, which is obtained by K-means Clustering or HMM method. Experiments on the data of 300 popular songs show that the proposed approach achieves satisfactory results.  2009 IEEE.",
      "title": "16653 A hybrid music recommendation system by M-LSA",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350550160&partnerID=40&md5=e5388121cd37ac21034f1649fb34eb1e"
    },
    {
      "abstract": "Current classification methods are based on the Bag of Words (BOW) representation, which only accounts for term frequency in the documents, and ignores important semantic relationships between key terms. In this paper, we proposed a system that uses ontologies and Natural Language Processing techniques to index texts. Traditional BOW matrix is replaced by Bag of Concepts(BOC). For this purpose, we developed fully automated methods for mapping keywords to their corresponding ontology concepts. Support Vector Machine a successful machine learning technique is used for classification. Experimental results shows that our proposed method dose improve text classification performance significantly.  2009 IEEE.",
      "title": "16656 Applying RDF ontologies to improve text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350521743&partnerID=40&md5=f075e2b1c02ff266ff3f0a19170a122b"
    },
    {
      "abstract": "This paper proposes the use of non-extensive entropy for text classification. Non-extensive entropy technique is used for text classification by estimating the conditional distribution of the class variable given the document. The underlying principle of non-extensive entropy is that without external knowledge, one should prefer distributions that are uniform. This paper proposes two models for text classification based on maximum entropy principle. The first model extends Shannon entropy into non-extensive entropy to simplify the form of classifier, the other one introduces high-level constraints into non-extensive model to impose constraints on the pairs of entities. Model with high-level constraints constructs relations between word pairs which builds semantic constraints, for the sake of advancing accuracy of text classification. Experiments on the 20-newsgroup set demonstrate the advantage of non-extensive model and non-extensive model with high-level constraints.  2009 Springer Berlin Heidelberg.",
      "title": "16657 Using non-extensive entropy for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350378279&partnerID=40&md5=6259b3c76c2121101299b07c0eb688db"
    },
    {
      "abstract": "Most past approaches to data mining have been based on association rules. However, the simple application of association rules usually only changes the users problem from dealing with millions of data points to dealing with thousands of rules. Although this may somewhat reduce the scale of the problem, it is not a completely satisfactory solution. This paper presents a new data mining technique, called knowledge cohesion (KC), which takes into account a domain ontology and the users interest in exploring certain data sets to extract knowledge, in the form of semantic nets, from large data sets. The KC method has been successfully applied to mine causal relations from oil platform accident reports. In a comparison with association rule techniques for the same domain, KC has shown a significant improvement in the extraction of relevant knowledge, using processing complexity and knowledge manageability as the evaluation criteria.  2009 Cambridge University Press.",
      "title": "16659 From data to knowledge mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-74149087511&partnerID=40&md5=e0abb112a53878d5549fd568807750d2"
    },
    {
      "abstract": "Information nowadays has become more and more accessible, so much as to give birth to an information overload issue. Yet important decisions have to be made, depending on the available information. As it is impossible to read all the relevant content that helps one stay informed, a possible solution would be condensing data and obtaining the kernel of a text by automatically summarizing it. We present an approach to analyzing text and retrieving valuable information in the form of a semantic graph based on subject-verb-object triplets extracted from sentences. Once triplets have been generated, we apply several techniques in order to obtain the semantic graph of the document: co-reference and anaphora resolution of named entities and semantic normalization of triplets. Finally, we describe the automatic document summarization process starting from the semantic representation of the text. The experimental evaluation carried out step by step on several Reuters newswire articles shows a comparable performance of the proposed approach with other existing methodologies. For the assessment of the document summaries we utilize an automatic summarization evaluation package, so as to show a ranking of various summarizers.",
      "title": "16665 Semantic graphs derived from triplets with application in document summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350138188&partnerID=40&md5=63211193eedb19cfa431f5e4f075bd4a"
    },
    {
      "abstract": "Protein-protein interaction (PPI) identification is an integral component of many biomedical research and database curation tools. Automation of this task through classification is one of the key goals of text mining (TM). However, labelled PPI corpora required to train classifiers are generally small. In order to overcome this sparsity in the training data, we propose a novel method of integrating corpora that do not contain relevance judgements. Our approach uses a semantic language model to gather word similarity from a large unlabelled corpus. This additional information is integrated into the sentence classification process using kernel transformations and has a re-weighting effect on the training features that leads to an 8% improvement in F-score over the baseline results. Furthermore, we discover that some words which are generally considered indicative of interactions are actually neutralised by this process.  2009 Springer Berlin Heidelberg.",
      "title": "16668 Semi-supervised prediction of protein interaction sentences exploiting semantically encoded metrics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349873530&partnerID=40&md5=3406cefbee9f04751c280cca5cf52f0c"
    },
    {
      "abstract": "As an unsupervised learning process, document clustering has been used to improve information retrieval performance by grouping similar documents and to help text mining approaches by providing a high-quality input for them. In this article, the authors propose a novel hybrid clustering technique that incorporates semantic smoothing of document models into a neural network framework. Recently, it has been reported that the semantic smoothing model enhances the retrieval quality in Information Retrieval (IR). Inspired by that, the authors developed and applied a context-sensitive semantic smoothing model to boost accuracy of clustering that is generated by a dynamic growing cell structure algorithm, a variation of the neural network technique. They evaluated the proposed technique on biomedical article sets from MEDLINE, the largest biomedical digital library in the world. Their experimental evaluations show that the proposed algorithm significantly improves the clustering quality over the traditional clustering techniques including k-means and self-organizing map (SOM). Copyright  2009, IGI Global.",
      "title": "16675 A dynamic and semantically-aware technique for document clustering in biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350064287&partnerID=40&md5=9dc7b0e9d25e06bd9e2ed386597c0d30"
    },
    {
      "abstract": "Background: Current search engines are keyword-based. Semantic technologies promise a next generation of semantic search engines, which will be able to answer questions. Current approaches either apply natural language processing to unstructured text or they assume the existence of structured statements over which they can reason. Results: Here, we introduce a third approach, GoWeb, which combines classical keyword-based Web search with text-mining and ontologies to navigate large results sets and facilitate question answering. We evaluate GoWeb on three benchmarks of questions on genes and functions, on symptoms and diseases, and on proteins and diseases. The first benchmark is based on the BioCreAtivE 1 Task 2 and links 457 gene names with 1352 functions. GoWeb finds 58% of the functional GeneOntology annotations. The second benchmark is based on 26 case reports and links symptoms with diseases. GoWeb achieves 77% success rate improving an existing approach by nearly 20%. The third benchmark is based on 28 questions in the TREC genomics challenge and links proteins to diseases. GoWeb achieves a success rate of 79%. Conclusion: GoWebs combination of classical Web search with text-mining and ontologies is a first step towards answering questions in the biomedical domain. GoWeb is online at: http://www.gopubmed.org/goweb.  2009 Dietze and Schroeder",
      "title": "16677 GoWeb: A semantic search engine for the life science web",
      "url": ""
    },
    {
      "abstract": "Corporations are extremely sensitive to issues such as brand stewardship and product reputation. Traditional brand image and reputation tracking is limited to news wires and contact centre analysis. However, with the emergence of web, Consumer Generated Media (CGM), such as blogs, news forums, message boards, and web pages/sites, is rapidly becoming the voice of the people. Effectively leveraging such readily available and massive amount of CGM content for brand and reputation insights can be extremely valuable to corporations. Yet the existing solutions in this space are commonly limited to keyword search based technologies that often result in excessive amount of information for users to digest manually. Some text mining based technologies exist for mining the web. However, they often target very narrow problems such as web page classification instead of looking at the overall stack of technologies required for web mining. This paper describes a holistically integrated brand and reputation analysis solution that mines CGM contents for insights, called COBRA (COrporate Brand and Reputation Analysis). COBRA contains a flexible ETL (Extract, Transform, and Load) engine that processes diverse sets of structured and unstructured information, a suite of analytical capabilities that mines CGM content to extract semantic entities and insights out of the data, and an alerting mechanism that utilizes a technique called orthogonal filtering to accurately generate brand and reputation alerts by filtering through orthogonal dimensions of information. We use a set of real-world case studies to demonstrate the effectiveness of our overall approach. We show that by applying such analytics techniques, we significantly improved the accuracy of alert identification by 10-200 times when compared to the popular keyword-search based techniques in todays alerting systems.  2009 - IOS Press and the authors. All rights reserved.",
      "title": "16681 COBRA - Mining web for COrporate brand and reputation analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349128527&partnerID=40&md5=b25159d36d4f0a6ad664e3f6e86eaf1a"
    },
    {
      "abstract": "Knowledge Management (KM) has become the focus of a lot of scientific research during the second half of the twentieth century as researchers discovered the importance of the knowledge resource to business organizations. Recent research recommended the use of semantic representation of knowledge that is expressed in natural language to enhance knowledge management systems (KMS). In order to address this need, the CRISP technique (Concept Relation Identification using Shallow Parsing) was developed utilizing a natural language processing tool for extracting concept and concept relations from construction contract documents. The extracted concepts and relations are then used to develop semantic representations of the important knowledge expressed in the documents. The process of knowledge extraction from textual documents is, however, a subjective task that may differ from one person to another. This paper presents the two evaluation methods used to compare the performance of the CRISP technique with human evaluators. In the first method, Kappa was used to measure the level of agreement of human evaluators on the concepts extracted from an evaluation set, and the level of agreement between human evaluators and CRISP on the same evaluation set. In the second method, a Gold Standard was developed from the results of the human evaluators and precision and recall for CRISP and the human evaluators were calculated based on the Gold Standard. F-measure was used to combine precision and recall and to consequently compare between the performance of CRISP and the performance of the humanevaluators. Results from both evaluation methods were relatively comparable. ",
      "title": "16683 Techniques for evaluating automated knowledge acquisition from contract documents",
      "url": ""
    },
    {
      "abstract": "An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper addresses the automatic classification of the semantic relations expressed by English genitives. A learning model is introduced based on the statistical analysis of the distribution of genitives semantic relations in a corpus. The semantic and contextual features of the genitives noun phrase constituents play a key role in the identification of the semantic relation. The algorithm was trained and tested on a corpus of approximately 20,000 sentences and achieved an f-measure of 79.80 per cent for of-genitives, far better than the 40.60 per cent obtained using a Decision Trees algorithm, the 50.55 per cent obtained using a Naive Bayes algorithm, or the 72.13 per cent obtained using a Support Vector Machines algorithm on the same corpus using the same features. The results were similar for s-genitives: 78.45 per cent using Semantic Scattering, 47.00 per cent using Decision Trees, 43.70 per cent using Naive Bayes, and 70.32 per cent using a Support Vector Machines algorithm. The results demonstrate the importance of word sense disambiguation and semantic generalization/ specialization for this task. They also demonstrate that different patterns (in our case the two types of genitive constructions) encode different semantic information and should be treated differently in the sense that different models should be built for different patterns.  2008 Cambridge University Press.",
      "title": "16684 A Semantic Scattering model for the automatic interpretation of English genitives",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-69849091611&partnerID=40&md5=0c003865df2314de29d5c269b92a2382"
    },
    {
      "abstract": "Since a decade, text categorization has become an active field of research in the machine learning community. Most of the approaches are based on the term occurrence frequency. The performance of such surface-based methods can decrease when the texts are too complex, i.e., ambiguous. One alternative is to use the semantic-based approaches to process textual documents according to their meaning. Furthermore, research in text categorization has mainly focused on flat texts whereas many documents are now semi-structured and especially under the XML format. In this paper, we propose a semantic kernel for semi-structured biomedical documents. The semantic meanings of words are extracted using the unified medical language system (UMLS) framework. The kernel, with a SVM classifier, has been applied to a text categorization task on a medical corpus of free text documents. The results have shown that the semantic kernel outperforms the linear kernel and the naive Bayes classifier. Moreover, this kernel was ranked in the top 10 of the best algorithms among 44 classification methods at the 2007 Computational Medicine Center (CMC) Medical NLP International Challenge.  2008 Elsevier Ltd. All rights reserved.",
      "title": "16686 Semi-structured document categorization with a semantic kernel",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67349170474&partnerID=40&md5=40b37359778475feed7e9362b1da9ba2"
    },
    {
      "abstract": "With the proliferation of Web page texts, it is important to fuse these texts to useful documents that users need. However, there is still no complete and unified theoretical model for studying the research issues including redundancy, localization, and fuzziness existing in the process of fusing Web page texts. This paper proposes a fusion framework called Web Pages Knowledge Fusion Framework (WP-KFF) to synthesize the knowledge of Web page texts. First, sentences in Web page texts are extracted and transformed into triple semantic net as knowledge representation. Then a semantic description of attribute fusion rules, description information fusion rules and attribute-value and description information fusion rules are defined in WPKFF. These rules are used to fuse the attributes of same domain concepts in triple semantic net. The features of attributes include description (string) and value data (number). The results of the experiments indicate that the fusion framework is a feasible model in terms of precision and recall.  2009 Higher Education Press and Springer-Verlag.",
      "title": "16690 Knowledge fusion framework based on web page texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77956979229&partnerID=40&md5=724526b66ccba9155a6eb007128e738b"
    },
    {
      "abstract": "This paper shows how Wikipedia and the semantic knowledge it contains can be exploited for document clustering. We first create a concept-based document representation by mapping the terms and phrases within documents to their corresponding articles (or concepts) in Wikipedia. We also developed a similarity measure that evaluates the semantic relatedness between concept sets for two documents. We test the concept-based representation and the similarity measure on two standard text document datasets. Empirical results show that although further optimizations could be performed, our approach already improves upon related techniques.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16694 Clustering documents using a wikipedia-based concept representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650705044&partnerID=40&md5=00449efd8ba8d20ea0d39dabd41ebf09"
    },
    {
      "abstract": "Document representation using the bag-of-words approach may require bringing the dimensionality of the representation down in order to be able to make effective use of various statistical classification methods. Latent Semantic Indexing (LSI) is one such method that is based on eigendecomposition of the covariance of the document-term matrix. This paper points out that LSI ignores discrimination while concentrating on representation.",
      "title": "16695 Forecasting and discriminant analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650667738&partnerID=40&md5=b26052c7173a3d6734761fbf62836f3e"
    },
    {
      "abstract": "In this paper, we present a new approach that incorporates semantic structure of sentences, in a form of verb-argument structure, to measure semantic similarity between sentences. The variability of natural language expression makes it difficult for existing text similarity measures to accurately identify semantically similar sentences since sentences conveying the same fact or concept may be composed lexically and syntactically different. Inversely, sentences which are lexically common may not necessarily convey the same meaning. This poses a significant impact on many text mining applications performance where sentence-level judgment is involved. The evaluation has shown that, by processing sentence at its semantic level, the performance of similarity measures is significantly improved.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16696 Addressing the variability of natural language Expression in sentence similarity with semantic structure of the sentences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650688967&partnerID=40&md5=dfea1683e9619b268ce332b748d4d283"
    },
    {
      "abstract": "In this work, we explore the combined use of latent semantic analysis (LSA) and multidimensional scaling (MDS) for identifying related concepts and terms. We approach the problem of related term identification by constructing low-dimensional embeddings where related terms are clustered together, and such clusters are spatially arranged according to the semantic relationships among the terms they include. In this work, we demonstrate the proposed methodology for a specific part-of-speech (verbs) of the Spanish language, by using dictionary-based definitions. We also comment on the future use of this experimental framework in the context of other natural language processing tasks such as opinion mining, topic detection and automatic summarization.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16699 Semantic mapping for related term identification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650547492&partnerID=40&md5=e1e4789d839e5b5e50196afe07ff34b9"
    },
    {
      "abstract": "This paper presents Unification-based Combinatory Categorial Grammar (UCCG): a grammar formalism that combines insights from Combinatory Categorial Grammar with feature structure unification. Various aspects of information structure are incorporated in the compositional semantics. Information structure in the semantic representation is worked out in enough detail to allow for the determination of accurate placement of pitch accents, making the representation a suitable starting point for speech generation with context appropriate intonation. UCCG can be used for parsing and generating prosodically annotated text, and uses a semantic representation that is compatible with the currently available off-the-shelf automatic inference tools. As such the framework has the potential to advance spoken dialogue systems.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16702 Information structure in a formal framework : Unification-based combinatory categorial grammar",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650550267&partnerID=40&md5=99323f416baaab486fafc3e277720bc3"
    },
    {
      "abstract": "In order to realise the Semantic Web vision, more and more information is being made available in formal knowledge representation languages such as OWL. Unfortunately, the gap between human users who want to retrieve information and the Semantic Web remains unsolved. This paper presents a method for querying ontological knowledge bases from natural language sentences. This approach is based on the combination of three key elements: Natural Language Processing techniques for analyzing text fragments, ontologies for knowledge representation and Semantic Web technologies for querying ontological knowledge bases. The results of the application of this approach in the e-tourism domain are also reported in this paper.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16705 Accessing touristic knowledge bases through a natural language interface",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650263828&partnerID=40&md5=d1c4618799cbf5e1065f48c5debc3d73"
    },
    {
      "abstract": "Conventional document mining systems mainly use the presence or absence of keywords to mine texts. However, simple word counting and frequency distributions of term appearances do not capture the meaning behind the words, which results in limiting the ability to mine the texts. In this paper, the application of a semantic understanding based approach to mine documents is presented. The approach is based on semantic notions to represent text, and to measure similarity between text documents. The representation scheme reflects existing relations between concepts and facilitates accurate similarity measurements that result in better mining performance. A document mining process, namely semantic document clustering, is investigated and tackled in various ways. The proposed representation scheme along with the proposed similarity measure were implemented as vital components of a mining system. The approach has enabled more effective document clustering than what conventional techniques would provide. The experimental work is reported, and its results are presented and analyzed.  2009 ACADEMY PUBLISHER.",
      "title": "16708 A semantic approach for document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78651543296&partnerID=40&md5=3efb0e1ffd818f7c87a9603c144c4c58"
    },
    {
      "abstract": "An emerging topic in Quantuam Interaction is the use of lexical semantic spaces, as Hilbert spaces, to capture the meaning of words. There has been some initial evidence that the phenomenon of quantum entanglement exists in a semantic space and can potentially play a crucial role in determining the embeded semantics. In this paper, we propose to consider pure high-order entanglements that cannot be reduced to the compositional effect of lower-order ones, as an indicator of high-level semantic entities. To characterize the intrinsic order of entanglements and distinguish pure high-order entanglements from lower-order ones, we develop a set of methods in the framework of Information Geometry. Based on the developed methods, we propose an expanded vector space model that involves context-sensitive high-order information and aims at characterizing high-level retrieval contexts. Some initial ideas on applying the proposed methods in query expansion and text classification are also presented.  Springer-Verlag Berlin Heidelberg 2009.",
      "title": "16709 Characterizing pure high-order entanglements in lexical semantic spaces via information geometry",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67049095569&partnerID=40&md5=55a063d5501e8fec637e8ab560680b93"
    },
    {
      "abstract": "Wide availability of electronic data has led to the vast interest in text analysis, information retrieval and text categorization methods. To provide a better service, there is a need for non-English based document analysis and categorizing systems, as is currently available for English text documents. This study is mainly focused on categorizing Indie Language documents. The main techniques examined in this study include data pre-processing and document clustering. The approach makes use of a transformation based on the text frequency and the inverse document frequency, which enhances the clustering performance. This approach is based on Latent Semantic Analysis, k-means clustering and Gaussian Mixture Model clustering. A text corpus categorized by human readers is utilized to test the validity of the suggested approach. The technique introduced in this work enables the processing of text documents written in Sinhala, and empowers citizens and organizations to do their daily work efficiently.",
      "title": "16711 Evaluation of partition-based text clustering techniques to categorize indie language documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-66249091242&partnerID=40&md5=cf6566d547236ec269f9cf07cf748bc8"
    },
    {
      "abstract": "The reusability in learning objects has always been a hot issue. However, we believe that current approaches to e-Learning failed to find a satisfying answer to this concern. This paper presents an approach that enables capitalization of existing learning resources by first creating content metadata through text mining and natural language processing and second by creating dynamically learning knowledge objects, i.e., active, adaptable, reusable, and independent learning objects. The proposed model also suggests integrating explicitly instructional theories in an on-the-fly composition process of learning objects. Semantic Web technologies are used to satisfy such an objective by creating an ontology-based organizational memory able to act as a knowledge base for multiple training environments.  2006 IEEE.",
      "title": "16713 Enhancing learning objects with an ontology-based memory",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-66149190823&partnerID=40&md5=0060b328390a3a6ae28f0a32f68e50db"
    },
    {
      "abstract": "Kernels are widely used in Natural Language Processing as similarity measures within inner-product based learning methods like the Support Vector Machine. The Vector Space Model (VSM) is extensively used for the spatial representation of the documents. However, it is purely a statistical representation. In this paper, we present a Concept Vector Space Model (CVSM) representation which uses linguistic prior knowledge to capture the meanings of the documents. We also propose a linear kernel and a latent kernel for this space. The linear kernel takes advantage of the linguistic concepts whereas the latent kernel combines statistical and linguistic concepts. Indeed, the latter kernel uses latent concepts extracted by the Latent Semantic Analysis (LSA) in the CVSM. The kernels were evaluated on a text categorization task in the biomedical domain. The Ohsumed corpus, well known for being difficult to categorize, was used. The results have shown that the CVSM improves performance compared to the VSM.  2009 World Scientific Publishing Company.",
      "title": "16719 A concept vector space model for semantic kernels",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-65849092305&partnerID=40&md5=c2941c25171ccff56c3249f02903117d"
    },
    {
      "abstract": "In this study a semi automatic acquisition of domain relevant terms from digital documents in e-newspaper related to Malaysian medicinal herbs is presented. This study proposes (1) TFIDF-based term classification method for acquiring single word terms, (2) recognition of multi-word using TerMine software to acquire multiword terms and (3) Hearsts methodology of acquiring semantic relationships of hyponym. The results show the benefits of using these methods in selecting relevant terms from domain specific corpus. From this study it is believed that the combination of these three methods might be helpful to select relevant terms as well as minimize the effort to discard irrelevant terms manually-from wide collection of terms from the corpus.  2009 Asian Network for Scientific Information.",
      "title": "16721 Knowledge acquisition from textual documents for the construction of medicinal herbs domain ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63049135095&partnerID=40&md5=362925bdf6d61b0b027b5cfe0bf22bab"
    },
    {
      "abstract": "Due to the availability of a huge amount of textual data from a variety of sources, users of internationally distributed information regions need effective methods and tools that enable them to discover, retrieve and categorize relevant information, in whatever language and form it may have been stored. This drives a convergence of numerous interests from diverse research communities focusing on the issues related to multilingual text categorization. In this work, we implemented and measured the performance of the leading supervised and unsupervised approaches for multilingual text categorization. We selected support vector machines (SVM) as representative of supervised techniques as well as latent semantic indexing (LSI) and self-organizing maps (SOM) techniques as our selective ones of unsupervised methods for system implementation. The preliminary results show that our platform models including both supervised and unsupervised learning methods have the potentials for multilingual text categorization.  2007 Elsevier Ltd. All rights reserved.",
      "title": "16722 Construction of supervised and unsupervised learning systems for multilingual text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56349094368&partnerID=40&md5=576243bc68a7761b3219ed3974a3374a"
    },
    {
      "abstract": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.  2009 AI Access Foundation. All rights reserved.",
      "title": "16730 Wikipedia-based semantic interpretation for natural language processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-65349111942&partnerID=40&md5=da31de86bfd2ce371901599f717dfc51"
    },
    {
      "abstract": "This article reports an experiment with a computational simulation of an Information Retrieval System constituted of a textual indexing base from a sample of documents, an artificial neural network software implementing Adaptive Resonance Theory concepts for the process of ordering and presenting outputs, and a human user interacting with the system in query processing. The goal of the experiment was to demonstrate (i) the usefulness of Carpenter and Grossberg (1988) neural networks based on that theory, and (ii) the power of semantic resolution based on sintagmatic indexing of the SiRILiCO approach proposed by Gottschalg-Duque (2005), for whom a noun phrase or proposition is a linguistic unity constituted of meaning larger than a word meaning and smaller than a story telling or a theory meaning. The experiment demonstrated the effectiveness and efficiency of an Information Retrieval System joining together those resources, and the conclusion is that such computational environment will be capable of dynamic and on-line clustering with continuing inputs and learning in a non-supervised fashion, without batch training needs (off-line), to answer user queries in computer networks with promising performance.",
      "title": "16731 The cognitive power of artificial neural networks model ART1 for information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350367981&partnerID=40&md5=cfa8d5f2eb78d6dbf90c51f756014c9c"
    },
    {
      "abstract": "We present a dimensional information retrieval model for combining concept-based semantics and term statistics within multiple levels of document context to identify concise, variable length passages of text that answer a user query. Our results demonstrate improved search results in the presence of varying levels of semantic evidence, and higher performance using retrieval functions that combine document, as well as sentence and passage level information. Experimental results are promising. When ranking documents based on the most relevant extracted passages, the results exceed the state-of-the-art by 15.28% as assessed by the TREC 2005 Genomics track collection of 4.5 million MEDLINE citations.  2008 Elsevier Ltd. All rights reserved.",
      "title": "16732 A dimensional retrieval model for integrating semantics and statistical evidence in context for genomics literature search",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58249099247&partnerID=40&md5=3251dd046a8be6c3fbc874c37ead6c43"
    },
    {
      "abstract": "As biomedical science progresses, ontologies play an increasingly important role in easing the understanding of biomedical information. Although much research, such as Gene Ontology annotation, has been proposed to utilize ontologies to help users understand biomedical information easily, most of the research does not focus on capturing gene-related terms and their relationships within biomedical document collections. Understanding key gene-related terms as well as their semantic relationships is essential for comprehending the conceptual structure of biomedical document collections and avoiding information overload for users. To address this issue, we propose a novel approach called GOClonto to automatically generate ontologies for conceptualization of biomedical document collections. Based on GO (Gene Ontology), GOClonto extracts gene-related terms from biomedical text, applies latent semantic analysis to identify key gene-related terms, allocates documents based on the key gene-related terms, and utilizes GO to automatically generate a corpus-related gene ontology. The experimental results show that GOClonto is able to identify key gene-related terms. For a test biomedical document collection, GOClonto shows better performance than other clustering algorithms in terms of F-measure. Moreover, the ontology generated by GOClonto shows a significant informative conceptual structure.  2008 Springer Berlin Heidelberg.",
      "title": "16733 Exploiting gene ontology to conceptualize biomedical document collections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049117786&partnerID=40&md5=e4770aa9e80643a6f27733a361c4500a"
    },
    {
      "abstract": "This paper discusses the problem of predicting the structural changes in an ontology. It addresses ontologies that contain instances in addition to concepts. The focus is on an ontology where the instances are textual documents, but the approach presented in this document is general enough to also work with other kinds of instances, as long as a similarity measure can be defined over them. We examine the changes in the Open Directory Project ontology of Web pages over a period of several years and analyze the most common types of structural changes that took place during that time. We then present an approach for predicting one of the more common types of structural changes, namely the addition of a new concept that becomes the subconcept of an existing parent concept and adopts a few instances of this existing parent concept. We describe how this task can be formulated as a machine-learning problem and present an experimental evaluation of this approach that shows promising results of the proposed approach.  2008 Springer Berlin Heidelberg.",
      "title": "16736 Predicting category additions in a topic hierarchy",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049107045&partnerID=40&md5=9d93111837623357df4896e9de4c5c05"
    },
    {
      "abstract": "This paper discusses the automatic ontology construction process in a digital library. Traditional automatic ontology construction uses hierarchical clustering to group similar terms, and the result hierarchy is usually not satisfactory for humans recognition. Human-provided knowledge network presents strong semantic features, but this generation process is both labor-intensive and inconsistent under large scale scenario. The method proposed in this paper combines the statistical correction and latent topic extraction of textual data in a digital library, which produces a semantic-oriented and OWL-based ontology. The experimental document collection used here is the Chinese Recorder, which served as a link between the various missions that were part of the rise and heyday of the Western effort to Christianize the Far East. The ontology construction process is described and a final ontology in OWL format is shown in our result.  2008 Springer Berlin Heidelberg.",
      "title": "16737 Ontology construction based on latent topic extraction in a digital library",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049120025&partnerID=40&md5=89cb32768d0df9b25d41a068d592b490"
    },
    {
      "abstract": "A feature based relation classification approach is presented, in which probabilistic and semantic relatedness features between patterns and relation types are employed with other linguistic information. The importance of each feature set is evaluated with Chi-square estimator, and the experiments show that, the relatedness features have big impact on the relation classification performance. A series experiments are also performed to evaluate the different machine learning approaches on relation classification, among which Bayesian outperformed other approaches including Support Vector Machine (SVM).  2008 Springer Berlin Heidelberg.",
      "title": "16738 An empirical research on extracting relations from Wikipedia text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049093445&partnerID=40&md5=ff074cd400bbe8dee30675d838f49d6c"
    },
    {
      "abstract": "This paper studies the problem of mining Chinese comparative sentences in text documents by using Semantic Role Labeling (SRL). The comparative opinion can be divided into six semantic roles: Holder, Entity 1, Comparative predicates, Entity 2, Attributes and Sentiments. These six opinion elements were recognized and labeled by using SRL. A corpus of Chinese comparative sentences was manually labeled at first. Then a Conditional Random Fields (CRFs) model was trained by learn from the corpus. Finally new comparative sentences were labeled by using this CRFs model, and comparative relations were extracted afterward.  2008 IEEE.",
      "title": "16741 Mining Chinese comparative sentences by Semantic Role Labeling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57849140824&partnerID=40&md5=90e67b5153a1b057df778dc6ac9131c0"
    },
    {
      "abstract": "Traditionally, text categorization has been studied as the problem of training of a classifier using labeled data. However, people can categorize documents into named categories without any explicit training because we know the meaning of category names. In this paper, we introduce Dataless Classification, a learning protocol that uses world knowledge to induce classifiers without the need for any labeled data. Like humans, a dataless classifier interprets a string of words as a set of semantic concepts. We propose a model for dataless classification and show that the label name alone is often sufficient to induce classifiers. Using Wikipedia as our source of world knowledge, we get 85.29% accuracy on tasks from the 20 Newsgroup dataset and 88.62% accuracy on tasks from a Yahoo! Answers dataset without any labeled or unlabeled data from the datasets. With unlabeled data, we can further improve the results and show quite competitive performance to a supervised learning algorithm that uses 100 labeled examples. Copyright  2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "16742 Importance of semantic representation: Dataless classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57749169921&partnerID=40&md5=edbc42bfc23429aa5307f3e15f9bac84"
    },
    {
      "abstract": "Measuring the similarity of short text snippets plays an important role in information retrieval and natural language processing. Measuring the similarity for short text snippets, such as search queries, remains a challenging task. In this paper, we develop a new similarity measure, which can further improve the accuracy of semantic similarity for short text snippets, especially in the case of insufficient content, such as web page snippets. Then we introduce our similarity measure combined with information entropy to the clustering search engine to automatically find the best clustering numbers. Meanwhile, we rank the clusters with our method and illustrate the results.  2008 IEEE.",
      "title": "16743 An improved measuring similarity for short text snippets and its application in clustering search engine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57749084812&partnerID=40&md5=8f3759996d077d850d0a009c5d57dc4a"
    },
    {
      "abstract": "Automatic text classification is one of important fields in intelligent information process. Most researchers focus on statistic method (Rocchio, SVM, KNN etc.) which is based on Vector Space Model (VSM) representing text. On the basis of analyzing their disadvantages, a new method -automatic text classification based on background knowledge is proposed in this paper. This method is to simulate the classification process of human being. And it includes background knowledge and classification algorithm in order to make computer cognitive ability. It combines text semantic structure and background knowledge to activate relative branches of knowledge tree and decide which classification it belongs to by reasoning. The experiment indicates that the model has higher classification precision and recall.  2008 IEEE.",
      "title": "16744 Automatic text classification based on knowledge tree",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57649180847&partnerID=40&md5=a237fd2a921a72ad140d75b7e36b62ce"
    },
    {
      "abstract": "In many Web applications, such as blog classification and newsgroup classification, labeled data are in short supply. It often happens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data in a related but different domain. Traditional text classification approaches are not able to cope well with learning across different domains. In this paper, we propose a novel cross-domain text classification algorithm which extends the traditional probabilistic latent semantic analysis (PLSA) algorithm to integrate labeled and unlabeled data, which come from different but related domains, into a unified probabilistic model. We call this new model Topic-bridged PLSA, or TPLSA. By exploiting the common topics between two domains, we transfer knowledge across different domains through a topic-bridge to help the text classification in the target domain. A unique advantage of our method is its ability to maximally mine knowledge that can be transferred between domains, resulting in superior performance when compared to other state-of-the-art text classification approaches. Experimental evaluation on different kinds of datasets shows that our proposed algorithm can improve the performance of cross-domain text classification significantly. Copyright 2008 ACM.",
      "title": "16749 Topic-bridged PLSA for cross-domain text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57349129298&partnerID=40&md5=0da9d72e7d82bca135b269c26aaad6aa"
    },
    {
      "abstract": "Learning ontologies from large text corpora is a well understood task while evolving ontologies dynamically from user-input has rarely been adressed so far. Evolution of ontologies has to deal with vague or incomplete information. Accordingly, the formalism used for knowledge representation must be able to handle this kind of information. Classical logical approaches such as description logics are particularly poor in adressing uncertainty. Ontology evolution may benefit from exploring probabilistic or fuzzy approaches to knowledge representation. In this thesis an approach to evolve and update ontologies is developed which uses explicit and implicit user-input and extends probabilistic approaches to ontology engineering.  2008 Springer Berlin Heidelberg.",
      "title": "16750 End-user assisted ontology evolution in uncertain domains",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-57349180721&partnerID=40&md5=eb1d9ff26aaf618fc14322365520ac9f"
    },
    {
      "abstract": "Background: The Gene Ontology is a controlled vocabulary for representing knowledge related to genes and proteins in a computable form. The current effort of manually annotating proteins with the Gene Ontology is outpaced by the rate of accumulation of biomedical knowledge in literature, which urges the development of text mining approaches to facilitate the process by automatically extracting the Gene Ontology annotation from literature. The task is usually cast as a text classification problem, and contemporary methods are confronted with unbalanced training data and the difficulties associated with multi-label classification. Results: In this research, we investigated the methods of enhancing automatic multi-label classification of biomedical literature by utilizing the structure of the Gene Ontology graph. We have studied three graph-based multi-label classification algorithms, including a novel stochastic algorithm and two top-down hierarchical classification methods for multi-label literature classification. We systematically evaluated and compared these graph-based classification algorithms to a conventional flat multi-label algorithm. The results indicate that, through utilizing the information from the structure of the Gene Ontology graph, the graph-based multi-label classification methods can significantly improve predictions of the Gene Ontology terms implied by the analyzed text. Furthermore, the graph-based multi-label classifiers are capable of suggesting Gene Ontology annotations (to curators) that are closely related to the true annotations even if they fail to predict the true ones directly. A software package implementing the studied algorithms is available for the research community. Conclusion: Through utilizing the information from the structure of the Gene Ontology graph, the graph-based multi-label classification methods have better potential than the conventional flat multi-label classification approach to facilitate protein annotation based on the literature.  2008 Jin et al",
      "title": "16754 Multi-label literature classification based on the Gene Ontology graph",
      "url": "Trata-se de classificaÃ§Ã£o de genes e proteÃnas."
    },
    {
      "abstract": "The most important prerequisite for the success of the Semantic Web research is the construction of complete and reliable domain ontologies. In this paper we describe an unsupervised framework for domain ontology enrichment based on mining domain text corpora. Specifically, we enrich the hierarchical backbone of an existing ontology, i.e. its taxonomy, with new domain-specific concepts. The framework is based on an extended model of hierarchical self-organizing maps. As being founded on an unsupervised neural network architecture, the framework can be applied to different languages and domains. Terms extracted by mining a text corpus encode contextual content information, in a distributional vector space. The enrichment behaves like a classification of the extracted terms into the existing taxonomy by attaching them as hyponyms for the nodes of the taxonomy. The experiments reported are in the Lonely Planet tourism domain. The taxonomy and the corpus are the ones proposed in the PASCAL ontology learning and population challenge. The experimental results prove that the quality of the enrichment is considerably improved by using semantics based vector representations for the classified (newly added) terms, like the document category histograms (DCH) and the document frequency times inverse term frequency (DF-ITF) weighting scheme.  2008 ACM.",
      "title": "16756 A neural model for unsupervised taxonomy enrichment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349097537&partnerID=40&md5=68c283b0f102cd222566ddf2b178c7f1"
    },
    {
      "abstract": "We conduct large-scale experiments to investigate optimal features for classification of verbs in biomedical texts. We introduce a range of feature sets and associated exttaction techniques, and evaluate them thoroughly using a robust method new to the task: cost-based framework for pair-wise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained with sophisticated feature sets which include lexical and semantic information about selectional preferences of verbs. The latter are acquired automatically from corpus data using a fully unsupervised method.  2008. Licensed under the Creative Commons.",
      "title": "16759 The choice of features for classification of verbs in biomedical texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053394342&partnerID=40&md5=ee5f3b5b396caf8ae9f39f8e43cfecb1"
    },
    {
      "abstract": "This paper describes a system for processing economic documents written in the ancient Sumerian language. The system is application-oriented and takes advantage of the simplicity of ancient economy. We have developed an ontology for a selected branch of economic activities. We translate the documents into a meaning representation language by means of a semantic grammar. The meaning representation language is constructed in a way that allows us to handle massive ambiguity caused by: the specifics of the Sumerian writing system (signs polyvalence, lack of mid-word signs), our incomplete knowledge of the Sumerian language and frequent damages of documents. The system is augmented with the capability of processing documents whose parts describe concepts not included in the ontology and grammar. As an effect we obtain a structural description of the documents contents in the meaning representation language, ready to use in historical research.  2008 Licensed under the Creative Commons.",
      "title": "16765 Contents modelling of neo-sumerian ur III economic text corpus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79956313864&partnerID=40&md5=c41ca8d64914ef0e072962b2d0681003"
    },
    {
      "abstract": "In this paper, we present and evaluate a hybrid document organization system based on Self-Organizing Maps. The proposed system uses Semantic Mapping to dimensionality reduction and K-means to volume reduction of document vectors of a medium text collection. The vectors obtained after dimensionality and volume reduction steps are used to train the document maps with the SOM algorithm, thus the training time is reduced without compromising the quality of the generated map. We compare experimentally the hybrid system with the correspondent SOM system in organization of documents of Reuters-21758 v1.0 collection. The performances of the systems were measured in terms of classification error in text categorization and training time. The experimental results show that the proposed system generates pretty good document maps with smallest training time. Copyright 2008 ACM.",
      "title": "16766 Semantic mapping and K-means applied to hybrid SOM-based document organization system construction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56749159242&partnerID=40&md5=e4cb03ac6ef4be7f7ad651c116a6627d"
    },
    {
      "abstract": "The growing popularity of Web 2.0 provides with increasing numbers of documents expressing opinions on different topics. Recently, new research approaches have been defined in order to automatically extract such opinions from the Internet. They usually consider opinions to be expressed through adjectives, and make extensive use of either general dictionaries or experts to provide the relevant adjectives. Unfortunately, these approaches suffer from the following drawback: in a specific domain, a given adjective may either not exist or have a different meaning from another domain. In this paper, we propose a new approach focusing on two steps. First, we automatically extract a learning dataset for a specific domain from the Internet. Secondly, from this learning set we extract the set of positive and negative adjectives relevant to the domain. The usefulness of our approach was demonstrated by experiments performed on real data. Copyright 2008 ACM.",
      "title": "16769 Web opinion mining: How to extract opinions from blogs?",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77951582166&partnerID=40&md5=c48b8d8d203de326c1069495177fda14"
    },
    {
      "abstract": "A main challenge for Web content classification is how to model the input data. This paper discusses the application of two text modeling approaches, Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), in the Web page classification task. We report results on a comparison of these two approaches using different vocabularies consisting of links and text. Both models are evaluated using different numbers of latent topics. Finally, we evaluate a hybrid latent variable model that combines the latent topics resulting from both LSA and LDA. This new approach turns out to be superior to the basic LSA and LDA models. In our experiments with categories and pages obtained from the ODP web directory the hybrid model achieves an averaged F-measure value of 0.852 and an averaged ROC value of 0.96.  2008 IEEE.",
      "title": "16770 A comparative Analysis of Latent variable models for Web page classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63149169065&partnerID=40&md5=0ed55fc365e52534bebe32b2a6308677"
    },
    {
      "abstract": "The number of users and the amount of information available has exploded since the advent of the World Wide Web (WWW). Most of Web users use various search engines to get specific information. A key factor in the success of Web search engines are their ability to rapidly find good quality results to the queries that are based on specific terms. This paper aims at retrieving more relevant documents from a huge corpus based on the required information. We propose a text mining framework that consists of four distinct stages: 1.Text preprocessing 2.Dimesionality Reduction using Latent Semantic Indexing 3.Clustering based on Hybrid combination of Particle Swarm Optimization (PSO) and k-means Algorithm 4.Information Retrieval Process using Simulated Annealing (SA).This framework provides more relevant documents to the user and reduces the irrelevant documents.  2008 IEEE.",
      "title": "16772 An efficient LSI based information retrieval framework usingp particle swarm optimization and simulated annealing approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63149139122&partnerID=40&md5=9d03add4ee7db6978fb345e5f7b58e76"
    },
    {
      "abstract": "Ontology (RDF/OWL) plays a foundational role of Semantic Web for knowledge representation. But nowadays there are few Chinese ontology bases available, which hinders the research and development of Chinese Semantic Web applications. This paper introduces an ontology knowledge discovery tool, named OntoLTCn, which supports semi-auto domain ontology acquisition from Chinese corpus. In brief OntoLTCn is a Protege plug-in based on OntoLT platform, which integrates Chinese NLP and XML pattern mapping technologies for knowledge discovery. A case study in Chinese e-Government domain is discussed as well, which shows the usability of OntoLTCn for ontology construction from digital archives.  2008 IEEE.",
      "title": "16775 Ontoltcn: A Chinese text oriented semi-auto ontology knowledge discovery tool",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-61849083003&partnerID=40&md5=a756c28ec36421a6916ee92a142c9597"
    },
    {
      "abstract": "The success of the Semantic Web research is dependent upon the construction of complete and reliable domain ontologies. In this paper we describe an unsupervised framework for domain ontology enrichment based on mining domain text corpora. Specifically, we enrich the hierarchical backbone of an existing ontology, i.e. its taxonomy, with new domain-specific concepts. The framework is based on an extended model of hierarchical self-organizing maps. As being founded on an unsupervised neural network architecture, the framework can be applied to different languages and domains. Terms extracted by mining a text corpus encode contextual content information, in a distributional vector space. The enrichment behaves like a classification of the extracted terms into the existing taxonomy by attaching them as hyponyms for the nodes of the taxonomy. The experiments reported are in the Lonely Planet tourism domain. The taxonomy and the corpus are the ones proposed in the PASCAL ontology learning and population challenge. The experimental results prove that the quality of the enrichment is considerably improved by using semantics based vector representations for the classified (newly added) terms, like the document category histograms (DCH) and the document frequency times inverse term frequency (DF-ITF) weighting scheme.",
      "title": "16780 Text-based ontology enrichment using hierarchical self-organizing maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885639314&partnerID=40&md5=ea26c8d4ea4c94d7979c708dc3924215"
    },
    {
      "abstract": "Generating alternative queries, also known as query suggestion, has long been proved useful to help a user explore and express his information need. In many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. However, how to generate suggestions while ensuring their semantic consistency with the original query remains a challenging problem. In this work, we propose a novel query suggestion algorithm based on ranking queries with the hitting time on a large scale bipartite graph. Without involvement of twisted heuristics or heavy tuning of parameters, this method clearly captures the semantic consistency between the suggested query and the original query. Empirical experiments on a large scale query log of a commercial search engine and a scientific literature collection show that hitting time is effective to generate semantically consistent query suggestions. The proposed algorithm and its variations can successfully boost long tail queries, accommodating personalized query suggestion, as well as finding related authors in research. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Text Mining.  2008 ACM.",
      "title": "16781 Query suggestion using hitting time",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67650083602&partnerID=40&md5=05b2b390168008c1bc1340968c337ba2"
    },
    {
      "abstract": "Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allocation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evidence that the document space is Euclidean, or flat. Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specifically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimental results show that LapPLSI provides better representation in the sense of semantic structure. Copyright 2008 ACM.",
      "title": "16783 Modeling hidden topics on document manifold",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349247055&partnerID=40&md5=03af1ccbdf23a34bbbb497f9ed2abf8c"
    },
    {
      "abstract": "When massive information is available for technology forecasting activities in WWW and other internet services, the smart technology forecasting supported by text-mining and other information technologies becomes one of the new approaches of technology forecasting. However, it is difficult to avoid the problems of confusion of information, disguise, and invalidation of correlation weighing when current text mining techniques based on keyword search are employed to conduct technology forecasting. In this paper, we present a three-layer model of smart technology forecasting based on text mining consisting of the collection layer, the analysis layer and the representation layer. Furthermore, by adapting current model to a new framework, we propose a semantic-web-based technology forecasting approach. Results show that the semantic-web-based framework is an appropriate and potential smart technology forecasting approach. 2008 IEEE.",
      "title": "16784 Applying semantic web into technology forecasting in enterprises",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58149139269&partnerID=40&md5=69ce3398d399d034746b5c56e62dd4f7"
    },
    {
      "abstract": "Standard Machine Learning approaches to text classification use the bag-of-words representation of documents to deceive the classification target function. Typical linguistic structures such as morphology, syntax and semantic are completely ignored in the learning process. This paper examines the role of these structures on the classifier construction applying the study to the Portuguese language. Classifiers are built using the SVM algorithm on a newspapers articles dataset. The results show that syntactic structure is not useful for text classification (as initially expected), but a novel structured representation that uses documents semantic information has the same discriminative power over classes as the traditional bag-of-words one.  2008 IEEE.",
      "title": "16785 Text classification using tree kernels and linguistic information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-60649110223&partnerID=40&md5=84bb2d7e7e5869c7c01b65a0feeb3b4b"
    },
    {
      "abstract": "Text representation is the basis of text processing. Most current text representation models ignore the words inter-relations, which result in the loss of texts structure information. This paper proposed a novel text representation model, which uses lexical network to represent the text and retains the texts structure. According to the different levels of words inter-relations, co-occurrence network, syntactic network and semantic network are introduced. To evaluate the representation ability of text network representation model, we investigated the applications of text network to two language processing tasks including unsupervised keyword extraction and text classification. The experimental results show how to use it for natural language processing successfully.  2008 IEEE.",
      "title": "16787 A text network representation model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049193747&partnerID=40&md5=78417f9ee63a4fa3e862accfd89efb90"
    },
    {
      "abstract": "Traditional approaches to document classification requires labeled data in order to construct reliable and accurate classifiers. Unfortunately, labeled data are seldom available, and often too expensive to obtain. Given a learning task for which training data are not available, abundant labeled data may exist for a different but related domain. One would like to use the related labeled data as auxiliary information to accomplish the classification task in the target domain. Recently, the paradigm of transfer learning has been introduced to enable effective learning strategies when auxiliary data obey a different probability distribution. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, we extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. This goal is achieved with the use of Wikipedia. As a result, the pathway that allows to propagate labels between the two domains not only captures common words, but also semantic concepts based on the content of documents. We empirically demonstrate the efficacy of our semantic-based approach to cross-domain classification using a variety of real data.  2008 IEEE.",
      "title": "16793 Using wikipedia for co-clustering based cross-domain text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67049165478&partnerID=40&md5=b4f551f3e1a083e0fbba2f089d20e7e5"
    },
    {
      "abstract": "Due to the continuous growth and crucial real-world impact of information produced in the biomedical and health domain, there is an acutely felt need for question-answering (QA) systems that can aid medical researchers and health care professionals in their information search. In order to provide users with accurate answers, such systems need to utilize semantic analysis and processing of medical texts and knowledge resources. Moreover, QA systems equipped with reasoning capabilities can derive more adequate answers by using inference mechanisms. We propose a logic-based medical QA system which uses Description Logic (DL) for knowledge representation and reasoning. As a first step in constructing the proposed QA system, we present a study on the semantic analysis and categorization of medical questions. 2008 IEEE.",
      "title": "16794 Semantic analysis and classification of medical questions for a logic-based medical question-answering system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049142556&partnerID=40&md5=1ca17906685ff85f68092c2783510edf"
    },
    {
      "abstract": "Today, users need pro-active information recovery mechanisms that can rapidly obtain the largest amount of relevant information possible. Several information recovery systems are available, but they still have semantic problems related to document treatment and retrieval. The proposed system uses statistical text-mining techniques to semantically associate documents to domains represented by ontologies, giving a similarity degree (relevance) between the document and the domain. A tool was implemented and tested using Jurisprudence documents from the Court of Justice of the State of Goias. These documents are manually indexed by specialists under predefined categories. A comparison between the results obtained using the tool and the manual indexation was thus possible, permitting an analysis of the tools efficiency using recall, precision and fall-out indexes.  ACM 2008.",
      "title": "16795 Automatic domain classification of jurisprudence documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70350630292&partnerID=40&md5=125fd985748b33c53a870603b0e7e556"
    },
    {
      "abstract": "Text classification has been mostly performed through implicit semantic correlation techniques, such as latent semantic analysis. This approach however, has proved insufficient for situations where there are short texts to be classified into one or more from many classes. That is the case of the classification of statements of purpose of Brazilian companies, according to the around one thousand and eight hundred categories of the government administration detailment of National Classification of Economical Activities (CNAE), CNAE-Subclasses. The impact of the order of words in a text is evaluated by comparing the performance of three classifiers based on the weightless artificial neural model, WISARD. Results point to the need of combining semantic with syntactic information in order to improve the classifiers performance.  2008 IEEE.",
      "title": "16797 The influence of order on a large bag of words",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67449161849&partnerID=40&md5=fc10e9efd9231d5e97b43deb043b1f0b"
    },
    {
      "abstract": "In this paper, we present the Pat Clust clustering solution for textual documents based on semantic criteria. Our proposition is dedicated to patent documents of the biomedical domain. We present three different approaches and we show that semantic web techniques clearly allow to improve the quality of resulting clusters.  2008 IEEE.",
      "title": "16798 Semantic patent clustering for biomedical communities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-62949150480&partnerID=40&md5=d12c67f8384b4ae07cad55d81e89974f"
    },
    {
      "abstract": "In the classification of text and retrieval of information, because the redundancy of information and complex computing problems are aroused from synonymous phenomenon between sentences, this paper give a calculation model about semantic similarity of sentences: start with the semantic similarity of words and phrases, then analyze sentence category, and introduce semantic similarity to syntaxes, finally calculate semantic similarity of a sentence. Through the analysis of examples, we can see that the grammar and semantics of sentence are inseparable, it is necessary to consider both.  2008 IEEE.",
      "title": "16801 The researching on similarity calculation based on HNC",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349606115&partnerID=40&md5=d3fa7b0c172e8020aba7e4d0518bb051"
    },
    {
      "abstract": "Text clustering techniques were usually used to structure the text documents into topic related groups which can facilitate users to get a comprehensive understanding on corpus or results from information retrieval system. Most of existing text clustering algorithm which derived from traditional formatted data clustering heavily rely on term analysis methods and adopted Vector Space Model (VSM) as their document representation. But because of the essential characteristic underlying text such as high dimensionality features vector space, the problem of sparseness has a strong impact on the clustering algorithm. So feature reduction is an important preprocess step for improving the efficiency and accuracy of clustering algorithm by removing redundant and irrelevant terms from corpus. Even the clustering is considered as an unsupervised learning method, but in text, there is still some priori knowledge we can use from NLP analysis based approach. In this paper, we propose a semantic analysis based feature reduction method which used in Chinese text clustering. Our method bases on a dedicated Part-of-Speech tags selection and synonyms consolidation and can reduce the feature space of documents more effectively compared with traditional feature reduction method tfidf and stopwords removal",
      "title": "16804 Semantic feature reduction in chinese document clustering",
      "url": ""
    },
    {
      "abstract": "Wikipedia has been applied as a background knowledge base to various text mining problems, but very few attempts have been made to utilize it for document clustering. In this paper we propose to exploit the semantic knowledge in Wikipedia for clustering, enabling the automatic grouping of documents with similar themes. Although clustering is intrinsically unsupervised, recent research has shown that incorporating supervision improves clustering performance, even when limited supervision is provided. The approach presented in this paper applies supervision using active learning. We first utilize Wikipedia to create a concept-based representation of a text document, with each concept associated to a Wikipedia article. We then exploit the semantic relatedness between Wikipedia concepts to find pair-wise instance-level constraints for supervised clustering, guiding clustering towards the direction indicated by the constraints. We test our approach on three standard text document datasets. Empirical results show that our basic document representation strategy yields comparable performance to previous attempts",
      "title": "16806 Clustering documents with active learning using wikipedia",
      "url": ""
    },
    {
      "abstract": "The calculation bottleneck problems of Kohonen self organizing feature map (SOFM) neural network in the high-dimensional vector environment of text processing and problems of input vector spaces had been analyzed in this paper, and then based on the theoretic analysis of RM (random mapping) and LSI (latent semantic indexing) method respectively, a RM-based fast latent semantic indexing method used in text processing was presented. The fast LSI method could greatly emerges original semantic links and settles the above mentioned problems in a low-cost, efficient and controllable way in the experiment. So the size and the calculation cost of Kohonen SOFM neural network were greatly reduced in text processing environment. 2008 IEEE.",
      "title": "16809 Fast latent semantic index using random mapping in text processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56749171373&partnerID=40&md5=6c101b5e610b94af4335fec422fb62b9"
    },
    {
      "abstract": "Text representation, which is a fundamental and necessary process for text-based intelligent information processing, includes the tasks of determining the index terms for documents and producing the numeric vectors corresponding to the documents. In this paper, multi-word, which is regarded as containing more contextual semantics than individual word and possessing the favorable statistical characteristics, is proposed as an alternative index terms in vector space model for text representation with theoretical support. We investigate the traditional indexing methods as TF*IDF (term frequency inverse document frequency) and LSI (latent semantic indexing) for comparative study. The performances of TF*IDF, LSI and multiword are examined on the tasks of text classification, which includes information retrieval (IR) and text categorization (TC), in Chinese and English document collection respectively. We also attempt to tune the rescaling factor of LSI and observe its effectiveness in text classification. The experimental results demonstrate that TF*IDF and multi-word are comparable when they are used for IR and TC and LSI is the poorest one of them. Moreover, the rescaling factor of LSI has an insignificant influence on its effectiveness on text classification for both Chinese and English text classification.  2008 IEEE.",
      "title": "16810 TFIDF, LSI and multi-word in information retrieval and text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-69949154338&partnerID=40&md5=0d620c57e303543395b8b1e20246b5dc"
    },
    {
      "abstract": "The traditional strategy performed by Information Retrieval (IR) systems is ranked keyword search: for a given query, a list of documents, ordered by relevance, is returned. Relevance computation is primarily driven by a basic string-matching operation. To date, several attempts have been made to deviate from the traditional keyword search paradigm, often by introducing some techniques to capture word meanings in documents and queries. The general feeling is that dealing explicitly with only semantic information does not improve significantly the performance of text retrieval systems. This paper presents SENSE (SEmantic N-levels Search Engine), an IR system that tries to overcome the limitations of the ranked keyword approach, by introducing semantic levels which integrate (and not simply replace) the lexical level represented by keywords. Semantic levels provide information about word meanings, as described in a reference dictionary, and named entities. We show how SENSE is able to manage documents indexed at three separate levels, keywords, word meanings, and entities, as well as to combine keyword search with semantic information provided by the two other indexing levels.",
      "title": "16811 Improving retrieval experience exploiting semantic representation of documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874274715&partnerID=40&md5=d88f5594fb376ad9b3280a0a5d85cd93"
    },
    {
      "abstract": "Previously topic models such as PLSI (Probabilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) were developed for modeling the contents of plain texts. Recently, topic models for processing hypertexts such as web pages were also proposed. The proposed hypertext models are generative models giving rise to both words and hyperlinks. This paper points out that to better represent the contents of hypertexts it is more essential to assume that the hyperlinks are fixed and to define the topic model as that of generating words only. The paper then proposes a new topic model for hypertext processing, referred to as Hypertext Topic Model (HTM). HTM defines the distribution of words in a document (i.e., the content of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites. The topics are further characterized as distributions of words, as in the conventional topic models. This paper further proposes a method for learning the HTM model. Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets.  2008 Association for Computational Linguistics.",
      "title": "16813 HTM: A topic model for hypertexts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-80053344908&partnerID=40&md5=038b4a33393e8eca43f85b4e05c610d8"
    },
    {
      "abstract": "The biomedical literature can be seen as a large integrated, but unstructured data repository. Extracting facts from literature and making them accessible is approached from two directions: manual curation efforts develop ontologies and vocabularies to annotate gene products based on statements in papers. Text mining aims to automatically identify entities and their relationships in text using information retrieval and natural language processing techniques. Manual curation is highly accurate but time consuming, and does not scale with the ever increasing growth of literature. Text mining as a high-throughput computational technique scales well, but is error-prone due to the complexity of natural language. How can both be married to combine scalability and accuracy? Here, we review the state-of-the-art text mining approaches that are relevant to annotation and discuss available online services analysing biomedical literature by means of text mining techniques, which could also be utilised by annotation projects. We then examine how far text mining has already been utilised in existing annotation projects and conclude how these techniques could be tightly integrated into the manual annotation process through novel authoring systems to scale-up high-quality manual curation.  The Author 2008. Published by Oxford University Press.",
      "title": "16820 Facts from text: Can text mining help to scale-up high-quality manual curation of gene products with ontologies?",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58149375832&partnerID=40&md5=ed401b192b010f1c6b149aeb8112fee8"
    },
    {
      "abstract": "Ontologies are an integral part of Knowledge and Information Management systems and there is increased interest in using ontologies for organizational memory. Ontology learning workbenches are used for semi-automatic learning of ontologies from representative text collections. This paper presents a new interactive workbench that gives the users more freedom in their ontology engineering process and frees them from knowing any ontology language syntax. The workbench is implemented as part of a search project, in which ontologies are used to search for movie information on the web. New techniques are steadily being added to the workbench, though early testing has already confirmed the validity of the ontology learning approach.  2008 ACM.",
      "title": "16821 An interactive ontology learning workbench for non-experts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349426190&partnerID=40&md5=7de2c5274bad999b2d794584887ba4d1"
    },
    {
      "abstract": "Text classification may be viewed as assigning texts in a predefined set of categories. However there are many digital documents that are not organized according to their contents. So it is difficult task to find relevant documents for a user. Automatic text classification problem can solve this problem. In this paper we introduce a new random walk term weighting method for improved text classification. In our approach to weight a term, we exploit the relationship of local (term position, term frequency) and global (inverse document frequency, information gain) information of terms (vertices). Moreover, we weight terms by considering co-occurrence and semantic relation of terms as a measure of dependency. To evaluate our term weighting approach we integrate it in Rocchio text classification algorithm and experimental results show that our method performs better than other random walk models.  2008 IEEE.",
      "title": "16826 An effective term weighting method using random walk model for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-66149173662&partnerID=40&md5=46f7f17768fe4a4ccb9f126ad8f46cb7"
    },
    {
      "abstract": "For the shortcomings in the method of traditional statistics-based feature extraction on PU issues, we put forward feature extraction based on ontology to improve the performance of PU classification. We improved PEBL algorithm, and get the document vector of positive set using ontology-based feature extraction, then find the strong positive features, which include the crossing semantics in the positive documents and have higher frequency in positive set. The improved algorithm scans the documents twice. First, we get the semantic of the documents by ontology. Second, we filtrate the terms which include none of these semantic to reduce the dimension and obtain the document vector. Experiments had shown that the improved PEBL classifier increases the F1 score by 0.7389%.  2008 IEEE.",
      "title": "16828 Research of PU text semi-supervised classification based on ontology feature extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-60649092319&partnerID=40&md5=eebeac7a9f945acdfae76b1e9cdcd1f0"
    },
    {
      "abstract": "Previous work on Natural Language Processing for Information Retrieval has shown the inadequateness of semantic and syntactic structures for both document retrieval and categorization. The main reason is the high reliability and effectiveness of language models, which are sufficient to accurately solve such retrieval tasks. However, when the latter involve the computation of relational semantics between text fragments simple statistical models may result ineffective. In this paper, we show that syntactic and semantic structures can be used to greatly improve complex categorization tasks such as determining if an answer correctly responds to a question. Given the high complexity of representing semantic/syntactic structures in learning algorithms, we applied kernel methods along with Support Vector Machines to better exploit the needed relational information. Our experiments on answer classification on Web and TREC data show that our models greatly improve on bagof-words. Copyright 2008 ACM.",
      "title": "16830 Kernel methods, syntax and semantics for relational text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349257998&partnerID=40&md5=bcc0432a78d0bf512aaf7858c7eda7b4"
    },
    {
      "abstract": "This paper presents the comparison of the text document space dimension reduction and the text document clustering and also the keyword space dimension reduction and keyword clustering by the Latent Semantic Analysis and by the Hebbian neural network with Oja learning rule. Results of this neural network are compared with the results of the Latent Semantic Analysis which uses the Singular Value Decomposition for dimension space reduction of the text documents in natural language. 2008 IEEE.",
      "title": "16833 Document space dimension reduction by Latent Semantic Analysis and Hebbian neural network",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58049216637&partnerID=40&md5=835ab79f84c1bfa945001c7751a5fa7e"
    },
    {
      "abstract": "We developed a model based on nonparametric Bayesian modeling for automatic discovery of semantic relationships between words taken from a corpus. It is aimed at discovering semantic knowledge about words in particular domains, which has become increasingly important with the growing use of text mining, information retrieval, and speech recognition. The subject-predicate structure is taken as a syntactic structure with the noun as the subject and the verb as the predicate. This structure is regarded as a graph structure. The generation of this graph can be modeled using the hierarchical Dirichlet process and the Pitman-Yor process. The probabilistic generative model we developed for this graph structure consists of subject-predicate structures extracted from a corpus. Evaluation of this model by measuring the performance of graph clustering based on WordNet similarities demonstrated that it outperforms other baseline models.  2008 ACM.",
      "title": "16834 Knowledge discovery of semantic relationships between words using nonparametric bayesian graph model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-65449127481&partnerID=40&md5=a1335e486b850c298f751aa2875edb40"
    },
    {
      "abstract": "A protein-protein interaction in a biomedical text is often described using a wide range of verbs, e.g. activate, bind, interact. In order to determine the specific type of interaction described, we must first determine the meaning of the verb used. In biomedical context, however, some verbs can be considered synonyms, yet may not be so in standard lexical databases, like WordNet. Furthermore, some verbs will not be mentioned at all in such a dictionary, since they are too area specific. We propose a simple classification scheme to predict the correct class (meaning) of the verb. With this, one can identify the types of protein-protein interactions described in subject-verb-object constructions in PubMed abstracts.",
      "title": "16835 Classifying verbs in biomedical text using subject-verb-object relationships",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874226252&partnerID=40&md5=c25a5cec05af19f605a7343e70d1b977"
    },
    {
      "abstract": "It is difficult for traditional data mining algorithms to mine semantic information from text set because of its complexity and high dimension. To solve this problem, the semantic categories of words appearing in tourism emergency reports are studied, and a semantic association rule mining algorithm is presented based on these categories. Association words are also gained from these rules, which can better describe the semantic contents of the texts. Quantum-inspired genetic algorithm is utilized to improve the effectiveness of rule-searching process. Experiments show the better results than traditional methods.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16837 Association rule mining based on the semantic categories of tourism information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-58849145085&partnerID=40&md5=e4004400ab7d7e4b697bfa225bd95958"
    },
    {
      "abstract": "Text clustering as a method of organizing retrieval results can organize large amounts of web search into a small number of clusters in order to facilitate users quickly browsing. In this paper, we propose a text clustering method based on ontology which is different from traditional text clustering and can improve clustering results performance. This method implements word clustering by calculating word relativity and then implements text classification. Experiments show that the proposed method clustering trends to perform better than only single term frequency based method.  2008 IEEE.",
      "title": "16839 Research on ontology-based text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-60349129344&partnerID=40&md5=bc0d0ec3f0546d880085537f571501e9"
    },
    {
      "abstract": "Subspace learning techniques for text analysis, such as Latent Semantic Indexing (LSI), have been widely studied in the past decade. However, to our best knowledge, no previous study has leveraged the rank information for subspace learning in ranking tasks. In this paper, we propose a novel algorithm, called Learning Latent Semantics for Ranking (LLSR), to seek the optimal Latent Semantic Space tailored to the ranking tasks. We first present a dual explanation for the classical Latent Semantic Indexing (LSI) algorithm, namely learning the so-called Latent Semantic Space (LSS) to encode the data information. Then, to handle the increasing amount of training data for the practical ranking tasks, we propose a novel objective function to derive the optimal LSS for ranking. Experimental results on two SMART sub-collections and a TREC dataset show that LLSR effectively improves the ranking performance compared with the classical LSI algorithm and ranking without subspace learning.  2008 IEEE.",
      "title": "16843 Learning the latent semantic space for ranking in text retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-67049162735&partnerID=40&md5=41b6d4d53bfea2e6547f1e2c74d8cf89"
    },
    {
      "abstract": "Since there is no standard naming convention for genes&gene products, gene symbol disambiguation (GSD) has become a big challenge when mining biomedical literature. Several GSD methods have been proposed based on MEDLINE references to genes. However, nowadays gene databases, e.g. Entrez Gene, provide plenty of information about genes, &many biomedical ontologies, e.g. UMLS Metathesaurus&Semantic Network, have been developed. These knowledge sources could be used for disambiguation, in this paper we propose a method which relies on information about gene candidates from gene databases, contexts of gene symbols&biomedical ontologies. We implement our method,&evaluate the performance of the implementation using BioCreAtIvE II data sets. Copyright 2008 ACM.",
      "title": "16844 Knowledge-based gene symbol disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349256088&partnerID=40&md5=d4088bdafd772209db4e3232e4a79d55"
    },
    {
      "abstract": "Ontology can provide a powerful representation of information space and solve many semantic problems. It is wonderful to apply ontology to text classification. This paper proposes a general framework for text classification, which can overcome the limitations of traditional text classification methods. The results of experiment prove that the general framework is applicable across different domains and this method produces better performance.  2008 IEEE.",
      "title": "16845 General framework for text classification based on domain ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-60349091270&partnerID=40&md5=f6798165a451d7e839c1b225f036a7cf"
    },
    {
      "abstract": "In order to extract the atmosphere in media, we model the latent semantics of TV synopses and affective terms as patterns of emotional components. Using a selection of affective last.fm tags and TV-Any time atmosphere terms as emotional buoys, we apply LSA latent semantic analysis to represent the correlation of terms and descriptions in a vector space that reflects the emotional context. Analyzing the resulting patterns of affective components, we propose that this approach could be applied to automatically generate affective user preferences based on synopsis, subtitles or other textual representations associated with media. Copyright 2008 ACM.",
      "title": "16851 Modeling emotional context from latent semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63049135688&partnerID=40&md5=bf68ffb4a1bc1a60637da687666f9e8c"
    },
    {
      "abstract": "Ontology learning is an important task in Artificial Intelligence, Semantic Web and Text Mining. This paper presents a novel framework for, and solutions to, three practical problems in ontology learning. An incremental clustering approach is used to solve the problem of unknown group names. Learned models at each level of an ontology address the problem of no control over concept abstractness. A metric learning module moves beyond the limitation of traditional use of features and incorporates heterogeneous semantic evidence into the learning process. The metric-based learning framework integrates these separate components into a single, unified solution. An extensive evaluation with WordNet and Open Directory Project data demonstrates that the method is more eective than a state-of-the-art baseline algorithm.  2008 ACM.",
      "title": "16853 Metric-based ontology learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-70349410705&partnerID=40&md5=c510740fd43c5d2f010e30a57e07d9c9"
    },
    {
      "abstract": "New text categorization models using back-propagation neural network (BPNN) and modified back-propagation neural network (MBPNN) are proposed. An efficient feature selection method is used to reduce the dimensionality as well as improve the performance. The basic BPNN learning algorithm has the drawback of slow training speed, so we modify the basic BPNN learning algorithm to accelerate the training speed. The categorization accuracy also has been improved consequently. Traditional word-matching based text categorization system uses vector space model (VSM) to represent the document. However, it needs a high dimensional space to represent the document, and does not take into account the semantic relationship between terms, which can also lead to poor classification accuracy. Latent semantic analysis (LSA) can overcome the problems caused by using statistically derived conceptual indices instead of individual words. It constructs a conceptual vector space in which each term or document is represented as a vector in the space. It not only greatly reduces the dimensionality but also discovers the important associative relationship between terms. We test our categorization models on 20-newsgroup data set, experimental results show that the models using MBPNN outperform than the basic BPNN. And the application of LSA for our system can lead to dramatic dimensionality reduction while achieving good classification results.  2008 Elsevier B.V. All rights reserved.",
      "title": "16854 Latent semantic analysis for text categorization using neural network",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54949154550&partnerID=40&md5=6cf31c47bccd011260824fe2f6fe4ec9"
    },
    {
      "abstract": "Information Extraction (IE) systems have been proposed in recent years, to extract genic interactions from bibliographical resources. But they are limited to single interaction relations, and have to face a tradeoff between recall and precision, by focusing either on specific interactions (for precision), or general and unspecified interactions of biological entities (for recall). Yet, biologists need to process more complex data from literature, in order to study biological pathways, so an ontology is an adequate formal representation to model this sophisticated knowledge. But the tight integration of IE systems and ontologies is still a current research issue, a fortiori with complex ones that go beyond hierarchies. Here, we propose a rich modeling of genic interactions with an ontology, and show how it can be used within an IE system. The ontology is seen as a language specifying a normalized representation of text. IE is performed by first extracting instances from Natural Language Processing (NLP) modules, then deductive inferences on the ontology language are completed. New instances may be infered, bringing together otherwise scattered textual information. We validated our approach on an annotated corpus of gene transcription regulations in Bacillus subtilis. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten semantic relations defined in the ontology.",
      "title": "16855 Genic interaction extraction by reasoning on an ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874270938&partnerID=40&md5=77b124315c54bd7fb7dd16569d004da8"
    },
    {
      "abstract": "Keyphrase extraction plays a key role in various fields such as information retrieval, text classification etc. However, most traditional keyphrase extraction methods relies on word frequency and position instead of document inherent semantic information, often results in inaccurate output. In this paper, we propose a novel automatic keyphrase extraction algorithm using semantic features mined from online Wikipedia. This algorithm first identifies candidate keyphrases based on lexical methods, and then a semantic graph which connects candidate keyphrases with document topics is constructed. Afterwards, a link analysis algorithm is applied to assign semantic feature weight to the candidate keyphrases. Finally, several statistical and semantic features are assembled by a regression model to predict the quality of candidates. Encouraging results are achieved in our experiments which show the effectiveness of our method.  2008 IEEE.",
      "title": "16856 Improving keyphrase extraction using wikipedia semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63149159385&partnerID=40&md5=f17cd4b23e53cbeea73d1bd75e7ab057"
    },
    {
      "abstract": "In computational linguistics, word sense disambiguation (WSD) is the problem of determining in which sense a word having a number of distinct senses is used in a given sentence. This paper handles text document clustering as one of the major tasks of text processing. Document clustering is the process of finding out groups of information from the text documents and cluster these documents into the most relevant groups. Large document corpus suffers from ambiguity problems like synonyms, polysemous and other semantic relations. For this reason we perform WSD task for all terms in all documents to get the best sense to be used as document features in the clustering process. Our experimental results proved that the efficiency of document clustering using WSD increases linearly with the size of the documents dataset. Different part of speech (POS) taggers were tested to determine the best",
      "title": "16857 Document clustering using word sense disambiguation",
      "url": ""
    },
    {
      "abstract": "Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. Research in text summarization has therefore often focused on measures of the usefulness of sentences for a summary. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentences information content. The classifier that maps sentences to the taxonomy is trained using search engines and is therefore very flexible and not bound to a specific domain. In our experiments, we train an SVM classifier to identify summary sentences using ontology-based sentence features. Our experimental results show that the ontology-based extraction of sentences outperforms baseline classifiers, leading to higher Rouge scores of summary extracts.  2008 IEEE.",
      "title": "16862 An ontology-based approach to text summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-62949201716&partnerID=40&md5=eb84019f64bdc81c62962f2ad904af97"
    },
    {
      "abstract": "Objectives: We compared the effects of two semantic terminology models on classification of clinical notes through a study in the domain of heart murmur findings. Methods: One schema was established from the existing SNOMED CT model (S-Model) and the other was from a template model (T-Model) which uses base concepts and non-hierarchical relationships to characterize the murmurs. A corpus of clinical notes (n=309) was collected and annotated using the two schemas. The annotations were coded for a decision tree classifier for text classification task. The standard information retrieval measures of precision, recall, f-score and accuracy and the paired t-test were used for evaluation. Results: The performance of S-Model was better than the original T-Model (p<0.05 for recall and f-score). A revised T-Model by extending its structure and corresponding values performed better than S-Model (p<0.05 for recall and accuracy). Conclusion: We discovered that content coverage is a more important factor than terminology model for classification",
      "title": "16863 Comparing the effects of two semantic terminology models on classification of clinical notes: A study of heart murmur findings",
      "url": ""
    },
    {
      "abstract": "Online collaboration among communities of practice using text-based tools, such as instant messaging, forums and web logs (blogs), has become very popular in the last years, but it is difficult to automatically analyze all their content due to the problems of natural language understanding software. However, useful socio-semantic data can be retrieved from a chat conversation using ontology-based text mining techniques. In this paper, a novel approach for detecting several kinds of semantic data from a chat conversation is presented. This method uses a combination of a dialogistic, socio-cultural perspective and of classical knowledge-based text processing methods. Lexical and domain ontologies are used. A tool has been developed for the discovery of the most important topics and of the contribution of each participant in the conversation. The system also discovers new, implicit references among the utterances of the chat in order to offer a multi-voiced representation of the conversation. The application offers a panel for visualizing the threading of the subjects in the chat and the contributions function. The system was experimented on chat sessions of small groups of students participating in courses on Human-Computer Interaction and Natural Language Processing in Politehnica University of Bucharest, Romania.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16865 Extraction of socio-semantic data from chat conversations in collaborative learning communities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56449125482&partnerID=40&md5=aa4d5245d4bf9f55d9ffa3db6d56370d"
    },
    {
      "abstract": "Traditional text mining techniques have weak ability to provide associated relations with rich semantics that is a foundation of the intelligent browsing of topics, discovery of semantic community and precise personalized recommendation in current Web and Knowledge Grid, etc. In this paper we propose an algorithm to generate and calculate the associated relations and their strengths between documents within a domain. Each document is represented by a bag of words and their weights. We first build domain knowledge background based on the association rules at keyword level, and then we apply those association rules to generate and calculate the documents semantic relations and their strengths at document level, which effectively shorten the semantic gap from keyword semantics to document semantics. Experimental results show that our proposed method is feasible and able to discover interesting facts within a domain. 2008 IEEE.",
      "title": "16866 Generating associated relation between documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56349119557&partnerID=40&md5=e41cb73537ccf510ba577c0600cc5340"
    },
    {
      "abstract": "Text data, which are represented as free text in World Wide Web (WWW), are inherently unstructured and hence it becomes difficult to directly process the text data by computer programs. There has been great interest in text mining techniques recently for helping users to quickly gain knowledge from the Web. Text mining technologies usually involve tasks such as text refining which transforms free text into an intermediate representation form which is machine-processable and knowledge distillation which deduces patterns or knowledge from the intermediate form. These text representation methodologies consider documents as bags of words and ignore the meanings and ideas their authors want to convey. As terms are treated as individual items in such simplistic representations, terms lose their semantic relations and texts lose their original meanings. In this paper, we propose a system that overcomes the limitations of the existing technologies to retrieve the information from the knowledge discovered through data mining based on the detailed meanings of the text. For this, we propose a Knowledge representation technique, which uses Resources Description Framework (RDF) metadata to represent the semantic relations, which are extracted from textual web document using natural language processing techniques. The main objective of the creation of RDF metadata in this system is to have flexibility for easy retrieval of the semantic information effectively. We also propose an effective SEMantic INformation RETrieval algorithm called SEMINRET algorithm. The experimental results obtained from this system show that the computations of Precision and Recall in RDF databases are highly accurate when compared to XML databases. Moreover, it is observed from our experiments that the document retrieval from the RDF database is more efficient than the document retrieval using XML databases.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16871 An intelligent system for semantic information retrieval information from textual web documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54349086914&partnerID=40&md5=c80fd0f1c4f40536111ad05cc4acae1d"
    },
    {
      "abstract": "With their expanding information assets and the growing importance of the knowledge factor, organizations are increasingly challenged to efficiently support knowledge management processes with appropriate structuring and retrieval technologies. Besides traditional information retrieval approaches, the use of semantic technologies like Topic Maps is also becoming more important. This paper compares the potential value of all these approaches, technological requirements, and applications. Thereafter, appropriate recommendations are derived for the most suitable choice of technology. In addition, early experiences from a practical case in the chemical industry provide further insights into the applicability of these recommendations in practice.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16872 Convergence of classical search and semantic technologies - Evidences from a practical case in the chemical industry",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54249144524&partnerID=40&md5=b4b5dcd581a5fb685a3f551158141441"
    },
    {
      "abstract": "At present time, Information plays a relevant role in current societies. In this context, Internet is one of the most extended mechanisms to communicate and distribute information around the word. Today, due to the extremely large number of information sources, automatic mechanisms are needed to filter the information that could be useful for each user. However, one of the problems that the usual techniques of automatic text categorization have not been able to handle is polysemy (words with two o more senses). In this paper, we have faced this problem by proposing a semantic analyzer for the automatic categorization of texts in Spanish. Context exploration techniques were used as a key mechanism for guiding the disambiguation process. A specific lexical database and its existing semantic relations fulfilled the objective of appropriately categorizing the analyzed text. To validate this analyzer, a tool was developed that classifies web pages by semantic sense. We present performance results for this classifier. Finally, a comparison with four other classification tools is reported.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16873 A method for automatic text categorization using word sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54249149573&partnerID=40&md5=0067ae9ff4a83ce5d8456c49c67bbef1"
    },
    {
      "abstract": "Within the last years the Web dramatically influenced biomedical research. Although it allows for almost instantaneous access to a huge amount of distributed information the problem how to retrieve useful information still persist. With semantic technologies (especially Topic Maps) the solution becomes tangible. We will discuss in this paper concepts and a technical realization for knowledge representation within the biomedical domain. This includes not only the semantic access of distributed and heterogeneous resources based on state-of-the-art enterprise integration technologies (J2EE, Web Services) but also an approach for Topic Map based views on unstructured information from scientific publications. We will furthermore present the implementation of an information portal based on the seamless semantic integration of ~ 500 genome databases and ~16.000.000 abstracts.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16874 Large scale knowledge representation of distributed biomedical information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54249089450&partnerID=40&md5=0c8c983b68b42cfcb501b0afb053b4f9"
    },
    {
      "abstract": "Most text categorization research exploit bag-of-words text representation. However, such representation makes it very hard to capture semantic similarity between text documents that share very little or even no vocabulary. In this paper we present preliminary results obtained with a novel approach that combines well established kernel text classifiers with external contextual commonsense knowledge. We propose a method for computing semantic similarity between words as a result of diffusion process in ConceptNet semantic space. Evaluation on a Reuters dataset show an improvement in precision of classification.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16875 Text categorization with semantic commonsense knowledge: First results",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-54049091997&partnerID=40&md5=a18cbdc5f5edf266d5b041ccfde6e42b"
    },
    {
      "abstract": "With more and more genomes being sequenced, a lot of effort is devoted to their annotation with terms from controlled vocabularies such as the GeneOntology. Manual annotation based on relevant literature is tedious, but automation of this process is difficult. One particularly challenging problem is word sense disambiguation. Terms such as development can refer to developmental biology or to the more general sense. Here, we present two approaches to address this problem by using term co-occurrences and document clustering. To evaluate our method we defined a corpus of 331 documents on development and developmental biology. Term co-occurrence analysis achieves an F-measure of 77%. Additionally, applying document clustering improves precision to 82%. We applied the same approach to disambiguate nucleus, transport, and spindle, and we achieved consistent results. Thus, our method is a viable approach towards the automation of literature-based genome annotation.  2008 Inderscience Enterprises Ltd.",
      "title": "16878 Word Sense Disambiguation in biomedical ontologies with term co-occurrence analysis and document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-53349143997&partnerID=40&md5=46566c3af1dd5f989a890052c20dd255"
    },
    {
      "abstract": "The Ontology registry system is developed to collect, manage, and compare ontological information for integrating global observation data. Data sharing and data service such as support of metadata deign, structuring of data contents, support of text mining are applied for better use of data as data interoperability. Semantic network dictionary and gazetteers are constructed as a trans-disciplinary dictionary. Ontological information is added to the system by digitalizing text based dictionaries, developing knowledge writing tool for experts, and extracting semantic relations from authoritative documents with natural language processing technique. The system is developed to collect lexicographic ontology and geographic ontology.  2008 Tsinghua University Press.",
      "title": "16883 Interoperability for Global Observation Data by Ontological Information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-62649090253&partnerID=40&md5=ef8ebea47a5fa6c2514e5998f5aba38b"
    },
    {
      "abstract": "Research work related to plagiarism detection methods in dealing with monolingual texts (e.g. English texts) have been well established in recent years. However, little attention has been paid to facilitate plagiarism detection in cross-lingual text collections (e.g. English and Chinese texts). In this paper we present a system platform to evaluating text similarity and relatedness in multilingual text collections for plagiarism detection. First, we utilized a number of selected texts in Chinese-English parallel corpora collected from internet to train text classifiers based on the Support Vector Machines (SVM) model. As such, the multilingual texts of unknown category can be classified by the trained classifiers. Subsequently, the resulting categorized texts were measured by means of a language-neutral clustering technique based on Self-Organizing Maps (SOM) method for evaluating semantic similarity among texts. The preliminary results show that our platform framework has the potential for cross-lingual text relatedness evaluation and plagiarism detection.  2008 IEEE.",
      "title": "16884 A platform framework for cross-lingual text relatedness evaluation and plagiarism detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52449127415&partnerID=40&md5=84ea63dceea78be919f3eab4c3e2a1c2"
    },
    {
      "abstract": "We present a new, ontology-based approach to the automatic text categorization. An important and novel aspect of this approach is that our categorization method does not require a training set, which is in contrast to the traditional statistical and probabilistic methods. In the presented method, the ontology, including the domain concepts organized into hierarchies of categories and interconnected by relationships, as well as instances and connections among them, effectively becomes the classifier. Our method focuses on (i) converting a text document into a thematic graph of entities occurring in the document, (ii) ontological classification of the entities in the graph, and (iii) determining the overall categorization of the thematic graph, and as a result, the document itself. In the presented experiments, we used an RDF ontology constructed from the full English version of Wikipedia. Our experiments, conducted on corpora of Reuters news articles, showed that our training-less categorization method achieved a very good overall accuracy.  2008 IEEE.",
      "title": "16888 Wikipedia in action: Ontological knowledge in text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52149090731&partnerID=40&md5=efcb164bf557253207fefc387bc679b0"
    },
    {
      "abstract": "Geo-temporal criteria are important for filtering, grouping and prioritizing information resources. This presents techniques for extracting semantic geo-temporal information from text, using simple text mining methods that leverage on a gazetteer. A prototype system, implementing the proposed methods and capable of displaying information over maps and timelines, is described. This prototype can take input in RSS, demonstrating the application to content from many different online sources. Experimental results demonstrate the efficiency and accuracy of the proposed approaches.  2008 IEEE.",
      "title": "16890 Extracting and exploring the geo-temporal semantics of textual resources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52149086540&partnerID=40&md5=729d6016286f916ff6ab6a09bb5afd49"
    },
    {
      "abstract": "In Biomedicine, provision of high-quality knowledge in terms of terminologies and consistent cross-references is a cornerstone for information retrieval, information extraction, text mining and federation of data repositories. Since the late 90s we have been building knowledge resources to support these tasks. Over the years, we have gathered valuable experience by running various applications and by discussing with our scientists from different areas. Based on this experience, we are currently enhancing the content of our terminologies as well as the conceptual structures to represent cross-references. In the biomedical domain, there are long standing efforts to formally conceive knowledge repositories. With the era of the Semantic Web, a limited number of frameworks have achieved acceptance in the community aiming at semantic interoperability. Inspired by the knowledge engineering approaches in biomedicine, we propose a unified conceptual model for knowledge representation and its implementation. Contrary to the widespread frameworks, our model excels in its modularity. In addition, our formal approach is designed to be compatible to the main biomedical frameworks so that content can be exchanged in both directions.  2008 IEEE.",
      "title": "16892 A unified conceptual model for representation of terminologies and references of terms across data repositories in biomedicine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52149084346&partnerID=40&md5=43b1d205383c360eac013b0e9e9e896a"
    },
    {
      "abstract": "As a result of advance in technology, there now exist a large amount of online documents in the form of surveys or called reviews. Most of the previous work on text classification is focusing on sentiment text classification. Sentiment classification requires the knowledge data of vocabularies semantic meaning and the relationships between the vocabularies. In this paper, sentiment features of text were divided into characteristic words and phrases, which were extracted from the training data. The method combining HowNet with sentiment classifier was proposed. It computed semantic similarity of characteristic words, phrases with tagged words in HowNet, and it adopted the positive and negative terms as features of sentiment classifier. In the test, a sentiment classifier was designed to compare with the other methods. Evaluation results show the effectiveness of our method.  2008 IEEE.",
      "title": "16897 Opinion mining: A study on semantic orientation analysis for online document",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-52149120939&partnerID=40&md5=382818a37274d148deeab518bc0124c0"
    },
    {
      "abstract": "Asynchronous discussion forums are used increasingly in e-learning in which learners interact in academic and social contexts, whereas some challenges and difficulties exist pertaining to the activity and variability in learner participation and collaboration. To motivating and facilitating active collaborative learning, this paper describes the design of an enhanced asynchronous discussion forum. Base on the domain ontology and text mining technologies, discussion transcripts are automatically processed for structural modeling with semantic association, which lays the foundation for the fulfillment of distinctive functionalities in the discussion forum (i.e. semantic search, thematic navigation and recommendation). An experimental study is conducted to show that the approach is effective to discover learning peers with same interests and search for related messages with thematic navigational guidance.  2008 IEEE.",
      "title": "16898 Semantic organization of online discussion transcripts for active collaborative learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849109900&partnerID=40&md5=591557f0c1083d145fe58b15161aec15"
    },
    {
      "abstract": "Learning objects are not a new concept, being currently produced either according to existent specifications, or in an ad hoc way. However, a system that can easily classify them for automatic storage is still missing. In this article we propose a repository of learning objects which is capable of automatic classification and categorization based on a semantic organization. The classification is based on a text extraction mechanism which then uses text mining methods. We present a methodology for the initial clustering of objects, and a technique for maintaining the organization with continuous arrival of new objects.  2008 IEEE.",
      "title": "16900 A repository with semantic organization for educational content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849112858&partnerID=40&md5=c3259e93e777c568ff2964d497494d2e"
    },
    {
      "abstract": "Question classification (QC) plays a key role in automated question answering (QA) systems. In Chinese QC, for example, a question is analyzed and then labeled with the question type it belongs to and the expected answer type. In this paper, we propose a novel method of Chinese QC that integrates syntactic tags and semantic tags into an alignment-based approach. We adopt a template alignment (TA) algorithm to process large collections of Chinese questions and compare the classification results with those of INFOMAP, a human annotated knowledge inference engine for Chinese questions. We experimented with two approaches for the proposed system: a majority algorithm and a machine learning method that uses Support Vector Machine (SVM). The TA algorithm performs well with both approaches. The experimental results show that the accuracy achieved by TA (85.5%) is comparable to that of INFOMAP (88%). In contrast, QC based on the SVM approach, which incorporates syntactic features and TA yields an accuracy rate of 91.5% 2008 IEEE.",
      "title": "16901 A template alignment algorithm for question classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849127327&partnerID=40&md5=43dde5586cc554b3030a7e64ea0636a0"
    },
    {
      "abstract": "In the present work, a knowledge representation of Computer Networks technical text, according to the Denhiere-Baudet text comprehension model, is presented. The semantic relations among units and events of a technical text can be expressed by structures, as microstructure and macrostructure. Furthermore, the explicit and implicit knowledge representation, and the micro and macrostructure representation of the functional system operations, depicted in this text, is provided. The presented methodology can support automated reasoning, through the knowledge representation, which leads to automated knowledge extraction from a technical text, and, sub sequentially, to automated normalized answers assessment.  2008 IEEE.",
      "title": "16902 Knowledge representation for an automated normalized answers assessment system, based on text comprehension theories",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849154292&partnerID=40&md5=1999220d8c16587cc490dcceb1019619"
    },
    {
      "abstract": "Under social tagging systems, a typical Web 2.0 application, users label digital data sources by using freely chosen textual descriptions (tags). Poor retrieval in the aforementioned systems remains a major problem mostly due to questionable tag validity and tag ambiguity. Earlier clustering techniques have shown limited improvements, since they were based mostly on tag co-occurrences. In this paper, a co-clustering approach is employed, that exploits joint groups of related tags and social data sources, in which both social and semantic aspects of tags are considered simultaneously. Experimental results demonstrate the efficiency and the beneficial outcome of the proposed approach in correlating relevant tags and resources.  2008 IEEE.",
      "title": "16903 Co-clustering tags and social data sources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849113797&partnerID=40&md5=8584910402735bb7993847afa70e10a0"
    },
    {
      "abstract": "Text Mining tasks include text categorization, text clustering, concept/entity extraction, document summarization, and entity relation modeling. In this paper, the focus is given to concept/entity extraction only. The major challenging issue in extracting concept/entity from texts is natural language words are always ambiguous. Up to now, not much research in text mining especially in concept/entity extraction has focused on the ambiguity problem. This paper addresses ambiguity issues in natural language texts, and presents a new technique for resolving ambiguity problem in extracting concept/entity from texts. The technique is developed by applying possibility theory, fuzzy set, and knowledge about the context to lexical semantics. 2008 IEEE.",
      "title": "16904 Ambiguity in text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-51849136487&partnerID=40&md5=8b7a27a9a97b661a487e9d18b0e06f77"
    },
    {
      "abstract": "Analysis of affective intensities in computer-mediated communication is important in order to allow a better understanding of online users emotions and preferences. Despite considerable research on textual affect classification, it is unclear which features and techniques are most effective. In this study, we compared several feature representations for affect analysis, including learned n-grams and various automatically and manually crafted affect lexicons. We also proposed the support vector regression correlation ensemble (SVRCE) method for enhanced classification of affect intensities. SVRCE uses an ensemble of classifiers each trained using a feature subset tailored toward classifying a single affect class. The ensemble is combined with affect correlation information to enable better prediction of emotive intensities. Experiments were conducted on four test beds encompassing web forums, blogs, and online stories. The results revealed that learned n-grams were more effective than lexicon-based affect representations. The findings also indicated that SVRCE outperformed comparison techniques, including Pace regression, semantic orientation, and WordNet models. Ablation testing showed that the improved performance of SVRCE was attributable to its use of feature ensembles as well as affect correlation information. A brief case study was conducted to illustrate the utility of the features and techniques for affect analysis of large archives of online discourse.  2008 IEEE.",
      "title": "16909 Affect analysis of web forums and blogs using correlation ensembles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50649122516&partnerID=40&md5=0115406671528a8da88d25bfb84f83e4"
    },
    {
      "abstract": "In this paper we present a framework for unified, personalized access to heterogeneous multimedia content in distributed repositories. Focusing on semantic analysis of multimedia documents, metadata, user queries and user profiles, it contributes to the bridging of the gap between the semantic nature of user queries and raw multimedia documents. The proposed approach utilizes as input visual content analysis results, as well as analyzes and exploits associated textual annotation, in order to extract the underlying semantics, construct a semantic index and classify documents to topics, based on a unified knowledge and semantics representation model. It may then accept user queries, and, carrying out semantic interpretation and expansion, retrieve documents from the index and rank them according to user preferences, similarly to text retrieval. All processes are based on a novel semantic processing methodology, employing fuzzy algebra and principles of taxonomic knowledge representation. The first part of this work presented in this paper deals with data and knowledge models, manipulation of multimedia content annotations and semantic indexing, while the second part will continue on the use of the extracted semantic information for personalized retrieval.  2007 Springer Science+Business Media, LLC.",
      "title": "16913 Semantic representation of multimedia content: Knowledge representation and semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-47949109505&partnerID=40&md5=17d54a054e4db90e20624ba5339aae18"
    },
    {
      "abstract": "The termvisual text analytics describes a class of information analysis techniques and processes that enable knowledge discovery via the use of interactive graphical representations of textual data. These techniques enable discovery and understanding via the recruitment of human visual pattern recognition and spatial reasoning capabilities. Visual text analytics is a subclass of visual data mining / visual analytics, which more generally encompasses analytical techniques that employ visualization of non-physically-based (or abstract) data of all types. Text visualization is a key component in visual text analytics. While the term text visualization has been used to describe a variety of methods for visualizing both structured and unstructured characteristics of text-based data, it is most closely associated with techniques for depicting the semantic characteristics of the free-text components of documents in large document collections. In contrast with text clustering techniques which serve only to partition text corpora into sets of related items, these so-called semantic mapping methods also typically strive to depict detailed inter- and intra-set similarity structure. Text analytics software typically couples semantic mapping techniques with additional visualization techniques to enable interactive comparison of semantic structure with other characteristics of the information, such as publication date or citation information. In this way, value can be derived from the material in the form of multidimensional relationship patterns existing among the discrete items in the collection. The ultimate goal of these techniques is to enable human understanding and reasoning about the contents of large and complexly related text collections.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16917 Text visualization for visual text analytics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50149112414&partnerID=40&md5=757c57ab4a3901496b04a76236236bb4"
    },
    {
      "abstract": "We investigate the effect of encoding additional semantic and syntactic information sources in a classification-based machine learning approach to the task of coreference resolution for Dutch. We experiment both with a memory-based learning approach and a maximum entropy modeling method. As an alternative to using external lexical resources, such as the low-coverage Dutch EuroWordNet, we evaluate the effect of automatically generated semantic clusters as information source. We compare these clusters, which group together semantically similar nouns, to two semantic features based on EuroWordNet encoding synonym and hypernym relations between nouns. The syntactic function of the anaphor and antecedent in the sentence can be an important clue for resolving coreferential relations. As baseline approach, we encode syntactic information as predicted by a memory-based shallow parser in a set of features. We contrast these shallow parse based features with features encoding richer syntactic information from a dependency parser. We show that using both the additional semantic information and syntactic information lead to small but significant performance improvement of our coreference resolution approach.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16918 Semantic and syntactic features for Dutch coreference resolution",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949091492&partnerID=40&md5=5b3508cbd927021bdfd8d766e25248c5"
    },
    {
      "abstract": "Knowledge representation systems aiming at full natural language understanding need to cover a wide range of semantic phenomena including lexical ambiguities, coreference, modalities, counterfactuals, and generic sentences. In order to achieve this goal, we argue for a multidimensional view on the representation of natural language semantics. The proposed approach, which has been successfully applied to various NLP tasks including text retrieval and question answering, tries to keep the balance between expressiveness and manageability by introducing separate semantic layers for capturing dimensions such as facticity, degree of generalization, and determination of reference. Layer specifications are also used to express the distinction between categorical and situational knowledge and the encapsulation of knowledge needed e.g. for a proper modeling of propositional attitudes. The paper describes the role of these classificational means for natural language understanding, knowledge representation, and reasoning, and exemplifies their use in NLP applications.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16919 Layer structures and conceptual hierarchies in semantic representations for NLP",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949109964&partnerID=40&md5=7bcdea366047124f7b3a5e073b433d3f"
    },
    {
      "abstract": "Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on logical representations for meaning, which may be viewed as being external to the natural language itself. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe a generic semantic inference framework that operates directly on language-based structures, particularly syntactic trees. New trees are inferred by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical evaluation in a Relation Extraction setting supports the validity and potential of our approach. Additionally, such inference is shown to improve the critical step of unsupervised learning of entailment rules, which in turn enhances the scope of the inference system.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16920 Natural language as the basis for meaning representation and inference",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949098274&partnerID=40&md5=0dc321a57e66336513a24f7ef739b477"
    },
    {
      "abstract": "The paper describes the neural network model which in the information retrieval process solves the document set dimension reduction for representation of text documents in Slovak language. This model comes out of the Vector Space Model, which for document set uses the full index representation. To decrease the matrix dimension for document set representation the Latent Semantic Model is used. Main advantage of Latent Semantic Model in relation to the Vector Space Model is the great reduction of the matrix dimension for document set representation. Described approach is performed by Cascade Neural Network.  2008 IEEE.",
      "title": "16922 Proposal of Cascade Neural Network model for text document space dimension reduction by Latent Semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49749091192&partnerID=40&md5=796b82cdceaf2e1f220c6e6c6d172409"
    },
    {
      "abstract": "Text representation is the basis of text processing. Most current text representation model didnt consider of the words relations and result in the loss of texts structure information, which is important to understand the text. This paper proposed a novel text representation model, which uses lexical network to represent the text and retains the texts structure. According to the different levels of words relations, co-occurrence network, syntactic network and semantic network are introduced. The text network representation was applied into text classification to measure the representation ability of this model. The experiment result shows that our text network representation is prior to vector space model.  2008 IEEE.",
      "title": "16924 Research on text network representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49249102216&partnerID=40&md5=6b515810503d5cef9d1d9691edd28dc5"
    },
    {
      "abstract": "The central problem addressed by this interdisciplinary paper is to predict related software artifacts that are usually changed together by a developer. The working focus of programmers is revealed by means of their interactions with a software repository that receives a set of cohesive artifact changes within one commit transaction. This implicit knowledge of interdependent changes can be exploited in order to recommend likely further changes, given a set of already changed artifacts. We suggest a hybrid approach based on Latent Semantic Indexing (LSI) and machine learning methods to recommend software development artifacts, that is predicting a sequence of configuration items that were committed together. As opposed to related approaches to repository mining that are mostly based on symbolic methods like Association Rule Mining (ARM), our connectionist method is able to generalize onto unseen artifacts. Text analysis methods are employed to consider their textual attributes. We applied our technique to three publicly available datasets from the PROMISE Repository of Software Engineering Databases. The evaluation showed that the connectionist LSI-approach achieves a significantly higher recommendation accuracy than existing methods based on ARM. Even when generalizing onto unseen artifacts, our approach still provides an accuracy of up to 72.7% on the given datasets.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16926 Recommending software artifacts from repository transactions",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-48249144780&partnerID=40&md5=4a75749a421c71e1b90fb4fb0ca5e118"
    },
    {
      "abstract": "In this paper, we present a novel textual case-based reasoning system called SOPHIA-TCBR which provides a means of clustering semantically related textual cases where individual clusters are formed through the discovery of narrow themes which then act as attractors for related cases. During this process, SOPHIA-TCBR automatically discovers appropriate case and similarity knowledge. It then is able to organize the cases within each cluster by forming a minimum spanning tree, based on their semantic similarity. SOPHIAs capability as a case-based text classifier is benchmarked against the well known and widely utilised k-Means approach. Results show that SOPHIA either equals or outperforms k-Means based on 2 different case-bases, and as such is an attractive approach for case-based classification. We demonstrate the quality of the knowledge discovery process by showing the high level of topic similarity between adjacent cases within the minimum spanning tree. We show that the formation of the minimum spanning tree makes it possible to identify a kernel region within the cluster, which has a higher level of similarity between cases than the cluster in its entirety, and that this corresponds directly to a higher level of topic homogeneity. We demonstrate that the topic homogeneity increases as the average semantic similarity between cases in the kernel increases. Finally having empirically demonstrated the quality of the knowledge discovery process in SOPHIA, we show how it can be competently applied to case-based retrieval.  2008 Elsevier B.V. All rights reserved.",
      "title": "16932 SOPHIA-TCBR: A knowledge discovery framework for textual case-based reasoning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-44149087861&partnerID=40&md5=f302b034116b63d5de565bed1876267c"
    },
    {
      "abstract": "The Internet holds huge amount of documents available for users. Effective utilization of this enormous repository means a need for tools and systems that support users in a process of finding documents related to their interests. For a human, a process of finding relevant documents is performed based on a network of interconnected and mutually dependent concepts and keywords. Concepts defined with keywords, words, or other concepts are part of the network. An ontology is a partially ordered set of concepts representing a specific domain. An ontology defined in the framework of the Semantic Web (Berners-Lee, Hendler, & Lassila, 2001) allows for specification of definitions of concepts, their instances, and a variety of relationships between concepts. Among various forms of knowledge representation, a hierarchy of concepts (Yager, 2000) is of a special interest. Graph-like structures of hierarchies provide a means for defining connections representing different dependencies between concepts. Copyright  2008, IGI Global.",
      "title": "16933 Ontology enhanced concept hierarchies for text identification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77952651583&partnerID=40&md5=590f7d596dadad908d2310d97468a2a2"
    },
    {
      "abstract": "We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.  2008 ACM.",
      "title": "16937 Semantic text similarity using corpus-based word similarity and string similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49149122316&partnerID=40&md5=052fd31558beb9c438141f91f8a5a4e1"
    },
    {
      "abstract": "Mining textual data in chat mediums is becoming more important because these mediums contain a vast amount of information, which is potentially relevant to a societys current interests, habits, social behaviors, crime tendency and other tendencies. Here, sex identification is taken as a base study in information mining in chat mediums. In order to do this, a simple discrimination function and semantic analysis method are proposed for sex identification in Turkish chat mediums. Then, the proposed sex identification method is compared with the Support Vector Machine (SVM) and Naive Bayes (NB) methods. Finally, results show that the proposed system has achieved accuracy over 90% in sex identification.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16940 A comparison of textual data mining methods for sex identification in chat conversations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-45449112960&partnerID=40&md5=938cd9b7e68d52c35d02e0ac9e4c7f52"
    },
    {
      "abstract": "A method is being developed to mine a text corpus for candidate linguistic patterns for information extraction. The candidate patterns can be used to improve the quality of extraction patterns constructed by a pseudo-supervised learning method - an automated method in which the system is provided with a high quality seed pattern or clue, which is used to generate a training set automatically. The study is carried out in the context of developing a system to extract disease-treatment information from medical abstracts retrieved from the Medline database. In an earlier study, the Apriori algorithm had been used to mine a sample of sentences containing a disease concept and a drug concept, to identify frequently occurring word patterns to see if these patterns could be used to identify treatment relations in text. Word patterns and statistical association measures alone were found to be insufficient for generating good extraction patterns, and need to be combined with syntactic and semantic constraints. In this study, we explore the use of syntactic, semantic and lexical constraints to improve the quality of extraction patterns.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16942 Pattern mining for information extraction using lexical, syntactic and semantic information: Preliminary results",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-45449087910&partnerID=40&md5=3ee745943b9ca68b3dfad16e004a5fce"
    },
    {
      "abstract": "The task of Text Classification (TC) is to automatically assign natural language texts with thematic categories from a predefined category set. And Latent Semantic Indexing (LSI) is a well known technique in Information Retrieval, especially in dealing with polysemy (one word can have different meanings) and synonymy (different words are used to describe the same concept), but it is not an optimal representation for text classification. It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation. Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information. However, their performance improvements over original term vectors are still very limited. In this paper, we propose a new local Latent Semantic Indexing method called Local Relevancy Ladder-Weighted LSI to improve text classification. And separate matrix singular value decomposition (SVD) was used to reduce the dimension of the vector space on the transformed local region of each class. Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16943 LRLW-LSI: An improved Latent Semantic Indexing (LSI) text classifier",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-44649100104&partnerID=40&md5=36b9be09ac8d50c549ed1b5ab589f172"
    },
    {
      "abstract": "Understanding the propagation of influence and the concept flow over a network in general has profound theoretical and practical implications. In this paper, we propose a novel approach to ranking individual members of a real-world communication network in terms of their roles in such propagation processes. We first improve the accuracy of the centrality measures by incorporating temporal attributes. Then, we integrate weighted PageRank and centrality scores to further improve the quality of these measures. We valid these ranking measures through a study of an email archive of a W3C working group against an independent list of experts. The results show that time-sensitive Degree, time-sensitive Betweenness and the integration of the weighted PageRank and these centrality measures yield the best ranking results. Our approach partially solves the rank sink problem of PageRank by adjusting flexible jumping probabilities with Betweenness centrality scores. Finally the text analysis based on Latent Semantic Indexing extracts key concepts distributed in different time frames and explores the evolution of the discussion topics in the social network. The overall study depicts an overview of the roles of the actors and conceptual evolution in the social network. These findings are important to understand the dynamics of the social networks.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16944 Analyzing the propagation of influence and concept evolution in enterprise social networks through centrality and latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-44649142517&partnerID=40&md5=179deb67a70b5eef9a8b244b6ef6b52d"
    },
    {
      "abstract": "Alexopoulou, D.a and Pickersgill, L.b  and Eyre, C.c  and Schroeder, M.a ",
      "title": "16951 Terminologies for text-mining",
      "url": ""
    },
    {
      "abstract": "Background: The increasing amount of published literature in biomedicine represents an immense source of knowledge, which can only efficiently be accessed by a new generation of automated information extraction tools. Named entity recognition of well-defined objects, such as genes or proteins, has achieved a sufficient level of maturity such that it can form the basis for the next step: the extraction of relations that exist between the recognized entities. Whereas most early work focused on the mere detection of relations, the classification of the type of relation is also of great importance and this is the focus of this work. In this paper we describe an approach that extracts both the existence of a relation and its type. Our work is based on Conditional Random Fields, which have been applied with much success to the task of named entity recognition. Results: We benchmark our approach on two different tasks. The first task is the identification of semantic relations between diseases and treatments. The available data set consists of manually annotated PubMed abstracts. The second task is the identification of relations between genes and diseases from a set of concise phrases, so-called GeneRIF (Gene Reference Into Function) phrases. In our experimental setting, we do not assume that the entities are given, as is often the case in previous relation extraction work. Rather the extraction of the entities is solved as a subproblem. Compared with other state-of-the-art approaches, we achieve very competitive results on both data sets. To demonstrate the scalability of our solution, we apply our approach to the complete human GeneRIF database. The resulting gene-disease network contains 34758 semantic associations between 4939 genes and 1745 diseases. The gene-disease network is publicly available as a machine-readable RDF graph. Conclusion: We extend the framework of Conditional Random Fields towards the annotation of semantic relations from text and apply it to the biomedical domain. Our approach is based on a rich set of textual features and achieves a performance that is competitive to leading approaches. The model is quite general and can be extended to handle arbitrary biological entities and relation types. The resulting gene-disease network shows that the GeneRIF database provides a rich knowledge source for text mining. Current work is focused on improving the accuracy of detection of entities as well as entity boundaries, which will also greatly improve the relation extraction performance.  2008 Bundschus et al",
      "title": "16952 Extraction of semantic biomedical relations from text using conditional random fields",
      "url": ""
    },
    {
      "abstract": "Current representation schemes for automatic text classification treat documents as syntactically unstructured collections of words or concepts. Past attempts to encode syntactic structure have treated part-of-speech information as another word-like feature, but have been shown to be less effective than non-structural approaches. We propose a new representation scheme using Holographic Reduced Representations (HRRs) as a technique to encode both semantic and syntactic structure. This method improves on previous attempts in the literature by encoding the structure across all features of the document vector while preserving text semantics. Our method does not increase the dimensionality of the document vectors, allowing for efficient computation and storage. We present classification results of our HRR text representations versus Bag-of-Concepts representations and show that our method of including structure improves text classification results.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "16954 Integrating structure and meaning: A new method for encoding structure for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-41849124522&partnerID=40&md5=e5018ff63a05abb371425b5387b4a230"
    },
    {
      "abstract": "Background: Efficient features play an important role in automated text classification, which definitely facilitates the access of large-scale data. In the bioscience field, biological structures and terminologies are described by a large number of features",
      "title": "16955 Exploiting and integrating rich features for biological literature classification",
      "url": ""
    },
    {
      "abstract": "Emergency department free-text chief complaints (CCs) are a major data source for syndromic surveillance. CCs need to be classified into syndromic categories for subsequent automatic analysis. However, the lack of a standard vocabulary and high-quality encodings of CCs hinder effective classification. This paper presents a new ontology-enhanced automatic CC classification approach. Exploiting semantic relations in a medical ontology, this approach is motivated to address the CC vocabulary variation problem in general and to meet the specific need for a classification approach capable of handling multiple sets of syndromic categories. We report an experimental study comparing our approach with two popular CC classification methods using a real-world dataset. This study indicates that our ontology-enhanced approach performs significantly better than the benchmark methods in terms of sensitivity, F measure, and F2 measure.  2007 Elsevier Inc. All rights reserved.",
      "title": "16957 Ontology-enhanced automatic chief complaint classification for syndromic surveillance",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-40049088436&partnerID=40&md5=324c51dfd01601ebf3d52d9aea569bf0"
    },
    {
      "abstract": "The goal of online event analysis is to detect events and track their associated documents in real time from a continuous stream of documents generated by multiple information sources. Unlike traditional text categorization methods, event analysis approaches consider the temporal relations among documents. However, such methods suffer from the threshold-dependency problem, so they only perform well for a narrow range of thresholds. In addition, If the contents of a document stream change, the optimal threshold (that is, the threshold that yields the best performance) often changes as well. In this paper, we propose a threshold-resilient online algorithm, called the Incremental Probabilistic Latent Semantic Indexing (IPLSI) algorithm, which alleviates the threshold-dependency problem and simultaneously maintains the continuity of the latent semantics to better capture the story line development of events. The IPLSI algorithm is theoretically sound and empirically efficient and effective for event analysis. The results of the performance evaluation performed on the Topic Detection and Tracking (TDT)-4 corpus show that the algorithm reduces the cost of event analysis by as much as 15 percent ~20 percent and increases the acceptable threshold range by 200 percent to 300 percent over the baseline.  2008 IEEE.",
      "title": "16960 Using incremental PLSI for threshold-resilient online event analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38749103565&partnerID=40&md5=af56c1fad73eb42acd8e34310bf67812"
    },
    {
      "abstract": "New techniques are discussed for enhancing the classification precision of deep Web databases, which include utilizing the content texts of the HTML pages containing the database entry forms as the context and a unification processing for the database attribute labels. An algorithm to find out the content texts in HTML pages is developed based on multiple statistic characteristics of the text blocks in HTML pages. The unification processing for database attributes is to let the attribute labels that are closed semantically be replaced with delegates. The domain and language knowledge found in learning samples is represented in hierarchical fuzzy sets and an algorithm for the unification processing is proposed based on the presentation. Based on the pre-computing a k-NN (k nearest neighbors) algorithm is given for deep Web database classification, where the semantic distance between two databases is calculated based on both the distance between the content texts of the HTML pages and the distance between database forms embedded in the pages. Various classification experiments are carried out to compare the classification results done by the algorithm with pre-computing and the one without the pre-computing in terms of classification precision, recall and F1 values.",
      "title": "16969 Classification of deep Web databases based on the context of Web pages",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-40949113215&partnerID=40&md5=61cc19bc6a7beb0e3bf1abda5839abd1"
    },
    {
      "abstract": "Background: Information extraction (IE) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge, particularly in areas where important factual information is published in a diverse literature. Here we report on the design, implementation and several evaluations of OpenDMAP, an ontology-driven, integrated concept analysis system. It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources, integrating diverse text processing applications, and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering. Results: OpenDMAP information extraction systems were produced for extracting protein transport assertions (transport), protein-protein interaction assertions (interaction) and assertions that a gene is expressed in a cell type (expression). Evaluations were performed on each system, resulting in F-scores ranging from .26 - .72 (precision .39 - .85, recall .16 - .85). Additionally, each of these systems was run over all abstracts in MEDLINE, producing a total of 72,460 transport instances, 265,795 interaction instances and 176,153 expression instances. Conclusion: OpenDMAP advances the performance standards for extracting protein-protein interaction predications from the full texts of biomedical research articles. Furthermore, this level of performance appears to generalize to other information extraction tasks, including extracting information about predicates of more than two arguments. The output of the information extraction system is always constructed from elements of an ontology, ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality. The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction. The open source OpenDMAP code library is freely available at http://bionlp.sourceforge.net/.  2008 Hunter et al",
      "title": "16971 OpenDMAP: An open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression",
      "url": ""
    },
    {
      "abstract": "Clustering text data streams is an important issue in data mining community and has a number of applications such as news group filtering, text crawling, document organization and topic detection and tracing etc. However, most methods are similarity-based approaches and only use the TF*IDF scheme to represent the semantics of text data and often lead to poor clustering quality. Recently, researchers argue that semantic smoothing model is more efficient than the existing TF*IDF scheme for improving text clustering quality. However, the existing semantic smoothing model is not suitable for dynamic text data context. In this paper, we extend the semantic smoothing model into text data streams context firstly. Based on the extended model, we then present two online clustering algorithms OCTS and OCTSM for the clustering of massive text data streams. In both algorithms, we also present a new cluster statistics structure named cluster profile which can capture the semantics of text data streams dynamically and at the same time speed up the clustering process. Some efficient implementations for our algorithms are also given. Finally, we present a series of experimental results illustrating the effectiveness of our technique.  2008 Science Press, Beijing, China and Springer Science + Business Media, LLC, USA.",
      "title": "16975 Clustering text data streams",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38849191210&partnerID=40&md5=8325254c789c9c125545a3baf2e172f1"
    },
    {
      "abstract": "Polarity mining provides an in-depth analysis of semantic orientations of text information. Motivated by its success in the area of topic mining, we propose an ontology-supported polarity mining (OSPM) approach. The approach aims to enhance polarity mining with ontology by providing detailed topic-specific information. OSPM was evaluated in the movie review domain using both supervised and unsupervised techniques. Results revealed that OSPM outperformed the baseline method without ontology support. The findings of this study not only advance the state of polarity mining research but also shed light on future research directions.",
      "title": "16976 Ontology-supported polarity mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38349100109&partnerID=40&md5=220dc688b4fa739c160c53fb6bdf51b1"
    },
    {
      "abstract": "In the novel conceptional self-organizing map model (ConSOM) proposed for text clustering in this paper, neurons and documents can be represented by two vectors: one in extended concept space, and the other in traditional feature space, and weight modification of neuron vector is guided by combination of similarities in both traditional and extended spaces. Experimental results show that by utilizing concept relevance knowledge effectively, ConSOM performs better than traditional SOM plus VSM mode in text clustering due to its semantic sensitivity.  2007 Elsevier B.V. All rights reserved.",
      "title": "16977 ConSOM: A conceptional self-organizing map model for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38649125634&partnerID=40&md5=9f2df162ab7826ca29a7887fa4e4e220"
    },
    {
      "abstract": "The paper reports on recent experiments in cross-lingual document processing (with a case study for Bulgarian-English-Romanian language pairs) and brings evidence on the benefits of using linguistic ontologies for achieving, with a high level of accuracy, difficult tasks in NLP such as word alignment, word sense disambiguation, document classification, cross-language information retrieval, etc. We provide brief descriptions of the parallel corpus we used, the multilingual lexical ontology which supports our research, the word alignment and word sense disambiguation systems we developed and a preliminary report on an ongoing development of a system for cross-lingual text-classification which takes advantage of these multilingual technologies. Unlike the keyword-based methods in document processing, the concept-based methods are supposed to better exploit the semantic information contained in a particular document and thus to provide more accurate results.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16981 Ontology-supported text classification based on cross-lingual word sense disambiguation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37249064719&partnerID=40&md5=af281e49c79cc46205084d67696b312f"
    },
    {
      "abstract": "Many researchers have used text classification method in solving the ontology mapping problem. Their mapping results heavily depend on the availability of quality exemplars used as training data. However, manual preparation of exemplars is costly. In this work, we propose to automatically extract text from web pages returned by a search engine. Search queries are formed according to the semantic information given in the ontology. We have implemented a prototype system that automates the entire process (from search query formation to conditional probability calculation) and conducted a series of experiments. We assessed the effectiveness of our approach by comparing the obtained conditional probabilities with human expectations. Our main contribution is that we explored the possibilities of utilizing web information for text classification based ontology mapping and made several valuable discoveries on its usefulness for future research.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16982 Learning the semantic meaning of a concept from the web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37249034289&partnerID=40&md5=156da650aa5a9d18e54d8ed07edda619"
    },
    {
      "abstract": "Text information processing depends critically on the proper representation of documents. Traditional models, like the vector space model, have significant limitations because they do not consider semantic relations amongst terms. In this paper we analyze a document representation using the association graph scheme and present a new approach called Global Association Distance Model (GADM). At the end, we compare GADM using K-NN classifier with the classical vector space model and the association graph model.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16985 Document representation using global association distance model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37149009640&partnerID=40&md5=0ca269b809caf5cfc6f705301cadfd55"
    },
    {
      "abstract": "Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant (most frequent) sense of a word when contextual clues are not strong enough. The topic domain of a document has a strong influence on the sense distribution of words. Unfortunately, it is not feasible to produce large manually sense-annotated corpora for every domain of interest. Previous experiments have shown that unsupervised estimation of the predominant sense of certain words using corpora whose domain has been determined by hand outperforms estimates based on domain-independent text for a subset of words and even outperforms the estimates based on counting occurrences in an annotated corpus. In this paper we address the question of whether we can automatically produce domain-specific corpora which could be used to acquire predominant senses appropriate for specific domains. We collect the corpora by automatically classifying documents from a very large corpus of newswire text. Using these corpora we estimate the predominant sense of words for each domain. We first compare with the results presented in [1]. Encouraged by the results we start exploring using text categorization for WSD by evaluating on a standard data set (documents from the SENSEVAL-2 and 3 English all-word tasks). We show that for these documents and using domain-specific predominant senses, we are able to improve on the results that we obtained with predominant senses estimated using general, non domain-specific text. We also show that the confidence of the text classifier is a good indication whether it is worth-while using the domain-specific predominant sense or not.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16987 Text categorization for improved priors of word meaning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-37149010506&partnerID=40&md5=bcfba0be8ce12ed0e3fcb4c50d48fb55"
    },
    {
      "abstract": "We propose the use of clustering to support the automatic definition of semantic hyperlinks. We present a linking service that relates documents pertaining to a specific cluster created by a clustering process. The results of preliminary experiments are positive and illustrate our contribution in terms of creating hyperlinks considering the homogeneous contents represented by clusters. Copyright 2007 ACM.",
      "title": "16988 Clustering as an approach to support the automatic definition of semantic hyperlinks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-36949013363&partnerID=40&md5=74ca92264fff42b1eab85723139b93a7"
    },
    {
      "abstract": "Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes moreto the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture these mantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation, and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-based model on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developed concept-based model is used to enhance the quality of the text categorization.  2007 ACM.",
      "title": "16989 A concept-based model for enhancing text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-36849063169&partnerID=40&md5=b54bf43c2ae7b7cb7e72546cd62af2af"
    },
    {
      "abstract": "This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl and CLUVI. The training data was annotated with contextual features based on two stateof- The-art classification tag sets.  2007 Association for Computational Linguistics.",
      "title": "16993 Experiments with an annotation scheme for a knowledge-rich noun phrase interpretation system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880321346&partnerID=40&md5=e7dad4834b264806a3426e3668e5f8e4"
    },
    {
      "abstract": "Typically, personalized information recommendation services automatically infer the user profile, a structured model of the user interests, from documents that were already deemed relevant by the user. We present an approach based on Word Sense Disambiguation (WSD) for the extraction of user profiles from documents. This approach relies on a knowledge-based WSD algorithm, called JIGSAW, for the semantic indexing of documents: JIGSAW exploits the WordNet lexical database to select, among all the possible meanings (senses) of a polysemous word, the correct one. Semantically indexed documents are used to train a Naive Bayes learner that infers semantic, sense-based user profiles as binary text classifiers (user-likes and user-dislikes). Two empirical evaluations are described in the paper. In the first experimental session, JIGSAW has been evaluated according to the parameters of the SENSEVAL-3 initiative, that provides a forum where the WSD systems are assessed against disambiguated datasets. The goal of the second empirical evaluation has been to measure the accuracy of the user profiles in selecting relevant documents to be recommended. Performance of classical keyword-based profiles has been compared to that of sense-based profiles in the task of recommending scientific papers. The results show that sense-based profiles outperform keyword-based ones.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16994 Discovering user profiles from semantically indexed scientific papers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38149011525&partnerID=40&md5=238583507fa6517fe50cd59df1b2c64a"
    },
    {
      "abstract": "Single Text Combinatorial Semantic Field (STCSF) is proposed to analyze semantic characteristics of a single text. By the acquisition of term combinatorial relations, especially non-neighbouring ones, and coefficients of term combinatorial relations in a single text, STCSF can be constructed. By visualization of STCSF, STCSF has the capabilities of intuitionistic semantic representation, composition, and decomposition. Thus, we can analyze semantic characteristics of a single text more effectively. STCSF has no complicated computation, so STCSF is apt to a single text analysis in Knowledge Grid.  2007 IEEE.",
      "title": "16995 Single Text Combinatorial Semantic Field (STCSF): Construction and visualization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50149101107&partnerID=40&md5=28faf850c5700227f3f6efe6eddbb1e9"
    },
    {
      "abstract": "The XML-based markup language TimeML encodes temporal and event-time information for use in automatic text processing. The TimeML annotation of a text contains information about the temporal intervals that are mentioned in the text as well as the relationship of these temporal intervals to the times and events mentioned in the text. We provide here a formal denotational semantics for TimeML, addressing problems of operator scope that arise in the context of a flat representation language and providing a sketch of an intensional extension to the main extensional semantics.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "16996 Towards a denotational semantics for TimeML",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38149033766&partnerID=40&md5=9ec25466d23ddb8126b36c7820c0a149"
    },
    {
      "abstract": "Huge biomedical literatures result in many new challenges on text classification, its efficiency and sparseness of data attract many researchers. Recent success of language modeling in information retrieval have let us consider again about multinomial Naive Bayes for text classification. In this paper, we propose a semantic smoothing method for Naive Bayes model, biomedical documents were indexed by the concept of UMLS, and at the same time concept pairs which are context-sensitive were extracted as topic signature, the translation between concept pair and concept is attained using EM algorithm. Then classification model is estimated by a mixture model combined with this semantic smoothing method. Ontology-based document representation can deal with synonym and reduce the concept vector. The semantic smoothing method can partly solve the sparseness of data. Our method is evaluated on OHSUMED and genomic track collection, and proper results were attained. We found this semantic smoothing method can attain better results than other simple smoothing method, also this method is significant because of its simpleness, comprehensibility.  2007 IEEE.",
      "title": "16999 Semantic smoothing the multinomial Naive Bayes for biomedical literature classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-46749115541&partnerID=40&md5=cedebf61c76b3589700d573c09897fa9"
    },
    {
      "abstract": "The amount of ontologies and meta data available on the Web is constantly growing. The successful application of machine learning techniques for learning of ontologies from textual data, i.e. mining for the Semantic Web, contributes to this trend. However, no principal approaches exist so far for mining from the Semantic Web. We investigate how machine learning algorithms can be made amenable for directly taking advantage of the rich knowledge expressed in ontologies and associated instance data. Kernel methods have been successfully employed in various learning tasks and provide a clean framework for interfacing between non-vectorial data and machine learning algorithms. In this spirit, we express the problem of mining instances in ontologies as the problem of defining valid corresponding kernels. We present a principled framework for designing such kernels by means of decomposing the kernel computation into specialized kernels for selected characteristics of an ontology which can be flexibly assembled and tuned. Initial experiments on real world Semantic Web data enjoy promising results and show the usefulness of our approach.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "17001 Kernel methods for mining instance data in ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949085013&partnerID=40&md5=ea8f4a9f6f42c9004b515d53cf317432"
    },
    {
      "abstract": "Keywords are viewed as the words that represent the topic and the content of the whole text. Keyword extraction is an important technology in many areas of document processing, such as text clustering, text summarization, and text retrieval. This paper provides a keyword extraction algorithm based on WordNet and PageRank. Firstly, a text is represented as a rough undirected weighted semantic graph with WordNet, which defines synsets as vertices and relations of vertices as edges, and assigns the weight of edges with the relatedness of connected synsets. Then we apply UW-PageRank in the rough graph to do word sense disambiguation, prune the graph, and finally apply UW-PageRank again on the pruned graph to extract keywords. The experimental results show our algorithm is practical and effective.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17002 Keyword extraction based on pagerank",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049161888&partnerID=40&md5=16b038622128bffdc645656ac9433403"
    },
    {
      "abstract": "Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed by using a set of manually classified documents, i.e. a training collection. Term-based representation of documents has found widespread use in TC. However, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage. In this paper we design, implement, and evaluate a new text classification technique. Our main idea consists in finding a series of projections of the training data by using a new, modified LSI algorithm, projecting all training instances to the low-dimensional subspace found in the previous step, and finally inducing a binary search on the projected low-dimensional data. Our conclusion is that, with all its simplicity and efficiency, our approach is comparable to SVM accuracy on classification.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17005 Text categorization in non-linear semantic space",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049169809&partnerID=40&md5=d0dadc1c940789947bb9093f15104e13"
    },
    {
      "abstract": "Extraction of concepts and identification of their semantic classes are useful in applications such as automatic instantiation of ontologies and construction of information extraction systems. Even though various techniques exist for the extraction of domain specific concepts from unstructured texts, very little concentration is in the semantic class labeling for concepts. In this paper we propose the Semantic Class Labeling (SCL) problem and differentiate it from the Named Entity Classification (NEC) problem. We also present a Naive Bayes solution to SCL. Experiments suggest that Naive Bayes learning method with specified features achieves high classification accuracy. Empirical and statistical evaluation on the significance of attributes for SCL is also presented.  2007 IEEE.",
      "title": "17006 Learning for semantic classification of conceptual terms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-46749110072&partnerID=40&md5=38a65c27391a327fbcb2950f1fba0223"
    },
    {
      "abstract": "In recent years, document sentiment analysis has attracted a great deal of research interest. One important aspect of this filed is the subjectivity analysis. This problem is different from traditional text categorization on that more linguistic or semantic information are required for better estimating the subjectivity of a document. Therefore, in this paper, focuses are on how to extract useful and meaningful language features and how to combine all of these language features efficiently. Under the well-known ngram language model framework, we investigated a series of language-grams having different n-order and various distances to find the most important ones. In addition, we have also tried several weighting methods to make features more meaningful. Based on various kinds of language features, we adopted a tailored Maximum Entropy modeling method to construct our subjectivity classifier. Detailed experiments given in this paper show that the well extracted language features are suit for the document subjectivity analysis task.  2007 IEEE.",
      "title": "17013 Language feature mining for document subjectivity analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-42149155870&partnerID=40&md5=61b12dfeec0329f93da82573f0891321"
    },
    {
      "abstract": "In this paper we explore an unsupervised approach to classify video content by analyzing the corresponding subtitles. The proposed method is based on the WordNet lexical database and the WordNet domains and applies natural language processing techniques on video subtitles. The method is divided into several steps. The first step includes subtitle text preprocessing. During the next steps, a keyword extraction method and a word sense disambiguation technique are applied. Subsequently, the WordNet domains that correspond to the correct word senses are identified. The final step assigns category labels to the video content based on the extracted domains. Experimental results with documentary videos show that the proposed method is quite effective in discovering the correct category for each video.",
      "title": "17015 Semantic video classification based on subtitles and domain terminologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884620659&partnerID=40&md5=1ac4cd8f14c5a7378110904f27f29dd4"
    },
    {
      "abstract": "There are increasingly visible demands for structured/unstructured information integration and advanced analytics. However, conventional database technology has not been able to present a robust and practical implementation of a truly integrated architecture for such purposes. After working on several industrial applications (in particular, in the healthcare and life sciences area), we have identified fundamental issues and technical approaches to tackle the issues. In this paper, we propose data representations and algebraic operations for integrating semantic information (e.g., ontologies) into OLAP systems, which allow us to analyze a huge set of textual documents with their underlying semantic information. The performance of the prototype implementation has been evaluated using real world datasets, and the high scalability and flexibility of our approach have been confirmed with respect to the computation time. Copyright 2007 ACM.",
      "title": "17017 A method for online analytical processing of text data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63449116142&partnerID=40&md5=05366ede8720d59319e20d213a20ba25"
    },
    {
      "abstract": "This paper introduces a framework to add a semantic web layer to legacy organizational information, and describes its application to the use case provided by the Italian National Research Council (CNR) intraweb. Building on a traditional web-based view of information from different legacy databases, we have performed a semantic porting of data into a knowledge base, dependent on an OWL domain ontology. We have enriched the knowledge base by means of text mining techniques, in order to discover on-topic relations. Several reasoning techniques have been applied, in order to infer relevant implicit relationships. Finally, the ontology and the knowledge base have been deployed on a semantic wiki by means of the WikiFactory tool, which allows users to browse the ontology and the knowledge base, to introduce new relations, to revise wrong assertions in a collaborative way, and to perform semantic queries. In our experiments, we have been able to easily implement several functionalities, such as expert finding, by simply formulating ad-hoc queries from either an ontology editor or the semantic wiki interface. The result is an intelligent and collaborative front end, which allow users to add information, fill gaps, or revise existing information on a semantic basis, while keeping the knowledge base automatically updated.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "17019 A collaborative Semantic Web layer to enhance legacy systems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949108541&partnerID=40&md5=6f4775585ec9b20c896dd75a09cfcb8d"
    },
    {
      "abstract": "The sentence similarity computation plays an important role in each field of Chinese Information Processing. A new approach to compute the Chinese question semantic similarity is presented, which is divided into two steps: the first step is to extract the Question Semantic Representation from the question, and the second step is to compute the question semantic similarity based on the Question Semantic Representation. This paper uses the method of Question Semantic Model Matching to extract the Question Semantic Representation from the question. The experimental results show that the proposed algorithm worked more reasonable in the real calculation. In addition, our approach is of great value for many potential applications in the future.  2007 IEEE.",
      "title": "17021 A new approach to compute the semantic similarity of Chinese question sentence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38048999069&partnerID=40&md5=dbff32ed29d52fd7065c5c155d78f3dc"
    },
    {
      "abstract": "This paper constructed a latent semantic text model using genetic algorithm (GA) for web clustering. The main difficulty in the application of GA for text clustering is thousands or even tens of thousands of dimensions in the feature space. Latent semantic indexing (LSI) is a successful technology which attempts to explore the latent semantics structure in textual data, and furthermore, it reduces this large space to smaller one and provides a robust space for clustering. GA belongs to search techniques that efficiently evolve the optimal solution for the problem. Evolved in the reduced latent semantic indexing model, GA can improve clustering accuracy and speed which is typically suitable for real time clustering. We used SSTRESS criteria to analyze the dissimilarity between original term-by-document corpus matrix and the approximate decomposition matrix with different ranks corresponding to the performance of our algorithm evolved in the reduced space. The superiority of GA applied in LSI model over K-means and conventional GA in the vector space model (VSM) is demonstrated by providing good Reuter text clustering results.  2007 IEEE.",
      "title": "17022 Analysis of web clustering based on genetic algorithm with latent semantic indexing technology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50149099664&partnerID=40&md5=f92b978715ad74594d3884a97dc0f7ca"
    },
    {
      "abstract": "Text mining concerns the discovery of knowledge from unstructured textual data. One important task is the discovery of rules that relate specific words and phrases. Textual entries in many database fields exhibit minor variations that may prevent mining algorithms from discovering important patterns. Variations can arise from typographical errors, misspellings, abbreviations, as well as other sources like ambiguity. Ambiguity may be due to the derivation feature, which is very common in the Arabic language. This paper introduces a new system developed to discover soft-matching association rules using a similarity measurements based on the derivation feature of the Arabic language. In addition, it presents the features of using Frequent Closed Itemsets (FCI) concept in mining the association rules rather than Frequent Itemsets (FI).",
      "title": "17025 Mining Arabic text using soft-matching association rules",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50249125366&partnerID=40&md5=f462f08f449e4decc264ccec2e31688d"
    },
    {
      "abstract": "This paper describes a new technique for obtaining measures of semantic relatedness. Like other recent approaches, it uses Wikipedia to provide a vast amount of structured world knowledge about the terms of interest. Our system, the Wikipedia Link Vector Model or WLVM, is unique in that it does so using only the hyperlink structure of Wikipedia rather than its full textual content. To evaluate the algorithm we use a large, widely used test set of manually defined measures of semantic relatedness as our bench-mark. This allows direct comparison of our system with other similar techniques.",
      "title": "17028 Computing semantic relatedness using Wikipedia Link structure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880084734&partnerID=40&md5=599c48a4b81b48585c558375de7e45c5"
    },
    {
      "abstract": "Two approaches to text classification based on summarization technique are proposed: in the first approach, the heuristic rules of auto-summarization are used to select and weight features for every category, and texts are classified by these features",
      "title": "17030 Chinese text classification based on summarization technique",
      "url": ""
    },
    {
      "abstract": "Semantic orientation (SO) for texts is often determined on the basis of the positive or negative polarity, or sentiment, found in the text. Polarity is typically extracted using the positive and negative words in the text, with a particular focus on adjectives, since they convey a high degree of opinion. Not all adjectives are created equal, however. Adjectives found in certain parts of the text, and adjectives that refer to particular aspects of what is being evaluated have more significance for the overall sentiment of the text. To capitalize upon this, we weigh adjectives according to their relevance and create three measures of SO: a baseline SO using all adjectives (no restriction)",
      "title": "17032 Not all words are created equal: Extracting semantic orientation as a function of adjective relevance",
      "url": "Conference Paper"
    },
    {
      "abstract": "The discovery of semantically associated groups of terms is important for many applications of text understanding, including document vectorization for text mining, semi-automated ontology extension from documents and ontology engineering with help of domain-specific texts. In [3], we have proposed a method for the discovery of such terms and shown that its performance is superior to other methods for the same task. However, we have observed that (a) the approach is sensitive to the term clustering method and (b) the performance improves with the size of the resultslist, thus incurring higher human overhead in the postprocessing phase. In this study, we address these issues by proposing the delivery of a hierarchically organized output, computed with Bisecting K-Means. We compared the results of the new algorithm with those delivered by the original method, which used K-Means using two ontologies as gold standards.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17033 Learning of semantic sibling group hierarchies - K-Means vs. Bi-secting-K-Means",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049033907&partnerID=40&md5=f275ef8c3b548952cec006c3a6b91a7f"
    },
    {
      "abstract": "This paper presents a robust classification of dialog acts from text utterances. Two different types, namely, bag-of-words and syntactic relationship among words, were used to extract the discourse level features from the transcript of utterances. Subsequently a number of feature mining methods have been used to identify the most relevant features and their roles in classifying dialog acts. The selected features are used to learn the underlying models of dialog acts using a number of existing machine learning algorithms from the WEKA toolbox. Empirical analyses using the HCRC Map Task Corpus dialog data was conducted to evaluate the performance of the proposed approach.  2007 IEEE.",
      "title": "17040 Robust classification of dialog acts from the transcription of utterances",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-47749151875&partnerID=40&md5=949eb3904631a8ace8b024c87f1997e2"
    },
    {
      "abstract": "How well the privacy policy follows a regulation is one of the current concerns of the user. Such a task can be accomplished by directly querying the policy statement with the regulation text. Automation of the process requires an expressive meaning-based framework for Natural Language Processing (NLP). This paper discusses the Ontological Semantics approach to the issue of verifying compliance and illustrates the potential of utilizing the framework in the domain of Privacy management for NLP-related tasks. As an example a section from BCBS and corresponding HIPAA regulations are used.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17042 Reconciling privacy policies and regulations: Ontological semantics perspective",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38149134412&partnerID=40&md5=7a068173f85dbfd4a39f910fe139496a"
    },
    {
      "abstract": "Issues of synonymy and strong relational semantic information increase the feature dimension of text vector, which embarrasses the efficiency and precision of text classification. In order to decrease the feature dimension of text vector, a method of Text Feature Extraction Based on Hybrid Parallel Genetic Clustering Algorithm was proposed in this paper. Firstly, K-means algorithm is used to perform thick-granularity clustering for feature words",
      "title": "17043 Research on text feature extraction based on hybrid parallel genetic algorithm",
      "url": "Conference Paper"
    },
    {
      "abstract": "It is generally thought that semantic and grammatical information was very significant to better understanding and processing of text. But in simple text categorization task, absence of this information does not always lead to the degradation of classifier performance. In this paper, we discuss the application of the character-level statistical method in text categorization, which extract character-level frequent pattern rather than consider the semantic and grammatical information. Compared with traditional n-gram model, the presented method is easy and convenient. Then by casting character-level statistical method in Bayesian theory framework, the proposed method was applied to spam detection. At last, we discuss the multiclass problem in short message categorization based on combination strategies. Effectiveness of the models and feasibility of the present method are verified.  2006 IEEE.",
      "title": "17049 Application of the character-level statistical method in text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38949140899&partnerID=40&md5=db85dd067abbe7b930dd0c66fedcb396"
    },
    {
      "abstract": "Extracting semantic relations is of great importance for the creation of the Semantic Web content. It is of great benefit to semi-automatically extract relations from the free text of Wikipedia using the structured content readily available in it. Pattern matching methods that employ information redundancy cannot work well since there is not much redundancy information in Wikipedia, compared to the Web. Multi-class classification methods are not reasonable since no classification of relation types is available in Wikipedia. In this paper, we propose PORE (Positive-Only Relation Extraction), for relation extraction from Wikipedia text. The core algorithm B-POL extends a state-of-the-art positive-only learning algorithm using bootstrapping, strong negative identification, and transductive inference to work with fewer positive training examples. We conducted experiments on several relations with different amount of training data. The experimental results show that B-POL can work effectively given only a small amount of positive training examples and it significantly outperforms the original positive learning approaches and a multi-class SVM. Furthermore, although PORE is applied in the context of Wikipedia, the core algorithm B-POL is a general approach for Ontology Population and can be adapted to other domains.  2008 Springer-Verlag Berlin Heidelberg.",
      "title": "17050 PORE: Positive-only relation extraction from wikipedia text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949095566&partnerID=40&md5=165f8758ae4f1fd711f7345b017ca1c9"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) has been shown to be effective in recovering from synonymy and polysemy in text retrieval applications. However, since LSI ignores class labels of training documents, LSI generated representations are not as effective in classification tasks. To address this limitation, a process called sprinkling is presented. Sprinkling is a simple extension of LSI based on augmenting the set of features using additional terms that encode class knowledge. However, a limitation of sprinkling is that it treats all classes (and classifiers) in the same way. To overcome this, we propose a more principled extension called Adaptive Sprinkling (AS). AS leverages confusion matrices to emphasise the differences between those classes which are hard to separate. The method is tested on diverse classification tasks, including those where classes share ordinal or hierarchical relationships. These experiments reveal that AS can significantly enhance the performance of instance-based techniques (kNN) to make them competitive with the state-of-the-art SVM classifier. The revised representations generated by AS also have a favourable impact on SVM performance.",
      "title": "17053 Supervised latent semantic indexing using adaptive sprinkling",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049012100&partnerID=40&md5=38557155fa83d70a418ba320186a5845"
    },
    {
      "abstract": "Probabilistic latent semantic indexing (PLSI) represents documents of a collection as mixture proportions of latent topics, which are learned from the collection by an expectation maximization (EM) algorithm. New documents or queries need to be folded into the latent topic space by a simplified version of the EM-algorithm. During PLSI-Folding-in of a new document, the topic mixtures of the known documents are ignored. This may lead to a suboptimal model of the extended collection. Our new approach incorporates the topic mixtures of the known documents in a Bayesian way during foldingin. That knowledge is modeled as prior distribution over the topic simplex using a kernel density estimate of Dirichlet kernels. We demonstrate the advantages of the new Bayesian folding-in using real text data.  2007 IEEE.",
      "title": "17060 Bayesian folding-in with Dirichlet kernels for PLSI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49849098677&partnerID=40&md5=50943d46528c26fb581f1229b07c0e07"
    },
    {
      "abstract": "The Web contains massive amount of documents to the point where it has become impossible to classify them manually. This projects goal is to find a new method for clustering documents that is as close to humans classification as possible and at the same time to reduce the size of the documents. This project uses a combination of Latent Semantic Indexing (LSI) with Singular Value Decomposition (SVD) calculation and Support Vector Machine (SVM) classification. Using SVD, data is decomposed and truncated to reduce the data size. The reduced data will be clustered into different categories. Using SVM, clustered data from SVD calculation is used for training to allow new data to be classified based on SVMs prediction. The projects result show that the method of combining SVD and SVM is able to reduce data size and classifies documents reasonably compared to humans classification.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17062 Clustering high dimensional data using SVM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38049049739&partnerID=40&md5=2281e4da55eff1f612558cf424fff90c"
    },
    {
      "abstract": "Latent semantic analysis (LSA) is an algorithm applied to approximate the meaning of texts, thereby exposing semantic structure to computation. LSA combines the classical vector-space model - well known in computational linguistics - with a singular value decomposition (SVD), a two-mode factor analysis. Thus, bag-of-words representations of texts can be mapped into a modified vector space that is assumed to reflect semantic structure. In this contribution the authors describe the lsa package for the statistical language and environment R and illustrate its proper use through examples from the areas of automated essay scoring and knowledge representation.",
      "title": "17066 Investigating unstructured texts with latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84879598704&partnerID=40&md5=e29134fc12e9ba9a6f7316798193393b"
    },
    {
      "abstract": "The current research on association rule based text classification neglected several key problems. First, weights of elements in profile vectors may have much impact on generating classification rules. Second, traditional association rule lacks semantics. Increasing semantic of association rule may help to improve the classification accuracy. Focusing on the above problems, we propose a new classification approach. This approach include: (1) Mining frequent item-sets on item-weighted transactions",
      "title": "17069 A novel text classification approach based on enhanced association rule",
      "url": ""
    },
    {
      "abstract": "In this paper we propose a document representation model based on latent semantic analysis (LSA) for text clustering. Most classic clustering systems represent document with a set of indices, which have been known as vector space model (VSM). In such a model, documents are encoded as vectors in N-dimensional space, where N is the number of unique terms. However, this method causes that the scalability will be poor and the cost of computational time will be high. Latent semantic analysis is a promising approach which attempts to construct a latent semantic structure in textual data and finds relevant documents such that they may not even share any common words, moreover, it reduces the large term-by-document matrix to a smaller one and provides a robust space for clustering. Two clustering algorithms, K-means and genetic algorithm (GA), are constructed in LSA space to demonstrate the effectiveness and validity of our text representation model. We use SSTRESS criteria to analyze the dissimilarity between the original corpus matrix and the approximate objective matrix with different ranks corresponding to the performance of the two clustering algorithms. The superiority of GA and K-means applied in LSA model over conventional GA and K-means in VSM is demonstrated by providing good text clustering results.  2007 IEEE.",
      "title": "17071 A novel document clustering model based on latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-50149104151&partnerID=40&md5=859a9dc1a6f5f1960e85b4d6b7993e7a"
    },
    {
      "abstract": "In this paper, we analyze some clustering algorithms that have been widely employed in the past to support the comprehension of web applications. To this end, we have defined an approach to identify static pages that are duplicated or cloned at the content level. This approach is based on a process that first computes the dissimilarity between web pages using Latent Semantic Indexing, a well known information retrieval technique, and then groups similar pages using clustering algorithms. We consider five instances of this process, each based on three variants of the agglomerative hierarchical clustering algorithm, a divisive clustering algorithm, k-means partitional clustering algorithm, and a widely employed partitional competitive clustering algorithm, namely Winner Takes All. In order to assess the proposed approach, we have used the static pages of three web applications and one static web site.  2007 IEEE.",
      "title": "17073 Clustering algorithms and latent semantic indexing to identify similar pages in web applications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56749106123&partnerID=40&md5=34806494949a0f6caa2b2c031af9885d"
    },
    {
      "abstract": "In this study, we design and prototype an automatic quiz generation system (auto-quiz for short) for a given English text to test learner comprehension of text content and English skills. The auto-quiz process parses an English text into a semantic network representation and enhances the semantic network iteratively with intrinsic knowledge, such as English grammar and writing styles, and extrinsic knowledge, such as the word relationship in WordNet and statistics from corpus or search engines. Then, the quiz generation process generates quiz from the text based on learner comprehension skills and according to the learner learning status and needs, such as English proficiency and frequent errors.  2007 IEEE.",
      "title": "17078 An automatic quiz generation system for English text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-47649125877&partnerID=40&md5=685aff6fa8e89057e38f3b5335c42367"
    },
    {
      "abstract": "Natural Language Processing has emerged as an active field of research in the machine learning community. Several methods based on statistical information have been proposed. However, with the linguistic complexity of the texts, semantic-based approaches have been investigated. In this paper, we propose a Semantic Kernel for semi-structured biomedical documents. The semantic meanings of words are extracted using the UMLS framework. The kernel, with a SVM classifier, has been applied to a text categorization task on a medical corpus of free text documents. The results have shown that the Semantic Kernel outperforms the Linear Kernel and the Naive Bayes classifier. Moreover, this kernel was ranked in the top ten of the best algorithms among 44 classification methods at the 2007 CMC Medical NLP International Challenge.  2007 IEEE.",
      "title": "17081 A Semantic Kernel for semi-structured documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49749152749&partnerID=40&md5=1f06fe41b36cbc53fdd769d72bc71ae7"
    },
    {
      "abstract": "Inspired by how search behavior works in human society, we propose CTO, a self-organized semantic overlay based on concept tree for P2P IR infrastructure, which is efficient for full text search in pure P2P environment without any central control or powerful peer as hub node. Especially, CTO performs very well on searching the unpopular resources shared by a few peers. In our experiment, while searching for the scarce documents shared by the peers, CTO achieves about 80% recall rate when the search covers less than 5% peers in the overlay. The search latency of CTO is also very low, which is controlled in the range about 5-12 hops. Copyright 2007 ACM.",
      "title": "17084 CTO: Concept tree based semantic overlay for pure peer-to-peer information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63449108197&partnerID=40&md5=5eca61a4330e68fd2e995eaa604218cd"
    },
    {
      "abstract": "Although using ontologies to assist information retrieval and text document processing has recently attracted more and more attention, existing ontologybased approaches have not shown advantages over the traditional keywords-based Latent Semantic Indexing (LSI) method. This paper proposes an algorithm to extract a concept forest (CF) from a document with the assistance of a natural language ontology, the WordNet lexical database. Using concept forests to represent the semantics of text documents, the semantic similarities of these documents are then measured as the commonalities of their concept forests. Performance studies of text document clustering based on different document similarity measurement methods show that the CF-based similarity measurement is an effective alternative to the existing keywords-based methods. In particular, this CF-based approach has obvious advantages over the existing keywords-based methods, including LSI, in processing short text documents or in P2P or live news environments where it is impractical to collect the entire document corpus for analysis.  2007 IEEE.",
      "title": "17086 Concept forest: A new ontology-assisted text document similarity measurement method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-48349122320&partnerID=40&md5=25a8c52282ee976eb564d97324ef0235"
    },
    {
      "abstract": "Several Text Categorization applications require a representation beyond the standard bag-of-words paradigm. Kernel-based learning has approached this problem by (i) considering information about syntactic structure or by (ii) incorporating knowledge about the semantic similarity of term features. We propose a generalized framework consisting of a family of kernels that jointly incorporate syntactic and semantic similarity and demonstrate the power of this approach in a series of experiments. Copyright 2007 ACM.",
      "title": "17088 Structure and semantics for expressive text kernels",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63449104310&partnerID=40&md5=d9d7a92d5e3d5061eadf4e0d30963447"
    },
    {
      "abstract": "Increasingly large text datasets and the high dimensionality associated with natural language create a great challenge in text mining. In this research, a systematic study is conducted, in which three different document representation methods for text are used, together with three Dimension Reduction Techniques (DRT), in the context of the text clustering pmblem. Several standard benchmark datasets are used. The three Document representation methods considered are based on the vector space model, and they include word, multi-word term, and character N-gram representations. The dimension reduction methods are independent component analysis (ICA), latent semantic indexing (LSI), and a feature selection technique based on Document Frequency (DF). Results are compared in terms of clustering performance, using the k-means clustering algorithm. Experiments show that ICA and LSI are clearly better than DF on all datasets. For word and N-gram representation, ICA generally gives better results compared with LSI. Experiments also show that the word representation gives better clustering results compared to term and N-gram representation. Finally, for the N-gram representation, it is demonstrated that a profile length (before dimensionality reduction) of 2000 is sufficient to capture the information and, in most cases, a 4-gram representation gives better performance than 3-gram representation.  2007 IEEE.",
      "title": "17093 Document representation and dimension reduction for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-48349100287&partnerID=40&md5=dd1c57fe509aff7004eaa04bb76f37e9"
    },
    {
      "abstract": "We consider the problem of document indexing and representation. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing (LSI) which is optimal in the sense of global Euclidean structure, LPI is optimal in the sense of local manifold structure. However, LPI is not efficient in time and memory which makes it difficult to be applied to very large data set. Specifically, the computation of LPI involves eigen-decompositions of two dense matrices which is expensive. In this paper, we propose a new algorithm called Regularized Locality Preserving Indexing (RLPI). Benefit from recent progresses on spectral graph analysis, we cast the original LPI algorithm into a regression framework which enable us to avoid eigen-decomposition of dense matrices. Also, with the regression based framework, different kinds of regularizers can be naturally incorporated into our algorithm which makes it more flexible. Extensive experimental results show that RLPI obtains similar or better results comparing to LPI and it is significantly faster, which makes it an efficient and effective data preprocessing method for large scale text clustering, classification and retrieval. Copyright 2007 ACM.",
      "title": "17094 Regularized locality preserving indexing via spectral regression",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-63449101804&partnerID=40&md5=25df860b1e62a351eec3174ffc419971"
    },
    {
      "abstract": "Background: The construction of literature-based networks of gene-gene interactions is one of the most important applications of text mining in bioinformatics. Extracting potential gene relationships from the biomedical literature may be helpful in building biological hypotheses that can be explored further experimentally. Recently, latent semantic indexing based on the singular value decomposition (LSI/SVD) has been applied to gene retrieval. However, the determination of the number of factors k used in the reduced rank matrix is still an open problem. Results: In this paper, we introduce a way to incorporate a priori knowledge of gene relationships into LSI/SVD to determine the number of factors. We also explore the utility of the non-negative matrix factorization (NMF) to extract unrecognized gene relationships from the biomedical literature by taking advantage of known gene relationships. A gene retrieval method based on NMF (GR/NMF) showed comparable performance with LSI/SVD. Conclusion: Using known gene relationships of a given gene, we can determine the number of factors used in the reduced rank matrix and retrieve unrecognized genes related with the given gene by LSI/SVD or GR/ NMF.  2007 Kim et al",
      "title": "17097 Extracting unrecognized gene relationships from the biomedical literature via matrix factorizations",
      "url": ""
    },
    {
      "abstract": "Background: A huge amount of biomedical textual information has been produced and collected in MEDLINE for decades. In order to easily utilize biomedical information in the free text, document clustering and text summarization together are used as a solution for text information overload problem. In this paper, we introduce a coherent graph-based semantic clustering and summarization approach for biomedical literature. Results: Our extensive experimental results show the approach shows 45% cluster quality improvement and 72% clustering reliability improvement, in terms of misclassification index, over Bisecting K-means as a leading document clustering approach. In addition, our approach provides concise but rich text summary in key concepts and sentences. Conclusion: Our coherent biomedical literature clustering and summarization approach that takes advantage of ontology-enriched graphical representations significantly improves the quality of document clusters and understandability of documents through summaries.  2007 Yoo et al",
      "title": "17098 A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method",
      "url": "17199 e 17209 foram considerados duplicados deste que Ã© mais novo."
    },
    {
      "abstract": "Aviation safety reports, such as those of the Aviation Safety Action Program (ASAP) and the Aviation Safety Reporting System (ASRS), provide a valuable record of safety incidents for industry analysts. Unfortunately, the sheer quantity of safety reports often makes it difficult to identify recurring safety issues and their causes. In response, NASA has explored the use of text classification techniques to automatically identify safety issues from the text of safety reports. One common approach for text classification is to use a statistical model of word occurrences in each report (often referred to as a bag of words), and use these models to train a classifier. We performed an experiment to evaluate whether simple semantic approaches could improve classification results with a support vector machine (SVM) classifier by factoring semantic relationships of words into the statistical model. Counter to our intuition, most of these semantic enhancements did not improve classification results. One method of combining meaningful words did show an improvement over the standard approach, however, and is presented along with the results of our study.",
      "title": "17100 Wordplay: An examination of semantic approaches to classify safety reports",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35648982939&partnerID=40&md5=0e3dc0ed3f01d379e51496d499b1dc82"
    },
    {
      "abstract": "Keywords can be considered as condensed versions of documents and short forms of their summaries. In this paper, the problem of automatic extraction of keywords from documents is treated as a supervised learning task. A lexical chain holds a set of semantically related words of a text and it can be said that a lexical chain represents the semantic content of a portion of the text. Although lexical chains have been extensively used in text summarization, their usage for keyword extraction problem has not been fully investigated. In this paper, a keyword extraction technique that uses lexical chains is described, and encouraging results are obtained.  2007 Elsevier Ltd. All rights reserved.",
      "title": "17101 Using lexical chains for keyword extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547930508&partnerID=40&md5=10f0b619c2748484e7114ec319854eef"
    },
    {
      "abstract": "This paper presents a model that incorporates contemporary theories of tense and aspect and develops a new framework for extracting temporal relations between two sentence-internal events, given their tense, aspect, and a temporal connecting word relating the two events. A linguistic constraint on event combination has been implemented to detect incorrect parser analyses and potentially apply syntactic reanalysis or semantic reinterpretation - in preparation for subsequent processing for multi-document summarization. An important contribution of this work is the extension of two different existing theoretical frameworks - Hornsteins 1990 theory of tense analysis and Allens 1984 theory on event ordering - and the combination of both into a unified system for representing and constraining combinations of different event types (points, closed intervals, and open-ended intervals). We show that our theoretical results have been verified in a large-scale corpus analysis. The framework is designed to inform a temporally motivated sentence-ordering module in an implemented multi-document summarization system.  2007 Elsevier Ltd. All rights reserved.",
      "title": "17102 Exploiting aspectual features and connecting words for summarization-inspired temporal-relation extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547931461&partnerID=40&md5=b882143906695288db1798d84644384c"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) is a favorite feature extraction method used in text classification. Since when important global features for all the classes can be determined by LSI, important local features for small classes may be ignored, this leads to poor performance on these small classes. To solve this problem, a novel method based on Partial Least Square (PLS) analysis is proposed by integrating class information into the latent classification structure. Important features are extracted according to both their descriptive power of document contents as in LSI, and their capacity of discriminating classes. The extracted features are applied to several classification algorithms: SVM, kNN, C4.5 and SMO. Experiments on Reuters prove that the features extracted by our method outperform those extracted by LSI in all the cases. In particular, the gain obtained by our method is the most apparent on small classes. Copyright 2007 ACM.",
      "title": "17105 Text classification based on partial least square analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35248885559&partnerID=40&md5=3804cafef5caea68c1732349f671dc83"
    },
    {
      "abstract": "Email has become one of the fastest and most economical forms of communication. However, the increase of email users has resulted in the dramatic increase of spam emails during the past few years. As spammers always try to find a way to evade existing filters, new filters need to be developed to catch spam. Ontologies allow for machine-understandable semantics of data. It is important to share information with each other for more effective spam filtering. Thus, it is necessary to build ontology and a framework for efficient email filtering. Using ontology that is specially designed to filter spam, bunch of unsolicited bulk email could be filtered out on the system. Similar to other filters, the ontology evolves with the user requests. Hence the ontology would be customized for the user. This paper proposes to find an efficient spam email filtering method using adaptive ontology.  2007 ACADEMY PUBLISHER.",
      "title": "17109 Spam email classification using an adaptive ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-72949106170&partnerID=40&md5=a8aed2f3e64ad6481814925f1aea720f"
    },
    {
      "abstract": "Extracting sentiment from text is a hard semantic problem. We develop a methodology for extracting small investor sentiment from stock message boards. The algorithm comprises different classifier algorithms coupled together by a voting scheme. Accuracy levels are similar to widely used Bayes classifiers, but false positives are lower and sentiment accuracy higher. Time series and cross-sectional aggregation of message information improves the quality of the resultant sentiment index, particularly in the presence of slang and ambiguity. Empirical applications evidence a relationship with stock values-tech-sector postings are related to stock index levels, and to volumes and volatility. The algorithms may be used to assess the impact on investor opinion of management announcements, press releases, third-party news, and regulatory changes.  2007 INFORMS.",
      "title": "17111 Yahoo! for amazon: Sentiment extraction from small talk on the Web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-38549141929&partnerID=40&md5=9f34a441c8fadb0379b8816f1c150ca1"
    },
    {
      "abstract": "Biological named entity recognition is a critical task for automatically mining knowledge from biological literature. In this paper, this task is cast as a sequential labeling problem and Conditional Random Fields model is introduced to solve it. Under the framework of Conditional Random Fields model, rich features including literal, context and semantics are involved. Among these features, shallow syntactic features are first introduced, which effectively improve the models performance. Experiments show that our method can achieve an F-measure of 71.2% in an open evaluation data, which is better than most of state-of-the-art systems.  2007 Elsevier Ltd. All rights reserved.",
      "title": "17113 Rich features based Conditional Random Fields for biological named entities recognition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34447291383&partnerID=40&md5=36d07ce4596e1c6366dc5e36d2286095"
    },
    {
      "abstract": "This work describes the construction of a conceptual structure, as a result of the domain analysis of the current research on Benign Prostatic Hyperplasia (BPH), through different techniques based on the citations and semantic analysis. A sample which covers all published articles about the disease in mainstream journals was chosen. A total of 1,968 articles from 2000 through 2004 were retrieved from Science Citation Index. Author Co-citation Analysis (ACA) techniques and semantic analysis were used and the results were represented by different informatic programs. A macrostructure of the domain of the current research on BPH was identified through bibliometric techniques. The text mining techniques allowed validation of the identified macrostructure and the obtainment of the most frequent words in the text. The semantic analysis of the most cited reviews on BPH during the studied period allowed the definition of the categories to be used in the structure. Finally, a conceptual structure to be used as controlled (structured) language in the information retrieval, inside of a specialized information system on the approach of the disease is shown.",
      "title": "17114 Domain analysis for the construction of a conceptual structure: A case study",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35349006328&partnerID=40&md5=7201fd541aee2bf405cde394bbaf3ebc"
    },
    {
      "abstract": "Software maintainers routinely have to deal with a multitude of artifacts, like source code or documents, which often end up disconnected, due to their different representations and the size and complexity of legacy systems. One of the main challenges in software maintenance is to establish and maintain the semantic connections among all the different artifacts. In this paper, we show how Semantic Web technologies can deliver a unified representation to explore, query and reason about a multitude of software artifacts. A novel feature is the automatic integration of two important types of software maintenance artifacts, source code and documents, by populating their corresponding sub-ontologies through code analysis and text mining. We demonstrate how the resulting Software Semantic Web can support typical maintenance tasks through ontology queries and Description Logic reasoning, such as security analysis, architectural evolution, and traceability recovery between code and documents.  Springer-Verlag Berlin Heidelberg 2007.",
      "title": "17116 Empowering software maintainers with semantic web technologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34548061634&partnerID=40&md5=3a5fbf23daea47025aa21d47c8735115"
    },
    {
      "abstract": "This paper provides the results of experiments on the detection of arguments in texts among which are legal texts. The detection is seen as a classification problem. A classifier is trained on a set of annotated arguments. Different feature sets are evaluated involving lexical, syntactic, semantic and discourse properties of the texts. The experiments are a first step in the context of automatically classifying arguments in legal texts according to their rhetorical type and their visualization for convenient access and search. Copyright 2007 ACM.",
      "title": "17119 Automatic detection of arguments in legal texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34548009846&partnerID=40&md5=6071b0aada55234ea49270a907efabeb"
    },
    {
      "abstract": "An ontology is a powerful way of representing knowledge for multiple purposes. There are several ontology languages for describing concepts, properties, objects, and relationships. However, ontologies in information systems are not primarily written for human reading and communication among humans. For many business, government, and scientific purposes, written documents are the primary description and communication media for human knowledge communication. Unfortunately, there is a significant gap between knowledge expressed as textual documents and knowledge represented as ontologies. Semantic documents aim at combining documents and ontologies, and allowing users to access the knowledge in multiple ways. By adding annotations to electronic-document formats and including ontologies in electronic documents, it is possible to reconcile documents and ontologies, and to provide new services, such as ontology-based searches of large document databases. To accomplish this goal, semantic documents require tools that support both complex ontologies and advanced document formats. The Protege ontology editor, together with a custom-tailored documentation-handling extension, enables developers to create semantic documents by linking preexisting documents to ontologies.  2007 Elsevier Ltd. All rights reserved.",
      "title": "17123 The semantic-document approach to combining documents and ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34249018240&partnerID=40&md5=87171015d30ce77502bfe5a2833e234e"
    },
    {
      "abstract": "Psychiatric consultation record retrieval attempts to help people to efficiently and effectively locate the consultation records relevant to their depressive problems. Consultation records can also make people aware that they are not alone, because many individuals have suffered from the same or similar problems. Additionally, people can understand how to alleviate their depressive symptoms according to recommendations from health professionals. To achieve this goal, this paper proposes the use of a scenario-based representation, i.e., a symptom-based structural representation, to capture the depressive symptoms and their semantic relations, such as cause-effect and temporal relations, for understanding users queries clearly. The symptoms and relations are identified from semantic mining and analysis of consultation records. The multilevel mixture model is adopted to estimate the relevance of queries and consultation records based on the structural information. Experimental results show that the proposed approach achieves higher precision than does a term-based flat representation. An experiment is also conducted to examine the effect of error propagation resulting from incorrect identification of symptoms and relations. Experimental results demonstrate that combining different approaches can improve the retrieval robustness.  2007 IEEE.",
      "title": "17124 Psychiatric consultation record retrieval using scenario-based representation and multilevel mixture model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547101977&partnerID=40&md5=f4241d737d25c272bcd8182367abe15e"
    },
    {
      "abstract": "This paper proposes a system for automatically categorizing terms or lexical entities into a predefined set of semantic domains. We present an approach that exploits the knowledge available in the Web to create a model of each term or entity (Entity Context Lexicons - ECLs). Each profile is simply a list of terms (similar to the Bag-Of-Words representation in text categorization) and it is composed primarily by the words often appearing in the same contexts of the entity. These profiles model the contexts in which the entity usually appears and they can be subsequently processed by an automatic classifier. Moreover, we propose and validate a profile-based categorization model developed for this particular task which uses the ECLs of the training entities to build a profile for each class (Class-Context lexicon - CCL). Finally, we propose a technique for dealing with multi-label classification based on a decision module that exploits a neural network. We show the effectiveness of the proposed approach on a term categorization task using a standard benchmark composed of a set of domain-specific lexicons (WordNetDomains).  2006 IEEE.",
      "title": "17126 Semantic labeling of data by using the Web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34250730716&partnerID=40&md5=891acefcba0845bc31fd0156b467f3eb"
    },
    {
      "abstract": "With the rapid development of the World Wide Web, people benefit more and more from the sharing of information. However, Web pages with obscene, harmful, or illegal content can be easily accessed. It is important to recognize such unsuitable, offensive, or pornographic Web pages. In this paper, a novel framework for recognizing pornographic Web pages is described. A C4.5 decision tree is used to divide Web pages, according to content representations, into continuous text pages, discrete text pages, and image pages. These three categories of Web pages are handled, respectively, by a continuous text classifier, a discrete text classifier, and an algorithm that fuses the results from the image classifier and the discrete text classifier. In the continuous text classifier, statistical and semantic features are used to recognize pornographic texts. In the discrete text classifier, the naive Bayes rule is used to calculate the probability that a discrete text is pornographic. In the image classifier, the objects contour-based features are extracted to recognize pornographic images. In the text and image fusion algorithm, the Bayes theory is used to combine the recognition results from images and texts. Experimental results demonstrate that the continuous text classifier outperforms the traditional keyword-statistics-based classifier, the contour-based image classifier outperforms the traditional skin-region-based image classifier, the results obtained by our fusion algorithm outperform those by either of the individual classifiers, and our framework can be adapted to different categories of Web pages.  2007 IEEE.",
      "title": "17128 Recognition of pornographic Web pages by classifying texts and images",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34247608298&partnerID=40&md5=f7777c6956019002568987d72b751d1b"
    },
    {
      "abstract": "An Informed recommender has been developed that uses prioritized consumer product reviews to make recommendations and maps each piece of review comment using text mining techniques. The recommender uses ontology that provides a controlled vocabulary and relationships among words to describe the consumers skill level and experience with the product in the review comment in the system. The recommender process involves the useful information to be useful for the recommendation process. The recommender system makes a recommendation based on the ontology data, and therefore, recommendation quality depends on accurately mapping the proper knowledge from the semantic features in the review comments. When a user requests an evaluation of a particular product based on certain features, the overall feature quality is calculated from reviews containing the valuation of this feature.",
      "title": "17133 Informed recommender: Basing recommendations on consumer product reviews",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34250007634&partnerID=40&md5=20c7ed9b4211b3662cc3b37375ca043e"
    },
    {
      "abstract": "Most text analysis and retrieval work to date has focused on the topic of a text",
      "title": "17137 Stylistic text classification using functional lexical features",
      "url": ""
    },
    {
      "abstract": "This article introduces a named entity matching model that makes use of both semantic and phonetic evidence. The matching of semantic and phonetic information is captured by a unified framework via a bipartite graph model. By considering various technical challenges of the problem, including order insensitivity and partial matching, this approach is less rigid than existing approaches and highly robust. One major component is a phonetic matching model which exploits similarity at the phoneme level. Two learning algorithms for learning the similarity information of basic phonemic matching units based on training examples are investigated. By applying the proposed named entity matching model, a mining system is developed for discovering new named entity translations from daily Web news. The system is able to discover new name translations that cannot be found in the existing bilingual dictionary.  2007 ACM.",
      "title": "17141 Named entity translation matching and learning: With application for mining unseen translations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33847662524&partnerID=40&md5=47faa9381dd10a3c6d39e2026e702240"
    },
    {
      "abstract": "The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text. Motivated by the problem of building a noun phrase-level semantic parser and adapting it to various applications, such as machine translation and multilingual question answering, in this paper we present a domain-independent model for noun phrase semantic interpretation. We investigate the problem based on cross-linguistic evidence from a set of four Romance languages: Spanish, Italian, French, and Romanian. The focus on Romance languages is well motivated. It is generally the case that English noun phrases translate into constructions of the form N P N in Romance languages where, as we will show, the P (preposition) varies in ways that correlate with the semantics. Thus, based on a set of 22 semantic interpretation categories (such as PART-WHOLE, AGENT, POSSESSION) we present empirical observations regarding the distribution of these semantic categories in a cross-lingual corpus and their mapping to various syntactic constructions in English and Romance. Furthermore, given a training set of English noun phrases along with their translations in the four Romance languages, our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation. Experimental results are compared against a state-of-the-art model reported in the literature. Copyright 2006 ACM.",
      "title": "17146 Out-of-context noun phrase semantic interpretation with cross-linguistic evidence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547641218&partnerID=40&md5=2f8340cf5e9b15c2430c63152a564b11"
    },
    {
      "abstract": "Document clustering is a widely researched area of information retrieval. As there are many possibilities for noise filtering and other performance improving tricks, this paper focuses on the comparison of some techniques in latent semantic indexing, which aims to track seman-tically related terms to decrease feature space dimensionality. 5 tile methods (term filtering, frequency quantizing, principal component analysis (PCA), term clustering and document clustering of course) are used to build various configurations. These are then compared based on maximal achieved F-measure and time consumption to find the best composition.",
      "title": "17147 Composition of methods in document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883754350&partnerID=40&md5=9e49c84ce248d3970733e9229bd24344"
    },
    {
      "abstract": "In this paper, we study the problem of content-based social network discovery among people who frequently appear in world news. Google news is used as the source of data. We describe a probabilistic framework for associating people with groups. A low-dimensional topic-based representation is first obtained for news stories via probabilistic latent semantic analysis (PLSA). This is followed by construction of semantic groups by clustering such representations. Unlike many existing social network analysis approaches, which discover groups based only on binary relations (e.g. co-occurrence of people in a news article), our model clusters people using their topic distribution, which introduces contextual information in the group formation process (e.g. some people belong to several groups depending on the specific subject). The model has been used to study evolution of people with respect to topics over time. We also illustrate the advantages of our approach over a simple co-occurrence-based social network extraction method. Copyright 2006 ACM.",
      "title": "17151 Discovering groups of people in Google news",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547424377&partnerID=40&md5=9ec9fd0444d39a0633066d1bbdbeee16"
    },
    {
      "abstract": "This paper discusses how knowledge technologies can be utilized in creating help desk services on the semantic web. To ease the content indexers work, we propose semi-automatic semantic annotation of natural language text for annotating question-answer (QA) pairs, and case-based reasoning techniques for finding similar questions. To provide answers matching with the indexers and end-users information needs, methods for combining case-based reasoning with semantic search and browsing are proposed. We integrate different data sources by using large ontologies of upper common concepts, places, and agents. Techniques to utilize these sources in authoring answers are suggested. A prototype implementation of a real life ontology-based help desk application is presented as a proof of concept. This system is based on the data set of over 20,000 QA pairs and the operational principles of an existing national library help desk service in Finland.",
      "title": "17152 A semi-automatic semantic annotation and authoring tool for a library help desk service",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84884629882&partnerID=40&md5=d552bec76ea01be7c9ed84644d7972d7"
    },
    {
      "abstract": "We explore scalability issues of the text classification problem where using (multi)labeled training documents we try to build classifiers that assign documents into classes permitting classification in multiple classes. A new class of classification problems, called scalable is introduced that models many problems from the area of Web mining. The property of scalability is defined as the ability of a classifier to adjust classification results on a per-user basis. Furthermore, we investigate on different ways to interpret personalization of classification results by analyzing well known text datasets and exploring existent classifiers. We present solutions for the scalable classification problem based on standard classification techniques and present an algorithm that relies on the semantic analysis using document decomposition into its sentences. Experimental results concerning the scalability property and the performance of these algorithms are provided using the 20newsgroup dataset and a dataset consisting of web news.  2010.",
      "title": "17153 Scalability of text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77954099752&partnerID=40&md5=f3f7c445cdd7a36857990d92fd2d0557"
    },
    {
      "abstract": "The usage of Wikis for the purpose of knowledge management within a business company is only of value if the stored information can be found easily. The fundamental characteristic of a Wiki, its easy and informal usage, results in large amounts of steadily changing, unstructured documents. The widely used full-text search often provides search results of insufficient accuracy. In this paper, we will present an approach likely to improve search quality, through the use of Semantic Web, Text Mining, and Case Based Reasoning (CBR) technologies. Search results are more precise and complete because, in contrast to full-text search, the proposed knowledge-based search operates on the semantic layer.",
      "title": "17156 Knowledge search within a company-WIKI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84874013760&partnerID=40&md5=594dd7c6ef1c9ba52b4021410ef4e7f5"
    },
    {
      "abstract": "VSM is a mature model of text representation for categorization. Words are commonly used as dimensions of feature space of VSM, but words only provide little semantic information. Sentence category theory is an important component of HNC theory and can provide abundant information about meaning, structure and style of a sentence. We use sentence categories as dimensions of feature space, reduce the dimensionality by dividing mixed sentence categories and reform the weights by tfc-weighting algorithm. By simple vector distance calculation, we can get the parameters of the classifier and execute the categorization. The average precision and recall of our classifier are acceptable and can be improved by other HNC techniques.",
      "title": "17157 A text classifier based on sentence category VSM",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863684270&partnerID=40&md5=dc822c429ef67fa3bf3a46ea5b2e5e79"
    },
    {
      "abstract": "Scientific texts domain keyword is one of the basic elements of the text high-level semantics acquisition, domain ontology building and the knowledge representation in semantic grid, knowledge grid and escience environment. It is also the indispensable foundation and prerequisite work of Web scientific texts automatic classification, clustering and personalized services. TFIDF based TDDF formula is proposed to extract scientific texts domain keyword. The experiments proved that TDDF formula extracting texts domain keyword is superior to the classic TFIDF formula does. Above discussions and achievements can provide certain support not only for the establishment of semantic grid, knowledge grid and escience environment, but also for the Web knowledge acquisition, representation and text information retrieval and so on.  2006 IEEE.",
      "title": "17161 Experiments study for scientific texts domain keyword acquisition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84867174077&partnerID=40&md5=6c9ba7d4c35fc3c7a36ab546eebc3e15"
    },
    {
      "abstract": "The amount of electronically stored textual information is continuously increasing both on the internet and in company assets, and there are no good solutions to easily locate the most needed information. Because search engines do not take into account the meaning of the word and its context, in the end the user has to select the right information from the unstructured result set. If the text is annotated and linked to the ontology of the annotation, then the user can directly navigate along the links of the semantic annotation to the desired information. In this paper we present a software framework to semi automatically generate a semantic representation of the knowledge of the Networkshop conference series and display on a web portal the generated ontology together with the references to the occurrences of the instances in the source text. The framework presented in this paper makes advances in the following fields: we do not assume that the source text has uniform and formally defined structure, we address English and Hungarian text as well, we incorporate machine learning techniques in the process, and provide a flexible content management system for the presentation of the generated Topic Map on a web based portal.",
      "title": "17166 Framework for semi automatically generating topic maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883699088&partnerID=40&md5=23c1d7e2e65a3aed0262120e7cb206d8"
    },
    {
      "abstract": "Traditionally text mining has had a strong link with information retrieval and classification and has largely aimed to classify documents according to embedded knowledge. Association rule mining and sequence mining, on the other hand, have had a different goal",
      "title": "17167 A multi-level framework for the analysis of sequential data",
      "url": ""
    },
    {
      "abstract": "Many executives wish that they had a more systematic means of using technology to help them identify external events that represent evidence of potential threats or opportunities. The Business Event Advisor is a prototype corporate radar kit that addresses this need. It can be used to create customized solutions to monitor the external business environment in which a company operates. It exploits Internet-based information sources to help decision-makers systematically detect and interpret simple patterns of external events relevant to their business concerns. To accomplish this, it integrates text mining components and a simple inference mechanism around a semantic model of a companys business environment. Applications built with the system can produce structured descriptions of events that may be relevant to a given company, and can then aggregate those events around its model of the companys business environment and suggest what impact(s) these events might have on that company. We discuss our motivation for creating this prototype, the architecture of the system, the kinds of information it can provide, the kinds of models it requires, and possible directions for future research (including use of pattern matching to auto-generate elements of our systems models). Copyright  2006, American Association for Artificial Intelligence (www.aaai.org), All rights reserved.",
      "title": "17168 Using lightweight NLP and semantic modeling to realize the internets potential as a corporate radar",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33947280560&partnerID=40&md5=8e4a61eccfd21696282826bf9d827445"
    },
    {
      "abstract": "Nearest-neighbor collaborative filtering (CF) algorithms are gaining widespread acceptance in recommender systems and e-commerce applications. User ratings are not expected to be independent, as users follow trends of similar rating behavior. In terms of Text Mining, this is analogous to the formation of higher-level concepts from plain terms. In this paper, we propose a novel CF algorithm which uses Latent Semantic Indexing (LSI) to detect rating trends and performs recommendations according to them. We perform an extensive experimental evaluation, with two real data sets, and produce results that indicate its superiority over existing CF algorithms. Copyright  2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17171 Scalable collaborative filtering based on latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33846003166&partnerID=40&md5=2d9fe1846f653fbb6cc7b04e0a01a538"
    },
    {
      "abstract": "We show that generation of contextually appropriate syntactic variation can be improved using a model based on automatically extracted features. We adapt a model for predicting dative alternation from (Bresnan et al. 2005)",
      "title": "17172 Modeling the dative alternation with automatically extracted features",
      "url": ""
    },
    {
      "abstract": "This paper describes the motivations, approach, and architecture for using ontologies in knowledge extraction and in applications that assist situated agents in complex information integration tasks. Our approach applies ontologies along with semantic analysis methods to extract task relevant knowledge from distributed, unstructured text sources. This knowledge is then applied to assist in information integration, sharing, and situation-awareness applications. We have developed a software architecture that provides support for automated ontology extraction, ontology conflict analysis and mapping, and knowledge integration and sharing. We are currently designing and building multiple applications that validate the practical benefits of this research.",
      "title": "17173 Methods for ontology-driven integration",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84866073300&partnerID=40&md5=2ef847e4ca4f812a43b910cb6d7460cb"
    },
    {
      "abstract": "This publication shows how the gap between the HTML based internet and the RDF based vision of the semantic web might be bridged, by linking words in texts to concepts of ontologies. Most current search engines use indexes that are built at the syntactical level and return hits based on simple string comparisons. However, the indexes do not contain synonyms, cannot differentiate between homonyms (mouse as a pointing vs. mouse as an animal) and users receive different search results when they use different conjugation forms of the same word. In this publication, we present a system that uses ontologies and Natural Language Processing techniques to index texts, and thus supports word sense disambiguation and the retrieval of texts that contain equivalent words, by indexing them to concepts of ontologies. For this purpose, we developed fully automated methods for mapping equivalent concepts of imported RDF ontologies (for this prototype WordNet, SUMO and OpenCyc). These methods will thus allow the seamless integration of domain specific ontologies for concept based information retrieval in different domains. To demonstrate the practical workability of this approach, a set of web pages that contain synonyms and homonyms were indexed and can be queried via a search engine like query frontend. However, the ontology based indexing approach can also be used for other data mining applications such text clustering, relation mining and for searching free text fields in biological databases. The ontology alignment methods and some of the text mining principles described in this publication are now incorporated into the ONDEX system http://ondex.sourceforge.net/.  2006 Elsevier B.V. All rights reserved.",
      "title": "17174 Ontology based text indexing and querying for the semantic web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33751221631&partnerID=40&md5=6b9791db1a3e17a516a6f33582a77c41"
    },
    {
      "abstract": "We propose here to develop a methodology to extract semantic knowledge from plain written English documents and represent It using a formal mathematical expression, In order to facilitate Its use In practical applications. Our fundamental conjecture Is that most of the semantic Information of a sentence lies In the action described by that sentence. Consequently, we focus on extracting from the text the words whose relationships represent actions, and on modelling those relationships. We then demonstrate the practicality of our methodology by applying It to a domain classifier.  2006 IEEE.",
      "title": "17176 A methodology for extracting and representing actions in texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34250693515&partnerID=40&md5=84a05a48ef4a803e72c53df02ecd6296"
    },
    {
      "abstract": "We use a combination of proven methods from time series analysis and machine learning to explore the relationship between temporal and semantic similarity in web query logs",
      "title": "17178 Measuring the meaning in time series clustering of text search queries",
      "url": ""
    },
    {
      "abstract": "In this paper, a novel method is proposed to extract key sentences of a document as its summary by estimating the relevance of sentences through the use of fuzzy-rough sets. This method uses senses rather than raw words to lessen the problem that sentences of the same or similar semantic meaning but written in synonyms are treated differently. Also included is semantic clustering, used to avoid selecting redundant key sentences. A prototype of this automatic text summarization scheme is constructed and an intrinsic method with criteria widely used in information retrieval systems is employed for measuring the summary quality. The results of applying the prototype to datasets with manually-generated summaries are shown. 2006 IEEE.",
      "title": "17180 Fuzzy-rough set aided sentence extraction summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-45849086587&partnerID=40&md5=394024bc7960f735a3d45137a765fb4f"
    },
    {
      "abstract": "Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks. While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy. We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific. It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts.  2006 Association for Computational Linguistics.",
      "title": "17181 Automatic classification of verbs in biomedical texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-49949116900&partnerID=40&md5=d98c5c131d79cf1ac34cdb8aa046e69d"
    },
    {
      "abstract": "This paper describes one of the ways how to overcome some of the major limitations of current fulltext search engines. It deals with synonymy of the web search engine results by clustering them into relevant synonym category of given word. It employs WordNet lexical database and several linguistic approaches to classify results in search engine result page (SERP) in appropriate synonym category according to WordNet synsets. Some methods to refine the classification are proposed and some initial experiments and results are described and discussed.",
      "title": "17183 Using WordNet glosses to refine google queries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84868693995&partnerID=40&md5=99378e89746db0c87aade71ec0741e58"
    },
    {
      "abstract": "This paper presents a machine-learning approach for the recognition of textual entailment. For our approach we model lexical and semantic features. We study the effect of stacking and voting joint classifier combination techniques which boost the final performance of the system. In an exhaustive experimental evaluation, the performance of the developed approach is measured. The obtained results demonstrate that an ensemble of classifiers achieves higher accuracy than an individual classifier and comparable results to already existing textual entailment systems.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17185 An approach for textual entailment recognition based on stacking and voting",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845925585&partnerID=40&md5=d9ec00df3fcea206efd012f008b8f38a"
    },
    {
      "abstract": "We propose a method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions, attempting to answer the question: given a seemingly new topic, from where does this topic evolve? In particular, we seek to discover the pair-wise probabilistic dependency in topics of documents which associate social actors from a latent social network, where these documents are being shared. By viewing the evolution of topics as a Markov chain, we estimate a Markov transition matrix of topics by leveraging social interactions and topic semantics. Metastable states in a Markov chain are applied to the clustering of topics. Applied to the CiteSeer dataset, a collection of documents in academia, we show the trends of research topics, how research topics are related and which are stable. We also show how certain social actors, authors, impact these topics and propose new ways for evaluating author impact. Copyright 2006 ACM.",
      "title": "17198 Topic evolution and social interactions: How authors effect research",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547640842&partnerID=40&md5=06e10ff72376909bf7f6c0726b829417"
    },
    {
      "abstract": "This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.  2006 Association for Computational Linguistics.",
      "title": "17201 Learning to recognize features of valid textual entailments",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84858413285&partnerID=40&md5=8b576eecf4a0d93fd59af14f8fac4f7e"
    },
    {
      "abstract": "In this paper, we review two techniques for topic discovery in collections of text documents (Latent Semantic Indexing and K-Means clustering) and present how we integrated them into a system for semi-automatic topic ontology construction. The OntoGen system offers support to the user during the construction process by suggesting topics and analyzing them in real time. It suggests names for the topics in two alternative ways both based on extracting keywords from a set of documents inside the topic. The first set of descriptive keyword is extracted using document centroid vectors, while the second set of distinctive keyword is extracted from the SVM classification model dividing documents in the topic from the neighboring documents.  2006 Springer-Verlag.",
      "title": "17204 Semi-automatic construction of topic ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77049084790&partnerID=40&md5=c7fe958e132067a042c9e80c9892a76f"
    },
    {
      "abstract": "Knowledge representation is essential for semantics modeling and intelligent information processing. For decades researchers have proposed many knowledge representation techniques. However, it is a daunting problem how to capture deep semantic information effectively and support the construction of a large-scale knowledge base efficiently. This paper describes a new knowledge representation model, SenseNet, which provides semantic support for commonsense reasoning and natural language processing. SenseNet is formalized with a Hidden Markov Model. An inference algorithm is proposed to simulate human-like text analysis procedure. A new measurement, confidence, is introduced to facilitate the text analysis. We present a detailed case study of applying SenseNet to retrieving compensation information from company proxy filings.  2006 IEEE.",
      "title": "17205 SenseNet: A knowledge representation model for computational semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-78650041571&partnerID=40&md5=df751a6858e9a19a56f80d4df6be2662"
    },
    {
      "abstract": "Most of text mining techniques are based on word and/or phrase analysis of the text. The statistical analysis of a term (word or phrase) frequency captures the importance of the term within a document. However, to achieve a more accurate analysis, the underlying mining technique should indicate terms that capture the semantics of the text from which the importance of a term in a sentence and in the document can be derived. A new concept-based mining model that relies on the analysis of both the sentence and the document, rather than, the traditional analysis of the document dataset only is introduced. The proposed mining model consists of a concept-based analysis of terms and a concept-based similarity measure. The term which contributes to the sentence semantics is analyzed with respect to its importance at the sentence and document levels. The model can efficiently find significant matching terms, either words or phrases, of the documents according to the semantics of the text. The similarity between documents relies on a new concept-based similarity measure which is applied to the matching terms between documents. Experiments using the proposed concept-based term analysis and similarity measure in text clustering are conducted. Experimental results demonstrate that the newly developed concept-based mining model enhances the clustering quality of sets of documents substantially.  2006 IEEE.",
      "title": "17207 Enhancing text clustering using concept-based mining model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-36848999406&partnerID=40&md5=c845a6314e2a45baf05728d6bad5f443"
    },
    {
      "abstract": "This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric. Copyright  2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17212 Corpus-based and knowledge-based measures of text semantic similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750693384&partnerID=40&md5=12d3e4c54726c1e2bc6746049d4e9234"
    },
    {
      "abstract": "Text classification is a key technique for handling and organizing text data. The support vector machine(SVM) is shown to be better for the classification among well-known methods. In this paper, the grouping method of the similar words, is proposed for the classification of documents, which is applied to Reuters news and it is shown that the grouping of words has equivalent ability to the Latent Semantic Analysis(LSA) in the classification accuracy. Further, a new combining method is proposed for the classification, which consists of Grouping, LSA followed by the k-Nearest Neighbor classification (k-NN). The combining method proposed here, shows the higher accuracy in the classification than the conventional methods of the kNN, and the LSA followed by the kNN. Then, the combining method shows almost same accuracies as SVM.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17213 Text classification: Combining grouping, LSA and kNN vs support vector machine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750682274&partnerID=40&md5=93387197fc209351e9f6532f8b2000f6"
    },
    {
      "abstract": "In this paper we deal with the main aspect of the problem of extracting meaning from documents, namely, with the problem of text categorization, outlining a novel and systematic approach to its solution. We present a text categorization system for non-domain specific full-text documents. The main contribution of this paper lies on the feature extraction methodology which, first, involves word semantic categories and not raw words as other rival approaches. As a consequence of coping with the problem of dimensionality reduction, the proposed approach introduces a novel second and third order feature extraction approach for text categorization by considering word semantic categories cooccurrence analysis. The suggested methodology compares favorably to widely accepted, raw word frequency based techniques in a collection of documents concerning the Dewey Decimal Classification (DDC) system. In these comparisons different Multilayer Perceptrons (MLP) algorithms, the LVQ and the conventional k-NN technique are involved.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17214 An improved text categorization methodology based on second and third order probabilistic feature extraction and neural network classifiers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750719379&partnerID=40&md5=d3ae0b4e2c110ef98ff01f7ce41b4428"
    },
    {
      "abstract": "The feature selection is an important part in automatic text classification. In this paper, we use a Chinese semantic dictionary - Hownet to extract the concepts from the word as the feature set, because it can better reflect the meaning of the text. We construct a combined feature set that consists of both sememes and the Chinese words, propose a CHI-MCOR weighing method according to the weighing theories and classification precision. The effectiveness of the competitive network and the Radial Basis Function (RBF) network in text classification are examined. Experimental result shows that if the words are extracted properly, not only the feature dimension is smaller but also the classification precision is higher, the RBF network outperform competitive network for automatic text classification because of the application of supervised learning. Besides its much shorter training time than the BP networks, the RBF network makes precision and recall rates that are almost at the same level as the BP networks.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17215 A RBF network for Chinese text classification based on concept feature extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750687515&partnerID=40&md5=e649db02020cc3606a134373f679d0e1"
    },
    {
      "abstract": "It has long been a dream to build computer systems that learn automatically by reading text. This dream is generally considered infeasible, but some surprising developments in the US over the past three years have led to the funding of several short-term investigations into whether and how much the best current practices in Natural Language Processing and Knowledge Representation and Reasoning, when combined, actually enable this dream. This paper very briefly describes one of these efforts, the Learning by Reading project at ISI, which has converted a high school textbook of Chemistry into very shallow logical form and is investigating which semantic features can plausibly be added to support the kinds of inference required for answering standard high school text questions.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17218 Learning by reading: An experiment in text analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750229203&partnerID=40&md5=aea11f9ca39d66e13ca6f2aecc68e134"
    },
    {
      "abstract": "This paper studies the effect of semantic knowledge expansion applied to the Textual Entailment Recognition task. In comparison to the already existing approaches we introduce a new set of similarity measures that captures hidden semantic relations among different syntactic categories in a sentence. The focus of our study is also centred on the synonym, antonym and verb entailment expansion of the initially generated pairs of words. The main objective for the realized expansion concerns the finding, the affirmation and the enlargement of the knowledge information. In addition, we applied Latent Semantic Analysis and the cosine measure to tune and improve the obtained relations. We conducted an exhaustive experimental study to evaluate the impact of the proposed new similarity relations for Textual Entailment Recognition.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17219 The effect of semantic knowledge expansion to Textual Entailment Recognition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750241521&partnerID=40&md5=faec08b4967dd858065dd9954468948d"
    },
    {
      "abstract": "We propose to use the n-multigram model to help the automatic text classification task. This model could automatically discover the latent semantic sequences contained in the document set of each category. Based on the n-multigram model and the n-gram language model, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram model can achieve the similar classification performance compared with the one based on n-gram model. However, the model size of our algorithm is only 4.21% of the latter one. Another proposed algorithm based on the combination of nmultigram model and n-gram model improves the microFl and macro-F1 values by 3.5% and 4.5% respectively which support the validity of our approach.  2006 IEEE.",
      "title": "17225 Text classification improved through automatically extracted sequences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749632760&partnerID=40&md5=95719db80bb81075ebce0ad08508c4c1"
    },
    {
      "abstract": "This paper presents a question answering (QA) system called Tikka. Tikkas approach to QA is based on question classification, semantic annotation and answer extraction pattern matching. Tikkas performance is evaluated by conducting experiments in the following tasks: monolingual Finnish and French and bilingual Finnish-English QA. Tikka is the first system ever reported to perform monolingual textual QA in the Finnish language. This is also the task in which its performance is best: 23% of all questions are answered correctly. Tikkas performance in the monolingual French task is a little inferior to its performance in the monolingual Finnish task, and when compared to the other systems evaluated with the same data in the same task, its performance is near the average. In the bilingual Finnish-English task, Tikka was the only participating system, and - as is expected - its performance was inferior to those attained in the monolingual tasks.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17226 Question answering experiments for Finnish and French",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749612509&partnerID=40&md5=262d6be06edd37192669a77d96a43188"
    },
    {
      "abstract": "Contextual text mining is concerned with extracting topical themes from a text collection with context information (e.g., time and location) and comparing/analyzing the variations of themes over different contexts. Since the topics covered in a document are usually related to the context of the document, analyzing topical themes within context can potentially reveal many interesting theme patterns. In this paper, we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases. Specifically, we extend the probabilistic latent semantic analysis (PLSA) model by introducing context variables to model the context of a document. The proposed mixture model, called contextual probabilistic latent semantic analysis (CPLSA) model, can be applied to many interesting mining tasks, such as temporal text mining, spatiotemporal text mining, author-topic analysis, and cross-collection comparative analysis. Empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively. Copyright 2006 ACM.",
      "title": "17228 A mixture model for contextual text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749551461&partnerID=40&md5=7f9af113ee01320900ba74ea220660ac"
    },
    {
      "abstract": "Due to the complexity and flexibility of natural language, linguistic knowledge representation, automatic acquisition and its application research becomes difficult. In this paper, a combination of ontology with statistical method is presented for linguistic knowledge representation and acquisition from training data. In this study, linguistic knowledge representaiton is firstly defined using ontology theory, and then, linguistical knowledge is automatically acquired by statistical method. In document processing, the semantic evaluation value of the document can be get by linguistic knowledge. The experimention in Chinese information retrieval and text classification shows the proposed method improves the precision of nature language processing.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17231 Linguistic knowledge representation and automatic acquisition based on a combination of ontology with statistical method",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749385931&partnerID=40&md5=f7da6c9d8cfef5740fc8fd48df1f4104"
    },
    {
      "abstract": "Discovering links and relationships is one of the main challenges in biomedical research, as scientists are interested in uncovering entities that have similar functions, take part in the same processes, or are coregulated. This article discusses the extraction of such semantically related entities (represented by domain terms) from biomedical literature. The method combines various text-based aspects, such as lexical, syntactic, and contextual similarities between terms. Lexical similarities are based on the level of sharing of word constituents. Syntactic similarities rely on expressions (such as term enumerations and conjunctions) in which a sequence of terms appears as a single syntactic unit. Finally, contextual similarities are based on automatic discovery of relevant contexts shared among terms. The approach is evaluated using the Genia resources, and the results of experiments are presented. Lexical and syntactic links have shown high precision and low recall, while contextual similarities have resulted in significantly higher recall with moderate precision. By combining the three metrics, we achieved F measures of 68% for semanticalty related terms and 37% for highly related entities.  2006 ACM.",
      "title": "17234 Mining semantically related terms from biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33748415214&partnerID=40&md5=f0ffea69a73cb260783d43bf7e1d48d6"
    },
    {
      "abstract": "Ontologies are nowadays used for many applications requiring data, services and resources in general to be interoperable and machine understandable. Such applications are for example web service discovery and composition, information integration across databases, intelligent search, etc. The general idea is that data and services are semantically described with respect to ontologies, which are formal specifications of a domain of interest, and can thus be shared and reused in a way such that the shared meaning specified by the ontology remains formally the same across different parties and applications. As the cost of creating ontologies is relatively high, different proposals have emerged for learning ontologies from structured and unstructured resources. In this article we examine the maturity of techniques for ontology learning from textual resources, addressing the question whether the state-of-the-art is mature enough to produce ontologies on demand.",
      "title": "17237 Ontologies on demand? A description of the state-of-the-art, applications, challenges and trends for ontology learning from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749657990&partnerID=40&md5=fe010c0f87b54bbee78e19abef483634"
    },
    {
      "abstract": "To respond correctly to a free form factual question given a large collection of text data, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This work presents a machine learning approach to question classification. Guided by a layered semantic hierarchy of answer types, we develop a hierarchical classifier that classifies questions into fine-grained classes. This work also performs a systematic study of the use of semantic information sources in natural language classification tasks. It is shown that, in the context of question classification, augmenting the input of the classifier with appropriate semantic category information results in significant improvements to classification accuracy. We show accurate results on a large collection of free-form questions used in TREC 10 and 11.  2005 Cambridge University Press.",
      "title": "17243 Learning question classifiers: The role of semantic information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746393719&partnerID=40&md5=2985588a7f051a5828b0919cd36dc747"
    },
    {
      "abstract": "This article addresses the task of mining named relationships between concepts from biomedical literature for indexing purposes or for scientific discovery from medical literature. This research builds on previous work on concept mining from medical literature for indexing purposes and proposes to learn semantic relationships names between concepts learnt. Previous ConceptMiner system did learn pairs of concepts, expressing a relationship between two concepts, but did not learn relationships semantic names. Building on ConceptMiner, RelationshipMiner is interested in learning as well the relationships with their name identified from the Unified Medical Language System (UMLS) knowledge-base as a basis for creating higher-level knowledge structures, such as rules, cases, and models, in future work. Current system is focused on learning semantically typed relationships as predefined in the UMLS, for which a dictionary of synonyms and variations has been created. An evaluation is presented showing that actually this relationship mining task improves the concept mining task results by enabling a better screening of the relationships between concepts for relevant ones.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17245 Named relationship mining from medical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746395080&partnerID=40&md5=6aa19570f40d39ebcf47d01d3e71d826"
    },
    {
      "abstract": "We discuss an approach to the automatic expansion of domain-specific lexicons, that is, to the problem of extending, for each ci in a predefined set C = {c1 , . . . , cm} of semantic domains, an initial lexicon L0i into a larger lexicon L 1i. Our approach relies on term categorization, defined as the task of labeling previously unlabeled terms according to a predefined set of domains. We approach this as a supervised learning problem in which term classifiers are built using the initial lexicons as training data. Dually to classic text categorization tasks in which documents are represented as vectors in a space of terms, we represent terms as vectors in a space of documents. We present the results of a number of experiments in which we use a boosting-based learning device for training our term classifiers. We test the effectiveness of our method by using WordNetDomains, a wellknown large set of domain-specific lexicons, as a benchmark. Our experiments are performed using the documents in the Reuters Corpus Volume 1 as implicit representations for our terms.  2006 ACM.",
      "title": "17246 Automatic expansion of domain-specific lexicons by term categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746443840&partnerID=40&md5=5646566e024ac38582f1f1de094035a2"
    },
    {
      "abstract": "Automatic extraction of semantic relationships between entity instances in an ontology is useful for attaching richer semantic meta-data to documents. In this paper we propose an SVM based approach to hierarchical relation extraction, using features derived automatically from a number of GATE-based open-source language processing tools. In comparison to the previous works, we use several new features including part of speech tag, entity subtype, entity class, entity role, semantic representation of sentence and WordNet synonym set. The impact of the features on the performance is investigated, as is the impact of the relation classification hierarchy. The results show there is a trade-off among these factors for relation extraction and the features containing more information such as semantic ones can improve the performance of the ontological relation extraction task.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17248 Automatic extraction of hierarchical relations from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746050681&partnerID=40&md5=59e990ff9dfe6e93263f00ebdf79984e"
    },
    {
      "abstract": "The paper discusses the lexical description and runtime disambiguation of homographous prepositions and verbal particles in English using the Ontological Semantic (OntoSem) text processing environment. It describes the knowledge resources and processors that permit - in all but the most genuinely ambiguous of cases - an unambiguous text-meaning representation (TMR) to be created from input containing such multifunctional elements. The example of back up in many of its senses is treated in particular detail.",
      "title": "17249 The semantics of backing up (Or: What to do with prepositions and particles?)",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746067486&partnerID=40&md5=a14adf9f3c89aaa01a00dea02f3710c7"
    },
    {
      "abstract": "This paper deals with categorization tasks where categories are partially ordered to form a hierarchy. First, it introduces the notion of consistent classification which takes into account the semantics of a class hierarchy. Then, it presents a novel global hierarchical approach that produces consistent classification. This algorithm with AdaBoost as the underlying learning procedure significantly outperforms the corresponding fiat approach, i.e. the approach that does not take into account the hierarchical information. In addition, the proposed algorithm surpasses the hierarchical local top-down approach on many synthetic and real tasks. For evaluation purposes, we use a novel hierarchical evaluation measure that has some attractive properties: it is simple, requires no parameter tuning, gives credit to partially correct classification and discriminates errors by both distance and depth in a class hierarchy.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17251 Learning and evaluation in the presence of class hierarchies: Application to text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746058639&partnerID=40&md5=f32de79c7d6d665543d0e2a209c020dc"
    },
    {
      "abstract": "Incorporating semantic features from the WordNet lexical database is among one of the many approaches that have been tried to improve the predictive performance of text classification models. The intuition behind this is that keywords in the training set alone may not be extensive enough to enable generation of u universal model for a category, but if we incorporate the word relationships in WordNet, a more accurate model may be possible. Other researchers have previously evaluated the effectiveness of incorporating WordNet synonyms, hypernyms, and hyponyms into text classification models. Generally, they have found that improvements in accuracy using features derived from these relationships are dependent upon the nature of the text corpora from which the document collections are extracted. In this paper, we not only reconsider the role of WordNet synonyms, hypernyms. and hyponyms in text classification models, we also consider the role of WordNet meronyms and holonyms. Incorporating these WordNet relationships into a Coordinate Matching classifier, a Naive Bayes classifier, and a Support Vector Machine classifier, we evaluate our approach on six document collections extracted from the Reuters-21578, USENET, and Digi-Trad text corpora. Experimental results show that none of the WordNet relationships were effective ut increasing the accuracy of the Naive Bayes classifier. Synonyms, hypernyms, and holonyms were effective at increasing the accuracy of the Coordinate Matching classifier, and hypernyms were effective at increasing the accuracy of the SVM classifier. Copyright  2006, American Association for Artificial Intelligence (www.aaai.org), All rights reserved.",
      "title": "17253 Evaluating WordNet features in text classification models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746088798&partnerID=40&md5=1f8f563644b3f9126e5b2e1950c3c3ca"
    },
    {
      "abstract": "The world wide web has a wealth of information that is related to almost any text classification task. This paper presents a method for mining the web to improve text classification, by creating a background text set. Our algorithm uses the information gain criterion to create lists of important words for each class of a text categorization problem. It then searches the web on various combinations of these words to produce a set of related data. We use this set of background text with Latent Semantic Indexing classification to create an expanded term by document matrix on which singular value decomposition is done. We provide empirical results that this approach improves accuracy on unseen test examples in different domains. Copyright  2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17254 Using web searches on important words to create background sets for LSI classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746097784&partnerID=40&md5=fe5d007272c20172595d84158b9bf816"
    },
    {
      "abstract": "In this paper we describe a system for semantic interpretation of noun compounds that relies on world and domain knowledge from a knowledge base. This architecture combines domain-independent compounding rules with a task-independent knowledge representation, allowing both components to be flexibly reused. We present examples from Scientific American text, on which the system was developed, and then describe an exercise that tests the portability of the architecture to a new domain: email text on the topic of conference planning. Copyright  2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17255 Resolving noun compounds with multi-use domain knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746067498&partnerID=40&md5=031bc79260d70c5b55d6932e73dec3e8"
    },
    {
      "abstract": "Many applications, such as word-sense disambiguation and information retrieval, can benefit from text classification. Text classifiers based on Independent Component Analysis (ICA) try to make the most of the independent components of text documents and give in many cases good classification effects. Short-text documents, however, usually have little overlap in their feature terms and, in this case, ICA can not work well. Our aim is to solve the shorttext problem in text classification by using Latent Semantic Analysis (LSA) as a data preprocessing method, then employing ICA for the preprocessed data. The experiment shows that using ICA and LSA together rather than only using ICA in Chinese short-text classification can provide better classification effects.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17257 Short-text classification based on ICA and LSA",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745888995&partnerID=40&md5=46674a313fca13bc4c8b74c0ab591c38"
    },
    {
      "abstract": "Text classification is a problem of assigning a document into one or more predefined classes. One of the most interesting issues in text categorization is feature selection. This paper proposes a novel approach in feature selection based on support vector machine(SVM) and latent semantic indexing(LSI), which can identify LSI-subspace that is suited for classification. Experimental results show that the proposed method can achieve higher classification accuracies and is of less training and prediction time.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17258 Feature selection in text classification via SVM and LSI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745885682&partnerID=40&md5=4975b0b5913237a0db3b8dc1ea94db53"
    },
    {
      "abstract": "Many popular Text Classification (TC) models use simple occurrence or words in a document as features to base their classifications. They commonly assume word occurrences to be statistically independent in their design. Although such assumption does not hold in general. these TC models are robust and efficient in their task. Some recent studies have shown context-sensitive TC approaches were able to perform better in general. On the other hand, although complex linguistic or semantic features may intuitively be more relevant in TC. studies on their effectiveness have produced mixed and inconclusive results. In this paper. we present our investigation on the use of some complex linguistic features with two context-sensitive TC methods. Our experimental results show potential advantages of such approach.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17263 Use of linguistic features in context-sensitive text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745776075&partnerID=40&md5=129b746644cdfa83a1ea7b4eda6e413e"
    },
    {
      "abstract": "This paper describes a technique for automatic creation of dictionaries using sub-symbolic representation of words in cross-language context. Semantic relationship among words of two languages is extracted from aligned bilingual text corpora. This feature is obtained applying the Latent Semantic Analysis technique to the matrices representing terms co-occurrences in aligned text fragments. The technique allows to find the best translation according to a properly defined geometric distance in an automatically created semantic space. Experiments show an interesting correctness of 95% obtained in the best case.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17264 Automatic dictionary creation by sub-symbolic encoding of words",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745788594&partnerID=40&md5=a47ab79854ee4c81c46ed663ee71a5f1"
    },
    {
      "abstract": "In this paper, we propose a novel multi-document summarization strategy based on Basic Element (BE) vector clustering. In this strategy, sentences are represented by BE vectors instead of word or term vectors before clustering. BE is a head-modifier-relation triple representation of sentence content, and it is more precise to use BE as semantic unit than to use word. The BE-vector clustering is realized by adopting the k-means clustering method, and a novel clustering analysis method is employed to automatically detect the number of clusters, K. The experimental results indicate a superiority of the proposed strategy over the traditional summarization strategy based on word vector clustering. The summaries generated by the proposed strategy achieve a ROUGE-1 score of 0.37291 that is better than those generated by traditional strategy (at 0.36936) on DUC04 task-2.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "17265 Multi-document summarization based on BE-vector clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745582082&partnerID=40&md5=bb98df358595fb1ade3c0447320a597c"
    },
    {
      "abstract": "Transliteration of Arabic numerals is not easily resolved. Arabic numerals occur frequently in scientific and informative texts and deliver significant meanings. Since readings of Arabic numerals depend largely on their context, generating accurate pronunciation of Arabic numerals is one of the critical criteria in evaluating TTS systems. In this paper, (1) contextual, pattern, and arithmetic features are extracted from a transliterated corpus",
      "title": "17266 Disambiguation based on wordnet for transliteration of Arabic Numerals for Korean TTS",
      "url": "Conference Paper"
    },
    {
      "abstract": "Two complementary and non-interactive literature sets of articles, when they are considered together, can reveal useful information of scientific interest not apparent in either of the two sets alone. Swanson called the existence of such hidden links as undiscovered public knowledge (UPK). The novel connection between Raynaud disease and fish oils was uncovered from complementary and non-interactive biomedical literature by Swanson in 1986. Since then, there have been many approaches to uncover UPK by mining the biomedical literature. These earlier works, however, required substantial manual intervention to reduce the number of possible connections. This paper proposes a semantic-based mining model for undiscovered public knowledge using the biomedical literature. Our method replaces manual ad-hoc pruning by using semantic knowledge from the biomedical ontologies. Using the semantic types and semantic relationships of the biomedical concepts, our prototype system can identify the relevant concepts collected from Medline and generate the novel hypothesis between these concepts. The system successfully replicates Swansons two famous discoveries: Raynaud disease/fish oils and migraine/magnesium. Compared with previous approaches such as LSI-based and traditional association rule-based methods, our method generates much fewer but more relevant novel hypotheses, and requires much less human intervention in the discovery procedure.",
      "title": "17267 A semantic approach for mining hidden links from complementary and non-interactive biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745438114&partnerID=40&md5=0c2d738fc3a16d123ebace7f098c178d"
    },
    {
      "abstract": "This paper presents a methodology for personalized knowledge discovery from text. It derives a users background knowledge from his/her background documents, and exploits such knowledge to evaluate the novelty of discovered knowledge in the form of association rules by measuring the semantic distance between the antecedent and the consequent of a rule in the background knowledge. The experiment results show that the proposed user-oriented novelty measure is highly correlated with the human subjective rule novelty and usefulness ratings. It outperforms seven major objective interestingness measures and the WordNet novelty measure for identifying novel and useful rules.",
      "title": "17268 Personalized knowledge discovery: Mining novel association rules from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745435611&partnerID=40&md5=98904e96649152455ff399b4987aac64"
    },
    {
      "abstract": "The new applications, contexts, and new twists of old themes and problems in the field of Knowledge Organization Systems (KSO) and Services was discussed. Diverse attempts have been applied to the outcomes of much work over the years in artificial subject languages and their intellectual structures to facilitate access to digital information in various setting. The KSOs, such as classification systems, gazetters, lexical databases, ontologies, taxonomies, and thesauri, attempt to model the underlying semantic structure of a domain for the purposes of retrieval. The controlled vocabularies attempt to reduce ambiguity by defining the scope of terms and more complex vocabularies provide a set of effective synonyms for each concept. KOS are also used as resources by natural language research community to address retrieval problems including free text search, query expansion, terminology extraction, text mining, summarization, and translation.",
      "title": "17269 Introduction to knowledge organization systems and services",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746791503&partnerID=40&md5=68159663cab971caec69b17816e9f510"
    },
    {
      "abstract": "This article addresses the task of mining concepts from biomedical literature to index and search through a documents base. This research takes place within the Telemakus project, which has for goal to support and facilitate the knowledge discovery process by providing retrieval, visual, and interaction tools to mine and map research findings from research literature in the field of aging. A concept mining component automating research findings extraction such as the one presented here, would permit Telemakus to be efficiently applied to other domains. The main strategy that has been followed in this project has been to mine from the legends of the documents the research findings as relationships between concepts from the medical literature. The concept mining proceeds through stages of syntactic analysis, semantic analysis, relationships building, and ranking. Evaluation results are presented at the end and show that the system learns concepts and relationships between them with good recall, and that these concepts can be used for indexing the documents. Future improvements of the system are also presented.  2006 Elsevier Ltd. All rights reserved.",
      "title": "17270 Concept mining for indexing medical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646049354&partnerID=40&md5=7c3d399b8ecfa055f1cef0d707c332c4"
    },
    {
      "abstract": "A new document representation model is presented in this paper. This model is based on the idea of representing a document by two or more pictures of the document taken from different perspectives. It is shown that by applying the stereo representation model, enhanced textual retrieval performance is achieved because the new model improves the capability of capturing individual features of the document. Experiments have been conducted on two standard corpora, TIME and ADI, using the standard term vector method and the latent semantic indexing (LSI) method based upon both the stereo representation model and the traditional representation model. Statistical t-tests on the experimental results have convincingly illustrated that these methods achieve significant improvements in retrieval performances with the stereo representation model over those with the traditional representation model.",
      "title": "17274 A Stereo document representation for textual information retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645741599&partnerID=40&md5=227c9618a97704751d0edb2b0ebcbff2"
    },
    {
      "abstract": "Background: Topic detection is a task that automatically identifies topics (e.g., biochemistry and protein structure) in scientific articles based on information content. Topic detection will benefit many other natural language processing tasks including information retrieval, text summarization and question answering",
      "title": "17275 Exploring supervised and unsupervised methods to detect topics in biomedical text",
      "url": "2006"
    },
    {
      "abstract": "A methodology for automatically identifying and clustering semantic features or topics in a heterogeneous text collection is presented. Textual data is encoded using a low rank nonnegative matrix factorization algorithm to retain natural data nonnegativity, thereby eliminating the need to use subtractive basis vector and encoding calculations present in other techniques such as principal component analysis for semantic feature abstraction. Existing techniques for nonnegative matrix factorization are reviewed and a new hybrid technique for nonnegative matrix factorization is proposed. Performance evaluations of the proposed method are conducted on a few benchmark text collections used in standard topic detection studies.  2004 Elsevier Ltd. All rights reserved.",
      "title": "17276 Document clustering using nonnegative matrix factorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-25844488029&partnerID=40&md5=fdb008d13b46206469201fa1e8411e80"
    },
    {
      "abstract": "Among various knowledge management initiatives, the creation of knowledge repositories has emerged as a prevalent approach in current knowledge management practices. In order to transfer tacit knowledge from individuals to a knowledge repository, organizations usually deploy community-based discussion forums. Typically, discussions among participants are organized into reply-replied structures, and reply semantic relationships among these discussion documents exist either explicitly or implicitly. Such relationships, once discovered or annotated in the knowledge repository, can facilitate subsequent knowledge navigation by providing a novel and more semantic mechanism and can support other organizational knowledge management activities (e.g., construction of expert networks). In this study, we propose a preliminary taxonomy of reply semantic relationships for discussion documents organized in reply-replied structures and develop a Semantic Enrichment between Knowledge-sharing documents (SEEK) technique that automatically annotates semantic relationships between reply pairs of documents. Specifically, we propose and evaluate six different feature models that combine keyword features, part-of-speech statistic features, and/or text statistic features. Copyright  2006, Idea Group Inc.",
      "title": "17279 Semantic enrichment in knowledge repositories: Annotating semantic relationships between discussion documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645519751&partnerID=40&md5=ed31e616f185cb50f01fb9df1ab67478"
    },
    {
      "abstract": "Popular online social networks such as Friendster and MySpace do more than simply reveal the superficial structure of social connectedness - the rich meanings bottled within social network profiles themselves imply deeper patterns of culture and taste. If these latent semantic fabrics of taste could be harvested formally, the resultant resource would afford completely novel ways for representing and reasoning about web users and people in general. This paper narrates the theory and technique of such a feat - the natural language text of 100,000 social network profiles were captured, mapped into a diverse ontology of music, books, films, foods, etc., and machine learning was applied to infer a semantic fabric of taste. Taste fabrics bring us closer to improvisational manipulations of meaning, and afford us at least three semantic functions - the creation of semantically flexible user representations, cross-domain taste-based recommendation, and the computation of taste-similarity between people - whose use cases are demonstrated within the context of three applications - the Interest Map, Ambient Semantics, and IdentityMirror. Finally, we evaluate the quality of the taste fabrics, and distill from this research reusable methodologies and techniques of consequence to the semantic mining and Semantic Web communities.  2006, Idea Group Inc.",
      "title": "17280 Unraveling the taste fabric of social networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33750740793&partnerID=40&md5=aa14b6833388778312d037231a6e259f"
    },
    {
      "abstract": "This paper presents a formalism for the representation of complex semantic relations among concepts of natural language. We define a semantic algebra as a set of atomic concepts together with an ordered set of semantic relations. Semantic trees are a graphical representation of a semantic algebra (comparable to Kantorovic trees for boolean or arithmetical expressions). A semantic tree is an ordered tree with nodes labeled with relation and concept names. We generate semantic trees from natural language texts in such a way that they represent the semantic relations which hold among the concepts occurring within that text. This generation process is carried out by a transformational grammar which transforms directly natural language sentences into semantic trees. We present an example for concepts and relations within the domain of computer science where we have generated semantic trees from definition texts by means of a metalanguage for transformational grammars (a sort of metacompiler for transformational grammars). The semantic trees generated so far serve for thesaurus entries in an information retrieval system.  1983.",
      "title": "17282 Semantic trees for natural language representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0020652547&partnerID=40&md5=8f498cb098c1415624e9145c67f7742d"
    },
    {
      "abstract": "This paper investigates certain problems in the production of text from a languaqe-free representation and proposes a model of generation to treat these problems. We deal with generation of connected text. We show that the generation of a connected text cannot be reduced to a simple combination of phrases expressing sub-parts of the representation, but must be based on patterns of discourse structure reflecting the whole representation.",
      "title": "17283 SOME ISSUES IN GENERATION FROM A SEMANTIC REPRESENTATION.",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0020905671&partnerID=40&md5=b4f81595f60727d9c3a952bd5804921a"
    },
    {
      "abstract": "We present a system which translates sentences from a subset of German into a database. This database will function as the basis for a question-answering-systern. The system is applied to a complete text and not to isolated sentences. As an intermediate stage between the German text and the database we use the Discourse Representation Structures (DRS) invented by Hans Kamp. Karnps system has been chosen because it handles intrasentential and intersentential relations uniformly. Within Kamps system one can account for certain types of anaphoric relations for which no other linguistic theory has provided a solution. The input to our system is analysed by a parser which is based on lexical functional grammar. This is the first attempt to combine research on discourse representation with lexical functional grammar with the help of the formalism of Definite Clause Grammar. For the construction of the database out of the DRSs, two solutions are proposed. First, a translation of the DRSs into a set of PROLOG clauses enriched with some additional deductive principles. Second, the formulation of inference rules which operate directly on the DRS. So far we have implemented the following components: parser of German, translation rules which map syntactic trees into DRSs and rules which translate DRSs into PROLOG-clauses.",
      "title": "17284 AUTOMATIC CONSTRUCTION OF A KNOWLEDGE BASE BY ANALYSING TEXTS IN NATURAL LANGUAGE",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0020938258&partnerID=40&md5=99290d6341aa9a2965840c6ebfd41ce3"
    },
    {
      "abstract": "This research aims at defining a consistent set of text representation conventions for organizing fifty pages of the AI handbook as an inferential knowledge base founded on a procedural logic system of general inference schemes for answering questions from it. As a result of research on the AI handbook project, we have developed a prototype, natural-language, text-knowledge system that includes a data base manager to compile the text knowledge and to make it available to navigational commands. The text is represented as logical propositions that form a set of text axioms to model its content. English questions and commands are translated to corresponding logical formulas and treated as theorems to be proved with respect to the text model. The logical form is that of semantic relations (SRs)-logical predicates with varying numbers and ordering of arguments. To compute effectively with such a free form, a relaxed unification procedure was defined as the basis of the SR theorem prover. The use of procedural logic, augmented with fast compiled LISP functions, has shown that questions can be answered in times ranging from a few tenths of a second to minutes of CPU time on a DEC2060 system. The navigational capabilities of the data base manager make available larger contexts surrounding the text and offer the user complete freedom to explore the text and to extract any desired information from it.  1987.",
      "title": "17287 A text knowledge base from the AI handbook",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0023495795&partnerID=40&md5=b219f4f3e15faefdbbf3888209b4c704"
    },
    {
      "abstract": "The Linguistic String Project-Medical Language Processor, a system for computer analysis of narrative patient documents in English, is being adapted for French Lettres de Sortie. The system converts the free-text input to a semantic representation which is then mapped into a relational database. Retrievals of clinical data from the database are described.",
      "title": "17289 Medical language processing for knowledge representation and retrievals",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0024884338&partnerID=40&md5=b2b4bcf1ab46a9ed85efd9fd7014c713"
    },
    {
      "abstract": "This article addresses the methodological problem of the non-linear representation of philosophical systems in a computerized knowledge base. It is a problem of knowledge representation as defined in the field of artificial intelligence. Instead of a purely theoretical discussion of the issue, we present selected results of a practical experiment which has in itself some theoretical significance. We show how one can represent different philosophies using CODE, a knowledge engineering system developed by artificial intelligence researchers. The hypothesis is that such a computer based representation of philosophical systems can give insight into their conceptual structure. We argue that computer aided text analysis can apply knowledge representation tools and techniques developed in artificial intelligence and we estimate how philosophers as well as knowledge engineers could gain from this cross-fertilization. This paper should be considered as an experiment report on the use of knowledge representation techniques in computer aided text analysis. It is part of a much broader project on the representation of conceptual structures in an expert system. However, we intentionally avoided technical issues related to either Computer Science or History of Philosophy to focus on the benefit to enhance traditional humanistic studies with tools and methods developed in AI on the one hand and the need to develop more appropriate tools on the other.  1993 Kluwer Academic Publishers.",
      "title": "17292 Frame-based representation of philosophical systems using a knowledge engineering tool",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-11544342820&partnerID=40&md5=dc63513f047b58d7b6989bcf20c0fb91"
    },
    {
      "abstract": "The text categorization module described here provides a front-end filtering function for the larger DR-LINK text retrieval system [Liddy and Myaeng 1993]. The module evaluates a large incoming stream of documents to determine which documents are sufficiently similar to a profile at the broad subject level to warrant more refined representation and matching. To accomplish this task, each substantive word in a text is first categorized using a feature set based on the semantic Subject Field Codes (SFCs) assigned to individual word senses in a machine-readable dictionary. When tested on 50 user profiles and 550 megabytes of documents, results indicate that the feature set that is the basis of the text categorization module and the algorithm that establishes the boundary of categories of potentially relevant documents accomplish their tasks with a high level of performance. This means that the category of potentially relevant documents for most profiles would contain at least 80% of all documents later determined to be relevant to the profile. The number of documents in this set would be uniquely determined by the systems category-boundary predictor, and this set is likely to contain less than 5% of the incoming stream of documents.",
      "title": "17295 Text categorization for multiple users based on semantic features from a machine-readable dictionary",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0028461596&partnerID=40&md5=f9a4e143b746e1a2f8377c4c51044c72"
    },
    {
      "abstract": "In knowledge-based machine translation (KBMT), the lexicon can be specified and acquired only in close connection with the specification and acquisition of the world model (ontology) and the specification of the text meaning representation (interlingua) language. The former supplies the atoms for the specification of text meaning and provides world knowledge to support the inference processes necessary for a variety of disambiguation and meaning assignment operations. The latter is necessary for the formulation of the semantic zone of the lexicon entries, which can be viewed as containing the static building blocks of the text meaning representation. This is the view taken in the Mikrokosmos KBMT project.  1995 Kluwer Academic Publishers.",
      "title": "17299 A lexicon for knowledge-based MT",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0029486970&partnerID=40&md5=f964e5973afa1e7e2302ce622c527124"
    },
    {
      "abstract": "HIRMA results in an integrated environment to query any full-text document base system by natural language sentences, obtaining a document set relevant to the query. Moreover it supports hypertextual navigation into the document base. The system uses content based document representation and retrieval methods. In this paper the representation framework as well as the retrieval and navigation algorithms used by HIRMA are described. Coverage and portability throughout application domains are supported by the lexical acquisition system ARIOSTO that provides the suitable lexical knowledge and processing methods to extract from raw text the semantic representation of documents content.  1995.",
      "title": "17300 HIRMA: Hypertextual information retrieval system managed by ARIOSTO",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0029532162&partnerID=40&md5=9ab76aab05ef56b83cb723725d707590"
    },
    {
      "abstract": "In this paper we describe a highly parallel method for extracting inferences from text. The method is based on a marker-propagation algorithm that establishes semantic paths between knowledge base concepts. The paper presents the structure of the system, the marker-propagation algorithm, and results that show a large degree of parallelism.",
      "title": "17301 Parallel algorithm for text inference",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0029714033&partnerID=40&md5=14324338fc3e3d622ac020b9567b5edf"
    },
    {
      "abstract": "Document analysis and understanding (DAU) systems aim not only at the recognition of text and document structures but also at the extraction of relevant information out of a scanned document. Depending on the class of a document, information to be extracted may be defined in advance in syntactic structures as well as in semantic structures. In this paper, we present a system for detecting such information and transforming it into a semantic representation. The basic component is a pattern matcher which incorporates geometric positions to detect phrases in the document. By defining a Levenshtein distance, the component reacts more generously in order to be error-tolerant against OCR failures.",
      "title": "17302 Supporting information extraction from printed documents by Lexico-semantic pattern matching",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0030690653&partnerID=40&md5=082c28675be78963eb6e271a587e66e5"
    },
    {
      "abstract": "This paper presents a lexical model dedicated to the semantic representation and interpretation of individual words in unrestricted text, where sense discrimination is difficult to assess. We discuss the need of a lexicon including local inference mechanisms and cooperating with as many other knowledge sources (about syntax, semantics and pragmatics) as possible. We suggest a minimal representation (that is, the smallest representation possible) acting as a bridge between a conceptual representation and the microscopic sense variations of lexical semantics. We describe an interpretation method providing one or many alternative candidate(s) to the word, as representatives of its meaning in the sentence (and text).",
      "title": "17304 An Intelligent Lexicon for Contextual Word Sense Discrimination",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-2342615721&partnerID=40&md5=09b918a3a696c44375da1d26ed206918"
    },
    {
      "abstract": "The Intelligent Essay Assessor (IEA) is a set of software tools for scoring the quality of essay content. The IEA uses Latent Semantic Analysis (LSA), which is both a computational model of human knowledge representation and a method for extracting semantic similarity of words and passages from text. Simulations of psycholinguistic phenomena show that LSA reflects similarities of human meaning effectively. To assess essay quality, LSA is first trained on domain-representative text. Then student essays are characterized by LSA representations of the meaning of the words used, and they are compared with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. Over many diverse topics, the IEA scores agreed with human experts as accurately as expert scores agreed with each other. Implications are discussed for incorporating automatic essay scoring in more general forms of educational technology.",
      "title": "17306 The intelligent essay assessor: Applications to educational technology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0038105092&partnerID=40&md5=8674193b1c391d662a49a744380dafa3"
    },
    {
      "abstract": "This paper presents an explicit connectionist-inspired, language learning model in which the process of settling on a particular interpretation for a sentence emerges from the interaction of a set of soft lexical, semantic, and syntactic primitives. We address how these distinct linguistic primitives can be encoded from different modular knowledge sources but strongly involved in an interactive processing in such a way as to make implicit linguistic information explicit. The learning of a quasi-logical form, called context-dependent representation, is inherently incremental and dynamical in such a way that every semantic interpretation will be related to what has already been presented in the context created by prior utterances. With the aid of the context-dependent representation, the capability of the language learning model in text understanding is strengthened. This approach also shows how the recursive and compositional role of a sentence as conveyed in the syntactic structure can be modeled in a neurobiologically motivated linguistics based on dynamical systems rather on combinatorial symbolic architecture. Experiments with more than 2,000 sentences in different languages illustrating the influences of the context-dependent representation on semantic interpretation, among other issues, are included.",
      "title": "17315 Integrating linguistic primitives in learning context-dependent representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035271353&partnerID=40&md5=acd709009479354744417c2b45d23bd0"
    },
    {
      "abstract": "We propose a novel, convenient fusion of natural language processing and fuzzy logic techniques for analyzing the affect content in free text. Our main goals are fast analysis and visualization of affect content for decision making. The main linguistic resource for fuzzy semantic typing is the fuzzy-affect lexicon, from which other important resources - the fuzzy thesaurus and affect category groups-are generated. Free text is tagged with affect categories from the lexicon and the affect categories centralities and intensities are combined using techniques from fuzzy logic to produce affect sets-fuzzy sets representing the affect quality of a document. We show different aspects of affect analysis using news content and movie reviews. Our experiments show a good correspondence between affect sets and human judgments of affect content. We ascribe this to the representation of ambiguity in our fuzzy affect lexicon and the ability of fuzzy logic to deal successfully with the ambiguity of words in a natural language. Planned extensions of the system include personalized profiles for Web-based content dissemination, fuzzy retrieval, clustering, and classification.",
      "title": "17317 Affect analysis of text using fuzzy semantic typing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035415706&partnerID=40&md5=b95d9617645cca0570df2565e9e9e383"
    },
    {
      "abstract": "This paper presents work that uses Latent Semantic Indexing (LSI) for text classification. However, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available background text in the classification process. Rather than performing LSIs singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text. We report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.",
      "title": "17319 Using LSI for text classification in the presence of background text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035751902&partnerID=40&md5=105c85bf1628e3494e48c2d47d6dc513"
    },
    {
      "abstract": "In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm. By computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another.",
      "title": "17320 Evaluating the novelty of text-mined rules using lexical knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035789294&partnerID=40&md5=2e968d899660648cd032916f9b0dfcd8"
    },
    {
      "abstract": "This paper analyses several recent treatises on hybridised self-organising map (SOM) theory. Each article proposes a solution to expedite the SOM mapping process and provide more accurate results within a shorter response time via hybridisation: including utilisation of Bayesian classification techniques",
      "title": "17322 A new approach to hybrid SOM implementations for text classification",
      "url": "Conference Paper"
    },
    {
      "abstract": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as X is author of Y -> X wrote Y, X solved Y -> X found a solution to Y, and X caused Y -> Y is triggered by X. Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.",
      "title": "17325 DIRT - Discovery of inference rules from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035789274&partnerID=40&md5=498f6c8042a480ce15ec6bfad383f5bf"
    },
    {
      "abstract": "Semantic paragraph partition is an important problem in text structure analysis in automatic abstracting system. For article containing distinct headings, this paper presents heading models in Chinese text to divide an article into semantic paragraphs based on recognition of headings. For article not containing headings, this paper establishes a vector space model for the whole article based on paragraphs, and then semantically relative paragraphs are clustered as semantic paragraphs.",
      "title": "17326 Study on semantic paragraph partition in automatic abstracting system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035723483&partnerID=40&md5=45c9a6877684df5aafd250cc1b2d17e7"
    },
    {
      "abstract": "Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.",
      "title": "17327 Induction of semantic classes from natural language text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0035789301&partnerID=40&md5=65a60a08e427a1d60f57825ff2629304"
    },
    {
      "abstract": "A map of text documents arranged using the Self-Organizing Map (SOM) algorithm (1) is organized in a meaningful manner so that items with similar content appear at nearby locations of the 2-dimensional map display, and (2) clusters the data, resulting in an approximate model of the data distribution in the high-dimensional document space. This article describes how a document map that is automatically organized for browsing and visualization can be success fully utilized also in speeding up document retrieval. Furthermore, experiments on the well-known CISI collection show significantly improved performance compared to Saltons vector space model, measured by average precision (AP) when retrieving a small, fixed number of best documents. Regarding comparison with Latent Semantic Indexing the results are inconclusive.",
      "title": "17338 Text retrieval using self-organized document maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0036470659&partnerID=40&md5=466bac0145d20cad9fdb81895b239321"
    },
    {
      "abstract": "This paper presents a strategy for building a morphological machine dictionary of English that infers meaning of derivations by considering morphological affixes and their semantic classification. Derivations are grouped into a frame that is accessible to semantic stem and knowledge base. This paper also proposes an efficient method for selecting compound Field Association (FA) terms from a large pool of single FA terms for some specialized fields. For single FA terms, five levels of association are defined and two ranks are defined, based on stability and inheritance. About 85% of redundant compound FA terms can be removed effectively by using levels and ranks proposed in this paper. Recall averages of 60-80% are achieved, depending on the type of text. The proposed methods are applied to 22,000 relationships between verbs and nouns extracted from the large tagged corpus.  2002 Elsevier Science Ltd. All rights reserved.",
      "title": "17345 A new method for selecting english field association terms of compound words and its knowledge representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0036833176&partnerID=40&md5=ffc717ba85a0b1a5b3b02e73d056fcc2"
    },
    {
      "abstract": "We describe an approach to retrieval of documents that contain of both free text and semantically enriched markup. In particular, we present the design and implementation prototype of a framework in which both documents and queries can be marked up with statements in the DAML+OIL semantic web language. These statements provide both structured and semi-structured information about the documents and their content. We claim that indexing text and semantic markup together will significantly improve retrieval performance. Our approach allows inferencing to be done over this information at several points: when a document is indexed, when a query is processed and when query results are evaluated.",
      "title": "17346 Information retrieval on the semantic web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0037480903&partnerID=40&md5=11482426a78258bbd6ea65e1bfa1e1b5"
    },
    {
      "abstract": "The paper deals with the problem of the identification of the specific answer to whatever question put to a search engine in the web exploration. The actual problem is to avoid the usual stream of thousands pages extracted by the search engine without the help of an a priori categorization of the theme to which the question is directed. The system,called IRAS, described in the paper, is a follow up of a previous theoretical framework,called TOM, presented at MIS 2002, Bellacicco [1]. The innovation of IRAS is the exploitation of parsing algorithms for the identification of the semantic organization of the statements so that the answer is specific for the real content of the question. Problem faced by IRAS is therefore to overcome besides the stream of thousands pages from the web, the stream of unspecific answers too, which are the fallout of the ambiguity related the use of the terms of the query without the specification of their role in the statement. The parsing of the query besides the parsing, selection and clustering of the statements is the primary tools of IRAS.",
      "title": "17350 Textual data mining by parsing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-2942679761&partnerID=40&md5=c14690d76c314a0e7fd07158c53423d7"
    },
    {
      "abstract": "We address the text content mining problem through a concept based framework by constructing a conceptual knowledge base and discovering knowledge therefrom. Defining a novel representation called the Concept Frame Graph (CFG), we propose a learning algorithm for constructing a CFG knowledge base from text documents. An interactive concept map visualization technique is presented for user-guided knowledge discovery from the knowledge base. Through experimental studies on real life documents, we observe that the proposed approach is promising for mining deeper knowledge.",
      "title": "17353 Knowledge discovery from texts: A concept frame graph approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0037480765&partnerID=40&md5=beaa680f22aee5777c414da424305d1d"
    },
    {
      "abstract": "In this study we show experimental results on using Independent Component Analysis (ICA) and the Self-Organizing Map (SOM) in document analysis. Our documents are segments of spoken dialogues carried out over the telephone in a customer service, transcribed into text. The task is to analyze the topics of the discussions, and to group the discussions into meaningful subsets. The quality of the grouping is studied by comparing to a manual topical classification of the documents.",
      "title": "17355 ICA and SOM in text document analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0036989433&partnerID=40&md5=80928747bdd33c61fbb2d912c0d3c4aa"
    },
    {
      "abstract": "In this paper we present an application of information extraction technologies to data mining. The system has the goal of producing reliable reports on opinions that customers express about companies and/or products. Information extraction is regarded as an enabling technology by which structured databases can be populated from unstructured texts available on the web. The approach can be considered as an alternative to standard text mining techniques: rather than applying data mining algorithms on textual inputs, we propose to apply syntactic and semantic processing in order to disclose structured information which abstracts completely from linear order of words and language dependent constructions.",
      "title": "17359 Opinion classification through information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-2942631624&partnerID=40&md5=45031794a36cc6689b730f99d712e51e"
    },
    {
      "abstract": "Much of knowledge modeling in the molecular biology domain involves interactions between proteins, genes, various forms of RNA, small molecules, etc. Interactions between these substances are typically extracted and codified manually, increasing the cost and time for modeling and substantially limiting the coverage of the resulting knowledge base. In this paper, we describe an automatic system that learns from text interaction verbs",
      "title": "17361 Learning anchor verbs for biological interaction patterns from published text articles",
      "url": ""
    },
    {
      "abstract": "A multilingual text-classification and hyperlinking system called Namic is presented. The system is based on a knowledge-based approach to information extraction. The system is intended to enable content-aware management and delivery of Web information.",
      "title": "17363 Personalizing Web publishing via information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0141837131&partnerID=40&md5=e7352762562b824dc57db7e25e481741"
    },
    {
      "abstract": "This work demonstrates how the World Wide Web can be mined in a fully automated manner for discovering the semantic similarity relationships among the concepts surfaced during an electronic brainstorming session, and thus improving the accuracy of automated clustering meeting messages. Our novel Context Sensitive Similarity Discovery (CSSD) method takes advantage of the meeting context when selecting a subset of Web pages for data mining, and then conducts regular concept co-occurrence analysis within that subset. Our results have implications on reducing information overload in applications of text technologies such as email filtering, document retrieval, text summarization, and knowledge management.  2002 Elsevier Science B.V. all rights reserved.",
      "title": "17365 Automatic discovery of similarity relationships through Web mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0037375280&partnerID=40&md5=760669470d46f152a6bfaeac0b693177"
    },
    {
      "abstract": "Most clinical data is narrative text and often not accessible and searchable at the clinical workstation. We have therefore developed a search engine that allows indexing, searching and linking different kinds of data using web technologies. Text matching methods fail to represent implicit relationships between data, e.g. the relationship between HIV and AIDS. The international organization for standardization (ISO) topic maps standard provides a data model that allows representing arbitrary relationships between resources. Such relationships form the basis for a context sensitive search and accurate search results. The extensible markup language (XML) standards are used for the interchange of data relationships. The approach has been applied to medical classification systems and clinical practice guidelines. The search engine is compared to other XML retrieval methods and the prospect of a semantic web is discussed.  2003 Elsevier Science B.V. All rights reserved.",
      "title": "17366 Linking clinical data using XML topic maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0038162244&partnerID=40&md5=7d3bf9fdd0f9395da618fb794bf70d00"
    },
    {
      "abstract": "An information retrieval system has to retrieve all and only those documents that are relevant to a user query, even if index terms and query terms are not matched exactly. However, term mismatches between index terms and query terms have been a serious obstacle to the enhancement of retrieval performance. In this article, we discuss automatic term normalization between words and phrases in text corpora and their application to a Korean information retrieval system. We perform three new types of term normalizations: transliterated word normalization, noun phrase normalization, and context-based term normalization. Transliterated words are normalized into equivalence classes by using contextual similarity to alleviate lexical term mismatches. Then, noun phrases are normalized into phrasal terms by segmenting compound nouns as well as normalizing noun phrases. Moreover, context-based terms are normalized by using a combination of mutual information and word context to establish word similarities. Next, unsupervised clustering is done by using the K-means algorithm and cooccurrence clusters are identified to alleviate semantic term mismatches. These term normalizations are used in both the indexing and the retrieval system. The experimental results show that our proposed system can alleviate three types of term mismatches and can also provide the appropriate similarity measurements. As a result, our system can improve the retrieval effectiveness of the information retrieval system.",
      "title": "17368 Semantic-based information retrieval for content management and security",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0142135126&partnerID=40&md5=9b15efa9c536613e0696051e73fbfcd2"
    },
    {
      "abstract": "Though the utility of domain ontologies is now widely acknowledged in the IT (Information Technology) community, several barriers must be overcome before ontologies become practical and useful tools. A critical issue is the ontology construction, i.e., the task of identifying, defining, and entering the concept definitions. In case of large and complex application domains this task can be lengthy, costly, and controversial (since different persons may have different points of view about the same concept). To reduce time, cost (and, sometimes, harsh discussions) it is highly advisable to refer, in constructing or updating an ontology, to the documents available in the field. Text mining tools may be of great help in this task. The work presented in this paper illustrates the guidelines of SymOntos, ontology management system, and the text mining approach adopted herein to support ontology building. The latter operates by extracting, from the related literature, the prominent domain concepts and the semantic relations among them.",
      "title": "17369 Text mining techniques to automatically enrich a domain ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0037687951&partnerID=40&md5=61e740707dc66c1afe43374aa9d83136"
    },
    {
      "abstract": "Past research has explored the effectiveness of a Naive Bayesian classifier when filtering unsolicited bulk email (spam). Results have shown that the degree of precision of this approach is generally superior to the degree of recall. This study evaluates the effectiveness of a classifier incorporating Latent Semantic Indexing (LSI) to filter spam email on corpus used in previous studies. Results show that email classifiers using LSI to filter spam enjoy a very high degree of both recall and precision, no matter if the corpus is treated using a stop list or a lemmatizer. While using LSI leads to precision roughly equal to that of using a Naive Bayesian approach, the LSI technique has a substantially higher recall and is more effective under certain conditions. Results show that incorporating LSI into an anti-spam filter is viable, particularly in implementations when misclassified legitimate messages are not arbitrarily deleted. Other inferences are drawn to the applicability of this method to other text mining tasks.",
      "title": "17371 Using latent semantic indexing to filter spam",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0038336922&partnerID=40&md5=6ad3e0a190cf1a0723645fc8ee7b0116"
    },
    {
      "abstract": "We have developed a program NeuroText to populate the neuroscience databases in SenseLab (http://senselab.med.yale.edu/senselab) by mining the natural language text of neuroscience articles. NeuroText uses a two-step approach to identify relevant articles. The first step (pre-processing), aimed at 100% sensitivity, identifies abstracts containing database keywords. In the second step, potentially relevant abstracts identified in the first step are processed for specificity dictated by database architecture, and neuroscience, lexical and semantic contexts. NeuroText results were presented to the experts for validation using a dynamically generated interface that also allows expert-validated articles to be automatically deposited into the databases. Of the test set of 912 articles, 735 were rejected at the pre-processing step. For the remaining articles, the accuracy of predicting database-relevant articles was 85%. Twenty-two articles were erroneously identified. NeuroText deferred decisions on 29 articles to the expert. A comparison of NeuroText results versus the experts analyses revealed that the program failed to correctly identify articles relevance due to concepts that did not yet exist in the knowledgebase or due to vaguely presented information in the abstracts. NeuroText uses two evolution techniques (supervised and unsupervised) that play an important role in the continual improvement of the retrieval results. Software that uses the NeuroText approach can facilitate the creation of curated, special-interest, bibliography databases.",
      "title": "17372 Text mining neuroscience journal articles to populate neuroscience databases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0042230987&partnerID=40&md5=98a497eaf53be431241baf8b4c98106e"
    },
    {
      "abstract": "This paper discusses the classification problems of text documents. Based on the concept of the proximity degree, the set of words is partitioned into some equivalence classes. Particularly, the concepts of the semantic field and association degree are given in this paper. Based on the above concepts, this paper presents a fuzzy classification approach for document categorization. Furthermore, applying the concept of the entropy of information, the approaches to select key words from the set of words covering the classification of documents and to construct the hierarchical structure of key words are obtained.",
      "title": "17375 A Fuzzy Approach to Classification of Text Documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0141867824&partnerID=40&md5=1dec81a0c6f34a664e1de91228c7c393"
    },
    {
      "abstract": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., honest, intrepid) and negative semantic orientation indicates criticism (e.g., disturbing, superfluous). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.",
      "title": "17376 Measuring praise and criticism: Inference of semantic orientation from association",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-2442507763&partnerID=40&md5=7bda5fbb202c15eea323c6c7c8d4a36f"
    },
    {
      "abstract": "Most of the text categorization algorithms in the literature represent documents as collections of words. An alternative which has not been sufficiently explored is the use of word meanings, also known as senses. In this paper, using several algorithms, we compare the categorization accuracy of classifiers based on words to that of classifiers based on senses. The document collection on which this comparison takes place is a subset of the annotated Brown Corpus semantic concordance. A series of experiments indicates that the use of senses does not result in any significant categorization improvement.",
      "title": "17379 A comparison of word- and sense-based text categorization using several classification algorithms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0142086575&partnerID=40&md5=46a2c29b0f91fee2adfa58614083222b"
    },
    {
      "abstract": "Automatic knowledge discovery from texts (KDT) is proving to be a promising method for businesses today to deal with the overload of textual information. In this paper, we first explore the possibilities for KDT to enhance communication in virtual communities, and then we present a practical case study with real-life Internet data. The problem in the case study is to manage the very successful virtual communities known as clubs of the largest Dutch Internet Service Provider. It is possible for anyone to start a club about any subject, resulting in over 10,000 active clubs today. At the beginning, the founder assigns the club to a predefined category. This often results in illogical or inconsistent placements, which means that interesting clubs may be hard to locate for potential new members. The ISP therefore is looking for an automated way to categorize clubs in a logical and consistent manner. The method used is the so-called bag-of-words approach, previously applied mostly to scientific texts and structured documents. Each club is described by a vector of word occurrences of all communications within that club. Latent Semantic Indexing (LSI) is applied to reduce the dimensionality problem prior to clustering. Clustering is done by the Within Groups Clustering method using a cosine distance measure appropriate for texts. The results show that KDT and the LSI method can successfully be applied for clustering the very volatile and unstructured textual communication on the Internet.",
      "title": "17382 Knowledge discovery in virtual community texts: Clustering virtual communities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0242468047&partnerID=40&md5=55f07ac6654c09d82603427eeaaaceb4"
    },
    {
      "abstract": "Large amounts of technical documentation are available in machine readable form, however there is a lack of effective ways to access them. In this paper we propose an approach based on linguistic techniques, geared towards the creation of a domain-specific Knowledge Base, starting from the available technical documentation. We then discuss an effective way to access the information encoded in the Knowledge Base. Given a user question phrased in natural language the system is capable of retrieving the encoded semantic information that most closely matches the user input, and present it by highlighting the textual elements that were used to deduct it.",
      "title": "17384 Knowledge-based question answering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-8344246963&partnerID=40&md5=a3b27edda9058b1f17231d5d36f17cc4"
    },
    {
      "abstract": "Automatic acquisition of information structures like Topic Maps or semantic networks from large document collections is an important issue in knowledge management. An inherent problem with automatic approaches is the treatment of multiword terms as single semantic entities. Taking company names as an example, we present a method for learning multiword terms from large text corpora exploiting their internal structure. Through the iteration of a search step and a verification step the single words typically forming company names are learnt. These name elements are used for recognizing compounds in order to use them for further processing. We give some evaluation of experiments on company name extraction and discuss some applications.  J.UCS.",
      "title": "17386 Automatic discovery and aggregation of compound names for the use in knowledge representations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-23844466239&partnerID=40&md5=093e3882ada689427716f1cd9c9a9f34"
    },
    {
      "abstract": "In this paper, we try to find empirically the optimal dimensionality in data-driven models, Latent Semantic Analysis (LSA) model and Probabilistic Latent Semantic Analysis (PLSA) model. These models are used for building linguistic semantic knowledge which could be used in estimating contextual semantic similarity for the target word selection in English-Korean machine translation. We also facilitate k-Nearest Neighbor learning algorithm. We diversify our experiments by analyzing the covariance between the value of k in k-NN learning and accuracy of selection, in addition to that between the dimensionality and the accuracy. While we could not find regular tendency of relationship between the dimensionality and the accuracy, however, we could find the optimal dimensionality having the most sound distribution of data during experiments.",
      "title": "17388 An empirical study on dimensionality optimization in text mining for linguistic knowledge acquisition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-7444251632&partnerID=40&md5=5c4ef9bf8d38f7e8b3cc4e88b897bfd4"
    },
    {
      "abstract": "Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. This paper first introduces ontologies in general and subsequently, in particular, shortly outlines the DOGMA ontology engineering approach that separates atomic conceptual relations from predicative domain rules. In the main part of the paper, we describe and experimentally evaluate work in progress on a potential method to automatically derive the atomic conceptual relations mentioned above from a corpus of English medical texts. Preliminary outcomes are presented based on the clustering of nouns and compound nouns according to co-occurrence frequencies in the subject-verb-object syntactic context.  Springer-Verlag Berlin Heidelberg 2003.",
      "title": "17391 Mining for lexons: Applying unsupervised learning methods to create ontology bases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0242628832&partnerID=40&md5=ca515cff6da16755b5752f1c67fa29cb"
    },
    {
      "abstract": "This paper presents a new method of text clustering based on the theory of latent semantic analysis (LSA) called TCBLSA method. The vector space model (VSM) of term weight is constructed by the theory of LSA and the TF.IDF method. The present method decreases the dimension of vector, and eliminates disadvantageous factors in the VSM. Furthermore, the method advances the speed and precision of text clustering. Through analyzing experimental data, we demonstrate that the TCBLSA method is effective and feasible for text clustering.",
      "title": "17393 TCBLSA: A new method of text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-1542328989&partnerID=40&md5=27560d70a59e2fdb25b3b69ba12725ac"
    },
    {
      "abstract": "Personalization based on Web usage mining can enhance the effectiveness and scalability of collaborative filtering. However, without semantic knowledge about the underlying domain, such systems cannot recommend different types of complex objects based in their underlying properties and attributes. This paper provides an overview of approaches for incorporating semantic knowledge into Web usage mining and personalization processes. We present two general approaches to integrate semantic knowledge extracted from the content features of pages into the usage-based personalization process. Next, we present a general framework of integrating domain ontologies with Web Usage Mining and Personalization. In each case, we discuss how semantic knowledge is leveraged and represented in the pre-processing and pattern discovery phases, as well as how it is used to enhance usage-based personalization.",
      "title": "17394 A road map to more effective web personalization: Integrating domain knowledge with web usage mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-1842586819&partnerID=40&md5=bdde4b100f1330413af42ea5a26b1cfc"
    },
    {
      "abstract": "We present a novel evolutionary model for knowledge discovery from texts (KDTs), which deals with issues concerning shallow text representation and processing for mining purposes in an integrated way. Its aims is to look for novel and interesting explanatory knowledge across text documents. The approach uses natural language technology and genetic algorithms to produce explanatory novel hypotheses. The proposed approach is interdisciplinary, involving concepts not only from evolutionary algorithms but also from many kinds of text mining methods. Accordingly, new kinds of genetic operations suitable for text mining are proposed. The principles behind the representation and a new proposal for using multiobjective evaluation at the semantic level are described. Some promising results and their assessment by human experts are also discussed which indicate the plausibility of the model for effective KDT.",
      "title": "17398 A semantically guided and domain-independent evolutionary model for knowledge discovery from texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0346502997&partnerID=40&md5=4b607d9c5e39171f7651def6e2a31f80"
    },
    {
      "abstract": "We seek insight into Latent Semantic Indexing by establishing a method to identify the optimal number of factors in the reduced matrix for representing a keyword. This method is demonstrated empirically by duplicating all documents containing a term t, and inserting new documents in the database that replace t with t'. By examining the number of times term t is identified for a search on term t' (precision) using differing ranges of dimensions, we find that lower ranked dimensions identify related terms and higher-ranked dimensions discriminate between the synonyms.",
      "title": "17399 Latent Concepts and the Number Orthogonal Factors in Latent Semantic Analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-1542287487&partnerID=40&md5=cc233a032eff53f018002af06ca6b444"
    },
    {
      "abstract": "Term-based representations of documents have found wide-spread use in information retrieval. However, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage. In this paper we investigate the use of concept-based document representations to supplement word- or phrase-based features. The utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis. We propose to use AdaBoost to optimally combine weak hypotheses based on both types of features. Experimental results on standard benchmarks confirm the validity of our approach, showing that AdaBoost achieves consistent improvements by including additional semantic features in the learned ensemble.",
      "title": "17402 Text Categorization by Boosting Automatically Extracted Concepts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-1542377542&partnerID=40&md5=2be7fe37673da551a8b9da92bfc18179"
    },
    {
      "abstract": "Most existing commercial applications of QA (Question and Answering) systems are restricted to dealing with some specific query domains and promoting precision is difficult. This study proposes a concept-based QA system with the understanding abilities, which combines several techniques such as FAQ corpus, text mining, concept space, and so on. The QA system can process the question statement in Chinese natural language fashion and find out the implicit intention of the user query. The question is split into four different terms, namely Subject term, Attribute term, Intention term and Interrogative term. These separate terms then are matched with the existing items in the FAQ corpus to get the proper or similar answers. The system interprets the document source to extract the semantic information for constructing Ontology, and this information is related to concepts such as human, event, time, place and entity. The system then contrasts the source documents with the question by heuristic rules and precisely retrieves passages that best fit the requirements of the users. This study collected 5000 Chinese News items from www.chinatimes.com and presented significant values on the precision and recall rate.",
      "title": "17403 An Effective Engine for Answering Questions Based upon Chinese Semantic Extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0345015383&partnerID=40&md5=4c9c010a785a1f7168e730069b4c8bd1"
    },
    {
      "abstract": "Clinical Guidelines are a major tool in improving the quality of medical care. However, most guidelines are in free text, not machine comprehensible, and are not easily accessible to clinicians at the point of care. We introduce a Web-based, modular, distributed architecture, the Digital Electronic Guideline Library (DeGeL), which facilitates gradual conversion of clinical guidelines from text to a formal representation in a chosen guideline ontology. The architecture supports guideline classification, semantic markup, context-sensitive search, browsing, run-time application, and retrospective quality assessment. The DeGeL hybrid meta-ontology includes elements common to all guideline ontologies, such as semantic classification, and domain knowledge. The hybrid meta-ontology also includes three guideline-content representation formats: free text, semi-structured text",
      "title": "17407 DEGEL: A hybrid, multiple-ontology framework for specification and retrieval of clinical guidelines",
      "url": ""
    },
    {
      "abstract": "A proposal of a new KDT approach that combines information extraction technology and genetic algorithms to produce a new integrated model for text mining is discussed. The proposed approach does not rely on external resources, rather it performs the discovery using only information from the original corpus of text documents and from training data computed from them. Taking into account the semantic and rhetorical constraints it handles the problem of the diversity of solutions. The model shows a good level of prediction in terms of its correlation with human judgements.",
      "title": "17420 Combining information extraction with genetic algorithms for text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-3042692199&partnerID=40&md5=37c2fbe4fb29db7cbd3aabb44e708938"
    },
    {
      "abstract": "Effective multilingual information filtering is required to alleviate users burden of information over-load resulting from the increasing flood of multilingual textual content available extensively over the World-Wide Web. This paper proposes a content-based self-organizing approach to multilingual information filtering using fuzzy logic and the self-organizing map. This approach screens and evaluates multilingual documents based on their semantic contents. Correlated multilingual documents are disseminated according to their corresponding themes or topics, thus enabling language-independent content-based information access efficiently and effectively. A Web-based multilingual online news-filtering system is developed to illustrate how the approach works.",
      "title": "17422 Filtering multilingual web content using fuzzy logic and self-organizing maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-3142696987&partnerID=40&md5=50feb201723f4d92160c156ee49bf701"
    },
    {
      "abstract": "This study involves a methodology for the automatic identification of semantic features and document clusters in a heterogeneous text collection. The methodology is based upon encoding the data using low rank nonnegative matrix factorization algorithms to preserve natural data non-negativity and thus avoid subtractive basis vector and encoding interactions present in techniques such as principal component analysis. Some existing non-negative matrix factorization techniques are reviewed and some new ones are proposed. Numerical experiments are reported on the use of a hybrid NMF algorithm to produce a parts-based approximation of a sparse term-by-document matrix. The resulting basis vectors and matrix projection can be used to identify underlying semantic features (topics) and document clusters of the corresponding text collection.",
      "title": "17424 Text mining using non-negative matrix factorizations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-2942588993&partnerID=40&md5=e7716db9705af84f51dd775da3a2f7df"
    },
    {
      "abstract": "In this paper, the FEELING-G system for extracting and building semantic representations of emotions, feelings and psychological states expressed in texts is presented, as well as the linguistic knowledge used by the system. Semantic representations are described by means of a set of feature structures. Letters to the Editor is taken as a domain for the evaluation of this work.",
      "title": "17426 Linguistic knowledge and automatic semantic representation of emotions and feelings",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-3042602431&partnerID=40&md5=9a638a4219b911012855fe62bc0edb61"
    },
    {
      "abstract": "Bayesian Networks and Influence Diagrams are effective methods for structuring clinical problems. Constructing a relevant structure without the numerical probabilities in itself is a challenging task. In addition, due to the rapid rate of innovations and new findings in the biomedical domain, constructing a relevant graphical model becomes even more challenging. Building a model structure from text with minimum intervention from domain experts and minimum training examples has always been a challenge for the researchers. In the biomedical domain, numerous advances have been made which may make this dream a possibility now. We are currently trying to build a general purpose system to automatically extract the model structure from scientific articles using a combination of ontological knowledge and data mining with natural language processing. This paper discusses the prototype system that we are working on. Previously, systems have used keyword features to extract knowledge from text. We, like Blake et al [4], argue that the choice of features used to represent a domain has a profound effect on the quality of model produced. Our system uses concepts and semantic types rather than keywords. We map complete sentences in the medical text to a conceptual level and a semantic level. We then, use Association Rule Mining (ARM) to extract relationships from text. Rules are then filtered and verified to improve precision of the obtained rules. Preliminary results applied to Colorectal Cancer medical domain are presented, which suggest the feasibility of our approach.",
      "title": "17436 Automatic model structuring from text using bioMedical ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-20344396694&partnerID=40&md5=4c90169c937d1ff02a3a4d151b0215f4"
    },
    {
      "abstract": "Document representations for text classification are typically based on the classical Bag-Of-Words paradigm. This approach comes with deficiencies that motivate the integration of features on a higher semantic level than single words. In this paper we propose an enhancement of the classical document representation through concepts extracted from background knowledge. Boosting is used for actual classification. Experimental evaluations on two well known text corpora support our approach through consistent improvement of the results.  2004 IEEE.",
      "title": "17440 Text classification by boosting weak learners based on terms and concepts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-19544368439&partnerID=40&md5=dd599ec09aab0093034b00fcb20012b7"
    },
    {
      "abstract": "We present a novel approach to incorporating semantic information to the problems of natural language processing, in particular to the document classification task. The approach builds on the intuition that semantic relatedness of words can be viewed as a non-static property of the words that depends on the particular task at hand. The semantic relatedness information is incorporated using feature transformations, where the transformations are based on a feature ontology and on the particular classification task and data. We demonstrate the approach on the problem of classifying MEDLINE-indexed documents using the MeSH ontology, The results suggest that the method is capable of improving the classification performance on most of the datasets.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "17441 Ontology-based feature transformations: A data-driven approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944457552&partnerID=40&md5=7d9c1d4afb4d07bd541611bc232a3326"
    },
    {
      "abstract": "Current article overviews questions of text theory, methods of text content modeling, basic formal models of content representation and their suitability for multilingual texts. The usage of an existing formal language (SEMSYNT) is suggested for overall text content description, syntactic and semantic levels of representation. The research is based on the material of English, French and Russian electronic documents with well-defined formal structure.",
      "title": "17446 Formal representation of the text content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-24644499382&partnerID=40&md5=051be798318801a3e346b960f2d61454"
    },
    {
      "abstract": "Language independent bag-of-words representations are surprisingly effective for text classification. The representation is high dimensional though, containing many non-consistent words for text categorization. These non-consistent words result in reduced generalization performance of sub-sequent classifiers, e.g., from ill-posed principal component transformations. In this communication our aim is to study the effect of reducing the least relevant words from the bag-of-words representation. We consider a new approach, using neural network based sensitivity maps and information gain for determination of term relevancy, when pruning the vocabularies. With reduced vocabularies documents are classified using a latent semantic indexing representation and a probabilistic neural network classifier. Reducing the bag-of-words vocabularies with 90%-98%, we find consistent classification improvement using two mid size data-sets. We also study the applicability of information gain and sensitivity maps for automated keyword generation.",
      "title": "17447 Pruning the vocabulary for better context recognition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10944229744&partnerID=40&md5=d56513e0bb9f6ec39fff6302ae58020f"
    },
    {
      "abstract": "We consider the problem of mining conversations between customers and call center representatives for automatically classifying calls into predefined categories. We analyze the conversations for speaker dependent information content using several multi-class classification technologies. The data consists of 539 manually transcribed conversations belonging to 15 categories. Classifiers were built using Support Vector Machines, Naive Bayes, Latent Semantic Analysis, Vector Space and K-Nearest Neighbor technologies. SVM classifiers were found to perform consistently well giving an accuracy of about 74% on the entire data and about 92% when considering only the 4 largest classes. It is observed that very high weightage to either the customer part of the dialog or that of the agent results in poor accuracy. Nearly equal weightage to the customer and agent provides the best results consistently. This approach has potential to identify cross sell and up-sell opportunities in real-time.",
      "title": "17448 Mining call center dialog data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-11844282222&partnerID=40&md5=4bd67c79609ae055bf8ed766fd20c99f"
    },
    {
      "abstract": "Automated discovery and extraction of biological knowledge from biomedical web documents has become essential because of the enormous amount of biomedical literature published each year, In this paper we present an ontology-based scalable and portable information extraction system to automatically extract biological knowledge from huge collection of online biomedical web documents. Our method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extract new tuples from the biomedical web documents based on the discovered patterns. A novel system SPIE (Scalable and Portable Information Extraction) is implemented and tested on the PuBMed to find the chromatin protein-protein interaction and the experimental results indicate our approach is very effective in extracting biological knowledge from huge collection of biomedical web documents.  2004 IEEE.",
      "title": "17452 Ontology-based scalable and portable information extraction system to extract biological knowledge from huge collection of biomedical Web documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-15544373800&partnerID=40&md5=9be0dc517e14cdb76ee2c7f0de7433ce"
    },
    {
      "abstract": "Text categorization is an important component in many information management tasks such as real-time sorting of emails or files. An important consideration in text categorization performance is the choice of feature sets for text representation. A popular approach for text representation is the vector space model. It represents the units of content of a document as a vector. In most situations, each distinct word is used as a content unit. However, such a representation, called the bag-of-word approach has drawbacks. Firstly, a large number of features are required for document representation. Secondly, it does not take into account the effects of synonymy and polysemy, which could have an impact on classification accuracy. Latent semantic analysis addresses the above shortcomings by simultaneously modelling all the interrelationships among terms and documents, using the singular value decomposition technique which allows the representation of the terms and documents in a reduced dimensional space. It has been widely used to enhance the performance of information retrieval systems and recently used for text classification purposes as well. In this study, we further explore its use, for the classification of call centre data sets obtained from a Multi-National Company. These spontaneously created documents exhibit characteristics different from benchmark data sets used in most studies, hence necessitating this investigation. Further, the effect on classification, of various weighting schemes as well as the number of dimensions was explored. Results revealed that the LSA approach marginally improved the classification accuracy. It was also found that the weighting scheme used did not significantly affect classification performance unlike in some retrieval applications where as much as a 40% average improvement in performance was observed. Further, the widely recommended use of 100 to 300 dimensions for document representation was found to be inapplicable for the investigated data sets.  2004 IEEE.",
      "title": "17456 On the effectiveness of latent semantic analysis for the categorization of call centre records",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-17644397079&partnerID=40&md5=2420557722da7fa9499cb24396326630"
    },
    {
      "abstract": "This paper presents a novel feature space enriching (FSE) technique to address the problem of sparse and noisy feature space in email classification. The (FSE) technique employs two semantic knowledge bases to enrich the original sparse feature space, which results in more semantic-richer features. From the enriched feature space, the classification algorithms can learn improved classifiers. Naive Bayes and support vector machine are selected as the classification algorithms. Experiments on an enterprise email dataset have shown that the FSE technique is effective for improving the email classification performance.  Springer-Verlag 2004.",
      "title": "17458 Improved email classification through enriched feature space",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048827952&partnerID=40&md5=9e1daf2d975685c2d14ec388962c60d1"
    },
    {
      "abstract": "In this paper we introduce a novel framework to compute jointly syntactic parses and semantic representations of a written sentence. To achieve this goal, we couple a syntactico-semantic grammar and a knowledge base. The knowledge base is implemented in Description Logics, in a polynomial variant. The grammar is a Range Concatenation Grammar, which combines expressive power and polynomial parsing time, and allows external predicate calls. These external calls are sent to the knowledge base, which is able either to answer these calls or to learn new information, this process taking place during parsing. Thus, only semantically acceptable parses are built, avoiding the costly a posteriori semantic check of all syntactically correct parses.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "17459 Coupling grammar and knowledge base: Range Concatenation Grammars and description logics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944484155&partnerID=40&md5=ee881ddc6e9fbc2bb30f0b0a8fe71795"
    },
    {
      "abstract": "This paper describes an approach that offers a more general and easier way to access the knowledge repository as a useful part of a knowledge management system. The approach applies a domain ontology in the query text analysis and ontology mapping technique in the knowledge search process. It sets up relationship between the query text in unstructured natural language expressions and the instances in the knowledge base to find those instances that match the condition expressed by the query most. The application magnifies the scope of ontology, and promotes the applicability of ontology based system.  2004 IEEE.",
      "title": "17463 Ontology based unstructured text query",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-15744399653&partnerID=40&md5=786baba508ba38d58d6627dcb4318945"
    },
    {
      "abstract": "This paper describes a technique which uses research into the use of existing linguistic resources (VerbNet and WordNet) to construct conceptual graph representations of texts. We use a two-step approach, firstly identifying the semantic roles in a sentence, and then using these roles, together with semi-automatically compiled domain-specific knowledge, to construct the conceptual graph representation.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "17466 Using linguistic resources to construct conceptual graph representation of texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944467533&partnerID=40&md5=ec623f522b314eb82e7efca530d2c659"
    },
    {
      "abstract": "Ontologies have been widely accepted as the most advanced knowledge representation model. They are among the most important building blocks of semantic web, hence, very crucial for the success of semantic web. This paper discusses a fast and efficient method to facilitate the evaluation and enrichment of domain ontologies using a text-mining approach. We exploit domain specific texts and glossaries or dictionaries in order to automatically generate g-groups and f-groups. These groups are sets of concepts/terms which have either taxonomic or non-taxonomic relationships among them. The domain expert ontology engineer reviews these generated groups and uses them to evaluate and enrich the domain ontology. We have developed an extensive and detailed ontology in the field of environmental science using this approach in interaction with domain expert. Empirical results show that our approach can support domain expert ontology engineers in building domain specific ontologies efficiently.",
      "title": "17470 Mining domain specific texts and glossaries to evaluate and enrich domain ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-12344299730&partnerID=40&md5=0fd21491affdef92621f5efd88e2b58d"
    },
    {
      "abstract": "Text processing in Serbian is based on the Intex format system of electronic dictionaries. Although lexical recognition is successful for 75% to 90% of word forms (depending on the type of text), some categories of words remain unrecognized. In this paper we present two aspects of e-dictionary enhancement that provide for additional recognition of two important categories of words: named entities and words generally not recorded in traditional dictionaries. We first describe the structure and content of dictionaries of proper names, both personal and geographic, developed to recognize the corresponding classes of named entities. Then we present a set of lexical transducers expressing morphological rules governing word formation, developed for the recognition of unknown words. The resources presented significantly improve the lexical recognition process.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "17473 Towards full lexical recognition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944442283&partnerID=40&md5=183581fa796370e8c2ce4fd408b47575"
    },
    {
      "abstract": "Automatic Summarization is an important research issue in natural language processing. This paper presents a special summarization method to generate single-document summary with maximum topic completeness and minimum redundancy. It initially implements the semantic-class-based vector representations of various kinds of linguistic units in a document by means of HowNet (an existing ontology), which can improve the representation quality of traditional term-based vector space model in a certain degree. Then, by adopting K-means clustering algorithm as well as a novel clustering analysis algorithm, we can capture the number of different latent topic regions in a document adoptively. Finally, topic representative sentences are selected from each topic region to form the final summary. In order to evaluate the effectiveness of the proposed summarization method, a novel metric which is known as representation entropy is used for summarization redundancy evaluation. Preliminary experimental results show that the proposed method outperforms the conventional basic summarization method under the evaluation scheme when dealing with diverse genres of Chinese documents with free writing style and flexible topic distribution.",
      "title": "17476 A study of Chinese text summarization using adaptive clustering of paragraphs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-9744222949&partnerID=40&md5=1937bc3ecbeb63722d42b0b6a624f8ef"
    },
    {
      "abstract": "This paper presents a novel algorithm for computing similarity between very short texts of sentence length. It will introduce a method that takes account of not only semantic information but also word order information implied in the sentences. Firstly, semantic similarity between two sentences is derived from information from a structured lexical database and from corpus statistics. Secondly, word order similarity is computed from the position of word appearance in the sentence. Finally, sentence similarity is computed as a combination of semantic similarity and word order similarity. The proposed algorithm is applied to a real world domain of conversational agents. Experimental results demonstrated that the proposed algorithm reduces the scripters effort to devise rule base for conversational agent.",
      "title": "17477 A method for measuring sentence similarity and its application to conversational agents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10044237967&partnerID=40&md5=005c6bad9069e6a2c1ab1848faae3557"
    },
    {
      "abstract": "Taxonomic case-based reasoning is a conversational case-based reasoning methodology that employs feature subsumption taxonomies for incremental case retrieval. Although this approach has several benefits over standard retrieval approaches, methods for automatically acquiring these taxonomies from text documents do not exist, which limits its widespread implementation. To accelerate and simplify feature acquisition and case indexing, we introduce FACIT, a domain independent framework that combines deep natural language processing techniques and generative lexicons to semi-automatically acquire case indexing taxonomies from text documents. FACIT employs a novel method to generate a logical form representation of text, and uses it to automatically extract and organize features. In contrast to standard information extraction approaches, FACITs knowledge extraction approach should be more accurate and robust to syntactic variations in text sources due to its use of logical forms. We detail FACIT and its implementation status.",
      "title": "17479 Towards acquiring case indexing taxonomies from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10044235337&partnerID=40&md5=2323faf02aaa9115a8b8249069076472"
    },
    {
      "abstract": "This paper presents work that uses Transductive Latent Semantic Indexing (LSI) for text classification. In addition to relying on labeled training data, we improve classification accuracy by incorporating the set of test examples in the classification process. Rather than performing LSIs singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available test examples. We report the performance of LSI on data sets both with and without the inclusion of the test examples, and we show that tailoring the SVD process to the test examples can be even more useful than adding additional training data. The test set can be a useful tool to combat the possible inclusion of unrelated data in the original corpus.",
      "title": "17480 Transductive LSI for short text classification problems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10044253372&partnerID=40&md5=e17020218b0b1c530a0ea6e72d7c9f66"
    },
    {
      "abstract": "In this paper I present a dynamic approach for constructing an unambiguous semantic representation for a text written in a controlled natural language called PENG. The semantic representation is built up incrementally - in left-to-right order - while the user of the PENG system writes the text. For each word form that the user types, the language processor creates a partial logical structure that is analysed in the context of a given information state and then either updates that information state or delays this process if necessary. After each processing step, the language processor reports what kind of syntactic categories can follow the current input. These predictive grammatical hints guide the writing process of the user and enforce the rules of the controlled language. The result is an unambiguous text in controlled natural language that has first-order equivalent properties and that can be further processed by a computer.",
      "title": "17482 Dynamic semantics for a controlled natural language",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10044266807&partnerID=40&md5=57f43935e95ef73e18cbc713062c7d3a"
    },
    {
      "abstract": "Enabling applications that exploit heterogeneous data in the Semantic Web will require us to harness a broad variety of semantics. Considering the role of semantics in a number of research areas in computer science, we organize semantics in three forms - implicit, formal, and powerful - and explore their roles in enabling some of the key capabilities related to the Semantic Web. The central message of this article is that building the Semantic Web purely on description logics will artificially limit its potential, and that we will need to both exploit well-known techniques that support implicit semantics, and develop more powerful semantic techniques. Copyright  2005, Idea Group Inc.",
      "title": "17484 Semantics for the semantic web: The implicit, the formal and the powerful",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-20344363698&partnerID=40&md5=bed834a9dd7c7efc4a2d9881dacf7108"
    },
    {
      "abstract": "Objective: Support vector machines (SVMs) have achieved state-of-the-art performance in several classification tasks. In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with large-scale terminologies such as those in medicine and related disciplines. Methods and materials: The foundation for the model is a sample of text annotated by a domain expert according to an ontology of concepts, properties and relations. The model then learns to annotate unseen terms in new texts and contexts. The results can be used for a variety of intelligent language processing applications. We illustrate SVMs capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Results: Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. Conclusion: Our experiments indicate a relationship between feature window size and the amount of training data and that a combination of surface words, orthographic features and head noun features achieve the best performance among the feature sets tested.  2004 Elsevier B.V. All rights reserved.",
      "title": "17487 Bio-medical entity extraction using support vector machines",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-16244362685&partnerID=40&md5=5572cade37bd7979f984155f725c6d43"
    },
    {
      "abstract": "It becomes increasingly important to automatically discover business knowledge from large databases in order to drastically reduce operators costs in the areas of CRM (Customer Relationship Management), knowledge management, Web marketing, etc. This paper introduces NECs technology concept of Knowledge Organization and data mining engines designed for it. They include text mining tool Survey Analyzer, key semantics mining, and topic analysis engine TopicAnalyzer. We briefly overview the principles of these engines and illustrate their applications to real domains.",
      "title": "17491 Data mining for knowledge organization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-23744450329&partnerID=40&md5=96f4f062a07d33fa66619bd0921fda4a"
    },
    {
      "abstract": "In this investigation, we propose a new method for text categorization (TC) based on a Bayesian approach with resolution of ambiguity. TC assigns weights to words whose meanings are ambiguous in the sense of synonymy and polysemy. We give weights to articles by examining dictionaries of thesaurus type and use dimensionality reduction to improve the quality of TC. We also utilize WordNet as a lexical reference tool and present some experiments to illustrate the effectiveness of our approach.  2005 Wiley Periodicals, Inc.",
      "title": "17494 Improving text categorization by resolving semantic ambiguity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-17144390195&partnerID=40&md5=6fa8854a99e13bc573c8e89473143d34"
    },
    {
      "abstract": "The Internet, together with the large amount of textual information available in document archives, has increased the relevance of information retrieval related tools. In this work we present an extension of the Gambal system for clustering and visualization of documents based on fuzzy clustering techniques. The tool allows to structure the set of documents in a hierarchical way (using a fuzzy hierarchical structure) and represent this structure in a graphical interface (a 3D sphere) over which the user can navigate. Gambal allows the analysis of the documents and the computation of their similarity not only on the basis of the syntactic similarity between words but also based on a dictionary (Wordnet 1.7) and latent semantics analysis.  2004 Elsevier Ltd. All rights reserved.",
      "title": "17496 Exploration of textual document archives using a fuzzy hierarchical clustering algorithm in the GAMBAL system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-10644269818&partnerID=40&md5=a57888c4136bfa49ee9ef821d8c705f3"
    },
    {
      "abstract": "Most Web and legacy paper-based documents are available in human comprehensible text form, not readily accessible to or understood by computer programs. Here, we investigate an approach to amalgamate XML technology with programming languages for representational purposes that can enhance traceability, thereby facilitating semiautomatic extraction and update. Specifically, we propose a modular technique to embed machine-processable semantics into a text document with tabular data via annotations, resulting sometimes in ill-formed XML fragments, and evaluate this technique vis a vis document querying, manipulation, and integration. The ultimate aim is to be able to author and extract human-readable and machine-comprehensible parts of a document hand in hand and keep them side by side.  2005 IEEE.",
      "title": "17497 On embedding machine-processable semantics into documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944433226&partnerID=40&md5=28af0ce6ef6547bba9e2ee5d5d42cae0"
    },
    {
      "abstract": "The volume of biomedical literature is increasing at such a rate that it is becoming difficult to locate, retrieve and manage the reported information without text mining, which aims to automatically distill information, extract facts, discover implicit links and generate hypotheses relevant to user needs. Ontologies, as conceptual models, provide the necessary framework for semantic representation of textual information. The principal link between text and an ontology is terminology, which maps terms to domain-specific concepts. This paper summarises different approaches in which ontologies have been used for text-mining applications in biomedicine.  Henry Stewart Publications.",
      "title": "17507 Text mining and ontologies in biomedicine: Making sense of raw text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-27744487007&partnerID=40&md5=a78844d1d1693db5ced6b48672448d6a"
    },
    {
      "abstract": "Enabling navigation via a hierarchy of conceptually related multilingual documents constitutes the fundamental support to global knowledge discovery. This requirement of organizing multilingual document by concepts makes the goal of supporting global knowledge discovery a concept-based multilingual text categorization task. In this paper, intelligent methods for enabling concept-based hierarchical multilingual text categorization using neural networks are proposed. First, a universal concept space, encapsulating the semantic knowledge of the relationship between all multilingual terms and concepts, which is required by concept-based multilingual text categorization, is generated using a self-organizing map. Second, a set of concept-based multilingual document categories, which acts as the hierarchical backbone of a browseable multilingual document directory, are generated using a hierarchical clustering algorithm. Third, a concept-based multilingual text classifier is developed using a 3-layer feed-forward neural network to facilitate the concept-based multilingual text categorization.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17509 A neural network model for hierarchical multilingual text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-24944509636&partnerID=40&md5=f45ab8aef5ebdb39ce4b50c0128e59a6"
    },
    {
      "abstract": "While classic information retrieval methods return whole documents as a result of a query, many information demands would be better satisfied by fine-grain access inside the documents. One way to support this goal is to make the semantics of small document regions explicit, e.g. as XML labels, so that query engines can exploit them. To this purpose, the topics of the small document regions must be discovered from the texts",
      "title": "17511 RELFIN - Topic discovery for ontology enhancement and annotation",
      "url": ""
    },
    {
      "abstract": "Query expansion techniques are used to find the desired set of query terms to improve retrieval performance. One of the limitations with the query expansion techniques is that a query is often expanded only by the linguistic features of terms. This paper presents a novel semantic query expansion technique that combines association rules with ontologies and information retrieval techniques. We propose to use the association rule discovery to find good candidate terms to improve the retrieval performance. These candidate terms are automatically derived from collections and added to the original query. Our method is differentiated from others in that 1) it utilizes the semantics as well as linguistic properties of unstructured text corpus and 2) it makes use of contextual properties of important terms discovered by association rules. Experiments conducted on a subset of TREC collections give quite encouraging results. We achieve from 15.49% to 20.98% improvement in term of P@20 with TREC5 ad hoc queries.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17512 Semantic query expansion combining association rules with ontologies and information retrieval techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-26844573379&partnerID=40&md5=e6e81adee3c76e07480ace87eafe111d"
    },
    {
      "abstract": "Recent studies reveal that associative classification can achieve higher accuracy than traditional approaches. The main drawback of this approach is that it generates a huge number of rules, which makes it difficult to select a subset of rules for accurate classification. In this study, we propose a novel association-based approach especially suitable for text classification. The approach first builds a classifier through a 2-PS (Two-Phase) method. The first phase aims for pruning rules locally, i.e., rules mined within every category are pruned by a sentence-level constraint, and this makes the rules more semantically correlated and less redundant. In the second phase, all the remaining rules are compared and selected with a global view, i.e., training examples from different categories are merged together to evaluate these rules. Moreover, when labeling a new document, the multiple sentence-level appearances of a rule are taken into account. Experimental results on the well-known text corpora show that our method can achieve higher accuracy than many well-known methods. In addition, the performance study shows that our method is quite efficient in comparison with other classification methods.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17515 2-PS based associative text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-26844497603&partnerID=40&md5=bbf33351a4cce22889eadff9f23f0ed0"
    },
    {
      "abstract": "The tools to analyze and visualize information from multiple, heterogeneous sources have often relied on innovations in statistical methods. The results from purely statistical methods, however, overlook relevant semantic features present within natural language and text-based information. Emerging research in ontology languages (e.g. RDF, RDFS, SUO-KIF, and OWL) offers promising avenues for overcoming these limitations by leveraging existing and future libraries of meta-data and semantic mark-up. Using semantic features (e.g. hypernyms, meronyms, synonyms, etc.) encoded in ontology languages, methods such as keyword search and clustering can be augmented to analyze and visualize documents at conceptually richer levels. We present findings from a hierarchical clustering system modified for ontological indexing and run on a topic-centric test collection of documents each with fewer than 200 words. Our findings show that ontologies can impose a complete interpretation or subjective clustering onto a document set that is at least as good as meta-word search.",
      "title": "17517 Using ontology in hierarchical information clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-27544437912&partnerID=40&md5=da7d43d56f5053f679ace606da4016fa"
    },
    {
      "abstract": "Semantic Web provides tools for expressing information in a machine accessible form where agents (human or software) can understand it. Ontology is required to describe the semantics of concepts and properties used in web documents. Ontology is needed to describe products, services, processes and practices in any e-commerce application. Ontology plays an essential role in recognizing the meaning of the information in Web documents. This paper attempts to deploy these concepts in an e-law application. E-laws ontology has been built using existing resources. It has been shown that extracting concepts is less hard than building relationships among them. A new algorithm has been proposed to reduce the number of relationships, so the domain knowledge expert (i.e. lawyer) can refine these relationships.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17524 Building e-Laws ontology: New approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646688702&partnerID=40&md5=36cc0044b967c21177a96359c9a8b51d"
    },
    {
      "abstract": "Document keyphrases provide semantic metadata characterizing documents and producing an overview of the content of a document. They can be used in many text-mining and knowledge management related applications. This paper describes a Keyphrase Identification Program (KIP), which extracts document keyphrases by using prior positive samples of human identified domain keyphrases to assign weights to the candidate keyphrases. The logic of our algorithm is: the more keywords a candidate keyphrase contains and the more significant these keywords are, the more likely this candidate phrase is a keyphrase. To obtain prior positive inputs, KIP first populates its glossary database using manually identified keyphrases and keywords. It then checks the composition of all noun phrases of a document, looks up the database and calculates scores for all these noun phrases. The ones having higher scores will be extracted as keyphrases.",
      "title": "17525 Domain-specific keyphrase extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745775246&partnerID=40&md5=fd5c257558cef213dd9919887c458006"
    },
    {
      "abstract": "The Semantic Web has emerged to replace the World Wide Web (WWW or the Web) as the unique platform for information sharing. Applications such as e-commerce will be and could be plausible only if we can annotate the Web pages with their semantics. For newly developed Semantic Web resources, such annotation can be done manually or by help of some authoring tools. However, it is not practical to semantically annotating existing Web pages due to the gigantic amount of them. To overcome this difficulty, we propose a machine learning approach to automatically generate semantic metadata for Web pages. The proposed automated process adopts the self-organizing map algorithm to cluster training Web pages and conducts a text mining process to discover some semantic descriptions about the Web pages. Preliminary experiments show that our method may generate semantically relevant metadata for the Web pages.  2005 IEEE.",
      "title": "17528 Automatic metadata generation for Web pages using a text mining approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33845367174&partnerID=40&md5=59e724c822276ad3f9a1b18bf5b8cc52"
    },
    {
      "abstract": "Traditional text mining systems employ shallow parsing techniques and focus on concept extraction and taxonomic relation extraction. This paper presents a novel system called CRCTOL for mining rich semantic knowledge in the form of ontology from domain-specific text documents. By using a full text parsing technique and incorporating both statistical and lexico-syntactic methods, the knowledge extracted by our system is more concise and contains a richer semantics compared with alternative systems. We conduct a case study wherein CRCTOL extracts ontological knowledge, specifically key concepts and semantic relations, from a terrorism domain text collection. Quantitative evaluation, by comparing with a state-of-the-art ontology learning system known as Text-To-Onto, has shown that CRCTOL produces much better precision and recall for both concept and relation extraction, especially from sentences with complex structures.  2005 IEEE.",
      "title": "17529 Mining ontological knowledge from domain-specific text documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34548548969&partnerID=40&md5=07706986a93e26c5da5547f06b73426c"
    },
    {
      "abstract": "The introduction of hierarchical thesauri (HT) that contain significant semantic information, has led researchers to investigate their potential for improving performance of the text classification task, extending the traditional bag of words representation, incorporating syntactic and semantic relationships among words. In this paper we address this problem by proposing a Word Sense Disambiguation (WSD) approach based on the intuition that word proximity in the document implies proximity also in the HT graph. We argue that the high precision exhibited by our WSD algorithm in various humanly-disambiguated benchmark datasets, is appropriate for the classification task. Moreover, we define a semantic kernel, based on the general concept of GVSM kernels, that captures the semantic relations contained in the hierarchical thesaurus. Finally, we conduct experiments using various corpora achieving a systematic improvement in classification accuracy using the SVM algorithm, especially when the training set is small.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17531 Word sense disambiguation for exploiting hierarchical thesauri in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646432735&partnerID=40&md5=d40a94902b9bb2c3df6851b79bd01929"
    },
    {
      "abstract": "Temporal expression contains crucial temporal information in texts. Understanding its temporal semantics is important in many NLP applications, such as information extraction, document summarization and question answering. Temporal expression normalization involves mapping from the different classes of expressions to the values of certain temporal attributes. A temporal expression may belong to one or more classes, but each class of expressions normally shares the same mapping procedure. In this paper, we explore the possibility of applying multi-label classification techniques in the context of temporal expression normalization. More specifically, two models, named independent binary classification model and compared binary classification model, are evaluated, compared and analyzed. Once the possible class(es) of a temporal expression is determined, the corresponding mapping rules are called to transform it into the corresponding attribute(s). Experiments on a substantiate data collection show that, based on the result of machine learning classification, the performance of temporal expression normalization is comparable with that of deliberate rule set.  2005 IEEE.",
      "title": "17533 Normalizing Chinese temporal expressions with multi-label classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33847316959&partnerID=40&md5=ec140d3a48eeb66b61b1dd4c368670c1"
    },
    {
      "abstract": "This paper studies the problem of mining relational data hidden in natural language text. In particular, it approaches the relation classification problem with the strategy of transductive learning. Different algorithms are presented and empirically evaluated on the ACE corpus. We show that transductive learners exploiting various lexical and syntactic features can achieve promising classification performance. More importantly, transductive learning performance can be significantly improved by using an induced similarity function.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17534 Mining inter-entity semantic relations using improved transductive learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645972570&partnerID=40&md5=87a3f88cb625887935fda2c09fd020e7"
    },
    {
      "abstract": "Correct and efficient text classification is a major challenge in todays world of rapidly increasing amount of accessible electronic text data. Kohonen networks have been applied to document classification with comparable success to other document clustering methods. An important challenge is to devise text similarity metrics that can improve the performance of text classification Kohonen networks by integrating more semantic information into the metric. Here we propose an augmented metric for text similarity that is based on the comparison of word consecutiveness graphs of documents. We show that using the proposed augmented similarity metric Kohonen networks perform better than Kohonen networks using usual Euclidean distance metric comparison of word frequency vectors. Our results indicate that word consecutiveness graph comparison includes more semantic information into the text similarity measure improving text classification performance.",
      "title": "17535 Kohonen networks with graph-based augmented metrics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890309026&partnerID=40&md5=ecdedd7c49f0b0595ea07494efe614d3"
    },
    {
      "abstract": "The similarity measure is a crucial step in many machine learning problems. The traditional cosine similarity suffers from its inability to represent the semantic relationship of terms. This paper explores the kernel-based similarity measure by using term clustering. An affinity matrix of terms is constructed via the co-occurrence of the terms in both unsupervised and supervised ways. Normalized cut is employed to do the clustering to cut off the noisy edges. Diffusion kernel is adopted to measure the kernel-like similarity of the terms in the same cluster. Experiments demonstrate our methods can give satisfactory results, even when the training set is small.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17538 Using term clustering and supervised term affinity construction to boost text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-26944476987&partnerID=40&md5=26845417d8937d83057dcc122ce7e86f"
    },
    {
      "abstract": "In the world of text document classification, the most general case is that in which a document can be classified into more than one category, the multi-label problem. This paper investigates the performance of two document classification systems applied to the task of multi-class multi-label document classification. Both systems consider the pattern of co-occurrences in documents of multiple categories. One system is based on a novel sequential data representation combined with a kNN classifier designed to make use of sequence information. The other is based on the Latent Semantic Indexing analysis combined with the traditional kNN classifier. The experimental results show that the first system performs better than the second on multi-labeled documents, while the second performs better on uni-labeled documents. Performance therefore depends on the dataset applied and the objective of the application.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17539 Evaluation of two systems on multi-class multi-label document classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-26944491460&partnerID=40&md5=9c9f27f85adccfe577b4fb5a949c8bc7"
    },
    {
      "abstract": "We present a method for classifying texts automatically, based on their subjective content. We apply a standard method for calculating semantic orientation (Turney 2002), and expand it by giving more prominence to certain parts of the text, where we believe most subjective content is concentrated. We also apply a linguistic classification of Appraisal and find that it could be helpful in distinguishing different types of subjective texts (e.g., movie reviews from consumer product reviews). Copyright  2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17542 Analyzing Appraisal automatically",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-32944460134&partnerID=40&md5=029783c3f22fa6c47501733282f11c38"
    },
    {
      "abstract": "In this paper we present an unsupervised model for learning arbitrary relations between concepts of a molecular biology ontology for the purpose of supporting text mining and manual ontology building. Relations between named-entities are learned from the GENIA corpus by means of several standard natural language processing techniques. An in-depth analysis of the output of the system shows that the model is accurate and has good potentials for text mining and ontology building applications.",
      "title": "17543 Unsupervised learning of semantic relations between concepts of a molecular biology ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84880761875&partnerID=40&md5=7375e89070a3fec59e903adbbb8d0626"
    },
    {
      "abstract": "Text clustering is an effective way of not only organizing textual information, but discovering interesting patterns. Most existing methods, however, suffer from two main drawbacks",
      "title": "17544 On the performance of feature weighting K-means for text subspace clustering",
      "url": ""
    },
    {
      "abstract": "Vertical search engines and web portals are gaining ground over the general-purpose engines due to their limited size and their high precision for the domain they cover. The number of vertical portals has rapidly increased over the last years, making the importance of a topic-driven (focused) crawler evident. In this paper, we develop a latent semantic indexing classifier that combines link analysis with text content in order to retrieve and index domain specific web documents. We compare its efficiency with other well-known web information retrieval techniques. Our implementation presents a different approach to focused crawling and aims to overcome the size limitations of the initial training data while maintaining a high recall/precision ratio.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17545 Focused crawling using latent semantic indexing - An application for vertical search engines",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645970163&partnerID=40&md5=ef7127ae3f5c9ef2b0b4addb5670f162"
    },
    {
      "abstract": "Little work to date in sentiment analysis (classifying texts by positive or negative orientation) has attempted to use fine-grained semantic distinctions in features used for classification. We present a new method for sentiment classification based on extracting and analyzing appraisal groups such as very good or not terribly funny. An appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on Appraisal Theory. Semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers. We classify movie reviews using features based upon these taxonomies combined with standard bag-of-words features, and report state-of-the-art accuracy of 90.2%. In addition, we find that some types of appraisal appear to be more significant for sentiment classification than others. Copyright 2005 ACM.",
      "title": "17548 Using appraisal groups for sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646880507&partnerID=40&md5=1c33b8aadcc167fd04f1f48b8d6182c9"
    },
    {
      "abstract": "In this paper, we set a shielded level in a concept tree to use both the concept attributes from a semantic dictionary and the Chinese words to make the feature set. After comparing the weight theories and classification precise, of the eight methods, we give a new selection method, the CHI-MCOR weight method, which is derived from two normal methods which present well in our experiments. Our former experiment result shows that if we can set a proper shielded level, we can not only reduce the feature dimension but also improve the classification precise. The later result shows that the combined weight method makes a good balance between the fuzzy words which have a high occurrence and the dividing words which have a middle or low occurrence, and the classification precise is higher than any one of the weight methods.  2005 IEEE.",
      "title": "17549 A combined weight method in automatic classification of chinese text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33847105528&partnerID=40&md5=7c06768a2589a9430c055dd9ba8e86c7"
    },
    {
      "abstract": "The semantic web envisions an Internet where data can be used by applications just as easily as it can be by humans. Ontologies are a key building block of the semantic web, and can enable applications to have a shared understanding of data, and to be semantically interoperable. However, ontologies from heterogeneous sources may use different terms to represent the same concept. This can present problems for applications which are required to work interoperably with independently produced ontologies. Manually determining semantically equivalent terms between ontologies is a laborious and error-prone process. In this paper, we present an architecture which uses machine-learning techniques to produce mappings between semantically equivalent terms in heterogeneous ontologies. The architecture combines the results of several machine-learning algorithms, in order to be effective with a wider range of data than if a single algorithm were used.  2005 IEEE.",
      "title": "17552 Mediating between heterogeneous ontologies using schema matching techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745725530&partnerID=40&md5=f83d489210e0b82d83f627920a86dcd2"
    },
    {
      "abstract": "F-measure is an indicator used since 25 years to evaluate classification algorithms in textmining, from precision and recall. For classification and information retrieval, some ones prefer to use the break even point. Nevertheless, these measures have some inconvenient: they use a binary logic and dont allow applying a user (judge) assessment. This paper proposes a new approach of evaluation. First, we distinguish classification and categorization from a semantic point of view. Then, we introduce a new measure: the K-measure, which is an overall of F-measure and break even point, and allows applying user requirements. Finally, we propose a methodology for evaluation.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17553 Evaluation and NLP",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745290014&partnerID=40&md5=9f4bb47b5d3cfd242e7ee04bc6c1e52a"
    },
    {
      "abstract": "Different from familiar clustering objects, text documents have sparse data spaces. A common way of representing a document is as a bag of its component words, but the semantic relations between words are ignored. In this paper, we propose a novel document representation approach to strengthen the discriminative feature of document objects. We replace terms of documents with concepts in WordNet and construct a model named Concept CHain Model(CCHM) for document representation. CCHM is applied in both partitioning and agglomerative clustering analysis. Hierarchical clustering processes in different levels of concept chains. The experimental evaluation on textual data sets demonstrates the validity and efficiency of CCHM. The results of experiments with concept show the superiority of our approach in hierarchical clustering.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17554 Concept chain based text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646826682&partnerID=40&md5=29c8d4a152ca17fed0fdb5e3d0570dc6"
    },
    {
      "abstract": "Understanding contextual behavior is very important in order to develop a context-aware retrieval system. This paper discusses the philosophy behind the development of the Evolutionary Behavior Of Textual Semantics (EBOTS) system. The EBOTS system is retrieval oriented knowledge representation and management system. This paper proposes a formal model of correlation that can be combined with traditional local and global weighing schemes. Intuitive contextual behavior is studied as apart of proposed research work. Context retrieval based on semantic knowledge allows abstract queries to be defined, instead of exact word-based queries. The results of the context retrieval for a classics and Time dataset using the EBOTS system have been discussed in this paper. The paper makes a contribution to the semantic knowledge representation and retrieval algorithms.  2005 IEEE.",
      "title": "17555 Learning contextual behavior of text data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33847303041&partnerID=40&md5=d2b221b0fdf4b349a0af429bbb9d056f"
    },
    {
      "abstract": "Non-negative Matrix Factorization (NMF, [5]) and Probabilistic Latent Semantic Analysis (PLSA, [4]) have been successfully applied to a number of text analysis tasks such as document clustering. Despite their different inspirations, both methods are instances of multinomial PCA [1]. We further explore this relationship and first show that PLSA solves the problem of NMF with KL divergence, and then explore the implications of this relationship.  2005 ACM.",
      "title": "17556 Relation between PLSA and NMF and implications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84885621082&partnerID=40&md5=497e24d49a54b29539b8fb7d35f28e5b"
    },
    {
      "abstract": "We propose a new text mining system which extracts characteristic contents from given documents. We define Key semantics as characteristic sub-structures of syntactic dependencies in the given documents, and consider the following three tasks in this paper: 1) Key semantics extraction: extracting characteristic syntactic dependency structures not only as ordered trees but also as unordered trees and free trees, 2) Redundancy reduction: from the result of extraction, deleting redundant dependency structures such as sub-structures or equivalent structures of the others, and 3) Phrase/sentence reconstruction: generating a phrase or sentence in a natural language corresponding to the extracted structure. Our system is a combination of natural language processing techniques and tree mining techniques. The system consists of the following five units: 1) syntactic dependency analysis unit, 2) input filters, 3) characteristic ordered subtree extraction unit, 4) output filters, and 5) phrase/sentence reconstruction unit. Although ordered trees are extracted in the third unit, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the input filters is/are applied in the second step. The output filters delete redundant trees from the extraction, result for efficient knowledge discovery. Finally, phrases or sentences corresponding to the extracted subtrees are reconstructed by utilizing the input documents. We demonstrate the validity of our system by showing experimental results using real data collected at a help desk and TDT pilot corpus. Copyright 2005 ACM.",
      "title": "17560 Key semantics extraction by dependency tree mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-32344452294&partnerID=40&md5=5f558eff2d9bc0ddc3fb89ce2d6312e8"
    },
    {
      "abstract": "Sentiment classification is a recent subdiscipline of text classification which is concerned not with the topic a document is about, but with the opinion it expresses. It has a rich set of applications, ranging from tracking users opinions about products or about political candidates as expressed in online forums, to customer relationship management. Functional to the extraction of opinions from text is the determination of the orientation of subjective terms contained in text, i.e. the determination of whether a term that carries opinionated content has a positive or a negative connotation. In this paper we present a new method for determining the orientation of subjective terms. The method is based on the quantitative analysis of the glosses of such terms, i.e. the definitions that these terms are given in on-line dictionaries, and on the use of the resulting term representations for semi-supervised term classification. The method we present outperforms all known methods when tested on the recognized standard benchmarks for this task. Copyright 2005 ACM.",
      "title": "17561 Determining the semantic orientation of terms through gloss classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646872279&partnerID=40&md5=27cdd171b7119396486bf30fa44cd85f"
    },
    {
      "abstract": "A significant part of medical data remains stored as unstructured texts. Semantic search requires introduction of markup tags. Experts use their background knowledge to categorize new documents, and knowing category of these documents disambiguate words and acronyms. A model of document similarity that includes a priori knowledge and captures intuition of an expert, is introduced. It has only a few parameters that may be evaluated using linear programming techniques. This approach applied to categorization of medical discharge summaries provided simpler and much more accurate model than alternative text categorization approaches.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17562 Medical document categorization using a priori knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646201919&partnerID=40&md5=bb2ad31f1a12d9f3d30c1678360e2169"
    },
    {
      "abstract": "Measuring semantic nearness of documents is important for accurate information retrieval, automated text categorization and classification. Inspired by the observation that text documents contain semantically coherent set of ideas/topics, this paper presents the design and experimental evaluation of a method to represent a text document as a set of concepts. Based on this, we propose a method to measure semantic nearness of texts. Our method makes use of WordNet which is a lexico-semantic network of words. We bypass word sense disambiguation. In order to show the effectiveness of our representation of texts, we compare experimental results of text classification and clustering with the results of classification and clustering with standard techniques.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17565 Text similarity measurement using concept representation of texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33646746823&partnerID=40&md5=456d32dfaa39bdfe240bbb1a5fffeffb"
    },
    {
      "abstract": "In this paper we review the main intermediate forms proposed in text mining, and we briey study some fuzzy counterparts. The concept of intermediate form applies to any knowledge representation employed to represent in a structured way the semantic content of a text corpus. Intermediate forms play a central role in the text mining process since it is necessary to transform plain text into a form in order to apply mining techniques. Since the semantics of text use to be imprecise, the use of fuzzy intermediate forms seems to be a natural solution in many cases. We discuss about fuzzy intermediate forms and the corresponding fuzzy text mining techniques that may be applicable on them.",
      "title": "17569 Text mining: Intermediate forms for knowledge representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33746221094&partnerID=40&md5=76c4fffcff2162541df40585a6d2f3f2"
    },
    {
      "abstract": "Enterprise Interoperability is the ability of different organizations to work together and exchange documents, data, services, best practices, etc. A preliminary step towards achieving this goal is the assessment of a common vocabulary of relevant domain concepts, a sort of initial, semi-formal, representation of shared domain knowledge. in this paper we describe an experiment, conducted within the INTEROP Network of Excellence (NoE), aimed at automatically extracting a thesaurus of interoperability terms from the web and a corpus of available documents. Statistical and text mining techniques have been used to extract a glossary of relevant terms and term definitions, as well as a preliminary taxonomic structure of the glossary. The result has been evaluated by a team of partners belonging to the NoE. Copyright  2005 IFAC.",
      "title": "17571 Automatic acquisition of a thesaurus of interoperability terms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-79960737115&partnerID=40&md5=f711792160742e956d013d10f8d0e0e1"
    },
    {
      "abstract": "The human solution to the understanding of text document requires sophisticated natural language processing, which is not currently an option for the computerbased solution. Instead, statistical keyword-based approaches have predominated. In this paper we describe an approach to capturing the semantics of text-based document by creating an asymmetric word similarity matrix and applying this to measure document similarity for document clustering.",
      "title": "17572 Asymmetric word similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84883305032&partnerID=40&md5=808164b8f48f363792233142b741c490"
    },
    {
      "abstract": "The semantic web is expected to have an impact at least as big as that of the existing HTML based web, if not greater. However, the challenge lays in creating this semantic web and in converting existing web information into the semantic paradigm. One of the core technologies that can help in migration process is automatic markup, the semantic markup of content, providing the semantic tags to describe the raw content. This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (Trainable Extraction Grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (Stochastic Context Free Grammar) based extraction language, and training them using an annotated corpus. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amount of training data. We also demonstrate the robustness of our system under conditions of poor training data quality. This makes the system very suitable for converting legacy web pages to semantic web pages.",
      "title": "17573 Hybrid semantic tagging for information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77953048400&partnerID=40&md5=fbb90c6fd5c11edfbd0d5782af4e7022"
    },
    {
      "abstract": "Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management. Beside textual features, the hierarchical structure of directories reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this information into text mining processes. In this paper, we propose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class proximity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCVI) benchmark and the Swissprot protein database.  2005 IEEE.",
      "title": "17574 Hierarchy-regularized latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34548570541&partnerID=40&md5=2cbbe8a51ee275ea1b4744d2c52765e0"
    },
    {
      "abstract": "Biological information embedded within the large repository of unstructured or semi-structured text documents can be extracted more efficiently through effective semantic analysis of the texts in collaboration with structured domain knowledge. The GENIA corpus houses tagged MEDLINE abstracts, manually annotated according to the GENIA ontology, for this purpose. However, manual tagging of all texts is impossible and special purpose storage and retrieval mechanisms are required to reduce information overload for users. In this paper we have proposed an ontology-based biological Information Extraction and Query Answering (BIEQA) system that has four components: an ontology-based tag analyzer for analyzing tagged texts to extract Biological and lexical patterns, an ontology-based tagger for tagging new texts, a knowledge base enhancer which enhances the ontology, and incorporates new knowledge in the form of biological entities and relationships into the knowledge base, and a query processor for handling user queries.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17575 An ontology-based pattern mining system for extracting information from biological texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645984852&partnerID=40&md5=20ca10a20378aa50437bd27fa79478d9"
    },
    {
      "abstract": "The recognition of biomedical concepts in natural text (named entity recognition, NER) is a key technology for automatic or semi-automatic analysis of textual resources. Precise NER tools are a prerequisite for many applications working on text, such as information retrieval, information extraction or document classification. Over the past years, the problem has achieved considerable attention in the bioinformatics community and experience has shown that NER in the life sciences is a rather difficult problem. Several systems and algorithms have been devised and implemented. In this paper, the problems and resources in NER research are described, the principal algorithms underlying most systems sketched, and the current state-of-the-art in the field surveyed.  Henry Stewart Publications.",
      "title": "17579 What makes a gene name? Named entity recognition in the biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-30744459862&partnerID=40&md5=0c586aad533d47c5daa7e26b1ad555f3"
    },
    {
      "abstract": "Design knowledge abstracted from cases is important for designers. This paper is aimed at building an agent to detect those correlations between explicit features of design cases and relevant design problems. Using the data mining algorithm, we have accumulated a list of keywords about design problems and their relevant concept from textual information of a case library, and established their semantic ontology by clustering their semantic and sentence structural relations from previous studies. Meanwhile, we have also established another hierarchical ontology of explicit design case features by applying design domain knowledge. Then, through mapping semantic relations of relevant keywords between two ontologies, the system will become more sensitive to the correlations of design case features and relevant design problems. Finally, a graphical interface is built to visualize these correlations and help users to recognize useful design knowledge cached in design cases.",
      "title": "17581 Ontology based design knowledge detective agent",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84875257839&partnerID=40&md5=60e70027c1b7ce5c44c1fc4b3d366f0f"
    },
    {
      "abstract": "We present experiments on modifying the semantic orientation of the near-synonyms in a text. We analyze a text into an interlingual representation and a set of attitudinal nuances, with particular focus on its near-synonyms. Then we use our text generator to produce a text with the same meaning but changed semantic orientation (more positive or more negative) by replacing, wherever possible, words with near-synonyms that differ in their expressed attitude. Copyright  2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17583 Generating more-positive and more-negative text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-32944474852&partnerID=40&md5=3416889f0bffb15c7a969cc429cf7fa6"
    },
    {
      "abstract": "We present a system for textual inference (the task of inferring whether a sentence follows from another text) that uses learning and a logical-formula semantic representation of the text. More precisely, our system begins by parsing and then transforming sentences into a logical formula-like representation similar to the one used by (Harabagiu et al., 2000). An abductive theorem prover then tries to find the minimum cost set of assumptions necessary to show that one statement follows from the other. These costs reflect how likely different assumptions are, and are learned automatically using information from syntactic/semantic features and from linguistic resources such as WordNet. If one sentence follows from the other given only highly plausible, low cost assumptions, then we conclude that it can be inferred. Our approach can be viewed as combining statistical machine learning and classical logical reasoning, in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving. We give experimental results from the recent PASCAL RTE 2005 challenge competition on recognizing textual inferences, where a system using this inference algorithm achieved the highest confidence weighted score. Copyright  2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "17584 Robust textual inference via learning and abductive reasoning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-29344457452&partnerID=40&md5=bc0ccbb9b8147e94e58ac13f27f72e18"
    },
    {
      "abstract": "When reading A text document, current knowledge workers have to manually look for additional information: they decide on concepts to further explore, access a web search engine, form a query and compose the search results with the original document. In this paper, we propose a framework aiming at transforming electronic documents from passive pieces of information into dynamic entities that search and retrieve relevant information, becoming active partners in the knowledge workers tasks. For this purpose, recent trends in natural language processing, semantic knowledge representation and agent-based technologies are combined.  2005 IEEE.",
      "title": "17585 eDocuments intelligent enrichment from distributed knowledge resources",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33947319201&partnerID=40&md5=f4761a297e789dc5c65c83284ef1e20e"
    },
    {
      "abstract": "We propose a framework for constructing semantic features for textual documents from tackling the problem of abstracting information in document representation. Semantic patterns are discovered from ontology-derived texts which provide rich contextual information regarding the concepts. The patterns represent the syntactic and semantic relationships implied in the textual documents which can help in extracting and representing the underlying concepts in texts. We also investigate the significance of using the patterns in automatic summarization of biomedical articles.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17589 Discovering patterns from ontology-derived texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33744915565&partnerID=40&md5=7b5f38b0646ef0db8e9dd29225e64b7d"
    },
    {
      "abstract": "In recent researches, an anchor text and texts near the anchor in the original pages are used for the purpose of summarizing or classifying the target pages. No research considers these text parts as the peoples views about the target page and tries to classify them. In our research, we try to extract the descriptions about the target pages from the original pages. We then classify these descriptions. In other manner, we classify the peoples views about the target pages into the hierarchical directory. We call the result the Multi-Peoples Views Web Directory. In this paper, we concentrate on describing our preliminary survey of semantic text portions. We also explain our method for extracting semantic text portions. The experimental results show that our method achieves high accuracy.  2005 IEEE.",
      "title": "17590 Survey of semantic text portion for building Web Directory from peoples views",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33645299146&partnerID=40&md5=0a3cc208d763e28ef26ec4d97a381210"
    },
    {
      "abstract": "Comprehension of semantic meaning is at the heart of modern natural language processing (NLP). Currently, research in statistical NLP has focused primarily on the statistical representation of lexical combinational occurrences. Due to the limitations of current computer technology, however, representing a lexical combination is restricted to a finite length. As such, we focus attention on obtaining approximate but simpler and satisfactory solutions through soft computing techniques",
      "title": "17591 Semantic context classification by means of fuzzy set theory",
      "url": ""
    },
    {
      "abstract": "Text information processing depends critically on the proper text representation. A common and Naive way of representing a document is a bag of its component words [1], but the semantic relations between words are ignored, such as synonymy and hypernymy-hyponymy between nouns. This paper presents a model for representing a document in terms of the synonymy sets (synsets) in WordNet [2]. The synsets stand for concepts corresponding to the words of the document. The Vector Space Model describes a document as orthogonal term vectors. We replace terms with concepts to build Concept Vector Space Model (CVSM) for the training set. Our experiments on the Reuters Corpus Volume I (RCV1) dataset have shown that the result is satisfactory.  Springer-Verlag Berlin Heidelberg 2005.",
      "title": "17592 A comparative study for WordNet guided text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33745613838&partnerID=40&md5=187ae473e2e600531b8eb32e1bf73e1d"
    },
    {
      "abstract": "Latent Semantic Indexing (LSI) is an important method for Information Retrieval (IR), in which we can automatically transform the original textual data to a smaller semantic space by take advantage of some of the implicit or latent higher-order structure in associations of words with customized objects, and it also has been successfully applied to text classification. LSI can resolve the problems of polysemy and synonymy, and can reduce noise in the raw document-term matrix. But LSI is not an optimal approach to text classification. Because LSI is a complete unsupervised method which ignores categories discrimination, it often drops the performance of text classification when it is applied to the whole training documents [1]. In this paper, in order to prevent the spreading of the unsolicited email and harmful message, under multi-languages (Chinese and English) circumstance we have developed a system based on customized email topic being filtered, and we represented topic in Latent Semantic model, and abstract features from predefined email categories and document categories in LSI method. It is able to filter and recognize customized or special unwanted Chinese and English emails in positive examples supervised learning approach. We propose an improving LSI to improve the classification performance by a separate Single Value Decomposition (SVD) on the transformed local region of each category. We apply Support Vector Machine (SVM) classification method to recognize and filter email based on text classifier. The result of the experiment showed that our approach is very effective and has a good filtering performance. 2005 IEEE.",
      "title": "17598 Support vector machine for customized email filtering based on improving latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-28444451558&partnerID=40&md5=6cd858a8ae27b5bfef5ea61a5035f3ed"
    },
    {
      "abstract": "Text Classification (TC) is the task to automatically classify documents based on learned document features. Many popular TC models use simple occurrence of words in a document as features. They also commonly assume word occurrences to be statistically independent in their design. Although it is obvious that such assumption does not hold in general, these TC models have been robust and efficient in their task. Some recent studies have shown context-sensitive TC approaches, which take into consideration contexts in the form of word co-occurrences, have been able to perform better in general. On the other hand, there have been many studies in the use of complex linguistic or semantic features instead of simple word occurrences as features for information retrieval and classification tasks. While these complex features may intuitively have more relevance to the tasks concerned, results of these studies on their effectiveness have been mixed and not been conclusive. In this paper we present our investigation on the use of some complex linguistic features with context-sensitive TC method. Our experiment results show some potential advantages of such approach. 2005 IEEE.",
      "title": "17599 Using complex linguistic features in context-sensitive text classification techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-28444465551&partnerID=40&md5=9ea89c06664c0d33cd3246a330a6a0e8"
    },
    {
      "abstract": "Many existing text clustering algorithms overlook the semantic information between words and so they possess a lower accuracy of text similarity computation. A new text hybrid clustering algorithm (HCA) based on HowNet semantics has been proposed in this paper. It calculates the semantic similarity of words by using the words semantic concept description in HowNet and then combines it with the method of maximum weight matching of bipartite graph to calculate a semantic-based text similarity. Based on the new text similarity and by combining an improved genetic algorithm with k-medoids algorithm, HCA has been designed. The comparative experiments show that: 1) compared with two existing traditional clustering algorithms, HCA can get better quality and 2) when their text cosine similarity is replaced with the new semantic-based text similarity, all the qualities of the three clustering algorithms can be improved significantly.",
      "title": "17619 A Text Hybrid Clustering Algorithm Based on HowNet Semantics",
      "url": ""
    },
    {
      "abstract": "The language modeling approach is widely used to improve the performanceof text mining in recent years because of its solid theoretical foundation and empirical effectiveness. In essence, this approach centers on the issue of estimating an accurate model by choosing appropriate language models as well as smooth techniques. Semantic smoothing, which incorporates semantic and contextual information into the language models, is effective and potentially significant to improvethe performance of text mining. In this paper, we proposed a high-order structure to represent text data by incorporating background knowledge, Wikipedia. The proposed structure consists of three types of objects, term, document and concept. Moreover, we firstly combined the high-order co-clustering algorithm with the proposed model to simultaneously cluster documents, terms and concepts. Experimental results on benchmark data sets (20Newsgroups and Reuters-21578) have shown that our proposed high-order co-clustering on high-order structure outperforms the general co-clustering algorithm on bipartite text data, such as document-term, document-concept and document-(term+concept).",
      "title": "17622 High-Order Co-clustering Text Data on Semantics-Based Representation Model",
      "url": ""
    },
    {
      "abstract": "This paper provides a framework for optimally representing student written essays in a vector space, based upon Latent Semantic Analysis and instructor evaluated grades. Comparing student essays to an authoritative source, a ranking scheme is optimized that allows for aunique vector space representation on the unit circle. Once such a representation has been found, traditional methods of circular data analysis and inference can be applied, as we demonstrate.",
      "title": "17646 Optimal Text Space Representation of Student Essays Using Latent Semantic Analysis",
      "url": ""
    },
    {
      "abstract": "Chinese text automatic classification is an important research topic in the chinese information processing field. The chinese text classification method can be divided into two types: based on the extension of the classification method, and based on the semantic Web classification. The semantic Web is an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. So this paper present a algorithm of chinese text classification on semantic Web. After getting keywords from the Web text, we get rid of ambiguity of the keywords.Then we get the semantic concept of the keywords base on hownet. Lastly, we classify the text after we integrate all the keywords semantic concept. We present experiments on different data set which demonstrates more effectiveness of our algorithm than traditional algorithm. It has been tested that this approach had good effect.",
      "title": "17658 Study of Chinese Text Classification Algorithm on Semantic Web",
      "url": ""
    },
    {
      "abstract": "Automatic text classification is an area that has received a great dealof attention in recent research due to current growth of Internet, which has resulted in huge amount of information that has become a challengeto access efficiently. This paper describes an experimental result on how to create an automatic efficient and effective tool that is able to classify large documents quickly. Our method is built on lexical chain of linking significant words that are about a particular topic with the help of hypernym relation in WordNet. We have tested for the Indian language Sanskrit using SanskritNet and also extracting and scoring lexical chain considering with necessary design decisions.",
      "title": "17696 Semantic Based Text Classification Using WordNets: Indian Language Perspective",
      "url": ""
    },
    {
      "abstract": "This paper describes a new application of a text-mining algorithm to the text sources of bilingual parallel corpora. The ultimate task, being undertaken in the context of a Chinese-English machine translation project, will be to develop a language-neutral method to discover similar documents from multilingual text collections. Using a variation of automatic clustering techniques, which apply a neural net approach namely the Self-Organizing Maps (SOM), we have conducted several experiments to uncover associated documents based on Chinese-English bilingual parallel corpora, and a hybrid Chinese-English corpus. The experiments show some interesting results and a couple of potential ways for future work towards the field of multilingual information discovery. Besides, for exploring the impacts on linguistics issues with the machine learning approach to mining sensible linguistics elements from multilingual texts, we have examined the resulting term associations and text associations from the view of cross-lingual text similarity. To evaluate semantic relatedness of the mined bilingual texts, we applied a measure technique of semantic similarity in the resulting bilingual document clusters and word clusters. This paper presents algorithms that enable multilingual text mining based on the serf-organizing map (SCM) for automatically grouping similar multilingual texts (i.e. Chinese and English texts), along with a means in measuring their semantic similarity to resolve the difficulties of syntactic and semantic ambiguity in multilingual information access.",
      "title": "17710 Text mining of bilingual parallel corpora with a measure of semantic similarity",
      "url": ""
    },
    {
      "abstract": "In the present paper we present the implementation and application of a system for computer semantic processing of Greek verbs occurring in Greek Stock Market Texts. The system has been implemented with the programming language Prolog and it accomplishes functions like the automatic detection of circular definitions and the automatic transformation of definitions into special forms in which verbs that express basic concepts are used. The application of the system concerns the interface of a user with an Intelligent Information Extraction and Text Mining System, which is under construction. The Computationa lLexicon constructed supports the Deductive Mining of Knowledge from Texts about the behaviour of companies. The knowledge acquired is used for the answering of questions about the behaviour of companies from the texts using the ARISTA method.",
      "title": "17717 Greek verb semantic processing for stock market text mining",
      "url": ""
    },
    {
      "abstract": "Many connectionist language processing models have now reached a level of detail at which more realistic representations of semantics are required. In this paper we discuss the extraction of semantic representations from the word co-occurrence statistics of large text corpora and present a preliminary investigation into the validation and optimisation of such representations. We find that there is significantly more variation across the extraction procedures and evaluation criteria than is commonly assumed.",
      "title": "17718 Extracting semantic representations from large text corpora",
      "url": ""
    },
    {
      "abstract": "KRISP is a representation system and set of interpretation protocol sthat is used in the Sparser natural language understanding system to embody the meaning of texts and their pragmatic contexts. It is based on a denotational notion of semantic interpretation, where the phrases of a text are directly projected onto a largely pre-existing set of individuals and categories in a model, rather than first going through a level of symbolic representation such as a logical form. It defines a small set of semantic object types, grounded in the lambda calculus, and it supports the principle of uniqueness and supplies first class objects to represent partially-saturated relationships. KRISP is being used to develop a core set of concepts for such things as names, amounts, time, and modality, which are part of a few larger models for domains including `Whos News and joint ventures. It is targeted at the task of information `traction, emphasizing the need to relate entities mentioned in new texts to a large set of pre-defined entities and those read about in earlier articles or in the same article.",
      "title": "17720 KRISP - A REPRESENTATION FOR THE SEMANTIC INTERPRETATION OF TEXTS",
      "url": ""
    },
    {
      "abstract": "Purpose Negative life events, such as the death of a family member, an argument with a spouse or the loss of a job, play an important role in triggering depressive episodes. Therefore, it is worthwhile to develop psychiatric services that can automatically identify such events. This study describes the use of association language patterns, i.e., meaningful combinations of words (e.g., <",
      "title": "17788 Mining association language patterns using a distributional semantic model for negative life event classification ",
      "url": ""
    },
    {
      "abstract": "Text categorization is one of the most common themes in data mining and machine learning fields. Unlike structured data, unstructured text data is more difficult to be analyzed because it contains complicated both syntactic and semantic information. In this paper, we propose a two-level representation model (2RM) to represent text data, one is for representing syntactic information and the other is for semantic information. Each document, in syntactic level, is represented as a term vector where the value of each component is the term frequency and inverse document frequency. The Wikipedia concepts related to terms in syntactic level are used to represent document in semantic level. Meanwhile, we designed a multi-layer classification framework (MLCLA) to make use of the semantic and syntactic information represented in 2RM model. The {MLCLA} framework contains three classifiers. Among them, two classifiers are applied on syntactic level and semantic level in parallel. The outputs of these two classifiers will be combined and input to the third classifier, so that the final results can be obtained. Experimental results on benchmark data sets (20Newsgroups, Reuters-21578 and Classic3) have shown that the proposed 2RM model plus {MLCLA} framework improves the text classification performance by comparing with the existing flat text representation models (Term-based VSM, Term Semantic Kernel Model, Concept-based VSM, Concept Semantic Kernel Model and Term ",
      "title": "17789 A multi-layer text classification framework based on two-level representation model ",
      "url": ""
    },
    {
      "abstract": "Abstract In this paper we present a new semantic smoothing vector space kernel (S-VSM) for text documents clustering. In the suggested approach semantic relatedness between words is used to smooth the similarity and the representation of text documents. The basic hypothesis examined is that considering semantic relatedness between two text documents may improve the performance of the text document clustering task. For our experimental evaluation we analyze the performance of several semantic relatedness measures when embedded in the proposed (S-VSM) and present results with respect to different experimental conditions, such as: (i) the datasets used, (ii) the underlying knowledge sources of the utilized measures, and (iii) the clustering algorithms employed. To the best of our knowledge, the current study is the first to systematically compare, analyze and evaluate the impact of semantic smoothing in text clustering based on wisdom of linguists, e.g., WordNets, wisdom of crowds, e.g., Wikipedia, and wisdom of corpora, e.g., large text corpora represented with the traditional Bag of Words (BoW) model. Three semantic relatedness measures for text are considered",
      "title": "17790 Semantic smoothing for text clustering",
      "url": ""
    },
    {
      "abstract": "The creation and deployment of knowledge repositories for managing, sharing, and reusing tacit knowledge within an organization has emerged as a prevalent approach in current knowledge management practices. A knowledge repository typically contains vast amounts of formal knowledge elements, which generally are available as documents. To facilitate users navigation of documents within a knowledge repository, knowledge maps, often created by document clustering techniques, represent an appealing and promising approach. Various document clustering techniques have been proposed in the literature, but most deal with monolingual documents (i.e., written in the same language). However, as a result of increased globalization and advances in Internet technology, an organization often maintains documents in different languages in its knowledge repositories, which necessitates multilingual document clustering (MLDC) to create organizational knowledge maps. Motivated by the significance of this demand, this study designs a Latent Semantic Indexing (LSI)-based {MLDC} technique capable of generating knowledge maps (i.e., document clusters) from multilingual documents. The empirical evaluation results show that the proposed LSI-based {MLDC} technique achieves satisfactory clustering effectiveness, measured by both cluster recall and cluster precision, and is capable of maintaining a good balance between monolingual and cross-lingual clustering effectiveness when clustering a multilingual document corpus. ",
      "title": "17792 A Latent Semantic Indexing-based approach to multilingual document clustering ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0167923607001339"
    },
    {
      "abstract": "Text document clustering plays an important role in providing better document retrieval, document browsing, and text mining. Traditionally, clustering techniques do not consider the semantic relationships between words, such as synonymy and hypernymy. To exploit semantic relationships, ontologies such as WordNet have been used to improve clustering results. However, WordNet-based clustering methods mostly rely on single-term analysis of text",
      "title": "17793 Exploiting noun phrases and semantic relationships for text document clustering ",
      "url": ""
    },
    {
      "abstract": "Abstract The aim of cross impact analysis (CIA) is to predict the impact of a first event on a second. For organizations strategic planning, it is helpful to identify the impacts among organizations internal events and to compare these impacts to the corresponding impacts of external events from organizations competitors. For this, literature has introduced compared cross impact analysis (CCIA) that depicts advantages and disadvantages of the relationships between organizations events to the relationships between competitors events. However, {CCIA} is restricted to the use of patent data as representative for competitor events and it applies a knowledge structure based text mining approach that does not allow considering semantic aspects from highly unstructured textual information. In contrast to related work, we propose an internet based environmental scanning procedure to identify textual patterns represent competitors events. To enable processing of this highly unstructured textual information, the proposed methodology uses latent semantic indexing (LSI) to calculate the compared cross impacts (CCI) for an organization. A latent semantic subspace is built that consists of semantic textual patterns. These patterns are selected that represent organizations events. A web mining approach is used for crawling textual information from the internet based on keywords extracted from each selected pattern. This textual information is projected into the same latent semantic subspace. Based on the relationships between the semantic textual patterns in the subspace, {CCI} is calculated for different events of an organization. A case study shows that the proposed approach successfully calculates the {CCI} for technologies processed by a governmental organization. This enables decision makers to direct their investments more targeted. ",
      "title": "17795 Semantic compared cross impact analysis ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417413008683"
    },
    {
      "abstract": "In this paper, we develop a genetic algorithm method based on a latent semantic model (GAL) for text clustering. The main difficulty in the application of genetic algorithms (GAs) for document clustering is thousands or even tens of thousands of dimensions in feature space which is typical for textual data. Because the most straightforward and popular approach represents texts with the vector space model (VSM), that is, each unique term in the vocabulary represents one dimension. Latent semantic indexing (LSI) is a successful technology in information retrieval which attempts to explore the latent semantics implied by a query or a document through representing them in a dimension-reduced space. Meanwhile, {LSI} takes into account the effects of synonymy and polysemy, which constructs a semantic structure in textual data. {GA} belongs to search techniques that can efficiently evolve the optimal solution in the reduced space. We propose a variable string length genetic algorithm which has been exploited for automatically evolving the proper number of clusters as well as providing near optimal data set clustering. {GA} can be used in conjunction with the reduced latent semantic structure and improve clustering efficiency and accuracy. The superiority of {GAL} approach over conventional {GA} applied in {VSM} model is demonstrated by providing good Reuter document clustering results. ",
      "title": "17796 Genetic algorithm for text clustering based on latent semantic indexing ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0898122108005300"
    },
    {
      "abstract": "The amount of digital information is created and used is steadily growing along with the development of sophisticated hardware and software. This has increased the need for powerful algorithms that can interpret and extract interesting knowledge from these data. Data mining is a technique that has been successfully exploited for this purpose. Text mining, a category of data mining, considers only digital documents or text. Text Clustering is the process of grouping text or documents such that the document in the same cluster are similar and are dissimilar from the one in other clusters. This paper studies the working of two sophisticated algorithms. The first work is a hybrid method that combines pattern recognition process with semantic driven methods for clustering documents, while the second uses an ontology-based approach to cluster documents. Through experiments, the performance of both the selected algorithms is analyzed in terms of clustering efficiency and speed of clustering. ",
      "title": "17797 Performance Evaluation of Semantic Based and Ontology Based Text Document Clustering Techniques",
      "url": ""
    },
    {
      "abstract": "Abstract The traditional short-text classifications accuracy usually highly relies on statistical feature selection. Owing to the fact that short-text has inherent defects such as short length, weak signal and less features. It is hard to avoid noise words when doing feature extension which will highly influence the accuracy of classification. In order to solve the above problem, this paper proposes a semantic dictionary method for short-text classification. The method builds a set of domain dictionary by analyzing the specific characteristics in certain field. As each words weight in the dictionary is designed according to the correlation between the word and the category, classification accuracy has improved to some extent. Then, in order to enhance dictionary vocabulary coverage, association rules are utilized to automatically extend semantic dictionary. Finally, an experiment based on micro-blog data is conducted which shows that the method has a good effect. ",
      "title": "17798 Semantic dictionary based method for short text classification ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1005888513602563"
    },
    {
      "abstract": "We propose a method for computing semantic relatedness between words or texts by using knowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is built by filtering the encyclopedia's articles, each concept corresponding to an article. Two types of weighted links between concepts are considered: one based on hyperlinks between the texts of the articles, and another one based on the lexical similarity between them. We propose and implement an efficient random walk algorithm that computes the distance between nodes, and then between sets of nodes, using the visiting probability from one (set of) node(s) to another. Moreover, to make the algorithm tractable, we propose and validate empirically two truncation methods, and then use an embedding space to learn an approximation of visiting probability. To evaluate the proposed distance, we apply our method to four important tasks in natural language processing: word similarity, document similarity, document clustering and classification, and ranking in information retrieval. The performance of the method is state-of-the-art or close to it for each task, thus demonstrating the generality of the knowledge resource. Moreover, using both hyperlinks and lexical similarity links improves the scores with respect to a method using only one of them, because hyperlinks bring additional real-world knowledge not captured by lexical similarity. ",
      "title": "17799 Computing text semantic relatedness using the contents and links of a hypertext encyclopedia ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0004370212000744"
    },
    {
      "abstract": "A number of techniques such as information extraction, document classification, document clustering and information visualization have been developed to ease extraction and understanding of information embedded within text documents. However, knowledge that is embedded in natural language texts is difficult to extract using simple pattern matching techniques and most of these methods do not help users directly understand key concepts and their semantic relationships in document corpora, which are critical for capturing their conceptual structures. The problem arises due to the fact that most of the information is embedded within unstructured or semi-structured texts that computers can not interpret very easily. In this paper, we have presented a novel Biomedical Knowledge Extraction and Visualization framework, BioKEVis to identify key information components from biomedical text documents. The information components are centered on key concepts. BioKEVis applies linguistic analysis and Latent Semantic Analysis (LSA) to identify key concepts. The information component extraction principle is based on natural language processing techniques and semantic-based analysis. The system is also integrated with a biomedical named entity recognizer, ABNER, to tag genes, proteins and other entity names in the text. We have also presented a method for collating information extracted from multiple sources to generate semantic network. The network provides distinct user perspectives and allows navigation over documents with similar information components and is also used to provide a comprehensive view of the collection. The system stores the extracted information components in a structured repository which is integrated with a query-processing module to handle biomedical queries over text documents. We have also proposed a document ranking mechanism to present retrieved documents in order of their relevance to the user query. ",
      "title": "17800 A concept-driven biomedical knowledge extraction and visualization framework for conceptualization of text corpora ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1532046410001401"
    },
    {
      "abstract": "Abstract Cross impact analysis (CIA) consists of a set of related methodologies that predict the occurrence probability of a specific event and that also predict the conditional probability of a first event given a second event. The conditional probability can be interpreted as the impact of the second event on the first. Most of the {CIA} methodologies are qualitative that means the occurrence and conditional probabilities are calculated based on estimations of human experts. In recent years, an increased number of quantitative methodologies can be seen that use a large number of data from databases and the internet. Nearly 80% of all data available in the internet are textual information and thus, knowledge structure based approaches on textual information for calculating the conditional probabilities are proposed in literature. In contrast to related methodologies, this work proposes a new quantitative {CIA} methodology to predict the conditional probability based on the semantic structure of given textual information. Latent semantic indexing is used to identify the hidden semantic patterns standing behind an event and to calculate the impact of the patterns on other semantic textual patterns representing a different event. This enables to calculate the conditional probabilities semantically. A case study shows that this semantic approach can be used to predict the conditional probability of a technology on a different technology. ",
      "title": "17801 Quantitative cross impact analysis with latent semantic indexing",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417413005538"
    },
    {
      "abstract": "A method of realization of multi-documents Automatic Abstracting based on text clustering and semantic analysis is brought forward, aimed at overcoming shortages of some current methods about multi-documents. The method makes use of semantic analysis and can realize Automatic Abstracting of multi-documents. The algorithm of twice word segmentation based on the title and first-sentences in paragraphs is brought forward. Its precision and recall is above 95%. For a specific domain on plastics, an Automatic Abstracting system named {TCAAS} is implemented. The precision and recall of multi-documents Automatic Abstracting is above 75%. And experiments do prove that it is feasible to use the method to develop a domain Automatic Abstracting system, which is valuable for further study in more depth. ",
      "title": "17802 Multi-documents Automatic Abstracting based on text clustering and semantic analysis",
      "url": "http://www.sciencedirect.com/science/article/pii/S0950705109000999"
    },
    {
      "abstract": "This paper proposes a self-organized genetic algorithm for text clustering based on ontology method. The common problem in the fields of text clustering is that the document is represented as a bag of words, while the conceptual similarity is ignored. We take advantage of thesaurus-based and corpus-based ontology to overcome this problem. However, the traditional corpus-based method is rather difficult to tackle. A transformed latent semantic indexing (LSI) model which can appropriately capture the associated semantic similarity is proposed and demonstrated as corpus-based ontology in this article. To investigate how ontology methods could be used effectively in text clustering, two hybrid strategies using various similarity measures are implemented. Experiments results show that our method of genetic algorithm in conjunction with the ontology strategy, the combination of the transformed LSI-based measure with the thesaurus-based measure, apparently outperforms that with traditional similarity measures. Our clustering algorithm also efficiently enhances the performance in comparison with standard {GA} and k-means in the same similarity environments. ",
      "title": "17804 Genetic algorithm for text clustering using ontology and evaluating the validity of various semantic similarity measures ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417408009123"
    },
    {
      "abstract": "Abstract Short-text classification is increasingly used in a wide range of applications. However, it still remains a challenging problem due to the insufficient nature of word occurrences in short-text documents, although some recently developed methods which exploit syntactic or semantic information have enhanced performance in short-text classification. The language-dependency problem, however, caused by the heavy use of grammatical tags and lexical databases, is considered the major drawback of the previous methods when they are applied to applications in diverse languages. In this article, we propose a novel kernel, called language independent semantic (LIS) kernel, which is able to effectively compute the similarity between short-text documents without using grammatical tags and lexical databases. From the experiment results on English and Korean datasets, it is shown that the {LIS} kernel has better performance than several existing kernels. ",
      "title": "17805 Language independent semantic kernels for short-text classification ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417413005988"
    },
    {
      "abstract": "This paper explores the role of named entities (NEs) in the classification of disease outbreak report. In the annotation schema of BioCaster, a text mining system for public health protection, important concepts that reflect information about infectious diseases were conceptually analyzed with a formal ontological methodology and classified into types and roles. Types are specified as {NE} classes and roles are integrated into {NEs} as attributes such as a chemical and whether it is being used as a therapy for some infectious disease. We focus on the roles of {NEs} and explore different ways to extract, combine and use them as features in a text classifier. In addition, we investigate the combination of roles with semantic categories of disease-related nouns and verbs. Experimental results using Naive Bayes and Support Vector Machine (SVM) algorithms show that: (1) roles in combination with {NEs} improve performance in text classification, (2) roles in combination with semantic categories of noun and verb features contribute substantially to the improvement of text classification. Both these results were statistically significant compared to the baseline raw text representation. We discuss in detail the effects of roles on each {NE} and on semantic categories of noun and verb features in terms of accuracy, precision/recall and F-score measures for the text classification task. ",
      "title": "17806 Towards role-based filtering of disease outbreak reports ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1532046408001494"
    },
    {
      "abstract": "A variety of services have recently been provided that depend on highly developed networks and personal equipment. With these advances, connecting this equipment has become increasingly more complicated. Customer enquiries about problems such as no-connection to an Internet/phone service will increase, and telecom operators will require the ability to understand such situations and act as quickly as possible. Therefore, it is important to analyze failure trends and establish, in advance, effective coping processes for complex problems conveyed in customer enquiries. We present a method for analyzing and classifying customer enquiries efficiently and precisely from the structural type of description in ontologies. Moreover, our method can reconstruct semantic content efficiently by extracting related terms through analysis and classification. This method is based on a dependency parsing and co-occurrence technique to enable classification of a large amount of unstructured data into patterns because customer enquiries are generally stored as unstructured textual data. The validity of the method is evaluated and the method to determine threshold values is developed by using a large amount of customer enquiries in the real operational field. ",
      "title": "17807 Semantic analysis and classification method for customer enquiries in telecommunication services ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0952197611000406"
    },
    {
      "abstract": "Sentiment analysis continues to be a most important research problem due to its abundant applications. Identifying the semantic orientation of subjective terms (words or phrases) is a fundamental task for sentiment analysis. In this paper, we propose a new method for identifying the semantic orientation of subjective terms to perform sentiment analysis. The method takes a classification approach that is based on a novel semantic orientation representation model called S-HAL (Sentiment Hyperspace Analogue to Language). S-HAL basically produces a set of weighted features based on surrounding words, and characterizes the semantic orientation information of words via a specific feature space. Because the method incorporates the idea underlying {HAL} and the hypothesis verified by the method of semantic orientation inference from pointwise mutual information (SO-PMI), it can quickly and accurately identify the semantic orientation of terms without the use of an Internet search engine. The results of an empirical evaluation show that our method outperforms other known methods. ",
      "title": "17809 Identifying the semantic orientation of terms using S-HAL for sentiment analysis",
      "url": "http://www.sciencedirect.com/science/article/pii/S0950705112001074"
    },
    {
      "abstract": "Wikipedia is a goldmine of information",
      "title": "17811 Mining meaning from Wikipedia ",
      "url": "Wikipedia, Text mining, Wikipedia mining, {NLP}, Information retrieval, Information extraction, Ontologies, Semantic web "
    },
    {
      "abstract": "The increasing amount of Web-based tasks is currently requiring personalization strategies to improve the user experience. However, building user profiles is a hard task, since users do not usually give explicit information about their interests. Therefore, interests must be mined implicitly from electronic sources, such as chat and discussion forums. In this work, we present a novel method for topic detection from online informal conversations. Our approach combines: (i) Wikipedia, an extensive source of knowledge, (ii) a concept association strategy, and (iii) a variety of text-mining techniques, such as {POS} tagging and named entities recognition. We performed a comparative evaluation procedure for searching the optimal combination of techniques, achieving encouraging results. ",
      "title": "17812 Mining interests for user profiling in electronic conversations ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417412009311"
    },
    {
      "abstract": "The helpfulness feature of online user reviews helps consumers cope with information overloads and facilitates decision-making. However, many online user reviews lack sufficient helpfulness votes for other users to evaluate their true helpfulness level. This study empirically examines the impact of the various features, that is, basic, stylistic, and semantic characteristics of online user reviews on the number of helpfulness votes those reviews receive. Text mining techniques are employed to extract semantic characteristics from review texts. Our findings show that the semantic characteristics are more influential than other characteristics in affecting how many helpfulness votes reviews receive. Our findings also suggest that reviews with extreme opinions receive more helpfulness votes than those with mixed or neutral opinions. This paper sheds light on the understanding of online users helpfulness voting behavior and the design of a better helpfulness voting mechanism for online user review systems. ",
      "title": "17814 Exploring determinants of voting for the helpfulness of online user reviews: A text mining approach ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0167923610001909"
    },
    {
      "abstract": "Abstract In this paper, we propose a novel approach to classify short texts by combining both their lexical and semantic features. We present an improved measurement method for lexical feature selection and furthermore obtain the semantic features with the background knowledge repository which covers target category domains. The combination of lexical and semantic features is achieved by mapping words to topics with different weights. In this way, the dimensionality of feature space is reduced to the number of topics. We here use Wikipedia as background knowledge and employ Support Vector Machine (SVM) as classifier. The experiment results show that our approach has better effectiveness compared with existing methods for classifying short texts. ",
      "title": "17815 Combining Lexical and Semantic Features for Short Text Classification",
      "url": "http://www.sciencedirect.com/science/article/pii/S1877050913008764"
    },
    {
      "abstract": "Latent semantic indexing (LSI) can be used in patent searching to overcome drawbacks of Boolean searching and to give more accurate retrieval. {LSI} combines the vector space model (VSM) of document retrieval with single value decomposition (SVD), using linear algebra techniques to uncover word relationships in the text. Results can be enhanced by using text clustering and tailoring {SVD} parameters to the specific corpus, in this case, patents, and by employing techniques to address ambiguities in language. ",
      "title": "17818 Advanced document retrieval techniques for patent research ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0172219008000069"
    },
    {
      "abstract": "Objective Access to the vast body of research literature that is available in biomedicine and related fields may be improved by automatic summarisation. This paper presents a method for summarising biomedical scientific literature that takes into consideration the characteristics of the domain and the type of documents. Methods To address the problem of identifying salient sentences in biomedical texts, concepts and relations derived from the Unified Medical Language System (UMLS) are arranged to construct a semantic graph that represents the document. A degree-based clustering algorithm is then used to identify different themes or topics within the text. Different heuristics for sentence selection, intended to generate different types of summaries, are tested. A real document case is drawn up to illustrate how the method works. Results A large-scale evaluation is performed using the recall-oriented understudy for gisting-evaluation (ROUGE) metrics. The results are compared with those achieved by three well-known summarisers (two research prototypes and a commercial application) and two baselines. Our method significantly outperforms all summarisers and baselines. The best of our heuristics achieves an improvement in performance of almost 7.7 percentage units in the ROUGE-1 score over the LexRank summariser (0.7862 versus 0.7302). A qualitative analysis of the summaries also shows that our method succeeds in identifying sentences that cover the main topic of the document and also considers other secondary or 'satellite' information that might be relevant to the user. Conclusion The method proposed is proved to be an efficient approach to biomedical literature summarisation, which confirms that the use of concepts rather than terms can be very useful in automatic summarisation, especially when dealing with highly specialised domains. ",
      "title": "17820 A semantic graph-based approach to biomedical summarisation ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0933365711000753"
    },
    {
      "abstract": "Nowadays most of the Web pages contain little amount of structure and supporting information that can reveal their semantics or meanings. To enable automated processing of the Web pages, semantic information such as metadata and tags regarding to each page should be added to it. Several authoring tools have been developed to help users tackling this task. However, manual or semi-automatic authoring is implausible when we intend to annotate large amount of Web pages. In this work, we proposed a method to automatically generate some descriptive metadata and tags for a Web page. The idea is to apply the self-organizing map algorithm to cluster the Web pages and discover the relationships between these clusters. In the mean time, the themes of each cluster are also identified. We then use such relationships and themes to tag the Web pages and generate metadata for the Web pages. The result of experiments shows that our method may generate semantically relevant metadata and tags for the Web pages. ",
      "title": "17821 Automatic generation of semantically enriched web pages by a text mining approach",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417409001729"
    },
    {
      "abstract": "This paper presents a least square support vector machine (LS-SVM) that performs text classification of noisy document titles according to different predetermined categories. The systems potential is demonstrated with a corpus of 91,229 words from University of Denvers Penrose Library catalogue. The classification accuracy of the proposed LS-SVM based system is found to be over 99.9%. The final classifier is an LS-SVM array with Gaussian radial basis function (GRBF) kernel, which uses the coefficients generated by the latent semantic indexing algorithm for classification of the text titles. These coefficients are also used to generate the confidence factors for the inference engine that present the final decision of the entire classifier. The system is also compared with a K-nearest neighbor (KNN) and Naive Bayes (NB) classifier and the comparison clearly claims that the proposed LS-SVM based architecture outperforms the {KNN} and {NB} based system. The comparison between the conventional linear {SVM} based classifiers and neural network based classifying agents shows that the LS-SVM with {LSI} based classifying agents improves text categorization performance significantly and holds a lot of potential for developing robust learning based agents for text classification. ",
      "title": "17822 Text classification: A least square support vector machine approach ",
      "url": "http://www.sciencedirect.com/science/article/pii/S156849460600038X"
    },
    {
      "abstract": "Literature-related discovery (LRD) is linking two or more literature concepts that have heretofore not been linked (i.e., disjoint), in order to produce novel, interesting, plausible, and intelligible knowledge. {LRD} has two components: Literature-based discovery (LBD) generates potential discovery through literature analysis alone, whereas literature-assisted discovery (LAD) generates potential discovery through a combination of literature analysis and interactions among selected literature authors. In turn, there are two types of {LBD} and LAD: open discovery systems (ODS), where one starts with a problem and arrives at a solution, and closed discovery systems (CDS), where one starts with a problem and a solution, then determines the mechanism(s) that links them. The generic methodology for identifying potential discovery candidates through {ODS} LRD, focusing mainly on its {ODS} {LBD} component, is described in this paper. A comprehensive flow chart showing the details of our systematic potential discovery generation process, including the evolution of the flow chart steps through each of the studies performed, is presented. Also shown is a vetting procedure that insures potential discoveries claimed are potential discoveries realized. The semantic filters that replace the numerical filters of other {ODS} {LBD} approaches are overviewed. The rationale for addressing the five topics studied (Raynauds Phenomenon (RP), Cataracts, Parkinsons Disease (PD), Multiple Sclerosis (MS), and Water Purification (WP)) is summarized. ",
      "title": "17823 Literature-related discovery (LRD): Methodology ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0040162507001977"
    },
    {
      "abstract": "The need for more comprehensive and easy to use search tools especially for patent documents still exists. In this paper, the developments of the ongoing project of voestalpine Stahl GmbH together with their partners regarding the implementation of a semantic search tool and an appropriate visualisation are revealed. The main focus is on the different methods of analysis and examples of applications of the software. The features are on the one hand, a semantic representation of the content of the patent documents, in combination with the users feedback and the use of landscape visualisation during the retrieval process. On the other hand a satisfactory quality is ensured by using among other features a document corpus which is finite and focused on the technology areas of voestalpine. ",
      "title": "17827 Semantic enrichment and added metadata - Examples of efficient usage in an industrial environment ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0172219008001452"
    },
    {
      "abstract": "Although machines perform much better than human beings in most of the tasks, it is not the case of natural language processing. Computational linguistic systems usually rely on mathematical and statistical formalisms, which are efficient and useful but far from human procedures and therefore not so skilled. This paper proposes a computational model of natural language reading, called Cognitive Reading Indexing Model (CRIM), inspired by some aspects of human cognition, that tries to become as more psychologically plausible as possible. The model relies on a semantic neural network and it produces not vectors but nets of activated concepts as text representations. Based on these representations, measures of semantic similarity are also defined. Human comparison results show that the system is suitable to model human reading. Additional results also point out that the system could be used in real applications concerning natural language processing tasks. ",
      "title": "17829 Dealing with written language semantics by a connectionist model of cognitive reading",
      "url": "http://www.sciencedirect.com/science/article/pii/S092523120800461X"
    },
    {
      "abstract": "The text retrieval method using latent semantic indexing (LSI) technique with truncated singular value decomposition (SVD) has been intensively studied in recent years. The {SVD} reduces the noise contained in the original representation of the term-document matrix and improves the information retrieval accuracy. Recent studies indicate that {SVD} is mostly useful for small homogeneous data collections. For large inhomogeneous datasets, the performance of the {SVD} based text retrieval technique may deteriorate. We propose to partition a large inhomogeneous dataset into several smaller ones with clustered structure, on which we apply the truncated SVD. Our experimental results show that the clustered {SVD} strategies may enhance the retrieval accuracy and reduce the computing and storage costs. ",
      "title": "17830 Clustered {SVD} strategies in latent semantic indexing ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0306457304001384"
    },
    {
      "abstract": "Based on the text orientation classification, a new measurement approach to semantic orientation of words was proposed. According to the integrated and detailed definition of words in HowNet, seed sets including the words with intense orientations were built up. The orientation similarity between the seed words and the given word was then calculated using the sentiment weight priority to recognize the semantic orientation of common words. Finally, the words semantic orientation and the context were combined to recognize the given words orientation. The experiments show that the measurement approach achieves better results for common words orientation classification and contributes particularly to the text orientation classification of large granularities. ",
      "title": "17831 Words semantic orientation classification based on HowNet ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1005888508601880"
    },
    {
      "abstract": "Motivation: The identification of events such as protein-protein interactions (PPIs) from the scientific literature is a complex task. One of the reasons is that there is no formal syntax to denote such relations in the scientific literature. Nonetheless, it is important to understand such relational event representations to improve information extraction solutions (e.g., for gene regulatory events). In this study, we analyze publicly available protein interaction corpora (AIMed, BioInfer, BioCreAtIve II) to determine the scope of verbs used to denote protein interactions and to measure their predictive capacity for the identification of {PPI} events. Our analysis is based on syntactical language patterns. This restriction has the advantage that the verb mention is used as the independent variable in the experiments enabling comparability of results in the usage of the verbs. The initial selection of verbs has been generated from a systematic analysis of the scientific literature and existing corpora for PPIs. We distinguish modifying interactions (MIs) such as posttranslational modifications (PTMs) from non-modifying interactions (NMIs) and assumed that {MIs} have a higher predictive capacity due to stronger scientific evidence proving the interaction. We found that {MIs} are less frequent in the corpus but can be extracted at the same precision levels as PPIs. A significant portion of correct {PPI} reportings in the BioCreAtIve {II} corpus use the verb 'associate', which semantically does not prove a relation. The performance of every monitored verb is listed and allows the selection of specific verbs to improve the performance of {PPI} extraction solutions. Programmatic access to the text processing modules is available online (www.ebi.ac.uk/webservices/whatizit/info.jsf) and the full analysis of Medline abstracts will be made through the Web pages of the Rebholz group. ",
      "title": "17833 Measuring prediction capacity of individual verbs for the identification of protein interactions ",
      "url": "http://www.sciencedirect.com/science/article/pii/S153204640900135X"
    },
    {
      "abstract": "Text representation is a necessary procedure for text categorization tasks. Currently, bag of words (BOW) is the most widely used text representation method but it suffers from two drawbacks. First, the quantity of words is huge",
      "title": "17834 Fast text categorization using concise semantic analysis",
      "url": ""
    },
    {
      "abstract": "This article reports on our experiments and results on the effectiveness of different feature sets and information fusion from some combinations of them in classifying free text documents into a given number of categories. We use different feature sets and integrate neural network learning into the method. The feature sets are based on the latent semantics of a reference library - a collection of documents adequately representing the desired concepts. We found that a larger reference library is not necessarily better. Information fusion almost always gives better results than the individual constituent feature sets, with certain combinations doing better than the others. ",
      "title": "17835 Information fusion for text classification - an experimental comparison",
      "url": "http://www.sciencedirect.com/science/article/pii/S0031320300001710"
    },
    {
      "abstract": "In this paper, we propose a new approach to automatic discovery of implicit rhetorical information from texts based on evolutionary computation methods. In order to guide the search for rhetorical connections from natural-language texts, the model uses previously obtained training information which involves semantic and structural criteria. The main features of the model and new designed operators and evaluation functions are discussed, and the different experiments assessing the robustness and accuracy of the approach are described. Experimental results show the promise of evolutionary methods for rhetorical role discovery. ",
      "title": "17836 Discovering implicit intention-level knowledge from natural-language texts ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0950705109000021"
    },
    {
      "abstract": "This paper addresses the process of semi-automatic text-driven ontology extension using ontology content, structure and co-occurrence information. A novel OntoPlus methodology is proposed for semi-automatic ontology extension based on text mining methods. It allows for the effective extension of the large ontologies, providing a ranked list of potentially relevant concepts and relationships given a new concept (e.g., glossary term) to be inserted in the ontology. A number of experiments are conducted, evaluating measures for ranking correspondence between existing ontology concepts and new domain concepts suggested for the ontology extension. Measures for ranking are based on incorporating ontology content, structure and co-occurrence information. The experiments are performed using a well known Cyc ontology and textual material from two domains - finances and, fisheries &",
      "title": "17837 OntoPlus: Text-driven ontology extension using ontology content, structure and co-occurrence information ",
      "url": ""
    },
    {
      "abstract": "Automatic emotion sensing in textual data is crucial for the development of intelligent interfaces in many interactive computer applications. This paper describes a high-precision, knowledgebase-independent approach for automatic emotion sensing for the subjects of events embedded within sentences. The proposed approach is based on the probability distribution of common mutual actions between the subject and the object of an event. We have incorporated web-based text mining and semantic role labeling techniques, together with a number of reference entity pairs and hand-crafted emotion generation rules to realize an event emotion detection system. The evaluation outcome reveals a satisfactory result with about 85% accuracy for detecting the positive, negative and neutral emotions. ",
      "title": "17838 Automatic event-level textual emotion sensing using mutual action histogram between entities",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417409006046"
    },
    {
      "abstract": "Many artificial intelligence tasks, such as automated question answering, reasoning, or heterogeneous database integration, involve verification of a semantic category (e.g. coffee is a drink, red is a color, while steak is not a drink and big is not a color). In this research, we explore completely automated on-the-fly verification of a membership in any arbitrary category which has not been expected a priori. Our approach does not rely on any manually codified knowledge (such as WordNet or Wikipedia) but instead capitalizes on the diversity of topics and word usage on the World Wide Web, thus can be considered knowledge-light and complementary to the knowledge-intensive approaches. We have created a quantitative verification model and established (1) what specific variables are important and (2) what ranges and upper limits of accuracy are attainable. While our semantic verification algorithm is entirely self-contained (not involving any previously reported components that are beyond the scope of this paper), we have tested it empirically within our fact seeking engine on the well known {TREC} conference test questions. Due to our implementation of semantic verification, the answer accuracy has improved by up to 16% depending on the specific models and metrics used. ",
      "title": "17841 Exploring models for semantic category verification",
      "url": "http://www.sciencedirect.com/science/article/pii/S0306437909000180"
    },
    {
      "abstract": "The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding. ",
      "title": "17847 Depth of processing in language comprehension: not noticing the evidence ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1364661302019587"
    },
    {
      "abstract": "How best to represent text data is an important problem in text mining tasks including information retrieval, clustering, classification and etc.. In this paper, we proposed a compact document representation with term semantic units which are identified from the implicit and explicit semantic information. Among it, the implicit semantic information is extracted from syntactic content via statistical methods such as latent semantic indexing and information bottleneck. The explicit semantic information is mined from the external semantic resource (Wikipedia). The proposed compact representation model can map a document collection in a low-dimension space (term semantic units which are much less than the number of all unique terms). Experimental results on real data sets have shown that the compact representation efficiently improve the performance of text clustering.",
      "title": "17849 Text Clustering via Term Semantic Units",
      "url": "Wikipedia"
    },
    {
      "abstract": "With the exponentially growing volume of digital documents and internet content, it becomes very challenging to locate right information when desired. We heavily rely on search engines but most existing search tools are key-word based and they often return search results with low precision and recall. The emerging semantic tagging technology provides an automatic way to generate semantic tags from text. It has drawn more and more interest from text mining research communities. It is critical to study how to utilize semantic tags to improve text mining including clustering, which helps users to enhance their experience of searching and browsing documents. Unfortunately, most previous works on text clustering merely based on content information. A few recent researches take user-generated tags into account, however user generated tags are often noisy, inconsistent, redundant and lack of semantic information and hierarchical structure. In this work, we propose a Semantic Text Mining (STeM) framework to generate semantic tags for given documents and then utilize the semantic tags to improve text clustering. Different from the previous works, we represent a document by a combination of domains and high quality noun phrases. We investigate the performance of our methods in two different datasets and the results are evaluated by normalized mutual information. Experiment results demonstrated that our proposed method substantially outperformed the traditional Term Frequency-Inverse Document Frequency (TF-IDF) term vector based clustering. We find that incorporating semantic information into document representation is critical to improve the performance of text clustering.",
      "title": "17850 An Exploratory Study of Enhancing Text Clustering with Auto-Generated Semantic Tags",
      "url": "semantic Web"
    },
    {
      "abstract": "This paper proposes an approach for mining the semantic relationships between terms. Using a dependency model based on syntactic parsing, the syntactic features of a term are first extracted from large scale corpus, and then the vector representation for this term is constructed. By the cosine similarities between vectors, we can get the semantically related words for a term. We apply the semantic knowledge to document vector representation in text classification. The experiment on the standard data sets shows that our approach gets a better performance compared with the traditional classifiers.",
      "title": "17851 The Mining of Term Semantic Relationships and its Application in Text Classification",
      "url": "cosine similarities"
    },
    {
      "abstract": "To overcome the limitations of traditional text classification approaches based on bag-of-words representation and to effectively incorporate linguistic knowledge and conceptual index into text vector space model, based on two thesaurus HowNet and Tongyici Cilin (hereinafter referred to Cilin), we use semantic vector to describe a document instead of traditional keywords vector, which is based on merging words with high similarity and using a concept to describe the semantic feature rather than a series of words. It not only reduces feature dimension but also adds semantic information to the vector. We also use sentence (document) similarity based on simple vector distance to classify the text and three groups of experiments are made respectively. The results show that the accuracy rates are generally improved along with semantic treatment, which indicates that semantic mining is very important and necessary to text classification.",
      "title": "17854 Research of Chinese Text Classification Methods Based on Semantic Vector and Semantic Similarity",
      "url": "thesauri"
    },
    {
      "abstract": "The majority of text retrieval and mining techniques are still based on exact feature (e.g. words) matching and unable to incorporate text semantics. Many researchers believe that the extension with semantic knowledge could improve the results and various methods (most of them are heuristic) have been proposed to account for concept hierarchy, synonymy, and other semantic relationships. However, the results with such semantic extension have been mixed, ranging from slight improvements to decreases in effectiveness, mostly likely due to the lack of a formal framework. Instead, we propose a novel method to address the semantic extension within the framework of language modeling. Our method extracts explicit topic signatures from documents and then statistically maps them into single- word features. The incorporation of semantic knowledge then reduces to the smoothing of unigram language models using semantic knowledge. The dragon toolkit reflects our method and its effectiveness is demonstrated by three tasks, text retrieval, text classification, and text clustering.",
      "title": "17855 Dragon Toolkit: Incorporating Auto-Learned Semantic Knowledge into Large-Scale Text Retrieval and Mining",
      "url": "pattern clustering"
    },
    {
      "abstract": "The paper addresses some roles of concept-based representations in document clustering to support knowledge discovery. Computational Intelligence algorithms can benefit from semantic networks in the definition of similarity between pairs of documents. After analyzing the tuning of semantic networks in a systematic fashion, the research defines and evaluates a novel semantic-based metrics, which integrates both classical and style-related features of texts. Experimental results confirm the effectiveness of the approach, showing that applying a refined semantic representation into a clustering engine yields consistent structures for information retrieval and knowledge acquisition.",
      "title": "17859 Semantic Models for Style-Based Text Clustering",
      "url": "semantic networks"
    },
    {
      "abstract": "Traditional text mining techniques transform free text into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. In this paper, we present a two-step procedure to mine generalized associations of semantic relations conveyed by the textual content of Web documents. First, RDF (resource description framework) metadata representing semantic relations are extracted from raw text using a myriad of natural language processing techniques. The relation extraction process also creates a term taxonomy in the form of a sense hierarchy inferred from WordNet. Then, a novel generalized association pattern mining algorithm (GP-Close) is applied to discover the underlying relation association patterns on RDF metadata. For pruning the large number of redundant overgeneralized patterns in relation pattern search space, the GP-Close algorithm adopts the notion of generalization closure for systematic overgeneralization reduction. The efficacy of our approach is demonstrated through empirical experiments conducted on an online database of terrorist activities",
      "title": "17860 Mining Generalized Associations of Semantic Relations from Textual Web Content",
      "url": "RDF mining"
    },
    {
      "abstract": "Most of text mining techniques are based on word and/or phrase analysis of the text. The statistical analysis of a term (word or phrase) frequency captures the importance of the term within a document. However, to achieve a more accurate analysis, the underlying mining technique should indicate terms that capture the semantics of the text from which the importance of a term in a sentence and in the document can be derived. Incorporating semantic features from the WordNet lexical database is one of many approaches that have been tried to improve the accuracy of text clustering techniques. A new semantic-based model that analyzes documents based on their meaning is introduced. The proposed model analyzes terms and their corresponding synonyms and/or hypernyms on the sentence and document levels. In this model, if two documents contain different words and these words are semantically related, the proposed model can measure the semantic-based similarity between the two documents. The similarity between documents relies on a new semantic-based similarity measure which is applied to the matching concepts between documents. Experiments using the proposed semantic-based model in text clustering are conducted. Experimental results demonstrate that the newly developed semantic-based model enhances the clustering quality of sets of documents substantially.",
      "title": "17862 A WordNet-Based Semantic Model for Enhancing Text Clustering",
      "url": "WordNet lexical database"
    },
    {
      "abstract": "One key step in text mining is the categorization of texts, i.e., to put texts of the same or similar contents into one group so as to distinguish texts of different contents. However, traditional word-frequency-based statistical approaches, such as VSM model, failed to reflect the complicated meaning in texts. This paper ushers in domain ontology and constructs new conceptual vector space model in the pre-processing stage of text clustering, substituting the initial matrix (lexicon-text matrix) in the latent semantic analysis with concept-text matrix. In the clustering analysis stage, this model adopts semantic similarity, partially overcoming the difficulty in accurately and effectively evaluating the degree of similarity of text due to simply taking into account the frequency of words and/or phrases in the text. Experimental results indicate that this method is helpful in improving the result of text clustering.",
      "title": "17865 Text Clustering Based on Domain Ontology and Latent Semantic Analysis",
      "url": "pattern clustering"
    },
    {
      "abstract": "To represent the textual knowledge more expressively, a kind of semantic-based graph structure is proposed for this issue and thereafter applied to clustering problems. Such graph structure for textual representation consists of nodes and directed edges, which stand for the feature terms derived from the texts and the semantic relationships between them, respectively. Moreover, the weight is assigned to each edge so that the strength of relationship between two terms can be measured. For this weighted directed graph structure, a novel graph similarity algorithm is developed by extracting the maximum common subgraph between two concerned graphs, which can therefore be used to measure the distance between two graph structures, i.e. two texts, and finally be used to sort the texts into different clusters. Some experiments have been done through the proposed semantic graph structure in clustering applications and the results have proved the high performance of our textual knowledge representation model.",
      "title": "17866 Textual Knowledge Representation through the Semantic-Based Graph Structure in Clustering Applications",
      "url": "clustering applications"
    },
    {
      "abstract": "Although an improvement of hierarchical text classification can be achieved by using hierarchical structure information, existing hierarchical text classification methods suffer from two problems: data skew (especially in large-scale hierarchy) and error propagation. In this paper, we first define the concept of path-based semantic vector for the presentation of categories. Then a set of additional reliable training data for data-sparse categories can be retrieved based on such representation and particular similarity metrics. This training data enhancement strategy is classifier independent and can improve the classification of categories without adequate training data. Second, we propose the occurrence probability based strategy for hierarchical text classification which can reduce error propagation efficiently. Cooccurrence probability is then introduced to correct the errors occurred on higher levels of the hierarchy. Extensive experiments show that our hierarchical classification strategies perform well on ODP dataset, even in the condition of having few training data.",
      "title": "17867 Large-Scale Hierarchical Text Classification Based on Path Semantic Vector and Prior Information",
      "url": "ODP dataset"
    },
    {
      "abstract": "For multi-view learning, existing methods usually exploit originally provided features for classifier training, which ignore the latent correlation between different views. In this paper, semantic features integrating information from multiple views are extracted for pattern representation. Canonical correlation analysis is used to learn the representation of semantic spaces where semantic features are projections of original features on the basis vectors of the spaces. We investigate the feasibility of semantic features on two learning paradigms: semi-supervised learning and active learning. Experiments on text classification with two state-of-the-art multi-view learning algorithms co-training and co-testing indicate that this use of semantic features can lead to a significant improvement of performance.",
      "title": "17873 Semantic Features for Multi-view Semi-supervised and Active Learning of Text Classification",
      "url": "active learning"
    },
    {
      "abstract": "A text classification model based on Latent Semantic Analysis and Improved Hyper-sphere Support Vector Machine, is proposed in order to improve the accuracy and efficiency of text classification. Latent Semantic Analysis is used in this model for feature extraction, eliminating the text representation errors caused by synonyms and polysemes, and reducing the dimension of text vector. At the same time, a new decision-making approach based on concentration, is designed for improving the classification of texts in overlapping regions. Experimental results show that the model will give good classification results when the number of classes is small. And the improved algorithm is effective and feasible.",
      "title": "17875 Research of Text Classification Model Based on Latent Semantic Analysis and Improved HS-SVM",
      "url": "text analysis"
    },
    {
      "abstract": "In the current thinking of the Chinese text clustering, most clustering algorithms are limited by the datas scalability and the results interpretability. This paper presents an efficient Chinese text clustering method based on semantic concepts. This method, proceeding from the text itself, by using classified hierarchy Subject Word in Thesaurus of Modern Chinese, extracts the conceptional tuple from a high-dimensional text vector collection to form the high-level concept expressing clustering results. Then samples are divided based on these high-level concepts which indicates the entire text clustering process has completed. On the premise of ensuring the clustering results accuracy, this method can greatly reduce the number of data needing to be processed and improve the clustering algorithms scalability. The experimental results show that this clustering algorithm has achieved a satisfactory clustering result and a higher implementation efficiency as well.",
      "title": "17877 Clustering Efficient Method on Mass Chinese Text Based on Semantic Concept",
      "url": "Chinese text clustering"
    },
    {
      "abstract": "The purpose of Web text mining is to find the potential knowledge from the immensity text information on the Internet. In this paper, a novel Web text mining method is proposed based on semantic polarity analysis. Firstly, the model for Web text mining is presented by using semantic polarity analysis, which includes three main parts: data acquisition, feature sentences analysis and semantic polarity analysis. Secondly, the procedure with semantic polarity analysis is introduced for Web text mining, and the related algorithms are also discussed. Thirdly, the method is applied into an actual case to try to find out the valuable products information for the consumers. The results show that the method is both reasonable and effective.",
      "title": "17885 A Novel Web Text Mining Method Based on Semantic Polarity Analysis",
      "url": "Internet"
    },
    {
      "abstract": "Through research on K-means algorithm of text clustering and semantic-based vector space model, a semantic-based K-means text clustering model is proposed to solve the problem on high-dimensional and sparse characteristics of text data set. The model reduces the semantic loss of the text data and improves the quality of text clustering. Experiments prove that semantic-based text clustering increases by more 6 percent than non-semantic-based one in the final evaluation of the F1 index value.",
      "title": "17887 Research on K-means Text Clustering Algorithm Based on Semantic",
      "url": "k-means clustering"
    },
    {
      "abstract": "Recent work into knowledge-mining algorithms has found that second order models can be used to approximate the inputs to systems by first calculating their time trajectories, and then obtaining the causal parameters of these trajectories. In this paper a new method of text analysis is proposed, and the application of knowledge-mining algorithms to text analysis is investigated. Mathcad is used to model relevant knowledge mining algorithms and test their performance with signals derived from ASCII codes of text excerpts. For the first time, time-varying signals relating to a series of input values have been analysed with knowledge mining algorithms. The Mathcad program which does this has been embedded into a user interface in MS Access to allow ease of text selection by users and automatic calculation of ASCII values for analysis.",
      "title": "17888 Semantic Mining for Language Text Analysis",
      "url": "ASCII codes"
    },
    {
      "abstract": "It has been shown that Latent Semantic Indexing (LSI) takes advantage of implicit higher-order (or latent) structure in the association of terms and documents. Higher order relations in LSI capture latent semantics. Inspired by this, a novel Bayesian framework for classification named Higher Order Naive Bayes (HONB), which can explicitly make use of these higher-order relations, has been introduced previously. We present a novel semantic smoothing method named Higher Order Smoothing (HOS) for the Naive Bayes algorithm. HOS is built on a similar graph based data representation of HONB which allows semantics in higher order paths to be exploited. Additionally, we take the concept one step further in HOS and exploited the relationships between instances of different classes in order to improve the parameter estimation when dealing with insufficient labeled data. As a result, we have not only been able to move beyond instance boundaries, but also class boundaries to exploit the latent information in higher-order paths. The results of our extensive experiments demonstrate the value of HOS on several benchmark datasets.",
      "title": "17892 A Novel Semantic Smoothing Method Based on Higher Order Paths for Text Classification",
      "url": "smoothing methods"
    },
    {
      "abstract": "3",
      "title": "17893 Mining opinions in Arabic text using an improved #x201C",
      "url": "ethical aspects"
    },
    {
      "abstract": "This paper discusses a approach of Chinese text classification on semantic Web. It is given one classified technology based on the semantic concept established on the How-net . It extracts keywords from text, analyses the full text using the keywords concept, and then the integrates to classify by categories of keywords semantic concept. It has been tested that this approach has good effect.",
      "title": "17897 A Chinese Text Classification Approach Based on Semantic Web",
      "url": "Chinese text classification"
    },
    {
      "abstract": "We use text semantic mining algorithms based on intelligent search engine framework to obtain media information data of stocks. Since media is significantly related to firm size, industry affiliation and whether belongs to important index, we adopt event study and use the residual attention model to examine the relationship between abnormal media information and stock returns with a special sample. We find that relative to stocks with high abnormal media information, those stocks with low abnormal media information have higher returns. The media effect exists in Chinese stock market. A long-short trading strategy can earn significant positive cumulative excess returns in the following 10 days. Furthermore, our findings show that the excess return from media effect is due to the significantly low returns of high abnormal media information stocks. We suggest that the explanation of this asymmetry phenomenon is possibly the stock prices overreaction to media reports caused by investor sentiment, which yields lower expected returns.",
      "title": "17898 The Relationship between Media Information and Stock Returns Based on Text Semantic Mining Algorithms",
      "url": "stock markets"
    },
    {
      "abstract": "In this paper we propose a new approach to the design of semantic smoothing kernels for text classification. These kernels implicitly encode a superconcept expansion in a semantic network using well-known measures of term similarity. The experimental evaluation on two different datasets indicates that our approach consistently improves performance in situations of little training data and data sparseness.",
      "title": "17899 Semantic Kernels for Text Classification Based on Topological Measures of Feature Similarity",
      "url": "data sparseness"
    },
    {
      "abstract": "Confronting the challenges of annotating naturally occurring text into a semantically structured form to facilitate automatic information extraction, current semantic role labeling (SRL) systems have been specifically examining a semantic predicate-argument structure. Based on the concept description language for natural language (CDL.nl) which is intended to describe the concept structure of text using a set of pre-defined semantic relations, we develop a parser to add a new layer of semantic annotation of natural language sentences as an extension of SRL. With the assumption that all relation instances are detected, we present a relation classification approach facing the challenges of CDL.nl relation extraction. Preliminary evaluation on a manual dataset, using support vector machine, shows that CDL.nl relations can be classified with good performance.",
      "title": "17900 Relation Classification for Semantic Structure Annotation of Text",
      "url": "text analysis"
    },
    {
      "abstract": "Linked Data is an open data space that emerges from the publication and interlinking of structured data on the Web using the Semantic Web technologies. How to utilize this wealth of data is currently a focused research theme of the Semantic Web community. In this paper, we aim to utilize Linked Data to generate semantic annotations for frequent patterns extracted from textual documents. First, we extract semantic relations from textual documents and merge them into a set of semantic graphs. Then, we apply a frequent subgraph discovery algorithm on the set of graphs to generate frequent patterns. Finally, we annotate the discovered patterns using Linked Data. Our approach can be applied in such domains as terrorist network analysis and biological network analysis. The efficacy of our approach is demonstrated through an empirical experiment that discovers and validates relationships between political figures from large number of news on the Web.",
      "title": "17903 Semantic Text Mining with Linked Data",
      "url": "semantic Web"
    },
    {
      "abstract": "Kernel methods are widely used for document classification in diverse domains. Popular kernels such as bag-of-word kernels and tree kernels show satisfactory results in classifying documents such as articles, e-mails or web pages. However, they provide less satisfactory performances in classifying short-text documents since the short documents have insufficient feature space. In order to cope with the problem, this paper presents a novel kernel function called semantic pattern tree kernel for classifying short-text documents. The proposed kernel extends the feature space of each document by incorporating syntactic and semantic information using three levels of semantic annotations. Experiments on the Open Directory Project dataset show that in classifying short-text documents the semantic pattern tree kernels achieve higher accuracy than the conventional kernels.",
      "title": "17904 Semantic Pattern Tree Kernels for Short-Text Classification",
      "url": "document feature space"
    },
    {
      "abstract": "Extraction of semantics from web documents has been becoming an essential technique for the implementation of Semantic Web. This paper aims to explore a method for automatically grouping semantically related term and documents in multiple languages and a representation of their cross-lingual semantic relations. We propose a novel algorithmic approach for multilingual text mining from a bilingual corpus (i.e. a Chinese and English text collection), based on a neural network technique, namely the self-orginizing map (SOM). When exposed to raw multilingual text, the model clusters words and documents according to their semantic relatedness to form a semantic network. In addition, we apply the semantics extracted from the resulting document clusters, along with a XML document technique, to carry out the task of re-categorization of web pages. As such, the multilingual web texts can be re-categorized based on the resulting semantic relatedness among the documents in the corpus.",
      "title": "17905 A multilingual text-mining approach based on self-orginizing maps for semantic web mining",
      "url": "Information management"
    },
    {
      "abstract": "With the ever-increasing growth of data and information, finding the appropriate knowledge becomes a real challenge and an urgent task. Traditional data and information retrieval systems that support the current web are no longer adequate for knowledge seeking tasks. Knowledge retrieval systems will be the next generation of retrieval systems to serve those purposes. However, the key problem becomes how to construct the basic unit of knowledge retrieval, crossing from data retrieval, information retrieval to knowledge retrieval. From the perspective of knowledge presentation and user needs, the knowledge is a category with context. This paper proposes a semantic text deep mining based on knowledge element. The basic unit of knowledge retrieval and the semantic triangle model of knowledge element are discussed. Application of semantic triangle of knowledge element is given by an example of mining electronic medical records. Experimental results verify the validity and feasibility of the design scheme.",
      "title": "17907 Semantic Text Deep Mining Based on Knowledge Element",
      "url": "information retrieval systems"
    },
    {
      "abstract": "This paper presents a new model of lexical-semantic representation of the lexicon which is intended for use in word sense disambiguation and semantic representation of text. It is usage-based, and part of it can be created automatically from an electronic dictionary. Although there remains more work to be done, the current model is provided with both syntactic and semantic contexts in which words occur, and thus effective in word sense disambiguation and the semantic representation of text. Drawing on XML technology, and designed in a simple data structure, it would be easy to implement and extend the lexicon computationally.",
      "title": "17909 Lexical-Semantic Representation of the Lexicon for Word Sense Disambiguation and Text Understanding",
      "url": "lexical-semantic representation"
    },
    {
      "abstract": "This paper describes a text representation and searching technique labeled as Semantic Vector Space Model (SVSM). The proposed technique combines Saltons VSM (1991) with distributed representation of semantic case structures of natural language text. It promises a way of abstracting and encoding richer semantic information of natural language text, and therefore, a better precision performance of IR, without involving sophisticated semantic processing.",
      "title": "17910 The semantic vector space model (SVSM): a text representation and searching technique",
      "url": "SVSM"
    },
    {
      "abstract": "We discuss here the search for inter-document references as an alternative to the grouping of document inventories based on a full text semantic analysis. The used document inventory, which is not publicly available, was provided to us by the European Union (EU) in the framework of an EU project, the aim of which was to analyse, classify, and visualise EU funded research in social sciences and humanities in EU framework programmes FP5 and FP6. This project, called the SSH project for short, was aimed at the evaluation of the contributions of research to the development of EU policies. For the semantic based grouping, we start from a Multi-Dimensional Scaling analysis of the document vectors, which is the result of a prior semantic analysis. As an alternative to a semantic analysis, we searched for inter-document references or direct references. Direct references are defined as terms that explicitly refer to other documents present in the inventory. We show that the grouping based on references is largely similar to the one based on semantics, but with considerably less computational efforts. In addition, the non-expert can make better use of the results, since the references are displayed as graphical webpages with hyperlinks pointing to both the referenced and the referencing document(s), and the reason of linkage. Finally, we show that the combination of a database, to store the data and the (intermediate) results, and a webserver, to visualise the results, offers a powerful platform to analyse the document inventory and to share the results with all participants/collaborators involved in a data- and computation intensive EU-project, thereby guaranteeing both data- and result-consistency.",
      "title": "17912 Inter-document reference detection as an alternative to full text semantic analysis in document clustering",
      "url": "file servers"
    },
    {
      "abstract": "This paper originally proposes a three-setp algorithm. First, CoTraining is employed for filtering out the likely positive data from the unlabeled dataset U. Second, we got vectors of documents in positive set using semantic-based feature extraction, then found the strong positive from likely positive set which is produced in first step. Those data picked out can be supplied to positive dataset P. Finally, a linear one-class SVM will learn from both the purified U as negative and the expanded P as positive. Because of the algorithms characteristic of automatic expanding positive dataset, the proposed algorithm especially performs well in situations where given positive dataset P is insufficient. A comprehensive experiment had proved that our algorithm is preferable to the existing ones.",
      "title": "17913 Using CoTraining and Semantic Feature Extraction for Positive and Unlabeled Text Classification",
      "url": "text analysis"
    },
    {
      "abstract": "This research proposes an expanded semantic graph definition that serves as a basis for an expanded semantic graph representation and graph matching approach. This representation separates the content and context and adds a number of semantic structures that encapsulate inferred information. The expanded semantic graph approach facilitates finding additional matches, identifying and eliminating poor matches, and prioritizing matches based on how much new knowledge is provided. By focusing on information of interest, doing pre-processing, and reducing processing requirements, the approach is applicable to problems where related information of interest is sought across a massive body of free text documents. Key aspects of the approach include (1) expanding the nodes and edges through inference using DL-Safe rules, abductive hypotheses, and syntactic patterns, (2) separating semantic content into nodes and semantic context into edges, and (3) applying relatedness measures on a node, edge, and sub graph basis. Results from tests using a ground-truthed subset of a large dataset of law enforcement investigator emails illustrate the benefits of these approaches.",
      "title": "17914 Expanded Semantic Graph Representation for Matching Related Information of Interest across Free Text Documents",
      "url": "information retrieval"
    },
    {
      "abstract": "In conventional text categorization algorithms, documents are symbolized as bag of words (BOW) with the fact that documents are supposed to be independent from each other. While this approach simplifies the models, it ignores the semantic information between terms of each document. In this study, we develop a novel method to measure semantic similarity based on higher-order dependencies between documents. We propose a kernel for Support Vector Machines (SVM) algorithm using these dependencies which is called Higher-Order Semantic Kernel. With the aim of presenting comparative performance of Higher-Order Semantic Kernel we performed many experiments not only with our algorithm but also with existing traditional first-order kernels such as Polynomial Kernel, Radial Basis Function Kernel, and Linear Kernel. The experiments using Higher-Order Semantic Kernel on several well-known datasets show that classification performance improves significantly over the first-order methods.",
      "title": "17917 A novel higher-order semantic kernel for text classification",
      "url": "Semantics"
    },
    {
      "abstract": "A search results ranking method that uses semantic features and a cluster coherence measure is introduced in this paper. The quality of the returned search results is improved by grouping semantically related texts into clusters displayed in descending cluster size order. First the term-document matrix is constructed where the documents correspond to individual texts. Then, nonnegative matrix factorization (NMF) is used to group the texts into semantically related clusters. Only those clusters whose coherence is greater than a threshold value are displayed. In this way trending conceptually similar texts that re-occur in the input of multiple users are identified. The advantage of this approach compared to other methods [6] consists in the fact that the clusters in the approach introduced in this paper are computed by semantic similarity and not only by texts counters.",
      "title": "17921 A new search method for ranking short text messages using semantic features and cluster coherence",
      "url": "Fabrics"
    },
    {
      "abstract": "To overcome the subjective factors of faceted classification representation, the method combined the faceted classification with text retrieval is used to describe the components. Meanwhile, from the semantic view and combined optimization techniques, a component clustering algorithm based on semantic similarity and optimization is proposed. This algorithm can reduce the subjective factors of faceted classification, and further improve the efficiency and accuracy of component search. And compared with component clustering effect based on vector space model, the experiments prove that this component clustering algorithm based on semantic similarity and optimization is effective which can improve the result of component clustering and raise the clustering quality.",
      "title": "17923 A Component Clustering Algorithm Based on Semantic Similarity and Optimization",
      "url": "pattern clustering"
    },
    {
      "abstract": "Text classification has been widely used to assist users with the discovery of useful information from the Internet. However, current text classification systems are based on the ldquoBag of Wordsrdquo (BOW) representation, which only accounts for term frequency in the documents, and ignores important semantic relationships between key terms. To overcome this problem, previous work attempted to enrich text representation by means of manual intervention or automatic document expansion. The achieved improvement is unfortunately very limited, due to the poor coverage capability of the dictionary, and to the ineffectiveness of term expansion. Fortunately, DBpedia appeared recently which contains rich semantic information. In this paper, we proposed a method compiling DBpedia knowledge into document representation to improve text classification. It facilitates the integration of the rich knowledge of DBpedia into text documents, by resolving synonyms and introducing more general and associative concepts. To evaluate the performance of the proposed method, we have performed an empirical evaluation using SVM calssifier on several real data sets. The experimental results show that our proposed framework, which integrates hierarchical relations, synonym and associative relations with traditional text similarity measures based on the BOW model, does improve text classification performance significantly.",
      "title": "17929 A Novel Conception Based Texts Classification Method",
      "url": "DBpedia"
    },
    {
      "abstract": "Through the Study of English semantic similarity, based on the previous, we designed the method of calculating English semantic similarity by Word Net. We proposed a vector space representation model of English standard information based on semantic similarity and applied the model on text clustering. This model resolves the problem of semantic correlation between the characteristics which is ignored by vector space model and reduces the semantic loss of practical application systems. Finally, by the experimentation we prove that this method improve the accuracy of text clustering.",
      "title": "17942 A Kind of Vector Space Representation Model Based on Semantic in the Field of English Standard Information",
      "url": "english standard information"
    },
    {
      "abstract": "Text sentimental mining is aim to obtain the orientation information of text, and studying users opinion is relevant because through it concerned departments could make correct responses. In this paper, we introduce methods to discriminatively learn semantic comprehension for use as features in text classification. Based on the existed HowNet Knowledge, we consider improving algorithm for calculating Chinese term semantic value. Then, conditions of adverbs, conjunctions and text-structure are added step by step. And the experimental results show that the proposed approach is suitable for judging texts sentimental orientation.",
      "title": "17944 Research on analyzing sentiment of texts based on semantic comprehension",
      "url": "semantic comprehension"
    },
    {
      "abstract": "This paper presents a new method of hierarchical text clustering based on combination of latent semantic analysis (LSA) and hierarchical TGSOM, which is called TCBLHT method. The text clustering result using traditional methods cannot show hierarchical structure, however, the hierarchical structure is very important in text clustering. The TCBLHT method can automatically achieve hierarchical text clustering, and establishes vector space model (VSM) of term weight by using the theory of LSA, then semantic relation is included in the vector space model. Both theory analysis and experimental results confirm that TCBLHT method decreases the number of vector, and enhances the efficiency and precision of text clustering.",
      "title": "17949 TCBLHT: a new method of hierarchical text clustering",
      "url": "text analysis"
    },
    {
      "abstract": "Most of the traditional text classification methods employ Bag of Words (BOW) approaches relying on the words frequencies existing within the training corpus and the testing documents. Recently, studies have examined using external knowledge to enrich the text representation of documents. Some have focused on using WordNet which suffers from different limitations including the available number of words, synsets and coverage. Other studies used different aspects of Wikipedia instead. Depending on the features being selected and evaluated and the external knowledge being used, a balance between recall, precision, noise reduction and information loss has to be applied. In this paper, we propose a new Centroid-based classification approach relying on Wikipedia to enrich the representation of documents through the use of Wikpedias concepts, categories structure, links, and articles text. We extract candidate concepts for each class with the help of Wikipedia and merge them with important features derived directly from the text documents. Different variations of the system were evaluated and the results show improvements in the performance of the system.",
      "title": "17951 Centroid-based Classification Enhanced with Wikipedia",
      "url": "Wikipedia"
    },
    {
      "abstract": "Extracting useful information from the web is the most significant issue of concern for the realization of semantic web. This may be achieved by several ways among which Web Usage Mining, Web Scrapping and Semantic Annotation plays an important role. Web mining enables to find out the relevant results from the web and is used to extract meaningful information from the discovery patterns kept back in the servers. Web usage mining is a type of web mining which mines the information of access routes/manners of users visiting the web sites. Web scraping, another technique, is a process of extracting useful information from HTML pages which may be implemented using a scripting language known as Prolog Server Pages(PSP) based on Prolog. Third, Semantic annotation is a technique which makes it possible to add semantics and a formal structure to unstructured textual documents, an important aspect in semantic information extraction which may be performed by a tool known as KIM(Knowledge Information Management). In this paper, we revisit, explore and discuss some information extraction techniques on web like web usage mining, web scrapping and semantic annotation for a better or efficient information extraction on the web illustrated with examples.",
      "title": "17952 Information Extraction Using Web Usage Mining, Web Scrapping and Semantic Annotation",
      "url": "data mining"
    },
    {
      "abstract": "How to build a text knowledge representation model, which carries rich knowledge and has a flexible reasoning ability as well as can be automatically constructed with a low computational complexity, is a fundamental challenge for reasoning-based knowledge services, especially with the rapid growth of web resources. However, current text knowledge representation models either lose much knowledge [e.g., vector space model (VSM)] or have a high complex computation [e.g., latent Dirichlet allocation (LDA)]",
      "title": "17953 Power Series Representation Model of Text Knowledge Based on Human Concept Learning",
      "url": "knowledge representation"
    },
    {
      "abstract": "Social Network Analysis (SNA) and Web Mining (WM) techniques are being applied to study the structures of social networks in order to manage their dynamics and predict their evolution. This paper describes how we used Semantically-Interlinked Online Communities (SIOC) ontology to represent the (latent) semantic relationships between the members of a large community forum (about 2,500), Plexilandia. We extended SIOC, taking advantage of topic-based text mining and developed data mining algorithms that used our SIOC extensions to provide a better understanding of the social dynamics of the members of the Plexilandia community. This new understanding helped us to detect and discover the key members of Plexilandia successfully.",
      "title": "17959 Leveraging Social Network Analysis with Topic Models and the Semantic Web",
      "url": "social networking (online)"
    },
    {
      "abstract": "This paper describes our work on developing a language-independent technique for discovery of implicit knowledge about patents from multilingual patent information sources. Traditional techniques of multi- and cross-language patent retrieval are mostly based on the process of translation. One major problem of those is that it is difficult to find related patents produced from other countries in a stand-alone patent information system. In this paper, we present a novel system platform to support locating similar and relevant multilingual patent documents. The platform is developed using a multilingual vector space based on the latent semantic indexing (LSI) model, and utilizing collected professional Chinese-English parallel corpora for training the system model. These multilingual patent documents can then be mapped into the semantic vector space for evaluating their similarity by means of text clustering techniques. The preliminary results show that our platform framework has potential for retrieval and relatedness evaluation of multilingual patent documents.",
      "title": "17961 A Multilingual Patent Text-Mining Approach for Computing Relatedness Evaluation of Patent Documents",
      "url": "patents"
    },
    {
      "abstract": "Medical Knowledge Discovery and Data Mining (KDD) over text is a promising yet difficult technology for unlocking meaning and uncovering associations in vast clinical text repositories. We report our experience in developing a new text analytic system called MEDAT or Medical Exploratory Data Analysis over Text, which overcomes several problems in text mining. The MEDAT system employs an annotated semantic index with a large number of assertions (propositions). The semantic index is able to capture complex assertions which encapsulate conceptual relationships including their modifiers at a granular level. The index represents semantically equivalent sentences with the same symbols, a necessary component for KDD semantic queries, including semantic Boolean and correlation queries. The graphical user interface enables users to perform complex semantic analysis of the Roentgen corpus, consisting of 594,000 de-identified radiology reports with 4.3 million sentences, without having to learn a programming language. The MEDAT architecture offers a novel framework for text mining in other medical domains.",
      "title": "17962 Knowledge Discovery and Data Mining of Free Text Radiology Reports",
      "url": "data mining"
    },
    {
      "abstract": "The cluster of search results can facilitate users in finding the needed from massive information. But the effect of the traditional text clustering has been verified not good enough. Lingo Algorithm, which adopts LSI for clustering, generates candidate labels first, then distributes the documents, and forms the clusters finally. On the basis of Lingo Algorithm, this paper presents a linear weighted method of Single-Pass improvement, which integrates HowNet semantic similarity and cosine similarity, fuses and rediscovers clusters, and extracting the cluster labels. The experiments have showed that our method it achieves a good results in clusters in the form of purity and F-measure.",
      "title": "17964 Search Results Clustering Based on a Linear Weighting Method of Similarity",
      "url": "text analysis"
    },
    {
      "abstract": "This paper introduces the concept algebra (CA) theory as a basis for the conceptual representation and the derivation of text processing to realize a semantic based retrieval system. We also take advantage of Hownet to create the concept attributes space for concept algebra. With the help of LTP, we get the key words and their dependent relations of every sentence to build the CA concept representation of the content with a five-tuple. Concepts make it possible to express both the keyword itself and the semantic relation with its context. According to the demands of text retrieval, some CA operations are optimized to calculate the relations and similarity between concepts. Besides, a text retrieval system framework which processes information based on the concept relations at a concept level is also proposed to verify the advantages of our method.",
      "title": "17966 A Text Representation and Retrieval Method Based on Concept Algebra",
      "url": "Hownet"
    },
    {
      "abstract": "Quality of a review can be identified by reviewing a review. Quantifiable factors that help identify the quality of a review include quality and tone of review comments, and the number of tokens each contains. We use machine-learning techniques such as latent semantic analysis (LSA) and cosine similarity to classify comments based on their quality and tone. Our paper details experiments that were conducted on student review and metareview data by using different data pre-processing steps. We compare these pre-processing steps and show that when applied to student review data, they help improve data quality by providing better text classification. Our technique helps predict metareview scores for student reviews.",
      "title": "17980 Automated Assessment of Review Quality Using Latent Semantic Analysis",
      "url": "reviews"
    },
    {
      "abstract": "In this paper, we present a document visualization technique for data analysis based on the semantic representation of text in the form of a directed graph, referred to as semantic graph. It is derived using natural language processing as follows. Firstly subject- verb-object triplets are automatically extracted from the Penn Treebank parse tree obtained for each sentence in the document. Secondly, the triplets are further enhanced by linking them to their corresponding co-referenced named entity, by resolving pronominal anaphors as well as attaching the associated WordNet synset. Starting from the documents semantic graph and the list of extracted triplets we automatically generate the document summary, for which we also derive the semantic representation.",
      "title": "17983 Document Visualization Based on Semantic Graphs",
      "url": "natural language processing"
    },
    {
      "abstract": "This paper presents a system for visualizing the information contained in the text of a web page. The goal of the visualization is to help the users better and faster understand the text on a web page and/or find related content on the internet. These visualizations are possible due to the use of text mining, natural language processing and semantic web technologies. Our system tries to make these technologies instantly accessible to a wide variety of users reading a wide variety of web pages. This high coverage of both users and content can be achieved because the system is implemented as an extension to Firefox, one of the most popular browsers, and because the visualizations are computed on the fly for any page the user happens to be reading at a given moment.",
      "title": "17984 Visualization of Web Page Content Using Semantic Technologies",
      "url": "natural language processing"
    },
    {
      "abstract": "Methods of semantic relatedness are essential for wide range of tasks such as information retrieval and text mining. This paper, concerned with these automated methods, attempts to improve Gloss Vector semantic relatedness measure for more reliable estimation of relatedness between two input concepts. Generally, this measure by considering frequency cut-off for big rams tries to remove low and high frequency words which usually do not end up being significant features. However, this naive cutting approach can lead to loss of valuable information. By employing point wise mutual information (PMI) as a measure of association between features, we will try to enforce the foregoing elimination step in a statistical fashion. Applying both approaches to the biomedical domain, using MEDLINE as corpus, MeSH as thesaurus, and available reference standard of 311 concept pairs manually rated for semantic relatedness, we will show that PMI for removing insignificant features is more effective approach than frequency cut-off.",
      "title": "17987 Improving Gloss Vector Semantic Relatedness Measure by Integrating Pointwise Mutual Information: Optimizing Second-Order Co-occurrence Vectors Computed from Biomedical Corpus and UMLS",
      "url": "Semantic Relatedness"
    },
    {
      "abstract": "With the development of Internet, more and more public users prefer to present their viewpoints of government policies. They often comment on some emergencies through news, blogs and so on. Their opinions influence decision makers of government to make right decisions. However, large numbers of news and related comments are produced when an emergency occurs and officers are very difficult to read and analyze all of them in seconds. Specially, comments usually are short texts and common clustering technologies are not suited to analyze them. In this paper, we firstly propose a framework based on semantic web technologies to recommend news and related comments in order to aid different officers to get their interesting news rapidly. Then, a new short text clustering method is discussed to analyze related comments. Finally, a news recommender system based on above approaches is introduced.",
      "title": "17989 Analyze and Recommend News Comments in E-Government",
      "url": "pattern clustering"
    },
    {
      "abstract": "In this paper we identify some limitations of contemporary information extraction mechanisms in the context of biomedical literature. We present an extraction mechanism that generates structured representations of textual content. Our extraction mechanism achieves this by extracting compound entities, and relationships between them, occuring in text. A detailed evaluation of the relationship and compound entities extracted is presented. Our results show over 62% average precision across 8 relationship types tested with over 82% average precision for compound entity identification.",
      "title": "17990 Joint Extraction of Compound Entities and Relationships from Biomedical Literature",
      "url": "biomedical literature"
    },
    {
      "abstract": "Hitherto, it has not been easy to interpret the meaning of the amount of discrimination information conveyed in a term rationally and explicitly within practical application contexts",
      "title": "17991 An Information-Theoretic Foundation for the Measurement of Discrimination Information",
      "url": "statistical analysis"
    },
    {
      "abstract": "The boom of opinion-rich resources such as online review Websites, discussion groups, personal blogs and forums on the Web has attracted many research efforts on opinion mining. Positive and negative opinions represented in review documents are helpful information for governments to improve their services, for companies to market their products, and for customers to purchase their commodities. In this paper, we introduce a new approach that employs finer granularity clustering for opinions extraction and clustering for the calculation of their sentiment orientation of opinions. The experimental result shows that the approach is qualitatively quite useful when used to analyze the netizens opinions to hot topics from some Websites.",
      "title": "17994 Finer Granularity Clustering for Opinion Mining",
      "url": "discussion groups"
    },
    {
      "abstract": "Living in the modern technology dependent world, we heavily rely on electronically stored data and information, to come up with sound and timely decisions. Considering the entire information technology world, there exists an unimaginable volume of data which contains a lot of information which is relevant to various kinds of fields. But the problem emerges when we are interested to find out about a particular subject. This is due to its scattered nature of relevant and non-relevant data. Therefore it is fair to say that there exists a critical need for a system which could create an ordered structure that provides a way of modeling the underlying relationships of data elements which will ultimately result in a much easier process of decision making. txtKnot is all about solving the above problem by generating a meaningful hierarchy of concepts from a set of unsorted text documents, thus enabling the visualization of relationships that exist within the set of documents. It consists of four main components namely, Data Extraction Module, Data Pre-processor Module, Text Clustering Module and Concept Hierarchy Generation Module. These four components are integrated together in order to fulfill the main objective of providing an easy to use method of organizing, visualizing, searching and filtering of the huge amount of electronically available unsorted textual data.",
      "title": "17996 txtKnot - Text clustering based concept hierarchy to generalize from different text sources",
      "url": "concept hierarchy generation module"
    },
    {
      "abstract": "The accelerating increase in the biomedical literature makes keeping up with recent advances challenging for researchers thus making automatic extraction and discovery of knowledge from this vast literature a necessity. Building such systems requires automatic detection of lexico-semantic event structures governed by the syntactic and semantic constraints of human languages in sentences of biomedical texts. The lexico-semantic event structures in sentences are centered around the predicates and most semantic role labeling (SRL) approaches focus only on the arguments of verb predicates and neglect argument taking nouns which also convey information in a sentence. In this article, a noun argument structure (NAS) annotated corpus named BioNom and a SRL system to identify and classify these structures is introduced. Also, a genetic algorithm-based feature selection (GAFS) method is introduced and global inference is applied to significantly improve the performance of the NAS Bio SRL system.",
      "title": "17998 Automatic Identification and Classification of Noun Argument Structures in Biomedical Literature",
      "url": "feature extraction"
    },
    {
      "abstract": "With the vast amount of digital text materials available on the Net, it is almost impractical for people to absorb all related information in a timely manner. This problem has been overcome by erstwhile researchers and scientists of data mining. The efficiency in the methods and exploratory analysis has to be ascertained yet. Document wise term frequencies and inverted frequencies are available to calculate the statistical importance among the documents. Determining the time line importance of the documents plays very essential role than just finding the documents importance. LSI is a basic PCA approach, which is proposed with time-line approach and has been discussed comparatively in this paper.",
      "title": "17999 Text mining: Finding hot topics TF*PDF vs. LSI",
      "url": "principal component analysis"
    },
    {
      "abstract": "Clustering semantically related terms is crucial for many applications such as document categorization, and word sense disambiguation. However, automatically identifying semantically similar terms is challenging. We present a novel approach for automatically determining the degree of relatedness between terms to facilitate their subsequent clustering. Using the analogy of ensemble classifiers in machine learning, we combine multiple techniques like contextual similarity and semantic relatedness to boost the accuracy of our computations. A new method, based on Yarowskypsilas word sense disambiguation approach, to generate high-quality topic signatures for contextual similarity computations, is presented. A technique to measure semantic relatedness between multi-word terms, based on the work of Hirst and St. Onge is also proposed. Experimental evaluation reveals that our method outperforms similar related works. We also investigate the effects of assigning different importance levels to the different similarity measures based on the corpus characteristics.",
      "title": "18000 Ensemble Similarity Measures for Clustering Terms",
      "url": "text analysis"
    },
    {
      "abstract": "The semantic analysis and context awareness in data mining can intensively increase results precision. In this research different semantic relatedness functions called 'Measure of Semantic Relatedness (MSR)' are discussed and compared. We found that the quality and accuracy of MSRs are different when applied in various contexts. Here we compared several MSR algorithms using different corpuses and have analyzed the results.",
      "title": "18003 A comparative study on Measure of Semantic Relatedness function",
      "url": "MSR"
    },
    {
      "abstract": "Ontology plays an essential role in the formalization of common information (e.g., products, services, relationships of businesses) for effective human-computer interactions. However, engineering of these ontologies turns out to be very labor intensive and time consuming. Although some text mining methods have been proposed for automatic or semi-automatic discovery of crisp ontologies, the robustness, accuracy, and computational efficiency of these methods need to be improved to support large scale ontology construction for real-world applications. This paper illustrates a novel fuzzy domain ontology mining algorithm for supporting real-world ontology engineering. In particular, contextual information of the knowledge sources is exploited for the extraction of high quality domain ontologies and the uncertainty embedded in the knowledge sources is modeled based on the notion of fuzzy sets. Empirical studies have confirmed that the proposed method can discover high quality fuzzy domain ontology which leads to significant improvement in information retrieval performance.",
      "title": "18004 Mining Fuzzy Domain Ontology from Textual Databases",
      "url": "Databases"
    },
    {
      "abstract": "Every research field (domain) has a lot of data or information in textual format. The main problem is how to optimally arrange and transform the textual data or information to explore research issues. The authors propose a framework that may help in exploration of research issues from text of specific field (domain). First textual data or information is transformed to semantic network by using some transformation functions or algorithms. Then the semantic network is transformed to frames using other transformation functions or algorithms. Knowledge base, semantic networks and frames repositories are updated from new concepts, semantic networks and frames respectively. By doing this, exploration of research issues from text may become quick, efficient and easy.",
      "title": "18006 Proposed Text Mining Framework to Explore Issues from Text in a Certain Domain",
      "url": "knowledge base repositories"
    },
    {
      "abstract": "Multi-task learning has proven to be useful to boost the learning of multiple related but different tasks. Meanwhile, latent semantic models such as LSA and LDA are popular and effective methods to extract discriminative semantic features of high dimensional dyadic data. In this paper, we present a method to combine these two techniques together by introducing a new matrix tri-factorization based formulation for semi-supervised latent semantic learning, which can incorporate labeled information into traditional unsupervised learning of latent semantics. Our inspiration for multi-task semantic feature learning comes from two facts, i.e., 1) multiple tasks generally share a set of common latent semantics, and 2) a semantic usually has a stable indication of categories no matter which task it is from. Thus to make multiple tasks learn from each other we wish to share the associations between categories and those common semantics among tasks. Along this line, we propose a novel joint Nonnegative matrix tri-factorization framework with the aforesaid associations shared among tasks in the form of a semantic-category relation matrix. Our new formulation for multi-task learning can simultaneously learn (1) discriminative semantic features of each task, (2) predictive structure and categories of unlabeled data in each task, (3) common semantics shared among tasks and specific semantics exclusive to each task. We give alternating iterative algorithm to optimize our objective and theoretically show its convergence. Finally extensive experiments on text data along with the comparison with various baselines and three state-of-the-art multi-task learning algorithms demonstrate the effectiveness of our method.",
      "title": "18008 Multi-task Semi-supervised Semantic Feature Learning for Classification",
      "url": "pattern classification"
    },
    {
      "abstract": "With the increasing development of web service technologies, more and more applications need web services to give strong capabilities. We divide the existing information space into two levels - semantic space and information space. It will make the tranditional operations on web services been abstracted from information space level into semantic space level. Through semantic computing and semantic reasoning, it is more effective to implement the service discovering, searching , and composing etc. We give a semantic resource description framework RDF4S and its description document WSDL4S for semantic web service. This paper presents an approach to establish semantic information space model based on WSDL4S document using data mining technologies and ontology technologies. The method constructs a mapping between general information space and semantic information space. A specific case is given to prove the validity of this method at the end of the paper.",
      "title": "18009 A Kind of Method of Establishing Semantic Information Space Model Based on WSDL4S Document",
      "url": "ontologies (artificial intelligence)"
    },
    {
      "abstract": "Subjective statements provide qualitative evaluation of the financial status of the reporting corporations, in addition to the quantitative information released in US 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax and simple semantics. The CRF models showed reasonable effectiveness to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. We also identified opinion patterns in the form of multi-word expressions (MWEs), which is a major contribution of our work. In a recent article published in a prestigious journal in Finance, single words, rather than MWEs, were reported to indicate positive and negative judgments in financial statements.",
      "title": "18012 Mining Opinion Holders and Opinion Patterns in US Financial Statements",
      "url": "emotion recognition"
    },
    {
      "abstract": "This paper proposes a novel framework for automatic text categorization problem based on the kernel density classifier. The overall goal is to tackle two main issues in automatic text categorization problems: the interpretability and the performance. Specifically, to solve the interpretability issue, the latent semantic analysis technique is used to construct a topic space, in which each dimension represents a single topic. The text features are extracted directly from this topic space. To solve the performance issue, classifierspsila parameters are optimized for either cost-sensitive or non-cost-sensitive categorization. We have experimentally evaluated the proposed framework by using a corpus of twenty newsgroups. The experimental results confirm the effectiveness of the framework to utilize the features from the topic model for cost-sensitive categorization.",
      "title": "18013 Using Kernel Density Classifier with Topic Model and Cost Sensitive Learning for Automatic Text Categorization",
      "url": "text analysis"
    },
    {
      "abstract": "This paper presents an intelligent corporate governance analysis and rating system, called ICGA, capable of retrieving SEC required documents of public companies and performing analysis and rating in terms of recommended corporate governance practices. With local knowledge bases, databases, and semantic networks, ICGA is able to automatically evaluate the strengths, deficiencies, and risks of a companys corporate governance practices and board of directors based on the documents stored in the SEC EDGAR database. The produced score reduces a complex corporate governance process and related policies into a single number which enables concerned government agencies, investors and legislators to assess the governance characteristics of individual companies.",
      "title": "18014 An Intelligent Text Mining System Applied to SEC Documents",
      "url": "knowledge based systems"
    },
    {
      "abstract": "The paper proposes a method of social tension detection and intention recognition based on natural language analysis of social networks, forums, blogs and news comments. Our approach combines natural language syntax and semantics analysis with statistical processing to identify possible indicators of social tension. The universal components of our method incorporate the general laws of natural language, general psychological, sociological and psycholinguistic rules and trends typical of social tension detection in virtual discussions. Automatic monitoring of the contents of discussions helps to timely unveil hidden signs of tension and makes it possible to predict the likely development of the situation.",
      "title": "18017 Social Tension Detection and Intention Recognition Using Natural Language Semantic Analysis: On the Material of Russian-Speaking Social Networks and Web Forums",
      "url": "social networking (online)"
    },
    {
      "abstract": "Most current translation assignments and tests are evaluated in manual work instead of machine, as automatic translation assessment has been considered to be very difficult. In this paper, a system was proposed to evaluate the quality of translation based on semantic similarities and fuzzy neartude. The experiment results are promising and indicate that the system can identify the semantic characteristics and yield scores that correlate with human judgments of translation quality. Our future work is to improve the evaluation strategies and introduce more linguistic resources.",
      "title": "18018 An Automatic Translation Evaluation System Based on Semantic Similarities and Fuzzy Neartude",
      "url": "automatic translation evaluation system"
    },
    {
      "abstract": "Measuring similarity between words using a search engine based on page counts alone is a challenging task. Search engines consider a document as a bag of words, ignoring the position of words in a document. In order to measure semantic similarity between two given words, this paper proposes a transformation function for web measures along with a new approach that exploits the documents title attribute and uses page counts alone returned by Web search engines. Experimental results on benchmark datasets show that the proposed approach outperforms snippets alone methods, achieving a correlation coefficient up to 71%.",
      "title": "18021 Word Semantic Similarity Based on Documents Title",
      "url": "Web search engines"
    },
    {
      "abstract": "Biomedical entity extraction from unstructured web documents is an important task that needs to be performed in order to discover knowledge in the veterinary medicine domain. In general, this task can be approached by applying domain specific ontologies, but a review of the literature shows that there is no universal dictionary, or ontology for this domain. To address this issue, we manually construct an ontology for extracting entities such as: animal disease names, viruses and serotypes. We then use an automated ontology expansion approach to extract semantic relationships between concepts. Such relationships include asserted synonymy, hyponymy and causality. Specifically, these relationships are extracted by using a set of syntactic patterns and part-of-speech tagging. The resulting ontology contains richer semantics compared to the manually constructed ontology. We compare our approach for extracting synonyms, hyponyms and other disease related concepts, with an approach where the ontology is expanded using GoogleSets, on the veterinary medicine entity extraction task. Experimental results show that our semantic relationship extraction approach produces a significant increase in precision and recall as compared to the GoogleSets approach.",
      "title": "18022 Boosting Biomedical Entity Extraction by Using Syntactic Patterns for Semantic Relation Discovery",
      "url": "information retrieval"
    },
    {
      "abstract": "Digitizing a historical document using ontologies and natural language processing techniques can transform it from arcane text to a useful knowledge base.The Handbook on Architecture (Handbuch der Architektur) was perhaps one of the most ambitious publishing projects ever. Like a 19thcentury Wikipedia, it attempted nothing less than a full account of all architectural knowledge available at the time, both past and present. It covers topics from Greek temples to contemporary hospitals and universities",
      "title": "18024 Converting a Historical Architecture Encyclopedia into a Semantic Knowledge Base",
      "url": "deductive databases"
    },
    {
      "abstract": "Social tags inherent in online music services such as Last.fm provide a rich source of information on musical moods. The abundance of social tags makes this data highly beneficial for developing techniques to manage and retrieve mood information, and enables study of the relationships between music content and mood representations with data substantially larger than that available for conventional emotion research. However, no systematic assessment has been done on the accuracy of social tags and derived semantic models at capturing mood information in music. We propose a novel technique called Affective Circumplex Transformation (ACT) for representing the moods of music tracks in an interpretable and robust fashion based on semantic computing of social tags and research in emotion modeling. We validate the technique by predicting listener ratings of moods in music tracks, and compare the results to prediction with the Vector Space Model (VSM), Singular Value Decomposition (SVD), Nonnegative Matrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA). The results show that ACT consistently outperforms the baseline techniques, and its performance is robust against a low number of track-level mood tags. The results give validity and analytical insights for harnessing millions of music tracks and associated mood data available through social tags in application development.",
      "title": "18025 Semantic Computing of Moods Based on Tags in Social Media of Music",
      "url": "Content Analysis and Indexing"
    },
    {
      "abstract": "Many complex information needs that arise in biomedical disciplines require exploring multiple documents in order to obtain information. While traditional information retrieval techniques that return a single ranked list of documents are quite common for such tasks, they may not always be adequate. The main issue is that ranked lists typically impose a significant burden on users to filter out irrelevant documents. Additionally, users must intuitively reformulate their search query when relevant documents have not been not highly ranked. Furthermore, even after interesting documents have been selected, very few mechanisms exist that enable document- to-document transitions. In this paper, we demonstrate the utility of assertions extracted from biomedical text (called semantic predications) to facilitate retrieving relevant documents for complex information needs. Our approach offers an alternative to query reformulation by establishing a framework for transitioning from one document to another. We evaluate this novel knowledge-driven approach using precision and recall metrics on the 2006 TREC Genomics Track.",
      "title": "18026 Semantic Predications for Complex Information Needs in Biomedical Literature",
      "url": "query formulation"
    },
    {
      "abstract": "A keywords Extraction algorithm of Chinese documents based on TEXT-NET is proposed. By using Semantic similarity computation of Howmet theory, a text is mapped a TEXT-NET, and then combined with complex network theory and statistical methods to extract keywords. Experimental results show that the recall and precision rate has increased compared with small-world model method and statistical methods, and this method is more flexible because it is independent of fields.",
      "title": "18028 Research on keywords Extraction of Chinese documents based on TEXT-NET",
      "url": "text analysis"
    },
    {
      "abstract": "This paper presents an approach using social semantics for the task of topic labelling by means of Open Topic Models. Our approach utilizes a social ontology to create an alignment of documents within a social network. Comprised category information is used to compute a topic generalization. We propose a feature-frequency-based method for measuring semantic relatedness which is needed in order to reduce the number of document features for the task of topic labelling. This method is evaluated against multiple human judgement experiments comprising two languages and three different resources. Overall the results show that social ontologies provide a rich source of terminological knowledge. The performance of the semantic relatedness measure with correlation values of up to .77 are quite promising. Results on the topic labelling experiment show, with an accuracy of up to .79, that our approach can be a valuable method for various NLP applications.",
      "title": "18029 Social Semantics and Its Evaluation by Means of Semantic Relatedness and Open Topic Models",
      "url": "Knowledge based systems"
    },
    {
      "abstract": "This paper introduces a new event information extraction approach under the research of a semantic search engine based on temporal and spatial elements. It proposes a method combining the sematic knowledge and statistical learning method. Moreover, it emphasizes the expandability for event tracing function as well as pays more attention to robustness and universality. Relative to the traditional research focusing on some special domain texts or news texts, it explores a method to process complex Chinese texts which are field independent.",
      "title": "18030 Event Information Extraction Approach Based on Complex Chinese Texts",
      "url": "search engines"
    },
    {
      "abstract": "The paper describes an approach to natural language watermarking and hashing based on semantic structures. In our method we are interested in the linguistic semantic phenomenon of presupposition. Presupposition is implicit information that is taken for granted by the reader and establishes common ground between the authors and readers situational knowledge",
      "title": "18031 Text Watermarking against Ownership Rights Violation",
      "url": "watermarking"
    },
    {
      "abstract": "A domain conceptual lexicon can be used for the construction of a domain ontology. In this paper, a method of auto-constructing a domain conceptual lexicon is proposed. The conceptual lexicon is represented by WordNet senses and their relations. Using the domain conceptual lexicon, an extractive summarization method is proposed. A characteristic of the scheme is that each sentence of a document is represented by a set of WordNet senses and constitutes a fuzzy transaction for mining conceptual lexicon and ranking relevance. A prototype of this automatic text summarization scheme is built and an intrinsic method with the information-retrieval criteria is used for measuring the summary quality.",
      "title": "18033 Towards Auto-Construction of Domain Ontology: An Auto-Constructed Domain Conceptual Lexicon and its Application to Extractive Summarization",
      "url": "ontologies (artificial intelligence)"
    },
    {
      "abstract": "Sentiment mining aims at extracting features on which users express their opinions in order to determine the users sentiment towards the query object. Movie sentiment in Twitter provides an excellent base upon which to evaluate sentiment mining methodologies both because of the pervasiveness of discussions devoted to movie topics and because of the brevity of expression induced by twitters 140 word limitation. In this paper we explore movie sentiment expressed in Twitter microblogs. A multi-knowledge based approach is proposed using, Self-Organizing Maps and movie knowledge in order to model opinion across a multi-dimensional sentiment space. We develop a visual model to express this taxonomy of sentiment vocabulary and then apply this model in test data. The results show the effectiveness of the proposed visualization in mining sentiment in the domain of Twitter tweets.",
      "title": "18035 Unsupervised Artificial Neural Nets for Modeling Movie Sentiment",
      "url": "social networking (online)"
    },
    {
      "abstract": "Sentiment mining aims at extracting features on which users express their opinions in order to determine the users sentiment towards the query object. We mine over 70 million Twitter microblogs to gain knowledge regarding tourist sentiment on the travel resort destination Cancun in the Yucatan Peninsula of Mexico. We measure sentiment using a binary choice keyword algorithm and a multi-knowledge based approach is proposed using, Self-Organizing Maps and tourism domain knowledge in order to model sentiment. We develop a visual model to express this taxonomy of sentiment vocabulary and then apply this model to maximums and minimums in the time sentiment data. The results show practical knowledge can be extracted.",
      "title": "18036 Naive Bayes and unsupervised artificial neural nets for Cancun tourism social media data analysis",
      "url": "query formulation"
    },
    {
      "abstract": "We present a method to extract semantic relations from text corpora in an unsupervised way and use the output as preprocessed material for the building of a virtual environment. The experiments have been carried out in the context of the Flemish OntoBasis project. We have used two small corpora. The first one has been gathered from the Internet and is describing the megalithic ruin of Stonehenge. The other one, considered as a test case, is composed in order to describe the appearance and content of a shop. Using a shallow parser, we select functional relations, such as the syntactic structure subject-verb-object, from which we extract spatial information, in an unsupervised way on the Stonehenge corpus, and using domain knowledge for the shop corpus. We then compare and discuss the results, considering the differences between both corpora and to what extent the size of the corpus and the presence of domain knowledge influence the results. The adequacy of the relations extracted has been evaluated manually",
      "title": "18038 Automatic extraction of spatial relations",
      "url": "text analysis"
    },
    {
      "abstract": "This paper presents an ontology-based health care information extraction system - VnHIES. In the system, we develop and use two effective algorithms called semantic elements extracting algorithm and new semantic elements learning algorithm for health care semantic words extraction and ontology enhancement. The former algorithm will extract concepts (Cs), descriptions of concepts (Ds), pairs of concept and description(C-D) and Names of diseases (Ns) in health care information domain from Web pages. Those extracted semantic elements are used by latter algorithm that will render suggestions in which might contain new semantic elements for later use by domain users to enrich ontology. After extracting semantic elements, a document weighting algorithm is applied to get summary information of document with respect to all extracted semantic words and then to be stored in knowledge base which contains ontology and database to be used later in other applications. Our experiment results show that the approach is very optimistic with high accuracy in semantic extracting and efficiency in ontology upgrade. VnHIES can be used in many health care information management systems such as medical document classification, health care information retrieval system. VnHIES is implemented in Vietnamese language.",
      "title": "18044 A Proposal of Ontology-based Health Care Information Extraction System: VnHIES",
      "url": "medical information systems"
    },
    {
      "abstract": "3",
      "title": "18052 Collecting positive instances of #x201C",
      "url": "ontologies (artificial intelligence)"
    },
    {
      "abstract": "This article presents a tool that allows the user to generate intelligent systems. Initially, we show a Multi-Agent system called SMAGS, which has the ability of integrating different information systems and manage the information inherent to the project of interest in real time, as well as acting as a 'Virtual Manager'. Next, we included new Artificial Intelligence (IA) techniques with semantic interpretation and metrical testing to supply the user with a CASE (Computer Aided Software Engineering) tool with the ability of generating intelligent computerized information systems, which can learn from information from intelligent searches in ontology servers, case bases, specialist in the area, or even from fragmented data from the Internet, with the purpose of enhancing decision taking.",
      "title": "18053 Enhancing Managing Decisions with Artificial Intelligence",
      "url": "Intelligent systems"
    },
    {
      "abstract": "Normalization procedures attempt to remove non- biological variance within microarray datasets. The choice of normalization procedure is important, as it has a dramatic effect on downstream data analysis. Although many normalization procedures have been developed, comparison and evaluation of their performance is difficult. We present a method to evaluate normalization procedures by utilizing gene- gene associations derived from the biomedical literature via Latent Semantic Indexing. The functional coherence of co- regulated genes obtained from different normalized data sets is calculated in order to evaluate the effectiveness of each normalization procedure. The method was tested on three popular normalization procedures (MAS5, PDNN and RMA) applied to gene expression data across 71 recombinant inbred mouse brain samples. Results show that, on average, MAS5 outperforms both PDNN and RMA by producing a higher number of functionally cohesive gene sets. These results demonstrate that our literature-based cohesion analysis can provide an objective method for evaluation of normalization procedures.",
      "title": "18056 Literature-based Evaluation of Microarray Normalization Procedures",
      "url": "genetics"
    },
    {
      "abstract": "Short posts on micro-blogs are characterized by high ambiguity and non-standard language. We focus on detecting life events from such micro-blogs, a type of event which have not been paid much attention so far. We discuss the corpus we assembled and our experiments. Simpler models based on unigrams perform better than models that include history, number of retweets and semantic roles.",
      "title": "18057 Detecting Life Events in Feeds from Twitter",
      "url": "Text Classification"
    },
    {
      "abstract": "Word semantics is gaining increasing interest within linguistics in view of both, more adequate representational structures of the semantic system and methods and procedures to analyse it empirically. Due to the fact formal and operational means have been devised to describe and represent word connotation and/or denotation, this paper discusses some of the empirical problems connected with natural languages varying and vague meanings, how these can be analysed statistically from discourse data, and represented formally as fuzzy system of vocabulary mappings. Some examples computed from East- and West-German newspaper texts will be given at the end to illustrate the approaches feasibility.",
      "title": "18092 Fuzzy word meaning analysis and representation in linguistic semantics, an empirical approach to the reconstruction of lexical meanings in East- and West-German newspaper texts",
      "url": ""
    },
    {
      "abstract": "This article presents the experiments carried out at Jadavpur University as part of the participation in Semantic Textual Similarity (STS) of Task 6 @ Semantic Evaluation Exercises (SemEval-2012). Task-6 of SemEval- 2012 focused on semantic relations of text pair. Task-6 provides five different text pair files to compare different semantic relations and judge these relations through a similarity and confidence score. Similarity score is one kind of multi way classification in the form of grade between 0 to 5. We have submitted one run for the STS task. Our system has two basic modules - one deals with lexical relations and another deals with dependency based syntactic relations of the text pair. Similarity score given to a pair is the average of the scores of the above-mentioned modules. The scores from each module are identified using rule based techniques. The Pearson Correlation of our system in the task is 0.3880.",
      "title": "18093 JU CSE NLP: multi-grade classification of semantic similarity between text pairs",
      "url": ""
    },
    {
      "abstract": "One of the issues of Artificial Intelligence is the transfer of the knowledge conveyed by Natural Language into formalisms that a computer can interpret. In the Natural Language Processing department of the IBM France Paris Scientific Center, we are developing and evaluating a system prototype whose purpose is to build a semantic representation of written French texts in a rigorous formal model (the Conceptual Graph model, introduced by J.F Sowa [10]).",
      "title": "18098 Solving ambiguities in the semantic representation of texts",
      "url": ""
    },
    {
      "abstract": "In this paper, we present BioEve a fully automated event extraction system for bio-medical text. It first semantically classifies each sentence to the class type of the event mentioned in the sentence, and then using high coverage hand-crafted rules, it extracts the participants of that event. We participated in Task 1 of BioNLP 2009 Shared task, and the final evaluation results are described here. Our experimentation with different approaches to classify a sentence to bio-interaction classes are also shared.",
      "title": "18102 BioEve: bio-molecular event extraction from text using semantic classification and dependency parsing",
      "url": ""
    },
    {
      "abstract": "The creation and maintenance of domain ontologies is a costly and time-consuming task. With the advent of ontologies being used in many different fields of computer science, developing appropriate algorithms and methods to support or automatize ontology engineering have become an increasingly important goal. Hence, we present a connectionist approach to visualize semantic relations inherent in free-from text documents related to a specific domain. In particular, we exploit word co-occurrences to capture relatedness of words in order to generate numeric representations of the words contexts. We use the self-organizing map, a well-known neural network model with unsupervised learning function, to map the high-dimensional data onto a two-dimensional representation for convenient browsing. This intuitive view on the domain vocabulary supports the construction and enrichment of domain ontologies by making relevant concepts and their relations evident. We underline this approach with an example from the tourism domain.",
      "title": "18105 Improving domain ontologies by mining semantics from text",
      "url": ""
    },
    {
      "abstract": "Text mining refers to the discovery of previously unknown knowledge that can be found in text collections. In recent years, the text mining field has received great attention due to the abundance of textual data. A researcher in this area is requested to cope with issues originating from the natural language particularities. This survey discusses such semantic issues along with the approaches and methodologies proposed in the existing literature. It covers syntactic matters, tokenization concerns and it focuses on the different text representation techniques, categorisation tasks and similarity measures suggested.",
      "title": "18106 Overview and Semantic Issues of Text Mining",
      "url": "http://doi.acm.org/10.1145/1324185.1324190"
    },
    {
      "abstract": "The vast amount of textual information available today is useless unless it can be effectively and efficiently searched. The goal in information retrieval is to find documents that are relevant to a given user query. We can represent and document collection by a matrix whose (i, j) entry is nonzero only if the ith term appears in the jth document",
      "title": "18117 A Semidiscrete Matrix Decomposition for Latent Semantic Indexing Information Retrieval",
      "url": ""
    },
    {
      "abstract": "One of the most relevant problems with Information Retrieval (IR) softwares is the correct processing of complex lexical units, today also known as multiword units. The shortcomings are mainly due to the fact that such units are often considered as extemporaneous combinations of words retrievable by means of statistical routines. On the contrary, several linguistic studies, also dating back to the 60s, show that multiword units, and mainly compound nouns, are almost always fixed meaning units, with specific formal, morphological, grammatical and semantic characteristics. Furthermore, these units can be processed as dictionary entries, thus becoming concrete lingware tools useful to achieve efficient semantic information retrieval (IR). Therefore, in this paper we will focus on CATALOGA, an automatic IR software which retrieves terminological information from digitized texts without any human intervention. CATALOGA is actually configured as a stand-alone software which can be integrated in Web sites and portals to be used online. More specifically, we will describe its lingware and software characteristics, discussing their usage as a possible solution to current IR software limitations. The analytical procedure here described will prove itself appropriate for any type of digitized text, and will also represent a relevant support for the building and implementing of Semantic Web (SW) interactive platforms",
      "title": "18127 CATALOGA: a software for semantic and terminological information retrieval",
      "url": ""
    },
    {
      "abstract": "Latent Semantic Analysis (LSA) can be used to reduce the dimensions of large Term-Document datasets using Singular Value Decomposition. However, with the ever expanding size of data sets, current implementations are not fast enough to quickly and easily compute the results on a standard PC. The Graphics Processing Unit (GPU) can solve some highly parallel problems much faster than the traditional sequential processor (CPU). Thus, a deployable system using a GPU to speedup large-scale LSA processes would be a much more effective choice (in terms of cost/performance ratio) than using a computer cluster. In this paper, we presented a parallel LSA implementation on the GPU, using NVIDIA R Compute Unified Device Architecture (CUDA) and Compute Unified Basic Linear Algebra Subprograms (CUBLAS). The performance of this implementation is compared to traditional LSA implementation on CPU using an optimized Basic Linear Algebra Subprograms library. For large matrices that have dimensions divisible by 16, the GPU algorithm ran five to six times faster than the CPU version.",
      "title": "18154 Parallel latent semantic analysis using a graphics processing unit",
      "url": ""
    },
    {
      "abstract": "We designed and implemented TAGME, a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of TAGME with respect to known systems [5,8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. This annotation is extremely informative, so any task that is currently addressed using the bag-of-words paradigm could benefit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.",
      "title": "18164 TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)",
      "url": ""
    },
    {
      "abstract": "For machine learning methods, processing and understanding Chinese texts are difficult, for that the basic unit of Chinese texts is not character but phrases, and there is no natural delimiter in Chinese texts to separate the phrases. The processing of a large number of Chinese Web texts is more difficult, because such texts are often less topic focused, short, irregular, sparse, and lacking in context. It poses a challenge for mining, clustering, and classification of Chinese Web texts. Typically, the recognition accuracy of the real meaning of such texts is low. In this paper, we propose a method that recognizes stable and abstract semantic topics that express the highly hierarchical relationship behind the Chinese texts from BaiduBaike. Then, based on these semantic topics, a discrete distribution model is established to convert analysis to a convex optimization problem by geometric programming. Our experiments demonstrated that the proposed approach outperforms many conventional machine learning methods, such as KNN, SVM, WIKI, CRFs, and LDA, regarding the recognition of mini training data and short Chinese Web texts.  2015, Springer Science+Business Media New York.",
      "title": "213238 Classification of Chinese Texts Based on Recognition of Semantic Topics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84957935051&partnerID=40&md5=b1ae971d40a8b0c30aebb7ba74f9f1af"
    },
    {
      "abstract": "Text classification can help users to effectively handle and exploit useful information hidden in large-scale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the classification performance of short texts. In order to overcome the weakness, we propose a unified framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we first discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units. 11Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual information can be exploited. in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs). 22In order to prevent outliers, a Euclidean distance threshold is preset between semantic cliques and semantic units, which is used as restricted condition. of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix. 33The projected matrix is obtained by table looking up, which encodes Unigram level features. and expanded matrices are combined and fed into CNN in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.  2015 Elsevier B.V..",
      "title": "213239 Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949672394&partnerID=40&md5=f5197fb1683af84e3ceadf44fe8692c3"
    },
    {
      "abstract": "Automated regulatory and contractual compliance checking requires automated rule extraction from regulatory and contractual textual documents (e.g., contract specifications). Automated rule extraction is a challenging task that requires complex processing of text. In the proposed automated compliance checking (ACC) approach, the first step in automating the rule extraction process is automatically classifying the different documents and parts of documents (e.g., contract clauses) into predefined categories (environmental, safety, health, etc.) for preparing it for further text analysis and rule extraction. These categories are defined in a semantic model for normative reasoning. This paper presents a semantic, machine learning-based text classification algorithm for classifying clauses and subclauses of general conditions for supporting ACC in construction. The multilabel classification problem was transformed into a set of binary classification problems. Different machine learning algorithms, text preprocessing techniques, methods of text feature scoring, methods of feature weighting, and feature sizes were implemented and evaluated at different thresholds. The developed classifier achieved 100 and 96% recall and precision, respectively, on the testing data.  2014 American Society of Civil Engineers.",
      "title": "213240 Semantic Text Classification for Supporting Automated Compliance Checking in Construction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84952361634&partnerID=40&md5=b13fe3a9e39823500de6a63b2478495a"
    },
    {
      "abstract": "The existing methods for text classification fail to achieve high accuracy in processing Chinese texts, for that the basic unit of Chinese texts is not hanzis but Chinese phrases, and there is no natural delimiter in Chinese texts to separate the phrases. Things go even worse in the case of processing large number of Chinese Web texts, for these texts often lack of enough context, because most of these text are often short, irregular and sparse. In this paper, a new classification method is proposed for Chinese texts based on apparent semantics and latent aspects (ASLA). First, the apparent semantics of Chinese text are extracted as features instead of hanzis by BaiduBaike",
      "title": "213242 A method for Chinese text classification based on apparent semantics and latent aspects",
      "url": ""
    },
    {
      "abstract": "Many text classifications depend on statistical term measures to implement document representation. Such document representations ignore the lexical semantic contents of terms and the distilled mutual information, leading to text classification errors. This work proposed a document representation method, WordNet-based lexical semantic VSM, to solve the problem. Using WordNet, this method constructed a data structure of semantic-element information to characterize lexical semantic contents, and adjusted EM modeling to disambiguate word stems. Then, in the lexical-semantic space of corpus, lexical-semantic eigenvector of document representation was built by calculating the weight of each synset, and applied to a widely-recognized algorithm NWKNN. On text corpus Reuter-21578 and its adjusted version of lexical replacement, the experimental results show that the lexical-semantic eigenvector performs F1 measure and scales of dimension better than term-statistic eigenvector based on TF-IDF. Formation of document representation eigenvectors ensures the method a wide prospect of classification applications in text corpus analysis.  2015, Central South University Press and Springer-Verlag Berlin Heidelberg.",
      "title": "213245 WordNet-based lexical semantic classification for text corpus analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84930005413&partnerID=40&md5=bc90726ba81c73b15d35dc3dc1a89ff5"
    },
    {
      "abstract": "In this paper we propose a novel model 'recursive directed graph' based on feature structure, and apply it to represent the semantic relations of postpositive attributive structures in biomedical texts. The usages of postpositive attributive are complex and variable, especially three categories: present participle phrase, past participle phrase, and preposition phrase as postpositive attributive, which always bring the difficulties of automatic parsing. We summarize these categories and annotate the semantic information. Compared with dependency structure, feature structure, being recursive directed graph, enhances semantic information extraction in biomedical field. The annotation results show that recursive directed graph is more suitable to extract complex semantic relations for biomedical text mining.  2015, Wuhan University and Springer-Verlag Berlin Heidelberg.",
      "title": "213246 Semantic relation annotation for biomedical text mining based on recursive directed graph",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929163441&partnerID=40&md5=ddb4802e96500b449224cac8fff70156"
    },
    {
      "abstract": "This paper presents a novel text space model that represents textual documents for document clustering, which contains the concept space independently of the document and term spaces. The text model described here represents documents as matrices (i.e., 2nd-order tensors), and a document corpus is represented as a 3rd-order tensor. For this, it is necessary to produce the concept vector for each term that occurs in a given document, which is related to word sense disambiguation. As an external knowledge source for concept weighting, we employ the Wikipedia encyclopedia. Copyright 2015 ACM.",
      "title": "213247 Semantically enriching text representation model for document clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84955486192&partnerID=40&md5=28857155e99d0a1cfe51d385066808e5"
    },
    {
      "abstract": "Corpus is a kind of important resource for knowledge acquisition in the natural language processing (NLP). However, up to now, in the biomedical domain comparatively fewer corpus focus on semantic association among all tokens in a sentence. We proposed an annotation scheme based on feature structure theory for enriching biomedical domain corpora with token semantic association (TSA). There are 227 documents of the BioNLP GE ST training data annotated to form TSA corpus in which each annotated item shows a token semantic association that appears as a triple. The annotation of token semantic association has the potential to significantly advance biomedical text mining by providing rich token semantic information for NLP systems especially for the sophisticated IE systems, such as bio-event extraction.  2015, Wuhan University and Springer-Verlag Berlin Heidelberg.",
      "title": "213248 BioTSA: Annotating token semantic association to support biomedical text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929147897&partnerID=40&md5=54fc34cd9877af1c636583f7af311491"
    },
    {
      "abstract": "The objective of text clustering is to divide document collections into clusters based on the similarity between documents. In this paper, an extension-based feature modeling approach towards semantically sensitive text clustering is proposed along with the corresponding feature space construction and similarity computation method. By combining the similarity in traditional feature space and that in extension space, the adverse effects of the complexity and diversity of natural language can be addressed and clustering semantic sensitivity can be improved correspondingly. The generated clusters can be organized using different granularities. The experimental evaluations on well-known clustering algorithms and datasets have verified the effectiveness of our approach.  2015 Liu et al.",
      "title": "213249 Towards semantically sensitive text clustering: A feature space modeling technology based on dimension extension",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925815334&partnerID=40&md5=d4704978ef164ec8f2102d6245dac151"
    },
    {
      "abstract": "We present the design, and analyze the performance of a multi-stage natural language processing system employing named entity recognition, Bayesian statistics, and rule logic to identify and characterize heart disease risk factor events in diabetic patients over time. The system was originally developed for the 2014 i2b2 Challenges in Natural Language in Clinical Data. The systems strengths included a high level of accuracy for identifying named entities associated with heart disease risk factor events. The systems primary weakness was due to inaccuracies when characterizing the attributes of some events. For example, determining the relative time of an event with respect to the record date, whether an event is attributable to the patients history or the patients family history, and differentiating between current and prior smoking status. We believe these inaccuracies were due in large part to the lack of an effective approach for integrating context into our event detection model. To address these inaccuracies, we explore the addition of a distributional semantic model for characterizing contextual evidence of heart disease risk factor events. Using this semantic model, we raise our initial 2014 i2b2 Challenges in Natural Language of Clinical data F1 score of 0.838 to 0.890 and increased precision by 10.3% without use of any lexicons that might bias our results.  2015 Elsevier Inc.",
      "title": "213250 Mining heart disease risk factors in clinical text with named entity recognition and distributional semantic models",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84941585295&partnerID=40&md5=8912bcc9fd367bb90f04e3617b098748"
    },
    {
      "abstract": "Arabic text classification methods have emerged as a natural result of the existence of a massive amount of varied textual information (written in Arabic language) on the web. In most text classification processes, feature selection is crucial task since it highly affects the classification accuracy. Generally, two types of features could be used: Statistical based features and semantic and concept features. The main interest of this paper is to specify the most effective semantic and concept features on Arabic text classification process. In this study, two novel features that use lexical, semantic and lexico-semantic relations of Arabic WordNet (AWN) ontology are suggested. The first feature set is List of Pertinent Synsets (LoPS), which is list of synsets that have a specific relation with the original terms. The second feature set is List of Pertinent Words (LoPW), which is list of words that have a specific relation with the original terms. Fifteen different relations (defined in AWN ontology) are used with both proposed features. Naive Bayes classifier is used to perform the classification process. The experimental results, which are conducted on BBC Arabic dataset, show that using LoPS feature set improves the accuracy of Arabic text classification compared with the well-known Bag-of-Word feature and the recent Bag-of-Concept (synset) features. Also, it was found that LoPW (especially with related-to relation) improves the classification accuracy compared with LoPS, Bagof- Word and Bag-of-Concept.  2015 Suhad A. Yousif, Venus W. Samawi, Islam Elkabani and Rached Zantout.",
      "title": "213251 Enhancement of Arabic text classification using semantic relations of Arabic WordNet",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929311238&partnerID=40&md5=3c6bc2cdc45ac0981db93443cc3b8ec8"
    },
    {
      "abstract": "Objectives: To develop an adaptive approach to mine frequent semantic tags (FSTs) from heterogeneous clinical research texts. Methods: We develop a plug-n-play framework that integrates replaceable unsupervised kernel algorithms with formatting, functional, and utility wrappers for FST mining. Temporal information identification and semantic equivalence detection were two example functional wrappers. We first compared this approachs recall and efficiency for mining FSTs from ClinicalTrials.gov to that of a recently published tag-mining algorithm. Then we assessed this approachs adaptability to two other types of clinical research texts: clinical data requests and clinical trial protocols, by comparing the prevalence trends of FSTs across three texts. Results: Our approach increased the average recall and speed by 12.8% and 47.02% respectively upon the baseline when mining FSTs from ClinicalTrials.gov, and maintained an overlap in relevant FSTs with the baseline ranging between 76.9% and 100% for varying FST frequency thresholds. The FSTs saturated when the data size reached 200 documents. Consistent trends in the prevalence of FST were observed across the three texts as the data size or frequency threshold changed. Conclusions: This paper contributes an adaptive tag-mining framework that is scalable and adaptable without sacrificing its recall. This component-based architectural design can be potentially generalizable to improve the adaptability of other clinical text mining methods.  Schattauer 2015.",
      "title": "213253 Adaptive semantic tag mining from heterogeneous clinical research texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928175241&partnerID=40&md5=7322e168c17be2aac06da288be75028f"
    },
    {
      "abstract": "Text summarization is of great importance to solve information overload. Salience and coverage are two most important issues for summaries. Most existing models extract summaries by selecting the top sentences with highest scores without using the relationships between sentences, and usually represent the sentences simply basing on lexical or statistical features. As a result, those models can not achieve salience or coverage very well. In this paper, we propose a novel summarization model called Sentence Selection with Semantic Representation (SSSR). SSSR ensures both salience and coverage by learning semantic representations for sentences and applying a well-designed selection strategy to select summary sentences. The selection strategy used in SSSR is to select sentences that can reconstruct the original document with least distortion by means of linear combination. Besides, we improve our selection strategy by reducing redundant information. Then we learn two semantic representations for sentences: (1) weighted mean of word embeddings, (2) deep coding. Both of them are semantic and compact, and can capture similarities between sentences. Extensive experiments on datasets DUC2006 and DUC2007 validate our model.  2014 IEEE.",
      "title": "213254 Text Summarization Based on Sentence Selection with Semantic Representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946555500&partnerID=40&md5=854cd6ad6c1331c03413f0f000dc4d8f"
    },
    {
      "abstract": "Text categorization plays a crucial role in both academic and commercial platforms due to the growing demand for automatic organization of documents. Kernel-based classification algorithms such as Support Vector Machines (SVM) have become highly popular in the task of text mining. This is mainly due to their relatively high classification accuracy on several application domains as well as their ability to handle high dimensional and sparse data which is the prohibitive characteristics of textual data representation. Recently, there is an increased interest in the exploitation of background knowledge such as ontologies and corpus-based statistical knowledge in text categorization. It has been shown that, by replacing the standard kernel functions such as linear kernel with customized kernel functions which take advantage of this background knowledge, it is possible to increase the performance of SVM in the text classification domain. Based on this, we propose a novel semantic smoothing kernel for SVM. The suggested approach is based on a meaning measure, which calculates the meaningfulness of the terms in the context of classes. The documents vectors are smoothed based on these meaning values of the terms in the context of classes. Since we efficiently make use of the class information in the smoothing process, it can be considered a supervised smoothing kernel. The meaning measure is based on the Helmholtz principle from Gestalt theory and has previously been applied to several text mining applications such as document summarization and feature extraction. However, to the best of our knowledge, ours is the first study to use meaning measure in a supervised setting to build a semantic kernel for SVM. We evaluated the proposed approach by conducting a large number of experiments on well-known textual datasets and present results with respect to different experimental conditions. We compare our results with traditional kernels used in SVM such as linear kernel as well as with several corpus-based semantic kernels. Our results show that classification performance of the proposed approach outperforms other kernels.  2015 Elsevier Ltd.",
      "title": "213256 A corpus-based semantic kernel for text classification by using meaning values of terms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929936194&partnerID=40&md5=9036893a5eba6be11174429adbf621f1"
    },
    {
      "abstract": "In this study, we propose a novel methodology to build a semantic smoothing kernel to use with Support Vector Machines (SVM) for text classification. The suggested approach is based on two key concepts",
      "title": "213258 A novel semantic smoothing kernel for text classification with class-based weighting",
      "url": ""
    },
    {
      "abstract": "Traditional clustering algorithms do not consider the semantic relationships among words so that cannot accurately represent the meaning of documents. To overcome this problem, introducing semantic information from ontology such as WordNet has been widely used to improve the quality of text clustering. However, there still exist several challenges, such as synonym and polysemy, high dimensionality, extracting core semantics from texts, and assigning appropriate description for the generated clusters. In this paper, we report our attempt towards integrating WordNet with lexical chains to alleviate these problems. The proposed approach exploits ontology hierarchical structure and relations to provide a more accurate assessment of the similarity between terms for word sense disambiguation. Furthermore, we introduce lexical chains to extract a set of semantically related words from texts, which can represent the semantic content of the texts. Although lexical chains have been extensively used in text summarization, their potential impact on text clustering problem has not been fully investigated. Our integrated way can identify the theme of documents based on the disambiguated core features extracted, and in parallel downsize the dimensions of feature space. The experimental results using the proposed framework on reuters-21578 show that clustering performance improves significantly compared to several classical methods.  2014 The Authors.",
      "title": "213259 A semantic approach for text clustering using WordNet and lexical chains",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84911126732&partnerID=40&md5=6c7a8cdbc62f86fa4f7580436ba6c857"
    },
    {
      "abstract": "The diversities of large-scale semistructured data make the extraction of implicit semantic information have enormous difficulties. This paper proposes an automatic and unsupervised method of text categorization, in which tree-shape structures are used to represent semantic knowledge and to explore implicit information by mining hidden structures without cumbersome lexical analysis. Mining implicit frequent structures in trees can discover both direct and indirect semantic relations, which largely enhances the accuracy of matching and classifying texts. The experimental results show that the proposed algorithm remarkably reduces the time and effort spent in training and classifying, which outperforms established competitors in correctness and effectiveness.  2015 Lin Guo et al.",
      "title": "213261 Text Matching and Categorization: Mining Implicit Semantic Knowledge from Tree-Shape Structures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944227269&partnerID=40&md5=e10886183e17449945f7343e9f297e8b"
    },
    {
      "abstract": "Learning cross-lingual semantic representations of relations from textual data is useful for tasks like cross-lingual information retrieval and question answering. So far, research has been mainly focused on cross-lingual entity linking, which is confined to linking between phrases in a text document and their corresponding entities in a knowledge base but cannot link to relations. In this paper, we present an approach for inducing clusters of semantically related relations expressed in text, where relation clusters (i) can be extracted from text of different languages, (ii) are embedded in a semantic representation of the context, and (iii) can be linked across languages to properties in a knowledge base. This is achieved by combining multi-lingual semantic role labeling (SRL) with cross-lingual entity linking followed by spectral clustering of the annotated SRL graphs. With our initial implementation we learned a cross-lingual lexicon of relation expressions from English and Spanish Wikipedia articles. To demonstrate its usefulness we apply it to crosslingual question answering over linked data.  Springer International Publishing Switzerland 2015.",
      "title": "213262 Learning a cross-lingual semantic representation of relations expressed in text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84937406143&partnerID=40&md5=74fdc702a9f50d162a15f8d2734582dd"
    },
    {
      "abstract": "In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones",
      "title": "213265 Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment",
      "url": ""
    },
    {
      "abstract": "In this paper, genetic algorithm oriented latent semantic features (GALSF) are proposed to obtain better representation of documents in text classification. The proposed approach consists of feature selection and feature transformation stages. The first stage is carried out using the state-of-the-art filter-based methods. The second stage employs latent semantic indexing (LSI) empowered by genetic algorithm such that a better projection is attained using appropriate singular vectors, which are not limited to the ones corresponding to the largest singular values, unlike standard LSI approach. In this way, the singular vectors with small singular values may also be used for projection whereas the vectors with large singular values may be eliminated as well to obtain better discrimination. Experimental results demonstrate that GALSF outperforms both LSI and filter-based feature selection methods on benchmark datasets for various feature dimensions.  2014 Elsevier Ltd. All rights reserved.",
      "title": "213266 Text classification using genetic algorithm oriented latent semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899668861&partnerID=40&md5=6e0a6ddf6db326fa0d46ee89d8ea9403"
    },
    {
      "abstract": "This paper proposes a strategy of sentence semantic clustering to measure similarities between texts to solve the problem of information loss on dependency relationship and structural information among terms in text caused by traditional approach of bag of words. Sentence is regarded as the smallest semantic unit because of relatively stable meaning of terms in text so that text could be divided into several sentences with preservation of latent information in text, extracting parts of speech of terms and structural feature of the sentence in text. The operation of semantic clustering among sentences is brought into use to construct semantic cluster, and the similarity between texts is acquired through optimal semantic cluster matching pair. The experimental results on an open benchmark datasets from Reuters21578 Top 10 and 20 Newsgroups show our proposed approach can improve the accuracy and F1 performance of classification compared to traditional approach.  2014 Binary Information Press.",
      "title": "213267 Research on text similarity algorithm based on sentence semantic clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901839880&partnerID=40&md5=11d29de2fe808e227b73d34ce3b4b6e1"
    },
    {
      "abstract": "This paper presents a method for enhancing text classification performance with semantic computing. It adopts conceptual primitives with semantic relations as knowledge expression. Based on the semantic expression, it mined the association relation of primitives among different text classification, and these association rules take association relation as text classification feature. The presented method not only considers what kind of the semantic primitives that a text contains, but also takes account of the association relation of the semantic primitives. Moreover, we test the method with public text classification text set. The experiment result shows that, comparing with the commonly used methods, this method prompts text classification performance.  2014 IEEE.",
      "title": "213270 Semantic conceptual primitives computing in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84941064871&partnerID=40&md5=b34619d11bc0d58d63fd3c3a23d6a5e6"
    },
    {
      "abstract": "Using traditional Random Forests in short text classification revealed a performance degradation compared to using them for standard texts. Shortness, sparseness and lack of contextual information in short texts are the reasons of this degradation. Existing solutions to overcome these issues are mainly based on data enrichment. However, data enrichment can also introduce noise. We propose a new approach that combines data enrichment with the introduction of semantics in Random Forests. Each short text is enriched with data semantically similar to its words. These data come from an external source of knowledge distributed into topics thanks to the Latent Dirichlet Allocation model. Learning process in Random Forests is adapted to consider semantic relations between words while building the trees. Tests performed on search-snippets using the new method showed significant improvements in the classification. The accuracy has increased by 34% compared to traditional Random Forests and by 20% compared to MaxEnt.  2014 Springer International Publishing.",
      "title": "213273 Short text classification using semantic random forest",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906861734&partnerID=40&md5=981b8c4317fb8254a2aaedf68aec1843"
    },
    {
      "abstract": "This paper describes the analysis of possible differentiation of the authors idiolect in the space of semantic fields",
      "title": "213274 Clustering of authors texts of English fiction in the vector space of semantic fields",
      "url": ""
    },
    {
      "abstract": "It is known that latent semantic indexing (LSI) takes advantage of implicit higher-order (or latent) structure in the association of terms and documents. Higher-order relations in LSI capture latent semantics. These findings have inspired a novel Bayesian framework for classification named Higher-Order Naive Bayes (HONB), which was introduced previously, that can explicitly make use of these higher-order relations. In this paper, we present a novel semantic smoothing method named Higher-Order Smoothing (HOS) for the Naive Bayes algorithm. HOS is built on a similar graph based data representation of the HONB which allows semantics in higher-order paths to be exploited. We take the concept one step further in HOS and exploit the relationships between instances of different classes. As a result, we move beyond not only instance boundaries, but also class boundaries to exploit the latent information in higher-order paths. This approach improves the parameter estimation when dealing with insufficient labeled data. Results of our extensive experiments demonstrate the value of HOS on several benchmark datasets.  2014 Springer Science+Business Media New York & Science Press, China.",
      "title": "213275 Higher-order smoothing: A novel semantic smoothing method for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901647961&partnerID=40&md5=252e7696a6c639e088314d56fe591654"
    },
    {
      "abstract": "This paper proposes a fuzzy control genetic algorithm (GA) in conjunction with a novel hybrid semantic similarity measure for document clustering. Since the common clustering algorithms use vector space model (VSM) to represent document, the conceptual relationships between related terms being ignored, we use semantic similarity measures to solve this problem. In general, the semantic similarity measures can be extensively categorized into two kinds: thesaurus-based methods and corpus-based methods. However, in practice the corpus-based method is rather complicated to tackle. We propose and demonstrate a semantic space model (SSM) as the corpus-based method, where the appropriately reduced dimensions in SSM can capture the true relationship between documents in terms of concepts, rather than specific terms. Thus, the thesaurus-based method is combined with our SSM as a hybrid strategy to represent the semantic similarity measure. In GA field, the balance between the capability to converge to an optimum and the capacity to explore new solutions affects the success of search for the global optimum. We utilize a fuzzy control GA to adaptively adjust the influence between these two factors. Two textual data sets from Reuter document collection and 20-newsgroup corpus are tested in our experiments, and the results show that our fuzzy control GA combined with the hybrid semantic similarity strategy apparently outperforms the conventional GA, FCM and K-means with the traditional cosine similarity in VSM. Moreover, the superiorities of the fuzzy control GA and our hybrid semantic strategy are demonstrated by their better performance, in comparison with conventional GA with the same similarity measures.  2014 Elsevier Inc. All rights reserved.",
      "title": "213276 Fuzzy control GA with a novel hybrid semantic similarity strategy for text clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899973460&partnerID=40&md5=af4e31c3a3c89736a99a1f42667fe642"
    },
    {
      "abstract": "The use of semantics in tasks related to information retrieval has become, in recent years, a vast field of research. Considering supervised text classification, which is the main interest of this work, semantics can be involved at different steps of text processing: during indexing step, during training step and during class prediction step. As for class prediction step, new text-to-text semantic similarity measures can replace classical similarity measures that are traditionally used by some classification methods for decision-making. In this paper we propose a new measure for assessing semantic similarity between texts based on TF/IDF with a new function that aggregates semantic similarities between concepts representing the compared text documents pair-to-pair. Experimental results demonstrate that our measure outperforms other semantic and classical measures with significant improvements.  Springer International Publishing Switzerland 2014.",
      "title": "213277 An effective TF/IDF-based text-to-text semantic similarity measure for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921675058&partnerID=40&md5=e3f322174dee30f32065d6912f795856"
    },
    {
      "abstract": "The Web has made possible many advanced text-mining applications, such as news summarization, essay grading, question answering, and semantic search. For many of such applications, statistical text-mining techniques are ineffective since they do not utilize the morphological structure of the text. Thus, many approaches use NLP-based techniques, that parse the text and use patterns to mine and analyze the parse trees which are often unnecessarily complex. Therefore, we propose a weighted-graph representation of text, called Text Graphs, which captures the grammatical and semantic relations between words and terms in the text. Text Graphs are generated using a new text mining framework which is the main focus of this paper. Our framework, SemScape, uses a statistical parser to generate few of the most probable parse trees for each sentence and employs a novel two-step pattern-based technique to extract from parse trees candidate terms and their grammatical relations. Moreover, SemScape resolves co references by a novel technique, generates domain-specific Text Graphs by consulting ontologies, and provides a SPARQL-like query language and an optimized engine for semantically querying and mining Text Graphs.  2014 IEEE.",
      "title": "213278 Mining semantic structures from syntactic structures in free text documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906969180&partnerID=40&md5=dbcc3a0d07963f7581b9e701c59b9f03"
    },
    {
      "abstract": "A semantic network is a graphical notation, for representing knowledge in form of interconnected nodes and arcs. In this paper we propose a novel approach to construct a semantic graph from a text document. Our approach considers all the nouns of a document and builds a semantic graph, such that it represents entire document. We think that our graph captures many properties of the text documents and can be used for different application in the field of text mining and NLP, such as keyword extraction and to know the nature of the document. Our approach to construct a semantic graph is independent of any language. We performed an experimental analysis to validate our results to extract keywords of document and to derive nature of graph. We present the experimental result on construction of graph on FIRE data set and present its application for keyword extraction and commenting on the nature of document.  2014 IEEE.",
      "title": "213279 Semantic graph based approach for text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899480895&partnerID=40&md5=5377b7abaecfda3779531a69757f06ef"
    },
    {
      "abstract": "The use of semantics in supervised text classification can improve its effectiveness especially in specific domains. Most state of the art works use concepts as an alternative to words in order to transform the classical bag of words (BOW) into a Bag of concepts (BOC). This transformation is done through conceptualization task. Furthermore, the resulting BOC can be enriched using other related concepts from semantic resources. This enrichment may enhance classification effectiveness as well. This paper focuses on two strategies for semantic enrichment of conceptualized text representation. The first one is based on semantic kernel method while the second one is based on enriching vectors method. These two semantic enrichment strategies are evaluated through experiments using Rocchio as the supervised classification method in the medical domain, using UMLS ontology and Ohsumed corpus. Copyright  2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "title": "213280 Semantic enrichments in text supervised classification: Application to medical domain",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84923917543&partnerID=40&md5=b64984bc24755895470ccd44f96fa586"
    },
    {
      "abstract": "The paper presents a formal sentence model of scientific and technical texts to be used in the process of semantic interpretation. The model is based on the word dependencies, formed by sequences of word combinations that are called dependency chains. Characteristic word sets are defined within the dependency chains and the discriminated transitive closure of direct dependence as well as its notation in the form of the incidence matrix are assigned. The procedures and algorithms of semantically cohesive sentence fragments detection are constructed on the basis of the introduced concepts.  2014 IEEE.",
      "title": "213283 Representation of semantically cohesive sentence fragments in scientific and technical texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902585380&partnerID=40&md5=d6558a3e8696ace475f7ad8282f363ec"
    },
    {
      "abstract": "Machine learning is a subfield of artificial intelligence that deals with the exploration and construction of systems that can learn from data. Machine learning trains the computers to manage the critical situations via examining, self-training, inference by observation and previous experience. This paper provides an overview of the development of an efficient classifier that represents the semantics in medical data (Medline) using a Machine Learning (ML) perspective. In recent days people are more concerned about their health and explore ways to identify health related information. But the process of identifying the semantic representation for the medical terms is a difficult task. The main goal of our work was to identify the semantic representation for the medical abstracts in the Medline repository using Machine Learning and Natural Language Processing (NLP).  2014 IEEE.",
      "title": "213287 Mining semantic representation from medical text: A Bayesian approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921044359&partnerID=40&md5=57688354a5e523e7cd334240fec371d8"
    },
    {
      "abstract": "We propose a semantic kernel for Support Vector Machines (SVM) that takes advantage of higher-order relations between the words and between the documents. Conventional approach in text categorization systems is to represent documents as a Bag of Words (BOW) in which the relations between the words and their positions are lost. Additionally, traditional machine learning algorithms assume that instances, in our case documents, are independent and identically distributed. This approach simplifies the underlying models, but nevertheless it ignores the semantic connections between words as well as the semantic relations between documents that stem from the words. In this study, we improve the semantic knowledge capture capability of a previous work in [1], which is called Chi-Sim Algorithm and use this method in the SVM as a semantic kernel. The proposed approach is evaluated on different benchmark textual datasets. Experiment results show that classification performance improves over the well-known traditional kernels used in the SVM such as the linear kernel (one of the state-of-the-art algorithms for text classification system), the polynomial kernel and the Radial Basis Function (RBF) kernel.  2014 Springer International Publishing.",
      "title": "213288 A semantic kernel for text classification based on iterative higher-order relations between words and documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902578931&partnerID=40&md5=224307d259732057a91bc0a8a301ccf4"
    },
    {
      "abstract": "Text clustering methods have been discussed with various factors. We have the goal and problem of making decision with rationality factors about how much information is available or how complete the document about the knowledge. We propose a new rational text clustering algorithm using the semantic ontology. The documents are processed to extract the key terms as feature vectors. Semantic frequency SF and inverse semantic frequency ISF are computed for each document. Using computed SF and ISF we compute semantic weight for each document towards various categories, based on which the document is identified to a class or category. The proposed method produces more accurate clusters with reduced overlap.  2005 - 2014 JATIT & LLS. All rights reserved.",
      "title": "213289 Graph based rational text clustering using semantic ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905898297&partnerID=40&md5=eb917e72e19f608a3efab18f9e76103b"
    },
    {
      "abstract": "In this paper, we present a novel and completely-unsupervised approach to unravel meanings (or senses) from linguistic constructions found in large corpora by introducing the concept of semantic vector. A semantic vector is a space-transformed vector where features represent fine-grained semantic information units, instead of values of co-occurrences within a collection of texts. More in detail, instead of seeing words as vectors of frequency values, we propose to first explode words into a multitude of tiny semantic information retrieved from existing resources like WordNet and ConceptNet, and then clustering them into frequent and diverse patterns. This way, on the one hand, we are able to model linguistic data with a larger but much more dense and informative semantic feature space. On the other hand, being the model based on basic and conceptual information, we are also able to generate new data by querying the above-mentioned semantic resources with the features contained in the extracted patterns. We experimented the idea on a dataset of 640 millions of triples subject-verb-object to automatically inducing senses for specific input verbs, demonstrating the validity and the potential of the presented approach in modeling and understanding natural language. Copyright  by the papers authors.",
      "title": "213291 Mining meaning from text by harvesting frequent and diverse semantic itemsets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84920897101&partnerID=40&md5=e0e30eaf57c4b6aaeae6196abf3544fe"
    },
    {
      "abstract": "In this paper, a new approach to automatically assessing patterns in text mining is proposed. It combines corpus based semantics and Formal Concept Analysis in order to deal with semantic and structural properties for concepts discovered in tasks such as generation of association rules. Experiments show the promise of our evaluation method to effectively assess discovered patterns when compared with other state-of-the-art evaluation methods.",
      "title": "213295 A semantically-based lattice approach for assessing patterns in text mining tasks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84892415454&partnerID=40&md5=eb35caf494715ddbee6930c8a0b6fff4"
    },
    {
      "abstract": "As the rapid development of the internet, we can collect more and more information. it also means we need the abitily to search the information which really useful to us from the amount of information quickly. Automatic summarization is useful to us for handling the huge amount of text information in the Web. This paper proposes a Chinese summarization method based on Affinity Propagation(AP)clustering and latent semantic analysis(LSA). AP is a new clustering algorithm raised by B. J. Frey on science in 2007 that takes as input measures of similarity between pairs of data points and simultaneously considers all data points as potential exemplars. LSA is a technique in natural language processing, in particular in vectorial semantics, of analyzing relationships between a set of sentences. Experiment results show that our method could get more comprehensive and high-quality summarization.  Springer-Verlag Berlin Heidelberg 2012.",
      "title": "213318 Automatic summarization for Chinese text using Affinity Propagation clustering and latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84894364782&partnerID=40&md5=919ffa3c78e167b501c8140dd64ff80b"
    },
    {
      "abstract": "Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR-a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to a lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70, 000 words mapped to more than 40, 000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and wordsense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.",
      "title": "213340 Joint learning of words and meaning representations for open-text semantic parsing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84954213973&partnerID=40&md5=e583e6ce8f15bd910dbe0578457e8a92"
    },
    {
      "abstract": "The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the first and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reflect some semantic similarity. Certain patterns of English words that relate to a specific meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we first demonstrate that the correlations detected between the two versions of the corpus are significantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reflect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and significantly superior to LSI on the same data.",
      "title": "213497 Inferring a semantic representation of text via cross-language correlation analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84873588359&partnerID=40&md5=efb701b7dc44e4b7fa3ae4c5485c4b89"
    },
    {
      "abstract": "Text summarization is either extractive or abstractive. Extractive summarization is to select the most salient pieces of information (words, phrases, and/or sentences) from a source document without adding any external information. Abstractive summarization allows an internal representation of the source document so as to produce a faithful summary of the source. In this case, external text can be inserted into the generated summary. Because of the complexity of the abstractive approach, the vast majority of work in text summarization has adopted an extractive approach. In this work, we focus on concepts fusion and generalization, i.e. where different concepts appearing in a sentence can be replaced by one concept which covers the meanings of all of them. This is one operation that can be used as part of an abstractive text summarization system. The main goal of this contribution is to enrich the research efforts on abstractive text summarization with a novel approach that allows the generalization of sentences using semantic resources. This work should be useful in intelligent systems more generally since it introduces a means to shorten sentences by producing more general (hence abstractions of the) sentences. It could be used, for instance, to display shorter texts in applications for mobile devices. It should also improve the quality of the generated text summaries by mentioning key (general) concepts. One can think of using the approach in reasoning systems where different concepts appearing in the same context are related to one another with the aim of finding a more general representation of the concepts. This could be in the context of Goal Formulation, expert systems, scenario recognition, and cognitive reasoning more generally. We present our methodology for the generalization and fusion of concepts that appear in sentences. This is achieved through (1) the detection and extraction of what we define as generalizable sentences and (2) the generation and reduction of the space of generalization versions. We introduce two approaches we have designed to select the best sentences from the space of generalization versions. Using four NLTK1 corpora, the first approach estimates the acceptability of a given generalization version. The second approach is Machine Learning-based and uses contextual and specific features. The recall, precision and F1-score measures resulting from the evaluation of the concept generalization and fusion approach are presented.  2016 Elsevier Ltd. All rights reserved.",
      "title": "222711 Concept generalization and fusion for abstractive sentence generation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84957598648&partnerID=40&md5=1b9c06528ade85a762cc5c5088add02f"
    },
    {
      "abstract": "Automatic Dialect Identification (DID) has recently gained substantial interest in the speech processing community. Studies have shown that the variation in speech due to dialect is a factor which significantly impacts speech system performance. Dialects differ in various ways such as acoustic traits (phonetic realization of vowels and consonants, rhythmical characteristics, prosody) and content based word selection (grammar, vocabulary, phonetic distribution, lexical distribution, semantics). The traditional DID classifier is usually based on Gaussian Mixture Modeling (GMM), which is employed as baseline system. We investigate various methods of improving the DID based on acoustic and text language sub-systems to further boost the performance. For acoustic approach, we propose to use i-Vector system. For text language based dialect classification, a series of natural language processing (NLP) techniques are explored to address word selection and grammar factors, which cannot be modeled using an acoustic modeling system. These NLP techniques include: two traditional approaches, including N-Gram modeling and Latent Semantic Analysis (LSA), and a novel approach based on Term Frequency-Inverse Document Frequency (TF-IDF) and logistic regression classification. Due to the sparsity of training data, traditional text approaches do not offer superior performance. However, the proposed TF-IDF approach shows comparable performance to the i-Vector acoustic system, which when fused with the i-Vector system results in a final audio-text combined solution that is more discriminative. Compared with the GMM baseline system, the proposed audio-text DID system provides a relative improvement in dialect classification performance of +40.1% and +47.1% on the self-collected corpus (UT-Podcast) and NIST LRE-2009 data, respectively. The experiment results validate the feasibility of leveraging both acoustic and textual information in achieving improved DID performance.  2015 Elsevier B.V.",
      "title": "222712 Unsupervised accent classification for deep data fusion of accent and language information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84957811281&partnerID=40&md5=d322be413aeda4f90017cbc1d27568c5"
    },
    {
      "abstract": "In recent years, topic models have been gaining popularity to perform classification of text from several web sources (from social networks to digital media). However, after working for many years in the web text mining area we have notice that assessing the quality of topics discovered is still an open problem, quite hard to solve. In this paper, we evaluated four latent semantic models using two metrics: coherence and interpretability which are the most used. We show how these pure mathematical metrics fall short to asses topics quality. Experiments were performed over a dataset of 21,863 text reclamation.  2015 Elsevier B.V.",
      "title": "222716 An empirical comparison of latent sematic models for applications in industry",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84955651481&partnerID=40&md5=bb8ef2f0ab361f20446944672250a82f"
    },
    {
      "abstract": "In community question answering (cQA), users pose queries (or questions) on portals like Yahoo! Answers which can then be answered by other users who are often knowledgeable on the subject. cQA is increasingly popular on the Web, due to its convenience and effectiveness in connecting users with queries and those with answers. In this article, we study the problem of finding previous queries (e.g., posed by other users) which may be similar to new queries, and adapting their answers as the answers to the new queries. A key challenge here is to the bridge the lexical gap between new queries and old answers. For example, company in the queries may correspond to firm in the answers. To address this challenge, past research has proposed techniques similar to machine translation that translate old answers to ones using the words in the new queries. However, a key limitation of these works is that they assume queries and answers are parallel texts, which is hardly true in reality. As a result, the translated or rephrased answers may not look intuitive. In this article, we propose a novel approach to learn the semantic representation of queries and answers by using a neural network architecture. The learned semantic level features are finally incorporated into a learning to rank framework. We have evaluated our approach using a large-scale data set. Results show that the approach can significantly outperform existing approaches.  2015 Elsevier B.V. All rights reserved.",
      "title": "222717 Learning semantic representation with neural networks for community question answering retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84955710691&partnerID=40&md5=51b0d8db6a123dee9bed5e6a3c2e42fe"
    },
    {
      "abstract": "Purpose - Syntax-based text classification (TC) mechanisms have been overtly replaced by semantic-based systems in recent years. Semantic-based TC systems are particularly useful in those scenarios where similarity among documents is computed considering semantic relationships among their terms. Kernel functions have received major attention because of the unprecedented popularity of SVMs in the field of TC. Most of the kernel functions exploit syntactic structures of the text, but quite a few also use a priori semantic information for knowledge extraction. The purpose of this paper is to investigate semantic kernel functions in the context of TC. Design/methodology/approach - This work presents performance and accuracy analysis of seven semantic kernel functions (Semantic Smoothing Kernel, Latent Semantic Kernel, Semantic WordNet-based Kernel, Semantic Smoothing Kernel having Implicit Superconcept Expansions, Compactness-based Disambiguation Kernel Function, Omiotis-based S-VSM semantic kernel function and Top-k S-VSM semantic kernel) being implemented with SVM as kernel method. All seven semantic kernels are implemented in SVM-Light tool. Findings - Performance and accuracy parameters of seven semantic kernel functions have been evaluated and compared. The experimental results show that Top-k S-VSM semantic kernel has the highest performance and accuracy among all the evaluated kernel functions which make it a preferred building block for kernel methods for TC and retrieval. Research limitations/implications - A combination of semantic kernel function with syntactic kernel function needs to be investigated as there is a scope of further improvement in terms of accuracy and performance in all the seven semantic kernel functions. Practical implications - This research provides an insight into TC using a priori semantic knowledge. Three commonly used data sets are being exploited. It will be quite interesting to explore these kernel functions on live web data which may test their actual utility in real business scenarios. Originality/value - Comparison of performance and accuracy parameters is the novel point of this research paper. To the best of the authors' knowledge, this type of comparison has not been done previously.  2016, Emerald Group Publishing Limited.",
      "title": "222718 Performance and accuracy analysis of semantic kernel functions",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84953325991&partnerID=40&md5=a81e0d99167db5f8982ad7b295364407"
    },
    {
      "abstract": "In the last years, users have become used to keyword-based search interfaces due to their ease of use. By matching input keywords against huge amounts of textual information and labeled multimedia files, current search engines satisfy most of users information needs. However, the principal problem of this kind of search is the semantic gap between the input and the real user need, as keywords are a simplification of the query intended by the user. Moreover, different users could use the same set of keywords to search different information",
      "title": "222719 QueryGen: Semantic interpretation of keyword queries over heterogeneous information systems",
      "url": ""
    },
    {
      "abstract": "Growing evidence is suggesting that postings on online stock forums affect stock prices, and alter investment decisions in capital markets, either because the postings contain new information or they might have predictive power to manipulate stock prices. In this paper, we propose a new intelligent trading support system based on sentiment prediction by combining text-mining techniques, feature selection and decision tree algorithms in an effort to analyze and extract semantic terms expressing a particular sentiment (sell, buy or hold) from stock-related micro-blogging messages called StockTwits. An attempt has been made to investigate whether the power of the collective sentiments of StockTwits might be predicted and how the changes in these predicted sentiments inform decisions on whether to sell, buy or hold the Dow Jones Industrial Average (DJIA) Index. In this paper, a filter approach of feature selection is first employed to identify the most relevant terms in tweet postings. The decision tree (DT) model is then built to determine the trading decisions of those terms or, more importantly, combinations of terms based on how they interact. Then a trading strategy based on a predetermined investment hypothesis is constructed to evaluate the profitability of the term trading decisions extracted from the DT model. The experiment results based on 122-tweet term trading (TTT) strategies achieve a promising performance and the (TTT) strategies dramatically outperform random investment strategies. Our findings also confirm that StockTwits postings contain valuable information and lead trading activities in capital markets.  2015 The Authors. Published by Elsevier Ltd.",
      "title": "222726 Quantifying StockTwits semantic terms trading behavior in financial markets: An effective application of decision tree algorithms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84940850993&partnerID=40&md5=f4596caba93f58f8a1a36eb49bde2b2e"
    },
    {
      "abstract": "The recent huge availability of data coming from mobile phones, social networks and urban sensors leads research scientists to new opportunities and challenges. For example, mining micro-blogs content to unveil latent information about people sentiment and opinions is drawing more and more attention, since it can improve the understanding of complex phenomena and paves the way to the development of new innovative and intelligent services. In this paper we present CrowdPulse, a domain-agnostic framework for text analytics of social streams. The framework extracts textual data from social networks and implements algorithms for semantic processing, sentiment analysis and classification of gathered data. The framework has been deployed in two real-world scenarios in order to identify the most at-risk areas of the Italian territory according to the content posted on social networks and to monitor the recovering state of the social capital of LAquilas city after the dreadful earthquake of April 20091, respectively. In both scenarios, the framework showed its effectiveness and confirmed the insight that the combination of technologies specifically designed for Big Data processing with state-of-the-art methodologies for semantic analysis of textual content can provide very interesting findings and permits the analysis of such phenomena in a totally new way.  2015 Elsevier Ltd.",
      "title": "222729 CrowdPulse: A framework for real-time semantic analysis of social streams",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84940767741&partnerID=40&md5=2fb965e44aaa445a9574d7e1f22d49ab"
    },
    {
      "abstract": "We present a Natural Language Processing extraction system called IESforPFP, which can retrieve useful information from biomedical abstracts. IESforPFP aims at enhancing the state of the art of biological text mining by applying novel linguistic computational technique. By retrieving significant patterns of associations between proteins and molecules from biomedical abstracts, IESforPFP can determine the functions of un-annotated proteins. The system determines the semantic relationship between each protein-molecule pair in sentences using novel semantic rules. It applies a semantic relationship extraction model that retrieves information from different structural forms of constituents in sentences. In the framework of IESforPFP, each protein p is represented by a vector of weights. Each weight reflects the significance of a molecule m in the biomedical abstracts associated with p. That is, each weight quantifies the likelihood of the association between m and p. IESforPFP determines the set of annotated proteins that is semantically similar to p by comparing their vectors. It then annotates p with the functions of these annotated proteins. We evaluated the quality of IESforPFP by comparing it experimentally with two other systems. Results showed marked improvement.  2015 IEEE.",
      "title": "222732 An information extraction system for protein function prediction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84953449730&partnerID=40&md5=354b7383e60f6d34f0aa5ffe1bf276fd"
    },
    {
      "abstract": "Motivation: Although full-text articles are provided by the publishers in electronic formats, it remains a challenge to find related work beyond the title and abstract context. Identifying related articles based on their abstract is indeed a good starting point",
      "title": "222734 In the pursuit of a semantic similarity metric based on UMLS annotations for articles in PubMed Central Open Access",
      "url": "Journal of Biomedical Informatics"
    },
    {
      "abstract": "Background: A plethora of publicly available biomedical resources do currently exist and are constantly increasing at a fast rate. In parallel, specialized repositories are been developed, indexing numerous clinical and biomedical tools. The main drawback of such repositories is the difficulty in locating appropriate resources for a clinical or biomedical decision task, especially for non-Information Technology expert users. In parallel, although NLP research in the clinical domain has been active since the 1960s, progress in the development of NLP applications has been slow and lags behind progress in the general NLP domain. The aim of the present study is to investigate the use of semantics for biomedical resources annotation with domain specific ontologies and exploit Natural Language Processing methods in empowering the non-Information Technology expert users to efficiently search for biomedical resources using natural language. Methods: A Natural Language Processing engine which can translate free text into targeted queries, automatically transforming a clinical research question into a request description that contains only terms of ontologies, has been implemented. The implementation is based on information extraction techniques for text in natural language, guided by integrated ontologies. Furthermore, knowledge from robust text mining methods has been incorporated to map descriptions into suitable domain ontologies in order to ensure that the biomedical resources descriptions are domain oriented and enhance the accuracy of services discovery. The framework is freely available as a web application at (http://calchas.ics.forth.gr/). Results: For our experiments, a range of clinical questions were established based on descriptions of clinical trials from the ClinicalTrials.gov registry as well as recommendations from clinicians. Domain experts manually identified the available tools in a tools repository which are suitable for addressing the clinical questions at hand, either individually or as a set of tools forming a computational pipeline. The results were compared with those obtained from an automated discovery of candidate biomedical tools. For the evaluation of the results, precision and recall measurements were used. Our results indicate that the proposed framework has a high precision and low recall, implying that the system returns essentially more relevant results than irrelevant. Conclusions: There are adequate biomedical ontologies already available, sufficiency of existing NLP tools and quality of biomedical annotation systems for the implementation of a biomedical resources discovery framework, based on the semantic annotation of resources and the use on NLP techniques. The results of the present study demonstrate the clinical utility of the application of the proposed framework which aims to bridge the gap between clinical question in natural language and efficient dynamic biomedical resources discovery.  2015 Sfakianaki et al.",
      "title": "222736 Semantic biomedical resource discovery: A Natural Language Processing framework",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942930127&partnerID=40&md5=835b1a5ee4de7d28b0b777b64adb3430"
    },
    {
      "abstract": "Recent years the distributed representations of words (i.e., word embeddings) have been shown to be able to significantly improve performance in many natural language processing tasks, such as pos-of-tag tagging, chunking, named entity recognition and sentiment polarity judgement, etc. However, previous tasks only involve a single sentence. In contrast, this paper evaluates the effectiveness of word embeddings in sentence pair classification or regression problems. Specifically, we propose novel simple yet effective features based on word embeddings and extract many traditional linguistic features. Then these features serve as input of a classification/regression algorithm in isolation and in combination. Evaluations are conducted on three sentence pair classification/regression tasks, i.e., textual entailment, cross-lingual textual entailment and semantic relatedness estimation. Experiments on benchmark datasets provided by Semantic Evaluation 2013 and 2014 showed that using word embeddings is able to significantly improve the performance and our results outperform the best achieved results so far.  2015 IEEE.",
      "title": "222737 Integrating word embeddings and traditional NLP features to measure textual entailment and semantic relatedness of sentence pairs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951083564&partnerID=40&md5=29dd1a462ed1f24c2fc2eaebf7ff222c"
    },
    {
      "abstract": "With the development of internet, there are billions of short texts generated each day. However, the accuracy of large scale short text classification is poor due to the data sparseness. Traditional methods used to use external dataset to enrich the representation of document and solve the data sparsity problem. But external dataset which matches the specific short texts is hard to find. In this paper, we propose a framework to solve the data sparsity problem without using external dataset. Our framework deal with large scale short text by making the most of semantic similarity of words which learned from the training short texts. First, we learn word distributed representation and measure the word semantic similarity from the training short texts. Then, we propose a method which enrich the document representation by using the word semantic similarity information. At last, we build classifiers based on the enriched representation. We evaluate our framework on both the benchmark dataset(Standford Sentiment Treebank) and the large scale Chinese news title dataset which collected by ourselves. For the benchmark dataset, using our framework can improve 3% classification accuracy. The result we tested on the large scale Chinese news title dataset shows that our framework achieve better result with the increase of the training set size.  2015 IEEE.",
      "title": "222739 A word distributed representation based framework for large-scale short text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951187201&partnerID=40&md5=d9383713d94be035fd9c3b0f18ef81f7"
    },
    {
      "abstract": "Nowadays, semantic lexical resources, like ontologies, are becoming increasingly important in many systems, in particular those providing access to unstructured textual data. Typically, such resources are built based on already existing repositories and by analyzing available texts. In practice, however, building new or enriching existing resources of such type cannot be accomplished without using an appropriate tool. In this paper the SAUText is presented",
      "title": "222740 SAUText - a system for analysis of unstructured textual data",
      "url": ""
    },
    {
      "abstract": "The paper addresses the problem of automatic dictionary translation.The proposed method translates a dictionary by means of mining repositories in the source and target languages, without any directly given relationships connecting the two languages. It consists of two stages: (1) translation by lexical similarity, where words are compared graphically, and (2) translation by semantic similarity, where contexts are compared. In the experiments Polish and English version of Wikipedia were used as text corpora. The method and its phases are thoroughly analyzed. The results allow implementing this method in human-in-the-middle systems.  2015 The Author(s)",
      "title": "222743 A novel method for dictionary translation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84940973614&partnerID=40&md5=322983bc0a2bb94a77cee84bdcde48e0"
    },
    {
      "abstract": "Multidocument summarization addresses the selection of a compact subset of highly informative sentences, i.e., the summary, from a collection of textual documents. To perform sentence selection, two parallel strategies have been proposed: (a) apply general-purpose techniques relying on datamining or information retrieval techniques, and/or (b) perform advanced linguistic analysis relying on semantics-based models (e.g., ontologies) to capture the actual sentence meaning. Since there is an increasing need for processing documents written in different languages, the attention of the research community has recently focused on summarizers based on strategy (a). This article presents a novelmultilingual summarizer, namely MWI-Sum (Multilingual Weighted Itemsetbased Summarizer), that exploits an itemset-based model to summarize collections of documents ranging over the same topic. Unlike previous approaches, it extracts frequent weighted itemsets tailored to the analyzed collection and uses them to drive the sentence selection process. Weighted itemsets represent correlations among multiple highly relevant terms that are neglected by previous approaches. The proposed approach makes minimal use of language-dependent analyses. Thus, it is easily applicable to document collections written in different languages. Experiments performed on benchmark and real-life collections, English-written and not, demonstrate that the proposed approach performs better than state-of-the-art multilingual document summarizers.  2015 ACM.",
      "title": "222744 MWI-sum: A multilingual summarizer based on frequent weighted itemsets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943646467&partnerID=40&md5=003302ede896d1bd8ee7c0f80f903c88"
    },
    {
      "abstract": "One of the successful approaches for developing TCBR applications is SOPHisticated Information Analysis (SOPHIA), which is distinguished by its ability to work without prior knowledge engineering, domain dependency, or language dependency. One of the critical challenges faced the application of TCBR is responding to enormous requests from users in acceptable performance. Another challenge is the complexity of adapting Arabic language. The main contribution of this paper is proposing an enhanced version of SOPHIA-TCBR, which provides higher accuracy and better time performance. The proposed approach is evaluated in the domain of Arabic Islamic Jurisprudence (fiqh), which is a challenge case study with its large case-base and enormous number of users requests (questions) daily. This task actually requires a certain smart system that can help in fulfilling peoples needs for answers by applying the proposed approach in this domain and overcoming challenges related to the language syntax and semantics.  2014 IEEE.",
      "title": "222747 Enhanced knowledge discovery approach in textual case based reasoning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951033793&partnerID=40&md5=89afa35bd1df928354ad0329bef9a1f8"
    },
    {
      "abstract": "Most of the exist Web search engines utilize matching the query keywords to pieces of information approach to identify of the data satisfying users request. These methods are not only inefficient, but also wasted a lot of users time to find a satisfactory results. In order to improve the problem above, we presented a different approach to identify users request that attracts more interest is called a ontology-based and implied sentiment search. Ontology is a tree structure which represent specifications of concepts and relations among them. Ontology play a central role in semantic web applications by providing a shared knowledge about the objects in real world. This paper proposes a method for determining semantic similarity between concepts and implied sentiment defined in ontology. Unlike method exist that use ontological definition of concepts for similarity assessment, the presented approach also focuses on the relations between concepts and their implied sentiment inclination. Our method is able to determine similarity not only at the definition level, but also is able to evaluate similarity of implied sentiment of information that are instances of concepts. In addition, the method allows for context-aware similarity assessment. Experimental comparison of our on-line text mining approach against other techniques known in the literature shows satisfying results.  2015 Global IT Research Institute (GiRI).",
      "title": "222749 On-line text mining and recommendation based on ontology and implied sentiment inclination",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949935397&partnerID=40&md5=be5a7b15406ef6373d7a1e59d868681c"
    },
    {
      "abstract": "We propose a framework for abstractive summarization of multi-documents, which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. In this framework, contents of the source documents are represented by predicate argument structures by employing semantic role labeling. Content selection for summary is made by ranking the predicate argument structures based on optimized features, and using language generation for generating sentences from predicate argument structures. Our proposed framework differs from other abstractive summarization approaches in a few aspects. First, it employs semantic role labeling for semantic representation of text. Secondly, it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text",
      "title": "222750 A framework for multi-document abstractive summarization based on semantic role labelling",
      "url": ""
    },
    {
      "abstract": "Scalability is one of the main challenges of social media analyses such as sentiment analysis. Micro logs as emerging opinion sharing platforms require new approaches that are more scalable and accurate. In this paper, we propose, implement, and evaluate SSSA, a Semantic Scoring Sentiment Analysis service, which matches the demand of scalability and efficiency of a sentiment analysis system for a large volume of micro logs. We first build an automated tweet tagging system to alleviate the cost of building a dataset for training. Our approach then calculates semantic scores of words in tweets. These scores are the baseline for the proposed subjectivity classification and feature selection steps. Finally, we build a polarity classifier to estimate the polarity of subjective tweets on top of them. Experiments show that SSSA, compared to conventional approaches that based on emoticon, hash tag, or semantic scoring, has improved the scalability and accuracy of micro log sentiment analysis.  2015 IEEE.",
      "title": "222751 Scalable Sentiment Analysis for Microblogs Based on Semantic Scoring",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84953432516&partnerID=40&md5=b5d37bce49897e13735101fd36f6ab7a"
    },
    {
      "abstract": "The growing popularity of social media provides a huge volume of social data including Tweets. These collections of social data can be potentially useful, but the extent of meaningful data in these collections has not been sufficiently researched, especially in South Korea Twitter data. In general, the South Korea Twitter data has been researched as a source of political media. Nonetheless, previous research on South Korea Twitter data has not adequately covered what kind of trend Twitter represents in terms of major topic categories such as politics, economics, or sports. In this paper, we present a cross-media approach to define the nature of South Korea Tweets by inferring the topic category distribution through short-text categorization. We select newspapers as cross-media, examine the categorization of news articles from major newspapers, and then train our classifier based on the features from each topic category. In addition, for grafting news topics onto South Korea Tweets, we propose a word clustering and filtering approach to exclude those words that do not provide semantic content for the topic categories. Based on the proposed procedures, we analyze the South Korea Tweets to determine the primary topic category focus of Twitter users. We observe the special behaviors of the South Korea Twitter users based on various parameters such as date, time slot, and day of the week. Because our research includes a macroscopic analysis of Twitter data using a cross-media strategy, our research can provide useful resources for other social media analysis as well.  2015 Springer Science+Business Media New York",
      "title": "222752 Topic category analysis on twitter via cross-media strategy",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939218718&partnerID=40&md5=5a19db03b216ae42fd97761cdedfe5d1"
    },
    {
      "abstract": "This paper presents and discusses some ensemble attribute selection methods for text document clustering. In this research, attributes are extracted in two levels. The first level document representation based on distribution of compact noun were constructed and relevance measure is applied on the distributed compact noun-document representation. it is used evaluate the importance of the noun. It has been widely studied in supervised learning, whereas it is still relatively rare researched in unsupervised learning. Vector Space Model has been used in many text mining tasks, where it has achieved good results as well as acceptable computational complexity. So the proposed document representations are exploited the nf-idf style equation. In this work, distributional nouns are selected into three different ways. First method, the distributed nouns are integrated with relevance measures, and secondly, relevance based distributional nouns are incorporated with Heuristic function to find importance of attributes. The distributional noun representations are used to discriminate the Nouns and measure the semantic similarity between documents. This proposed feature selection method have been successfully applied for Text clustering with flocking algorithm, HCLK Means Clustering and evaluated the efficiency of our Document representation with both synthetic and a real datasets. It is found that the proposed algorithm identifies better feature sets and improves the clustering quality.  2015 IEEE.",
      "title": "222753 Text document clustering with distributed noun with its compactness using relevance measure and Heuristic function",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84956972000&partnerID=40&md5=01b947e03f6732d91913237eb1ac39f6"
    },
    {
      "abstract": "We propose a new unified framework for monolingual (MoIR) and cross-lingual information retrieval (CLIR) which relies on the induction of dense real-valued word vectors known as word embeddings (WE) from comparable data. To this end, we make several important contributions: (1) We present a novel word representation learning model called BilingualWord Embeddings Skip-Gram (BWESG) which is the first model able to learn bilingual word embeddings solely on the basis of document-aligned comparable data",
      "title": "222756 Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings",
      "url": ""
    },
    {
      "abstract": "In this paper we use Twitter data to assess customers early reactions to the launch of two new products by Apple and Samsung by analyzing the streams generated in a 72 h window around the two events. We present a methodology based on conversational analysis to extract concept maps from Twitter streams and use semantic and topological metrics to compare the conversations. Our findings show that there are significant differences in the structural patterns of the two conversations and that the analysis of these differences can be highly informative about early customers perceptions and value judgments associated with the competing products.  2015 Elsevier Ltd. All rights reserved.",
      "title": "222757 Extracting and evaluating conversational patterns in social media: A socio-semantic analysis of customers reactions to the launch of new products using Twitter streams",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929145299&partnerID=40&md5=b8cd92401865f98ff944335a3c3899d1"
    },
    {
      "abstract": "The design and operating principles of an interactive system of semantic processing, i.e., understanding the meaning, of scientific and technical texts on chemical technology of reagents and ultrapure substances have been presented. Understanding of the meaning of scientific and technical texts has been implemented through the development of intelligent software of a dialogue system, which includes semantic and topological models of knowledge representation, as well as special languages of the automated generation of meaning. Understanding of the basic terms, concepts, and phrases in the limited natural language of the problem area has been presented in the form of the internal representation of the knowledge and language of control directives.  2015, Pleiades Publishing, Ltd.",
      "title": "222759 Principles of developing an interactive system for the semantic processing of scientific and technical texts on chemical technology of reagents and ultrapure substances",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939173314&partnerID=40&md5=39491822782c13efb518d75cb07da2db"
    },
    {
      "abstract": "Named-entity recognition (NER) plays an important role in the development of biomedical databases. However, the existing NER tools produce multifarious named-entities which may result in both curatable and non-curatable markers. To facilitate biocuration with a straightforward approach, classifying curatable named-entities is helpful with regard to accelerating the biocuration workflow. Co-occurrence Interaction Nexus with Named-entity Recognition (CoINNER) is a web-based tool that allows users to identify genes, chemicals, diseases, and action term mentions in the Comparative Toxicogenomic Database (CTD). To further discover interactions, CoINNER uses multiple advanced algorithms to recognize the mentions in the BioCreative IV CTD Track. CoINNER is developed based on a prototype system that annotated gene, chemical, and disease mentions in PubMed abstracts at BioCreative 2012 Track I (literature triage). We extended our previous system in developing CoINNER. The pre-tagging results of CoINNER were developed based on the state-of-the-art named entity recognition tools in BioCreative III. Next, a method based on conditional random fields (CRFs) is proposed to predict chemical and disease mentions in the articles. Finally, action term mentions were collected by latent Dirichlet allocation (LDA). At the BioCreative IV CTD Track, the best F-measures reached for gene/protein, chemical/drug and disease NER were 54 percent while CoINNER achieved a 61.5 percent F-measure. System URL: http://ikmbio.csie.ncku.edu.tw/coinner/introduction.htm.  2015 IEEE.",
      "title": "222762 Curatable Named-Entity Recognition Using Semantic Relations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939184380&partnerID=40&md5=97aa37d45b49d3434cd36b867fc72057"
    },
    {
      "abstract": "This article summarizes the results of an empirical study focusing on the value of product-enabled services in intensive R&D spenders. The focus is on product-driven firms for which new service development is expected to be particularly promising but also quite challenging. Part of the motivation is based on the fact that existing studies on the value attributes of hybrid offerings are mostly conceptual and need to be further substantiated through more systematic empirical studies. The research includes two samples with a total of 83 product-driven firms selected among the top R&D spenders in Canada and Europe. It adopts an innovative methodology based on online textual data that could be implemented in advanced business intelligence tools aiming at the facilitation of innovation, marketing and business decision making. Combinations of keywords referring to different aspects of service value were designed and used in a web search resulting in the frequency of their use on companies websites. Principal component analysis was applied to identify distinctive groups of keyword combinations that were interpreted in terms of specific service value attributes. Finally, the firms were classified by means of K-means cluster analysis in order to identify the firms with a high degree of articulation of their service value attributes. This work articulates a relatively simple and intuitive method for quantitative and qualitative semantic analysis of online textual data that is similar to latent semantic analysis but could be used as part of more user-friendly expert system solutions and business intelligence tools based on easily accessible business statistics packages. The results show that the main service value attributes of the Canadian firms are: better service effectiveness, higher market share, higher service quality, and customer satisfaction. The service value attributes for the European firms include, among others, product added-value, product modernization and optimization of customer time and efforts. Canadian firms focus on collaboration and co-creation with suppliers and customers for the sake of product-service innovation as a competitive advantage on the marketplace. On the other hand, the focus of EU firms on innovative hybrid offerings is not explicitly related to business differentiation and competitiveness.  2015 Elsevier Ltd. All rights reserved.",
      "title": "222763 A business intelligence approach using web search tools and online data reduction techniques to examine the value of product-enabled services",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84934301266&partnerID=40&md5=d08a490d503d38da6a922e5f4d67b722"
    },
    {
      "abstract": "When users post links to web pages in Twitter there is a time delta between when the post was shared (t tweet ) and when it was read (t click ). Ideally, when this time delta is small there is often no change in the pages state. However upon reading shared content in the past and due to the dynamic nature of the web, the pages state could change and the intention of the author need to be inferred. In this work, we enhance a prior temporal intention model and tackle its shortcomings by incorporating extended linguistic feature analysis, replacing the prior textual similarity measure with semantic similarity one based on latent topic detection trained on Wikipedia English corpus, and finally by enriching and balancing the training dataset. We uncovered three different intention behaviors in respect to time: Stable Intention, Changing Intention from current to past, and Undefined intention. Using these classes and only the information available at posting time from the tweet and the current state of the resource, we correctly predict the temporal intention classification and strength with 77% accuracy.  2015 ACM.",
      "title": "222766 Predicting Temporal Intention in Resource Sharing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84952049267&partnerID=40&md5=fc2e908d431860ae1b06576f14f14f45"
    },
    {
      "abstract": "Text data are ubiquitous and play an essential role in big data applications. However, text data are mostly unstructured. Transforming unstructured text into structured units (e.g., semantically meaningful phrases) will substantially reduce semantic ambiguity and enhance the power and efficiency at manipulating such data using database technology. Thus mining quality phrases is a critical research problem in the field of databases. In this paper, we propose a new framework that extracts quality phrases from text corpora integrated with phrasal segmentation. The framework requires only limited training but the quality of phrases so generated is close to human judgment. Moreover, the method is scalable: both computation time and required space grow linearly as corpus size increases. Our experiments on large text corpora demonstrate the quality and efficiency of the new method. Copyright  2015 ACM.",
      "title": "222770 Mining quality phrases from massive text corpora",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84952656631&partnerID=40&md5=953d028b45a04e8dfb567c64ce384317"
    },
    {
      "abstract": "Large quantities of data, often referred to as big data, are now held by companies. This big data includes statements of customer opinion regarding product or service quality in an unstructured textual form. While many tools exist to extract meaningful information from big data, automation tools do not exist to monitor the ongoing conceptual content of that data. We use latent semantic analysis to extract concept factors related to service quality categories. Customer comments found in the data that express dissatisfaction are then considered as representing a non-conforming observation in a process. Once factors are extracted, proportions of nonconformities for service quality failure categories are plotted on a control chart. The results are easily interpreted and the approach allows for the quantitative evaluation of customer acceptance of system process improvement initiatives.  2014, Springer Science+Business Media Dordrecht.",
      "title": "222773 Quantitative quality control from qualitative data: control charts with latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939873991&partnerID=40&md5=4d8c1e208e55d78fde7f444274c600c9"
    },
    {
      "abstract": "Background: This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies. Results: The 2013 BioASQ competition comprised two tasks, Task 1a and Task 1b. In Task 1a participants were asked to automatically annotate new PubMed documents with MeSH headings. Twelve teams participated in Task 1a, with a total of 46 system runs submitted, and one of the teams performing consistently better than the MTI indexer used by NLM to suggest MeSH headings to curators. Task 1b used benchmark datasets containing 29 development and 282 test English questions, along with gold standard (reference) answers, prepared by a team of biomedical experts from around Europe and participants had to automatically produce answers. Three teams participated in Task 1b, with 11 system runs. The BioASQ infrastructure, including benchmark datasets, evaluation mechanisms, and the results of the participants and baseline methods, is publicly available. Conclusions: A publicly available evaluation infrastructure for biomedical semantic indexing and QA has been developed, which includes benchmark datasets, and can be used to evaluate systems that: assign MeSH headings to published articles or to English questions",
      "title": "222774 An overview of the BioASQ large-scale biomedical semantic indexing and question answering competition",
      "url": ""
    },
    {
      "abstract": "Background: Research in biomedical text categorization has mostly used the bag-of-words representation. Other more sophisticated representations of text based on syntactic, semantic and argumentative properties have been less studied. In this paper, we evaluate the impact of different text representations of biomedical texts as features for reproducing the MeSH annotations of some of the most frequent MeSH headings. In addition to unigrams and bigrams, these features include noun phrases, citation meta-data, citation structure, and semantic annotation of the citations. Results: Traditional features like unigrams and bigrams exhibit strong performance compared to other feature sets. Little or no improvement is obtained when using meta-data or citation structure. Noun phrases are too sparse and thus have lower performance compared to more traditional features. Conceptual annotation of the texts by MetaMap shows similar performance compared to unigrams, but adding concepts from the UMLS taxonomy does not improve the performance of using only mapped concepts. The combination of all the features performs largely better than any individual feature set considered. In addition, this combination improves the performance of a state-of-the-art MeSH indexer. Concerning the machine learning algorithms, we find that those that are more resilient to class imbalance largely obtain better performance. Conclusions: We conclude that even though traditional features such as unigrams and bigrams have strong performance compared to other features, it is possible to combine them to effectively improve the performance of the bag-of-words representation. We have also found that the combination of the learning algorithm and feature sets has an influence in the overall performance of the system. Moreover, using learning algorithms resilient to class imbalance largely improves performance. However, when using a large set of features, consideration needs to be taken with algorithms due to the risk of over-fitting. Specific combinations of learning algorithms and features for individual MeSH headings could further increase the performance of an indexing system.  Jimeno Yepes et al.",
      "title": "222777 Feature engineering for MEDLINE citation categorization with MeSH",
      "url": ""
    },
    {
      "abstract": "Text classification often faces the problem of imbalanced training data. This is true in sentiment analysis and particularly prominent in emotion classification where multiple emotion categories are very likely to produce naturally skewed training data. Different sampling methods have been proposed to improve classification performance by reducing the imbalance ratio between training classes. However, data sparseness and the small disjunct problem remain obstacles in generating new samples for minority classes when the data are skewed and limited. Methods to produce meaningful samples for smaller classes rather than simple duplication are essential in overcoming this problem. In this paper, we present an oversampling method based on word embedding compositionality which produces meaningful balanced training data. We first use a large corpus to train a continuous skip-gram model to form a word embedding model maintaining the syntactic and semantic integrity of the word features. Then, a compositional algorithm based on recursive neural tensor networks is used to construct sentence vectors based on the word embedding model. Finally, we use the SMOTE algorithm as an oversampling method to generate samples for the minority classes and produce a fully balanced training set. Evaluation results on two quite different tasks show that the feature composition method and the oversampling method are both important in obtaining improved classification results. Our method effectively addresses the data imbalance issue and consequently achieves improved results for both sentiment and emotion classification.  2015, Springer Science+Business Media New York.",
      "title": "222778 Word Embedding Composition for Data Imbalances in Sentiment and Emotion Classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939941491&partnerID=40&md5=2470c060ca3209bbd67c7834d878f61b"
    },
    {
      "abstract": "Background: Relation extraction is a fundamental technology in biomedical text mining. Most of the previous studies on relation extraction from biomedical literature have focused on specific or predefined types of relations, which inherently limits the types of the extracted relations. With the aim of fully leveraging the knowledge described in the literature, we address much broader types of semantic relations using a single extraction framework. Results: Our system, which we name PASMED, extracts diverse types of binary relations from biomedical literature using deep syntactic patterns. Our experimental results demonstrate that it achieves a level of recall considerably higher than the state of the art, while maintaining reasonable precision. We have then applied PASMED to the whole MEDLINE corpus and extracted more than 137 million semantic relations. The extracted relations provide a quantitative understanding of what kinds of semantic relations are actually described in MEDLINE and can be ultimately extracted by (possibly type-specific) relation extraction systems. Conclusion: PASMED extracts a large number of relations that have previously been missed by existing text mining systems. The entire collection of the relations extracted from MEDLINE is publicly available in machine-readable form, so that it can serve as a potential knowledge base for high-level text-mining applications.  Nguyen et al.",
      "title": "222779 Wide-coverage relation extraction from MEDLINE using deep syntax",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84927557695&partnerID=40&md5=98fda6c5c15435d3631c6a6c015df695"
    },
    {
      "abstract": "We computed linguistic information at the lexical, syntactic, and semantic levels for Recognizing Inference in Text (RITE) tasks for both traditional and simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing, named-entity recognition, and near synonym recognition were employed, and features like counts of common words, statement lengths, negation words, and antonyms were considered to judge the entailment relationships of two statements, while we explored both heuristics-based functions and machine-learning approaches. The reported systems showed their robustness by simultaneously achieving second positions in the binary-classification subtasks for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted more experiments with the test data of NTCIR-9 RITE, with good results. We also extended our work to search for better configurations of our classifiers and investigated contributions of individual features. This extended work showed interesting results and should encourage further discussions.  2015 Springer-Verlag Berlin Heidelberg",
      "title": "222781 Exploring lexical, syntactic, and semantic features for Chinese textual entailment in NTCIR RITE evaluation tasks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925597111&partnerID=40&md5=c2a20e69f22bad31c8941690c6549e5c"
    },
    {
      "abstract": "Clustering is one of the most important techniques in machine learning and data mining responsibilities. Similar documents are grouped by performing clustering techniques. Similarity measure is used to determine transaction associations. Hierarchical clustering method produces tree structured results. Partition based clustering model produces the results in grid format. Text documents are formless data values with high dimensional attributes. Document clustering group the unlabeled text documents into meaningful clusters. Traditionally clustering methods need cluster count (K) before the document grouping process. Clustering accuracy decreases drastically with reference to the unsuitable cluster count. Document word features are automatically partitioned into two groups discriminative words and non-discriminative words. But only discriminative words are useful for grouping documents. The contribution of nondiscriminative words confuses the clustering process and leads to poor cluster solutions. The variational inference algorithm is used to infer the document collection structure and partition of document words at the same time. Dirichlet Process Mixture (DPM) model is used to partition documents. DPM clustering model utilizes both the data likelihood and the clustering property of the Dirichlet Process (DP). Dirichlet Process Mixture Model for Feature Partition (DPMFP) is used to discover the latent cluster structure based on the DPM model. DPMFP clustering model is performed without requiring the no. of clusters as input. The Discriminative word identification process is enhanced with the labeled document analysis mechanism. The concept relationships are analyzed with Ontology support. Semantic weight analysis is used for the document similarity measure. This method increases the scalability with the support of labels and concept relations for dimensionality cutback process.  2014 IEEE.",
      "title": "222787 Document grouping with concept based discriminative analysis and feature partition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925325511&partnerID=40&md5=b7af4923b6e1cf2c71c584e07d305bda"
    },
    {
      "abstract": "Patent search is a substantial basis for many operational questions and scientometric evaluations. We consider it as a sequence of distinct stages. The 'patent wide search' involves a definition of system boundaries by means of classifications and a keyword search producing a patent set with a high recall level (see Schmitz in Patentinformetrie: Analyse und Verdichtung von technischen Schutzrechtsinformationen, DGI, Frankfurt (Main), 2010 with an overview of searchable patent meta data). In this set of patents a 'patent near search' takes place, producing a patent set with high(er) precision. Hence, the question arises how the researcher has to operate within this patent set to efficiently identify patents that contain paraphrased descriptions of the sought inventive elements in contextual information and whether this produces different results compared to a conventional search. We present a semiautomatic iterative method for the identification of such patents, based on semantic similarity. In order to test our method we generate an initial dataset in the course of a patent wide search. This dataset is then analyzed by means of the semiautomatic iterative method as well as by an alternative method emulating the conventional process of keyword refinement. It thus becomes obvious that both methods have their particular 'raison d'etre', and that the semiautomatic iterative method seems to be able to support a conventional patent search very effectively.  2014, Budapest, Hungary.",
      "title": "222792 Completing keyword patent search with semantic patent search: introducing a semiautomatic iterative method for patent near search based on semantic similarities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84937774625&partnerID=40&md5=13de49c55bf203bb3f3538a1c2ec5386"
    },
    {
      "abstract": "Lexical relations, or semantic relations of words, are useful knowledge fundamental to all applications since they help to capture inherent semantic variations of vocabulary in human languages. Discovering such knowledge in a robust way from arbitrary text data is a significant challenge in big text data mining. In this paper, we propose a novel general probabilistic approach based on random walks on word adjacency graphs to systematically mine two fundamental and complementary lexical relations, i.e., paradigmatic and syntagmatic relations between words from arbitrary text data. We show that representing text data as an adjacency graph opens up many opportunities to define interesting random walks for mining lexical relation patterns, and propose specific random walk algorithms for mining paradigmatic and syntagmatic relations. Evaluation results on multiple corpora show that the proposed random walk-based algorithms can discover meaningful paradigmatic and syntagmatic relations of words from text data.  2014 IEEE.",
      "title": "222793 Random walks on adjacency graphs for mining lexical relations from big text data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921814809&partnerID=40&md5=4141a3a02a1037a7d155be9b4467bdfb"
    },
    {
      "abstract": "We present an experimental study of the performance of a hybrid semantic and linguistic system for recognizing and linking entities from formal and informal texts. In the current literature, systems are generally tailored to one or a few types of textual documents (e.g. narrative texts, newswire articles, informal text such as microposts). In contrast, we assess the performance of a hybrid approach that adapts the entity extraction, recognition and linking process to the type of document being analyzed. The hybrid system relies on POS taggers, gazetteers and Twitter user account dereferencing modules to extract entities, NER modules to recognize and type entities and entity popularity, string distance measures and scoring functions to disambiguate entities by ranking potential link candidates. The evaluation results show the robustness of our proposed approach in terms of text-independence compared to the current state-of-the art.",
      "title": "222795 An experimental study of a hybrid entity recognition and linking system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84955599745&partnerID=40&md5=3383ff871a8a30672798b9502470d295"
    },
    {
      "abstract": "One of the key goals of biological Natural Language Processing (NLP) is the automatic information extraction from biomedical publications. Most current constituency and dependency parsers overlook the semantic relationships between the constituents comprising a sentence and may not be well suited for capturing complex long-distance dependencies. We propose in this paper a hybrid constituency-dependency parser for biological NLP information extraction called BioHCDP. BioHCDP aims at enhancing the state of the art of biological text mining by applying novel linguistic computational techniques that overcome the limitations of current constituency and dependency parsers outlined above, as follows: (1) it determines the semantic relationship between each pair of constituents in a sentence using novel semantic rules, and (2) it applies semantic relationship extraction models that represent the relationships of different patterns of usage in different contexts. BioHCDP can be used to extract various classes of data from biological texts, including protein function assignments, genetic networks, and protein-protein interactions. We compared BioHCDP experimentally with three systems. Results showed marked improvement.  2014 IEEE.",
      "title": "222798 BioHCDP: A hybrid constituency-dependency parser for biological NLP information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925061108&partnerID=40&md5=4d486eebf17857e61518f0ed1b236433"
    },
    {
      "abstract": "In this paper we consider the problem of retrieving the concepts of an ontology that are most relevant to a given textual query. In our setting the concepts are associated with textual fragments, such as labels, descriptions, and links to other relevant concepts. The main task to be solved is the definition of a similarity measure between the single text of the query and the set of texts associated with an ontology concept. We experimentally study this problem on a particular scenario with a socio-pedagogic domain ontology and Italian language texts. We investigate how the basic cosine similarity measure on the bag-of-words text representations can be improved in three distinct ways by (i) taking into account the context of the ontology nodes, (ii) using the linear combination of various measures, and (iii) exploiting semantic resources. The experimental evaluation confirms the improvement of the presented methods upon the baseline. Beside discussing some issues to consider in applying these methods, we point out some directions for further improvement.",
      "title": "222801 Exploring an ontology via text similarity: An experimental study",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939632833&partnerID=40&md5=5d808e7e593ff806995e55178f504e54"
    },
    {
      "abstract": "The distributed vector representations learned from the deep learning framework have shown its great power in capturing the semantic meaning of words, phrases and sentences, from which multiple NLP applications have benefited. As words combine to form the meaning of sentences, so do sentences combine to form the meaning of documents, the idea of representing each document with a dense distributed representation holds promise. In this paper, we propose a supervised framework (Compound RNN) for document classification based on document-level distributed representations learned from deep learning architecture. Our framework first obtains the distributed representation at sentence-level by operating on the parse tree structure from recursive neural network, and then obtains the document presentation-level by convoluting the sentence vectors from a recurrent neural network. Our framework (Compound RNN) outperforms existing document representations such as bag-of-words, LDA in multiple text classification/regression tasks.  Springer International Publishing Switzerland 2015.",
      "title": "222802 Distributed document representation for document classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84945904507&partnerID=40&md5=e81f4cb2bfef56a98c6b93f532ef2494"
    },
    {
      "abstract": "Homogeneous unstructured data (HUD) are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values. Building on such properties, it would be desirable to automatically process HUD to access the main information through a semantic layer - typically an ontology - called semantic view. Hence, we propose an ontology-based approach for extracting semantically rich information from HUD, by integrating and extending recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. Moreover, we design and implement a system, named KnowRex, that has been successfully applied to curriculum vitae in the Europass style to offer a semantic view of them, and be able, for example, to select those which exhibit required skills.  Springer International Publishing Switzerland 2015.",
      "title": "222803 Semantic views of homogeneous unstructured data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951153594&partnerID=40&md5=218bb44e3fa6ba9efd75ba3ed8aacdd8"
    },
    {
      "abstract": "Capturing knowledge from customer reviews about products is an important object of interest for a company. This paper describes an approach to target extraction from user reviews of products. In contrast to other works, based on machine learning approaches, our system is defined by syntactic and semantic connections between possible targets and problem indicators. We present an approach where domain-specific targets are extracted using a problem phrase structure with dependency trees and semantic knowledge from a lexical database. The algorithm achieves an average F1-measure of 77%, evaluated on reviews from four different domains (reviews of electronic products, automobiles, home tools, and baby products). The F1-measure ranges from 76% for the reviews about baby products to 79% for automobile reviews.  Springer International Publishing Switzerland 2015.",
      "title": "222804 Dependency-based problem phrase extraction from user reviews of products",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951825659&partnerID=40&md5=6bdda384d69369d5f7b9618847cd5467"
    },
    {
      "abstract": "Frequent quizzes are used to motivate students to study throughout the academic term instead of waiting until just before the examination. Teachers have been relying on the use of multiple-choice questions to reduce their grading effort. We have developed a tool that grades answers of essay questions automatically. The tool should be a welcome addition to a teacher's arsenal for quiz preparation. The teacher will provide the model answer of an essay question. Incorporating some heuristics into the Stanford Parser, our tool recognizes the parts of speech of each word and creates a parse tree from each sentence. It builds a semantic graph from the sentences in the model answer. The graph uses nodes to represent words and phrases. A directed arc connects two nodes to represent a relation. Currently, four types of arcs are used: attribute, possession, classification and action. In the same way, our tool determines the semantic graph of the student answer. Our tool compares the graph of the model answer with the graph of the student answer. It calculates a score to reflect their similarity. The relative weights of nodes and arcs are adjustable. WordNet helps us to identify synonyms so that the two answers need not be using the same wording to be considered similar. Our tool has some limitations. First, our semantic graph cannot handle timing sequences, for example, 'event A happens before event B'. Second, our graph cannot handle conditional knowledge like 'if X, then Y'. In the future, we may be able to introduce new arc types to address these limitations. Our prototype is not yet ready to replace the grading performed by teachers in formal assessment. But it may be useful to allow students to check their understanding during their self-study.  Springer-Verlag Berlin Heidelberg 2015.",
      "title": "222805 Applying the semantic graph approach to automatic essay scoring",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84952815237&partnerID=40&md5=cafdc523a2a639c62ec0d0d43043138c"
    },
    {
      "abstract": "In this survey, we review different text mining techniques to discover various textual patterns from the social networking sites. Social network applications create opportunities to establish interaction among people leading to mutual learning and sharing of valuable knowledge, such as chat, comments, and discussion boards. Data in social networking websites is inherently unstructured and fuzzy in nature. In everyday life conversations, people do not care about the spellings and accurate grammatical construction of a sentence that may lead to different types of ambiguities, such as lexical, syntactic, and semantic. Therefore, analyzing and extracting information patterns from such data sets are more complex. Several surveys have been conducted to analyze different methods for the information extraction. Most of the surveys emphasized on the application of different text mining techniques for unstructured data sets reside in the form of text documents, but do not specifically target the data sets in social networking website. This survey attempts to provide a thorough understanding of different text mining techniques as well as the application of these techniques in the social networking websites. This survey investigates the recent advancement in the field of text analysis and covers two basic approaches of text mining, such as classification and clustering that are widely used for the exploration of the unstructured text available on the Web.  Cambridge University Press, 2015.",
      "title": "222810 A survey on text mining in social networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925387234&partnerID=40&md5=b5aac58c07d23e6d7556c2e76ea7db44"
    },
    {
      "abstract": "Textual entailment (TE) relation determines whether a text can be inferred from another. Given two texts, one is called the 'Text' denoted as T and the other one is called 'Hypothesis' denoted as H, the process of textual entailment is to decide whether or not the meaning of H can be logically inferred from the meaning of T. Different semantic, lexical and vector based similarity metrics are used as features for different machine learning classifiers to take the entailment decision in this study. We also considered two machine translation evaluation metrics, namely BLEU and METEOR, as similarity metrics for this task. We carried out the experiments on the datasets released in the shared tasks on textual entailment organized in RTE-1, RTE-2, and RTE-3. We experimented with different feature combinations. Best accuracies were obtained on different feature combinations by different classifiers. The best classification accuracies obtained by our system on the RTE-1, RTE-2 and RTE-3 dataset are 55.91%, 58.88% and 63.38% respectively. MT evaluation metrics based feature alone produced the best classification accuracies of 53.9%, 59.3%, and 62.8% on the RTE-1, RTE-2, and RTE-3 datasets respectively.  Springer International Publishing Switzerland 2015.",
      "title": "222812 Textual entailment using different similarity metrics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942645772&partnerID=40&md5=52b862a34bc444ac7190a3e959822707"
    },
    {
      "abstract": "Substantial amount of work has been done on measuring word-to-word relatedness which is also commonly referred as similarity. Though relatedness and similarity are closely related, they are not the same as illustrated by the words lemon and tea which are related but not similar. The relatedness takes into account a broader ranLemge of relations while similarity only considers subsumption relations to assess how two objects are similar. We present in this paper a method for measuring the semantic similarity of words as a combination of various techniques including knowledge-based and corpus-based methods that capture different aspects of similarity. Our corpus based method exploits state-of-the-art word representations. We performed experiments with a recently published significantly large dataset called Simlex-999 and achieved a significantly better correlation (rho = 0.642, P < 0.001) with human judgment compared to the individual performance.  Springer International Publishing Switzerland 2015.",
      "title": "222813 Lemon and tea are not similar: Measuring word-to-word similarity by combining different methods",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942626502&partnerID=40&md5=13a6b8c025317ffc160ac0a1e06526ae"
    },
    {
      "abstract": "Companies have been collecting innovative ideas that can help them to develop new products and services through co-creation with their customers. As more customers participate in suggesting ideas, companies are likely to acquire more valuable ones. At the same time, however, some fundamental problems occur such as managing and selecting useful ideas from a large number of collected ideas. Semantic web mining techniques allow us to manage a large number of customers ideas effectively, extract meaningful information from the ideas, and provide useful information for idea selection. In order to cope with such problems and enhance the value of co-creation, we propose an ontology-based co-creation enhancing system (OnCES) developed using semantic web mining techniques. To this end, we 1) defined a co-creation idea ontology (CCIO) that includes common concepts related to customers ideas from MyStarbucksIdea.com, their attributes, and relationships between them",
      "title": "222815 An ontology-based co-creation enhancing system for idea recommendation in an online community",
      "url": ""
    },
    {
      "abstract": "Neural network methods have achieved promising results for sentiment classification of text. However, these models only use semantics of texts, while ignoring users who express the sentiment and products which are evaluated, both of which have great influences on interpreting the sentiment of text. In this paper, we address this issue by incorporating user-and product-level information into a neural network approach for document level sentiment classification. Users and products are modeled using vector space models, the representations of which capture important global clues such as individual preferences of users or overall qualities of products. Such global evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product-and documentlevel in a unified neural framework, the proposed model achieves state-of-The-Art performances on IMDB and Yelp datasets1.  2015 Association for Computationl Linguisticss.",
      "title": "222817 Learning semantic representations of users and products for document level sentiment classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943784260&partnerID=40&md5=165996cb54084dce2625bf3876f8d891"
    },
    {
      "abstract": "In this paper, we propose a novel learning classifier which utilizes a staged learning-based resource allocation network (SLRAN) for text categorization. In the light of its learning progress, SLRAN is divided into a preliminary learning phase and a refined learning phase. In the former phase, to reduce the sensitivity corresponding to input data an agglomerate hierarchical k-means method is utilized to create the initial structure of hidden layer. Subsequently, a novelty criterion is put forward to dynamically regulate the hidden layer centers. In the latter phase a least square method is used to enhance the convergence rate of network and further improve its ability for classification. Such staged learning-based approach builds a compact structure which decreases the computational complexity of network and boosts its learning capability. In order to implement SLRAN to text categorization, we utilize a semantic similarity approach which reduces the input scales of neural network and reveals the latent semantics between text features. The benchmark Reuter and 20-newsgroup datasets are tested in our experiments and the extensive experimental results reveal that the dynamic learning process of SLRAN improves its classifying performance in comparison with conventional classifiers, e.g. RAN, BP, RBF neural networks and SVM.  2014 Elsevier B.V.",
      "title": "222820 Application of a staged learning-based resource allocation network to automatic text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922485569&partnerID=40&md5=5afe602e9f269c853e41c098a98477c2"
    },
    {
      "abstract": "The computerized methods for document similarity estimation (or plagiarism detection) in natural languages, evolved during the last two decades, have focused on English language in particular and some other languages such as German and Chinese. On the other hand, there are several language-independent methods, but the accuracy of these methods is not satisfactory, especially with morphological and complicated languages such as Arabic. This paper proposes an innovative content-based method for document similarity analysis devoted to Arabic language in order to bridge the existing gap in such software solutions. The proposed method is based on modeling the relation between documents and their n-gram phrases. These phrases are generated from the normalized text, exploiting Arabic morphology analysis and lexical lookup. Resolving possible morphological ambiguity is carried out through applying Part-of-Speech (PoS) tagging on the examined documents. Text indexing and stop-words removal are performed, employing a new method based on text morphological analysis. The examined documents TF-IDF model is constructed using Heuristic based pair-wise matching algorithm, considering lexical and syntactic changes. Then, the hidden associations between the unique n-gram phrases and their documents are investigated using Latent Semantic Analysis (LSA). Next, the pairwise document subset and similarity measures are derived from the Singular Value Decomposition (SVD) computations. The performance of the proposed method was confirmed through experiments with various data sets, exhibiting promising capabilities in estimating literal and some types of intelligent similarities. Finally, the results of the proposed method was compared to that of Plagiarism-Checker-X, and the proposed method outperformed Plagiarism-Checker-X, especially for the intelligent similarity cases with syntactic changes.  2015 IEEE.",
      "title": "222823 Arabic document similarity analysis using n-grams and singular value decomposition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84937887901&partnerID=40&md5=7b4db3091e5c55a71b2c39c0a7f8795a"
    },
    {
      "abstract": "Evaluating semantic similarity between concepts is a very common component in many applications dealing with textual data such as information extraction, information retrieval, natural language processing, or knowledge acquisition. This paper presents an approach to assess semantic similarity between Vietnamese concepts using Vietnamese Wikipedia. Firstly, the Vietnamese Wikipedia' structure is exploited to derive a Vietnamese ontology. Next, based on the obtained ontology, we employ similarity measures in literature to evaluate the semantic similarity between Vietnamese concepts. Then we conduct an experiment providing 30 Vietnamese concept pairs to 18 human subjects to assess similarity of these pairs. Finally, we use Pearson product-moment correlation coefficient to estimate the correlation between human judgments and the results of similarity measures employed. The experiment results show that our system achieves quite good performance and that similarity measures between Vietnamese concepts are potential in enhancing the performance of applications dealing with textual data.  Springer International Publishing Switzerland 2015.",
      "title": "222824 Computing semantic similarity for vietnamese concepts using wikipedia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943264466&partnerID=40&md5=0bf98acb301b7d846f9234cbe8a136a3"
    },
    {
      "abstract": "In social media, recommender systems are becoming more and more important. Different techniques have been designed for recommendations under various scenarios, but many of them do not use user-generated content, which potentially reflects users' opinions and interests. Although a few studies have tried to combine user-generated content with rating or adoption data, they mostly reply on lexical similarity to calculate textual similarity. However, in social media, a diverse range of words is used. This renders the traditional ways of calculating textual similarity ineffective. In this work, we apply vector representation of words to measure the semantic similarity between text. We design a model that seamlessly integrates word vectors into a joint model of user feedback and text content. Extensive experiments on datasets from various domains prove that our model is effective in both recommendation and topic discovery in social media.  Springer International Publishing Switzerland 2015.",
      "title": "222825 Modeling social media content with word vectors for recommendation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951869941&partnerID=40&md5=1823fb2a26500e758ba1503c20d0f9d3"
    },
    {
      "abstract": "Statistical methods have shown a remarkable ability to capture semantics. The word2vec method is a frequently cited method for capturing meaningful semantic relations between words from a large text corpus. It has the advantage of not requiring any tagging while training. The prevailing view is, however, that it lacks the ability to capture semantics of word sequences and is virtually useless for most purposes, unless combined with heavy machinery. This paper challenges that view, by showing that by augmenting the word2vec representation with one of a few pooling techniques, results are obtained surpassing or comparable with the best literature algorithms. This improved performance is justified by theory and verified by extensive experiments on well studied NLP benchmarks (This work is inspired by [10]).  Springer International Publishing Switzerland 2015.",
      "title": "222828 In defense of word embedding for generic text representation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84948844165&partnerID=40&md5=5571c00f52bc5e5012e961e5bff8896a"
    },
    {
      "abstract": "As one of the entity disambiguation tasks, Web Person Disambiguation (WPD) identifies different persons with the same name by grouping search results for different persons into different clusters. Most of current research works use clustering methods to conduct WPD. These approaches require the tuning of thresholds that are biased towards training data and may not work well for different datasets. In this paper, we propose a novel approach by using pairwise co-reference modeling for WPD without the need to do threshold tuning. Because person names are named entities, disambiguation of person names can use semantic measures using the so called co-reference resolution criterion across different documents. The algorithm first forms a forest with person names as observable leaf nodes. It then stochastically tries to form an entity hierarchy by merging names into a sub-tree as a latent entity group if they have co-referential relationship across documents. As the joining/partition of nodes is based on co-reference-based comparative values, our method is independent of training data, and thus parameter tuning is not required. Experiments show that this semantic based method has achieved comparable performance with the top two state-of-the-art systems without using any training data. The stochastic approach also makes our algorithm to exhibit near linear processing time much more efficient than HAC based clustering method. Because our model allows a small number of upper-level entity nodes to summarize a large number of name mentions, the model has much higher semantic representation power and it is much more scalable over large collections of name mentions compared to HAC based algorithms.  Springer International Publishing Switzerland 2015.",
      "title": "222831 Web person disambiguation using hierarchical co-reference model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942584940&partnerID=40&md5=a962f3069d3fd01ba15704e16e1751fb"
    },
    {
      "abstract": "This study leverages the syntactic, semantic and contextual featuresof online hotel and restaurant reviews to extract information aspects and summarizethem into meaningful feature groups. We have designed a set of syntacticrules to extract aspects and their descriptors. Further, we test the precision ofa modified algorithm for clustering aspects into closely related feature groups,on a dataset provided by Yelp.com. Our method uses a combination of semanticsimilarity methods- distributional similarity, co-occurrence and knowledge basebased similarity, and performs better than two state-of-the-art approaches. It isshown that opinion words and the context provided by them can prove to begood features for measuring the semantic similarity and relationship of theirproduct features. Our approach successfully generates thematic aspect groupsabout food quality, decor and service quality.  Springer International Publishing Switzerland 2015.",
      "title": "222834 Summarizing customer reviews through aspects and contexts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942509547&partnerID=40&md5=7316ca2918c2f8fc2bd190ad62673498"
    },
    {
      "abstract": "New words and new senses are produced quickly and are used widely in micro blogs, so to automatically extract new words and predict their semantic orientations is vital to sentiment analysis in micro blogs. This paperproposes Extractor and PolarityAssigner to tackle this task in an unsupervisedmanner. Extractor is a pattern-based method which extracts sentiment-bearingwords from large-scale raw micro blog corpus, where the main task is to eliminatethe huge ambiguities in the un-segmented raw texts. PolarityAssignerpredicts the semantic orientations of words by exploiting emoticons and latentpolarities, using a LDA model which treats each sentiment-bearing wordas a document and each co-occurring emoticon as a word in that document.The experimental results are promising: many new sentiment-bearing wordsare extracted and are given proper semantic orientations with a relatively highprecision, and the automatically extracted sentiment lexicon improves theperformance of sentiment analysis on an open opinion mining task in micro blog corpus.  Springer International Publishing Switzerland 2015.",
      "title": "222839 Sentiment-bearing new words mining: Exploiting emoticons and latent polarities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942513750&partnerID=40&md5=39241d7bc43b77df38a91f1567847bc4"
    },
    {
      "abstract": "The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.  2014 IEEE.",
      "title": "222841 Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84936938573&partnerID=40&md5=b1b56c7f09d21b15dc7db10a80bc5ab6"
    },
    {
      "abstract": "Measures of Semantic Similarity of two sets of words that describe two entities is an important problem in Web Mining. Semantic Similarity measures are used in various applications in Information Retrieval (IR) , Natural Language Processing (NLP) such as Word Sense Disambiguation (WSD), synonym extraction, query expansion and automatic thesauri extraction. The Computer being a syntactic machine, it cannot understand the semantics. Ontology is the explicit specialization of concepts, attributes and the relationships between them. It is for providing relevant and accurate information to the users for a particular domain. A new Semantic Similarity measure based on the domain Ontology is proposed here. It brings out a more accurate relationship between the two words The main purpose of finding Semantic Similarity is to enhance the integration and retrieval of resources in a more meaningful and accurate way. The performance analysis in terms of Precision and Recall for Traditional Search and Semantic Similarity Search is done. The Precision value of Semantic Similarity Search is high compared with the Traditional Search. This paper focuses on the approaches that differentiates the Semantic Similarity Research from other related areas.  Springer International Publishing Switzerland 2015.",
      "title": "222843 An enhanced ontology based measure of similarity between words and semantic similarity search",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84915745165&partnerID=40&md5=10506dc5bfd7d707e0cbbb12e7dd82eb"
    },
    {
      "abstract": "Computational linguistics encompasses a broad range of ideas and research areas, and only a brief introduction is possible here. We chose to include areas in computational linguistics where statisticians can contribute, hoping to provide inspiration to the reader. We describe three main aspects of this discipline-formal languages, information retrieval, and machine learning. These support the overarching goal, which is the representation and analysis of meaning from unstructured text. We then provide an example where text analysis has been applied to unstructured text fields in survey records and conclude with some applications and computational resources.  2015 Wiley Periodicals, Inc.",
      "title": "222844 At the interface of computational linguistics and statistics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84948709969&partnerID=40&md5=740d4478ea003b79dd0accb3d815c6a2"
    },
    {
      "abstract": "Explicit Semantic Analysis (ESA) is an effective method that utilizesWikipedia entries (articles) to represent text and compute semantic relatedness (SR) for text pairs. Analogous to ordinary web search techniques, ESA also suffers from the redundancy issues due to the ongoing expansion of the amount of Wikipedia entries. Entries redundancy could lead to biased representation that lay particular emphasis on semantics from a large number of similar entries. On the other hand, original ESA for SR has a weak point that it does not consider the correlations or similarities between the Wikipedia articles of the text representations. To tackle these problems, We develop a novel method to cluster the redundant or similar entries by similarity measurement based on Paragraph Vector (PV), a neural network language model. Results of experiments on four datasets show that our framework could gain better performance in relatedness accuracy against ESA.  Springer International Publishing Switzerland 2015.",
      "title": "222846 Boosting explicit semantic analysis by clustering paragraph vectors of Wikipedia articles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84950295034&partnerID=40&md5=d0836daff4c5d5b4bccd6349acbc4862"
    },
    {
      "abstract": "One of the biggest challenges in Big Data is the exploitation of Value from large volumes of data that are constantly changing. To exploit value, one must focus on extracting knowledge from these Big Data sources. To extract knowledge and value from unstructured text we propose using a Hierarchical Multi-Label Classification process called Semantic HMC that uses ontologies to describe the predictive model including the label hierarchy and the classification rules. To not overload the user, this process automatically learns the ontologydescribed label hierarchy from a very large set of text documents. This paper aims to present a maintenance process of the ontology-described label hierarchy relations with regards to a stream of unstructured text documents in the context of Big Data that incrementally updates the label hierarchy.  Springer International Publishing Switzerland 2015.",
      "title": "222847 Semantic HMC: Ontology-described hierarchy maintenance in big data context",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951179474&partnerID=40&md5=65cacb992e0a9644709eedb2963bd483"
    },
    {
      "abstract": "A proposal for text mining as a support for knowledge discovery on biological descriptions is introduced. Our aim is both to sustain the curation of databases and to offer an alternative representation frame for accessing information in the biodiversity domain. We work on raw texts with minimum human intervention, applying natural language processing to integrate linguistic and domain knowledge in a mathematical model that makes it possible to capture concepts and relationships between them in a computable form, using conceptual graphs. This provides a reasoning basis for determining semantic disjointedness or subsumption, as well as sub and super-concept relationships.  2015 Elsevier B.V. All rights reserved.",
      "title": "222848 Supporting knowledge discovery for biodiversity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946484620&partnerID=40&md5=39b2d9552daa8ecbaddfe08f9cbfbcad"
    },
    {
      "abstract": "The emergence of Web 2.0 technologies has changed dramatically not only the way users perceive the Internet and interact on it but also the way they influence a community and act in real life aspects. With the rapid rise in use and popularity of social media, people tend to share opinions and observations for almost any subject or event in their everyday life. Consequently, microblogging websites have become a rich data source for user-generated information. The leading opportunity is to take advantage of the wisdom of the crowd and to benefit from collective intelligence in any applicable domain. Towards this direction, we focus on the problem of mining and extracting knowledge from unstructured textual content, for the atmospheric environment domain and its effect to quality of life. As the main contribution, we propose a combined methodology of unsupervised learning methods for analyzing posts from Twitter and clustering textual data into concepts with semantically similar context. By applying Self-Organizing Maps and k-means clustering, we identify possible inter-relationships and patterns of words used in tweets that can form upper concepts of atmospheric and health related topics of discussion. We achieve to group together tweets, from more generic to more specific description levels of their content, according to the selected number of clusters. Strong clusters with significant semantic relatedness among their content are revealed, and hidden relations between concepts and their related semantics are acquired. The results highlight the potential use of social media text streams as a highly-valued supplement source of environmental information and situation awareness.  2015 ISEIS All rights reserved.",
      "title": "222849 Atmospheric environment and quality of life information extraction from twitter with the use of self-organizing maps",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943235171&partnerID=40&md5=11a03d077f21a3216608ca4c7dd12eba"
    },
    {
      "abstract": "Knowledge exaction and text representation are considered as the main concepts concerning organizations nowadays. The estimation of the semantic similarity between words provides a valuable method to enable the understanding of texts. In the field of biomedical domains, using Ontologies have been very effective due to their scalability and efficiency. In this paper, we aim to cluster and classify medical thesis data to better discover the commonalities between theses data and hence, improve the accuracy of the similarity estimation which in return improves the scientific research sector. Experimental evaluations using 4,878 theses data set in the medical sector at Cairo University indicate that the proposed approach yields results that correlate more closely with human assessments than other by using the standard ontology (MeSH). Two different algorithms were used",
      "title": "222852 Lexical similarity using fuzzy Euclidean distance",
      "url": ""
    },
    {
      "abstract": "In this paper, we propose a flexible principle-based approach (PBA) for reader-emotion classification and writing assistance. PBA is a highly automated process that learns emotion templates from raw texts to characterize an emotion and is comprehensible for humans. These templates are adopted to predict reader-emotion, and may further assist in emotional resonance writing. Results demonstrate that PBA can effectively detect reader-emotions by exploiting the syntactic structures and semantic associations in the context, thus outperforming wellknown statistical text classification methods and the state-of-the-art reader-emotion classification method. Moreover, writers are able to create more emotional resonance in articles under the assistance of the generated emotion templates. These templates have been proven to be highly interpretable, which is an attribute that is difficult to accomplish in traditional statistical methods.  2015 Association for Computational Linguistics.",
      "title": "222853 Linguistic template extraction for recognizing reader-emotion and emotional resonance writing assistance",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944051298&partnerID=40&md5=041ce1ca9a8f3deac159a54527bd638c"
    },
    {
      "abstract": "Summary: We propose a parallelization scheme for an existing algorithm for constructing a web-directory, that contains categories of web documents organized hierarchically. The clustering algorithm automatically infers the number of clusters using a quality function based on graph cuts. A parallel implementation of the algorithm has been developed to run on a cluster of multi-core processors interconnected by an intranet. The effect of the well-known Latent Semantic Indexing on the performance of the clustering algorithm is also considered. The parallelized graph-cut based clustering algorithm achieves an F-measure in the range [0.69,0.91] for the generated leaf-level clusters while yielding a precision-recall performance in the range [0.66,0.84] for the entire hierarchy of the generated clusters. As measured via empirical observations, the parallel algorithm achieves an average speedup of 7.38 over its sequential variant, at the same time yielding a better clustering performance than the sequential algorithm in terms of F-measure.  2015 John Wiley & Sons, Ltd.",
      "title": "222855 Parallelization of a graph-cut based algorithm for hierarchical clustering of web documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929428350&partnerID=40&md5=bef6cda905e6428a77f7e1cfe9153c39"
    },
    {
      "abstract": "Developing information technology led to raise the intricacy of information systems intensive, hence techniques with effectiveness and efficiency are required. These techniques are used to support users in using the information for rapid and correct decision-making. Conventional text mining and managing systems mainly use the presence or absence of to discover and analyze useful information from textual documents. However, simple word counting and frequency distributions of term appearances do not capture the meaning behind the words, which results in limiting the ability to mine the texts. This paper has been primarily concerned with constructing text representation model and exploiting that in mining and managing operations such as gathering, searching, filtering, retrieving, extracting, clustering, classifying, and summarizing. This representation model is based on semantic notions to represent text in documents, to infer unknown dependencies and relationships among concepts in a text, to measure the relatedness between text documents, and to apply mining processes using the representation and the relatedness measure. This model reflects the existing relations among concepts and facilitates accurate relatedness measurements that result in better mining performance. The experimental evaluations were carried out on real datasets from various domains, showing the importance of the proposed model.",
      "title": "222856 An efficient approach to construct object model of static textual structure with dynamic behavior based on Q-learning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84933525085&partnerID=40&md5=b7cc1320afb6edf169a86632587e53e7"
    },
    {
      "abstract": "In this paper an attempt has been made to predict the gaze fixation duration at source text words using supervised learning method, namely Support Vector Machine. The machine learning models used in the present work make use of lexical, syntactic and semantic information for predicting the gaze fixation duration. Different features are extracted from the data and models are built by combining the features. Our best set up achieves close to 50% classification accuracy.  2015 IEEE.",
      "title": "222857 Predicting source gaze fixation duration: A machine learning approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84930400616&partnerID=40&md5=39ff7102c399233a33f3f9eaa9aa23fd"
    },
    {
      "abstract": "Analyzing the structure of legal sentences in legal document is an important phase to build a knowledge management system in Legal Engineering. This paper proposes a new approach to recognize logical parts in Vietnamese legal documents based on a statistic machine learning method - Conditional Random Fields. Beside linguistic features such as word features, part of speech features, we use semantic features of logical parts such as trigger features and ontology features to improve the result of the annotation system. Experiments were conducted in a Vietnamese Business Law data set and obtained 78.12% at precision and 68.72% at recall measure. Compare to state-of-the-art systems, it improves the result for recognizing some logical parts.  2015 IEEE.",
      "title": "222860 Recognizing logical parts in Vietnamese legal texts using conditional random fields",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925841423&partnerID=40&md5=c030f7164ff4a4cbf22f9eba9b1a2495"
    },
    {
      "abstract": "Most of the researches on text categorization are focus on using bag of words. Some researches provided other methods for classification such as term phrase, Latent Semantic Indexing, and term clustering. Term clustering is an effective way for classification, and had been proved as a good method for decreasing the dimensions in term vectors. The authors used hierarchical term clustering and aggregating similar terms. In order to enhance the performance, they present a modify indexing with terms in cluster. Their test collection extracted from Chinese NETNEWS, and used the Centroid-Based classifier to deal with the problems of categorization. The results had shown that term clustering is not only reducing the dimensions but also outperform than bag of words. Thus, term clustering can be applied to text classification by using any large corpus, its objective is to save times and increase the efficiency and effectiveness. In addition to performance, these clusters can be considered as conceptual knowledge base, and kept related terms of real world. Copyright  2015, IGI Global.",
      "title": "222861 Chinese text categorization via bottom-up weighted word clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84924407603&partnerID=40&md5=2058d4995b211bae3725bb6f57d6690c"
    },
    {
      "abstract": "Query subtopic mining aims to find aspects to represent people's potential intents for a query. Clustering query reformulations is the most common approach for subtopic mining these days. However, there are some challenges that the existing approaches have to face in finding both relevant and diverse subtopics, such as term mismatch and data sparseness. In this paper, a novel semantic representations for query subtopics is introduced, which including phrase embedding representation and query category distributional representation, to solve those problems mentioned above. Furthermore, we also combine multiple semantic representations into vector space model and compute a joint similarity for clustering query reformulations. To evaluate our theory an experiment is conducted on a public dataset offered by NTCIR subtopic mining project, the experimental results show that phrase embedding representation is the most effective representation while combining multiple semantics benefits short text clustering and improves the performance of query subtopic mining.  2015 SERSC.",
      "title": "222863 Query subtopic mining by combining multiple semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84956865805&partnerID=40&md5=2f18d8fe742c1d9563ea8abdc8091b95"
    },
    {
      "abstract": "Tackling the ever-growing amount of specialized literature in the life sciences domain is a paramount challenge. Various scientific workflows depend on using domain knowledge from resources that summarize, in structured form, validated information extracted from scientific publications. Manual curation of these data is a demanding task, and latest strategies use computerized solutions to aid in the analysis, extraction and storage of relevant concepts and their respective attributes and relationships. The outcome of these complex document curation workflows provides valuable insights into the overwhelming amount of biomedical information being produced. Yet, the majority of automated and interactive annotation tools are not open, limiting access to knowledge and reducing the potential scope of the manually curated information. In this manuscript, we propose an interoperable semantic layer to unify document curation results and enable their proper exploration through multiple interfaces geared towards bioinformatics developers and general life sciences researchers. This enables a unique scenario where results from computational annotation tools are harmonized and further integrated into rich semantic knowledge bases, providing a solid foundation for discovering knowledge.  Springer International Publishing Switzerland 2015.",
      "title": "222865 A semantic layer for unifying and exploring biomedical document curation results",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944460806&partnerID=40&md5=c0bc8311b1a5aff56f4dc406ce4cc336"
    },
    {
      "abstract": "With the purpose of improving the accuracy of text categorization and reducing the dimension of the feature space, this paper proposes a two-stage feature selection method based on a novel category correlation degree (CCD) method and latent semantic indexing (LSI). In the first stage, a novel CCD method is proposed to select the most effective features for text classification, which is more effective than the traditional feature selection method. In the second stage, document representation requires a high dimensionality of the feature space and does not take into account the semantic relation between features, which leads to a poor categorization accuracy. So LSI method is proposed to solve these problems by using statistically derived conceptual indices to replace the individual terms which can discover the important correlative relationship between features and reduce the feature space dimension. Firstly, each feature in our algorithm is ranked depending on their importance of classification using CCD method. Secondly, we construct a new semantic space based on LSI method among features. The experimental results have proved that our method can reduce effectively the dimension of text vector and improve the performance of text categorization.  2015, Shanghai Jiaotong University and Springer-Verlag Berlin Heidelberg.",
      "title": "222866 A two-stage feature selection method for text categorization by using category correlation degree and latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949324997&partnerID=40&md5=fa27aec04336e704bfb985199a7a4302"
    },
    {
      "abstract": "Daily deals have emerged in the last three years as a successful form of online advertising. The downside of this success is that users are increasingly overloaded by the many thousands of deals offered each day by dozens of deal providers and aggregators. The challenge is thus offering the right deals to the right users i.e., the relevance ranking of deals. This is the problem we address in our paper. Exploiting the characteristics of deals data, we propose a combination of a term- and a concept-based retrieval model that closes the semantic gap between queries and documents expanding both of them with category information. The method consistently outperforms state-of-the-art methods based on term-matching alone and existing approaches for ad classification and ranking.  2015 Elsevier Ltd. All rights reserved.",
      "title": "222871 Ranking of daily deals with concept expansion",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84924416878&partnerID=40&md5=3f2d696c301027098f5d3dea38f6c3aa"
    },
    {
      "abstract": "When searching a query in the microblogging, a user would typically receive an archive of tweets as part of a retrospective piece on the impact of social media. For ease of understanding the retrieved tweets, it is useful to produce a summarized timeline about a given topic. However, tweet timeline generation is quite challenging due to the noisy and temporal characteristics of microblogs. In this paper, we propose a graph-based dynamic greedy clustering approach, which considers the coverage, relevance and novelty of the tweet timeline. First, tweet embedding representation is learned in order to construct the tweet semantic graph. Based on the graph, we estimate the coverage of timeline according to the graph connectivity. Furthermore, we integrate a noise tweet elimination component to remove noisy tweets with the lexical and semantic features based on relevance and novelty. Experimental results on public Text Retrieval Conference (TREC) Twitter corpora demonstrate the effectiveness of the proposed approach.  Springer International Publishing Switzerland 2015.",
      "title": "222874 Tweet timeline generation via graph-based dynamic greedy clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84958057757&partnerID=40&md5=7d5f299a5ecf04702a747b5314307f8f"
    },
    {
      "abstract": "Textual entailment has been proposed as a unifying generic framework for modeling language variability and semantic inference in different Natural Language Processing (NLP) tasks. By evaluating on NTCIR-11 RITE3 Simplified Chinese subtask data set, this paper firstly demonstrates and compares the performance of Chinese textual entailment recognition models that combine different lexical, syntactic, and semantic features. Then a word embedding based lexical entailment module is added to enhance classification ability of our system further. The experimental results show that the word embedding for lexical semantic relation reasoning is effective and efficient in Chinese textual entailment.  Springer International Publishing Switzerland 2015.",
      "title": "222875 Chinese textual entailment recognition enhanced with word embedding",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84952655804&partnerID=40&md5=260b26ed4265daf7ac3b30cfe3613b97"
    },
    {
      "abstract": "Training sets of images for object recognition are the pillars on which classifiers base their performances. We have built a framework to support the entire process of image and textual retrieval from search engines, which, giving an input keyword, performs a statistical and a semantic analysis and automatically builds a training set. We have focused our attention on textual information and we have explored, with several experiments, three different approaches to automatically discriminate between positive and negative images: keyword position, tag frequency and semantic analysis. We present the best results for each approach.  Springer International Publishing Switzerland 2015.",
      "title": "222877 Semantic-analysis object recognition: Automatic training set generation using textual tags",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928798532&partnerID=40&md5=9fc037e42004f6d33abe110831561a15"
    },
    {
      "abstract": "The text document classification employs either text based approach or semantic based approach to index and retrieve text documents. The former uses keywords and therefore provides limited capabilities to capture and exploit the conceptualization involved in user information needs and content meanings. The latter aims to solve these limitations using content meanings, rather than keywords. More formally, the semantic based approach uses the domain ontology to exploit the content meanings of a particular domain. This approach however has some drawbacks. It lacks enrichment of ontology concepts with new lexical resources and evaluation of the importance indicated by weights of those concepts. Therefore to address these issues, this paper proposes a new ontology based text document classification framework. The proposed framework incorporates a newly developed objective metric called SEMCON to enrich the domain ontology with new concepts by combining contextual as well as semantic information of a term within a text document. The framework also introduces a new approach to automatically estimate the importance of ontology concepts which is indicated by the weights of these concepts, and to enhance the concept vector space model using automatically estimated weights.  Springer International Publishing Switzerland 2015.",
      "title": "222880 A general framework for text document classification using SEMCON and ACVSR",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84947240268&partnerID=40&md5=f912b3c0955790ec3a62744ed75f7c25"
    },
    {
      "abstract": "Education data mining (EDM) is an emerging field which take advantage of natural language processing, data mining, statistics and machine learning algorithms for different types of educational data especially related to medical e-learning. Medical text categorization is the one of the major component to help students more easily and effectively search medical text for e-Learning. In this paper, spectral based Kernel Discriminant Analysis has been introduced for medical text categorization. We evaluated the proposed approach on 10 most frequent categories of cardiovascular diseases group from Ohsumed data sets. When compared with existing approaches, the results have indicated significant increase in performance. In order to further refine the search, Semantic Engine that uses Brain-Like approach (SEBLA) is also introduced in this paper.  2015 IEEE.",
      "title": "222881 Medical text categorization using SEBLA and Kernel Discriminant Analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943328025&partnerID=40&md5=cb285a35b0b497329c7a83592760b2a8"
    },
    {
      "abstract": "Feature selection is an important problem for any pattern classification task. In this paper, we developed an ensemble of two Maximum Entropy classifiers for Twitter sentiment analysis: one for subjectivity and the other for polarity classification. Our ensemble employs surface-form, semantic and sentiment features. The classification complexity of this ensemble of linear models is linear with respect to the number of features. Our goal is to select a compact feature subset from the exhaustive list of extracted features in order to reduce the computational complexity without scarifying the classification accuracy. We evaluate the performance on two benchmark datasets, CrowdScale and SemEval. Our selected 20K features have shown very similar results in subjectivity classification to the NRC state-of-the-art system with 4 million features that has ranked first in 2013 SemEval competition. Also, our selected features have shown a relative performance gain in the ensemble classification over the baseline of uni-gram and bi-gram features of 9.9% on CrowdScale and 11.9% on SemEval.  Springer International Publishing Switzerland 2015.",
      "title": "222884 Feature selection for twitter sentiment analysis: An experimental study",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942508773&partnerID=40&md5=7d6949ed7045c8cbd30aa90e397d127f"
    },
    {
      "abstract": "The bag-of-words model is the most common text representation model used in information retrieval and text classification. It represents a text as a bag of words without regard to word order and relationship. We propose a new text classification approach that is based on a more efficient text representation than the Bag-of-words model. Our approach employs automatic relation extraction via statistical co-occurrence of words in a given text collection. We extract relations in the form of word-word proximity matrix and then use the matrix elements as correction factors for the Bag-of-words text representation. Thereby we obtain a new text representation model enriched by semantic relations. Combining this model with Support Vector Machine classifier we perform a series of experiments on text classification. The experimental results prove that our approach enhances accuracy of text classification compared to the Bag-of-words approach.  Springer International Publishing Switzerland 2015.",
      "title": "222888 Enhancement of binary text classification using automatic relation extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951787602&partnerID=40&md5=605200fff40f38d7d5bc599a0596b8c6"
    },
    {
      "abstract": "This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: The second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of stateof-the-Art models for this type of relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition.  2015 Association for Computationl Linguisticss.",
      "title": "222891 Structural representations for learning relations between pairs of texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943805083&partnerID=40&md5=4757b7e90f2a69d552c1d45e5f282adf"
    },
    {
      "abstract": "Language suffers an everlasting process of change, both at a semantic level, where existing words acquire new meanings, and at a lexical level, where new concepts appear and old ones disappear or are used less frequently. New words (terms/concepts) may be added as a result of scientific discoveries or socio-cultural influences, while other words are forgotten or are assigned alternative meanings. These changes in a vocabulary usually characterize important shifts in the environment or the domain they are used in. For experts there is an evident connection between a new concept and some of the existing ones, but for regular people these relations remain hidden and need to be identified. In particular, in the medical domain new terms appear as a result of new discoveries and it becomes an important challenge to establish the connections between different concepts. Moreover, it is important to detect if such a relation even exists. In this paper, we present a graph-based approach to identify the semantic path (which is a chain of semantically related words) between the concepts that appeared in the bio-medicine publications available in the Pub Med corpus over a time period of 20 years.  2015 IEEE.",
      "title": "222892 Tracing the paths between concepts in large bio-medical corpora",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84947087190&partnerID=40&md5=8322df4b6f7c8d0de240a3677a7c04e8"
    },
    {
      "abstract": "With the tremendous growth of unstructured data in the Business Intelligence, there is a need for incorporating textual data into data warehouses, to provide an appropriate multidimensional analysis (OLAP) and develop new approaches that take into account the textual content of data. This will provide textual measures to users who wish to analyse documents online. In this paper, we propose a new aggregation function for textual data in an OLAP context. For aggregating keywords, our contribution is to use a data mining technique, such as kmeans, but with a distance based on the Google similarity distance. Thus our approach considers the semantic similarity of keywords for their aggregation. The performance of our approach is analyzed and compared to another method using the k-bisecting clustering algorithm and based on the Jensen-Shannon divergence for the probability distributions. The experimental study shows that our approach achieves better performances in terms of recall, precision,F-measure complexity and runtime.",
      "title": "222893 GOTA: Using the Google similarity distance for OLAP textual aggregation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939499994&partnerID=40&md5=6d18161e278aea38d05ec9e232c1b1bc"
    },
    {
      "abstract": "Organizing construction project documents based on semantic similarities offers several advantages over traditional metadata criteria, including facilitating document retrieval and enhancing knowledge reuse. In this study, the use of text classifiers for automatically classifying documents according to their corresponding group of semantically related documents is evaluated. Supporting documents of claims were used as representations of document discourses. The evaluation was performed under varying general conditions (such as dimensionality level and weighting method) to assess the effect of such conditions on performance, and varying classifier-specific parameters. The highest performance in terms of classification accuracy was achieved by a Rocchio classifier and a kNN classifier with the application of dimensionality reduction and using the tf-idf weighting method. A combined classifier approach was also evaluated in which the classification outcome is based on a majority vote strategy between the outcomes of three different classifiers. The evaluation demonstrated that classification accuracy of standard text classifiers can be refined by applying an appropriate level of dimensionality reduction to the training and testing sets and by combining the results of several classifiers. Accordingly, such application enables effective utilization of standard text classifiers for automatic organization of project documents based on text content.  2014 American Society of Civil Engineers.",
      "title": "222894 Automatic classification of project documents on the basis of text content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928015856&partnerID=40&md5=09ba79e16af873117aeb0f3c47131bf9"
    },
    {
      "abstract": "Categorization of text documents plays a vital role in information retrieval systems. Clustering the text documents which supports for effective classification and extracting semantic knowledge is a tedious task. Most of the existing methods perform the clustering based on factors like term frequency, document frequency and feature selection methods. But still accuracy of clustering is not up to mark. In this paper we proposed an integrated approach with a metric named as Term Rank Identifier (TRI). TRI measures the frequent terms and indexes them based on their frequency. For those ranked terms TRI will finds the semantics and corresponding class labels. In this paper, we proposed a Semantically Enriched Terms Clustering (SETC) Algorithm, it is integrated with TRI improves the clustering accuracy which leads to incremental text categorization. Our experimental analysis on different data sets proved that the proposed SETC performing better.  Springer India 2015.",
      "title": "222895 An integrated approach to improve the text categorization using semantic measures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84917707246&partnerID=40&md5=d9acf583a52145818b9b62f4c19c9420"
    },
    {
      "abstract": "Irony is a fundamental rhetorical device. It is a uniquely human mode of communication, curious in that the speaker says something other than what he or she intends. Recently, computationally detecting irony has attracted attention from the natural language processing (NLP) and machine learning (ML) communities. While some progress has been made toward this end, I argue that current machine learning methods rely too heavily on shallow, unstructured, syntactic modeling of text to consistently discern ironic intent. Irony detection is an interesting machine learning problem because, in contrast to most text classification tasks, it requires a semantics that cannot be inferred directly from word counts over documents alone. To support this position, I survey the large body of existing philosophical/literary work investigating ironic communication. I then survey more recent computational efforts to operationalize irony detection in the fields of NLP and ML. I identify the disparities of the latter with respect to the former. Specifically, I highlight a major conceptual problem in all existing computational models of irony: none maintain an explicit model of the speaker/environment. I argue that without such an internal model of the speaker, irony detection is hopeless, as this model is necessary to represent expectations, which play a key role in ironic communication. I sketch possible means of embedding such models into computational approaches to irony detection. In particular, I introduce the pragmatic context model, which looks to operationalize computationally existing theories of irony. This work is a step toward unifying work on irony from literary, empirical and philosophical perspectives with modern computational models.  Springer Science+Business Media Dordrecht 2013.",
      "title": "222896 Computational irony: A survey and new perspectives",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84924851941&partnerID=40&md5=fc3e29a74e96057c01cd93335feccbb2"
    },
    {
      "abstract": "Comprehensive analysis of multi-domain texts has generated an important effect on text mining. Although the objects described by these multi-domain texts belong to different fields, they sometimes are overlapped partially",
      "title": "222897 Graph-based approach for cross domain text linking",
      "url": "Conference Paper"
    },
    {
      "abstract": "Cross-domain recommendation systems exploit tags, textual descriptions or ratings available for items in one domain to recommend items in multiple domains. Handling unstructured/ unannotated item information is, however, a challenge. Topic modeling offer a popular method for deducing structure in such data corpora. In this paper, we introduce the concept of a common latent semantic space, spanning multiple domains, using topic modeling of semantic clustered vocabularies of distinct domains. The intuition here is to use explicitly-determined semantic relationships between non-identical, but possibly semantically equivalent, words in multiple domain vocabularies, in order to capture relationships across information obtained in distinct domains. The popular WordNet based ontology is used to measure semantic relatedness between textual words. The experimental results shows that there is a marked improvement in the precision of predicting user preferences for items in one domain when given the preferences in another domain.  2014 IEEE.",
      "title": "222901 Semantic clustering-based cross-domain recommendation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925149393&partnerID=40&md5=a676a1a39f158604909af2ca4acc778e"
    },
    {
      "abstract": "Event is the coarse-grained form of knowledge representation compared with that of word or concept. Event has more semantic information than word or concept, which is the basic unit of knowing and understanding the real-world for human. There are inherent relations between different events in objective world. At present, there is no relatively full-fledged corpus about event relation in Natural Language Processing field. Therefore, in this paper, we regarded event as the unit of knowledge representation, then we semi-automatically annotated event relation in the aspects of semantic, timing, co-reference and co-participant relation for Chinese newswire news text documents. The purpose of this paper is to supply an evaluation benchmark for event relation detection and platform for text mining. The mean Kappa-score, which illustrated the consistency of annotation, is 93.5%. , 2015, Journal of Computational Information Systems. All right reserved.",
      "title": "222909 Annotation event relation for chinese newswire text document",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84940778555&partnerID=40&md5=895a3bb9882389d0747d135357e8f9c2"
    },
    {
      "abstract": "With the growth of the Internet community, textual data has proven to be the main tool of communication in human-machine and human-human interaction. This communication is constantly evolving towards the goal of making it as human and real as possible. One way of humanizing such interaction is to provide a framework that can recognize the emotions present in the communication or the emotions of the involved users in order to enrich user experience. For example, by providing insights to users for personal preferences and automated recommendations based on their emotional state. In this work, we propose a framework for emotion classification in English sentences where emotions are treated as generalized concepts extracted from the sentences. We start by generating an intermediate emotional data representation of a given input sentence based on its syntactic and semantic structure. We then generalize this representation using various ontologies such as Word Net and Concept Net, which results in an emotion seed that we call an emotion recognition rule (ERR). Finally, we use a suite of classifiers to compare the generated ERR with a set of reference ERRs extracted from a training set in a similar fashion. The used classifiers are k-nearest neighbors (KNN) with handcrafted similarity measure, Point Mutual Information (PMI), and PMI with Information Retrieval (PMI-IR). When applied on different datasets, the proposed approach significantly outperformed the existing state-of- The art machine learning and rule-based classifiers with an average F-Score of 84%.  2014 IEEE.",
      "title": "222910 Emotion recognition from text based on automatically generated rules",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84936854417&partnerID=40&md5=1d15d512a72f639d4e736a9349af5bc3"
    },
    {
      "abstract": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarities as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurrence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.  Springer International Publishing Switzerland 2015.",
      "title": "222912 Rehabilitation of count-based models for word vector representations",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942673026&partnerID=40&md5=21aabb033c6994a7e50f7972ab0e102b"
    },
    {
      "abstract": "During the last few years, the investigation of methodologies to automatically detect and characterise the figurative traits of textual contents has attracted a growing interest. Indeed, the capability to correctly deal with figurative language and more specifically with satire is fundamental to build robust approaches in several sub-fields of Artificial Intelligence including Sentiment Analysis and Affective Computing. In this paper we investigate the automatic detection of Tweets that advertise satirical news in English, Spanish and Italian. To this purpose we present a system that models Tweets from different languages by a set of language independent features that describe lexical, semantic and usage-related properties of the words of each Tweet. We approach the satire identification problem as binary classification of Tweets as satirical or not satirical messages. We test the performance of our system by performing experiments of both monolingual and cross-language classifications, evaluating the satire detection effectiveness of our features. Our system outperforms a word-based baseline and it is able to recognise if a news in Twitter is satirical or not with good accuracy. Moreover, we analyse the behaviour of the system across the different languages, obtaining interesting results.",
      "title": "222915 Do we criticise (and Laugh) in the same way? Automatic detection of multi-lingual satirical news in twitter",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949802029&partnerID=40&md5=ebda2de5737cfa26acc8140523a01f46"
    },
    {
      "abstract": "Measuring the similarity between objects is considered one of the main hot topics nowadays and the main core requirement for several data mining and knowledge discovery task. For better performance most organizations are in need on semantic similarity and similarity measures. This article presents different distance metrics used for measuring the similarity between qualitative data within a text. The case study represents a qualitative data of Faculty of medicine Cairo University for theses. The dataset is about 5,000 thesis document with 35 departments and about 16,000 keyword. As a result, we are able to better discover the commonalities between theses data and hence, improve the accuracy of the similarity estimation which in return improves the scientific research sector. The experimental results show that Kulczynksi distance yields better with a 92.51% without normalization that correlate more closely with human assessments compared to other distance measures.  Springer India 2015.",
      "title": "222916 Effective classification and categorization for categorical sets: Distance similarity measures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922055593&partnerID=40&md5=b6b43cb58d06b3108a46bcbb3aebadf8"
    },
    {
      "abstract": "Domain knowledge bases are a basis for advanced knowledge-based systems, manually creating a formal knowledge base for a certain domain is both resource consuming and non-trivial. In this paper, we propose an approach that provides support to extract, select, and disambiguate terms embedded in domain specific documents. The extracted terms are later used to en-rich existing ontologies/taxonomies, as well as to bridge domain specific knowledge base with a generic knowledge base such as Word Net. The proposed approach addresses two major issues in the term extraction domain, namely quality and efficiency. Also, the proposed approach adopts a feature-based method that assists in topic extraction and integration with existing ontologies in the given domain. The proposed approach is realized in a research prototype, and then a case study is conducted in order to illustrate the feasibility and the efficiency of the proposed method in the finance domain. A preliminary empirical validation by the domain experts is also conducted to determine the accuracy of the proposed approach. The results from the case study indicate the advantages and potential of the proposed approach.  2015 IEEE.",
      "title": "222917 Term extraction and disambiguation for semantic knowledge enrichment: A case study on initial public offering (IPO) prospectus corpus",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944196661&partnerID=40&md5=cd6f76ebf32fbc936111a1cebaf987e5"
    },
    {
      "abstract": "In this paper, we present our method of lexical enrichment applied on a semantic network in the context of query disambiguation. This network represents the list of relevant sentences in French (noted by listRSF) that respond to a given Arabic query. In a first step we generate the semantic network covering the content of the listRSF. The generation of the network is based on our approach of semantic and conceptual indexing. In a second step, we apply a contextual enrichment on this network using association rules model. The evaluation of our method shows the impact of this model on the semantic network enrichment. As a result, this enrichment increases the F-measure from 71% to 81% in terms of the (liste<inf>RSF</inf>) coverage.  Springer International Publishing Switzerland 2015.",
      "title": "222919 Lexical network enrichment using association rules model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942694239&partnerID=40&md5=b0279cc41295cf6db828dd861b4acc09"
    },
    {
      "abstract": "Full-text publications in an electronic form become more prevalent than ever before. It is a difficult challenge to extract concepts from unstructured document collections data because different concepts and their relationships are buried in them and ample term variations make the challenge compound. Extracted concepts are useful instruments of managing and searching large document collections and play a pivotal role in indexing electronic documents and building digital libraries. In this paper we explore a biomedical concept extraction technique based on a ranking algorithm of concept graphs. The proposed technique comprises two major steps: the first step is to represent documents with graphs whose nodes and edges are created by Named Entity Recognition and UMLS Semantic Network. The second step is rank concepts with relative importance algorithms. We evaluate our technique with a set of biomedical full-texts and compare it to various different key-phrase extraction and graph ranking techniques. The experimental results show that our technique achieves the best performance over other compared algorithms. We further take a close look at the properties of the network to examine how concepts are related to each other and what concept plays a dominant role in the network. To this end, we build the network with 526 full-text articles published in PubMed Central and measure the significance of nodes by centrality.  2015 IEEE.",
      "title": "222920 Exploring concept graphs for biomedical literature mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928103764&partnerID=40&md5=fffa62e81b5fdd3faad96fa98d403a44"
    },
    {
      "abstract": "In this work, we made an experimental study for compare two approaches of reduction dimensionality and verify their effectiveness in Arabic document classification. Firstly, we apply latent Dirichlet allocation (LDA) and latent semantic indexing (LSI) for modeling our document sets OATC (open Arabic Tunisian corpus) contained 20.000 documents collected from Tunisian newspapers. We generate two matrices LDA (documents/topics) and LSI (documents/topics). Then, we use the SVM algorithm for document classification, which is known as an efficient method for text mining. Classification results are evaluated by precision, recall and F-measure. The evaluation of classification results was performed on OATC corpus (70 % training set and 30 % testing set). Our experiment shows that the results of dimensionality reduction via LDA outperform LSI in Arabic topic classification.  Springer International Publishing Switzerland 2015.",
      "title": "222927 LDA and LSI as a dimensionality reduction method in arabic document classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951859574&partnerID=40&md5=86822997617860b4f0fc90d64ab25b93"
    },
    {
      "abstract": "Experts state that a detailed analysis of rhetorical structure highlights crucial sentiment-carrying text segments. A better understanding of a texts sentiment can be obtained by guiding the analysis by the texts rhetorical structure. They state that automated sentiment analysis is related to natural language processing, computational linguistics, and text mining. Deep linguistic analysis is a key success factor for sentiment-analysis systems, as it helps in dealing with compositionality or the way the semantic orientation of text is determined by the combined semantic orientations of its constituent phrases.",
      "title": "222932 Using rhetorical structure in sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84934783073&partnerID=40&md5=7c2069b9ded3429291303c5779f52041"
    },
    {
      "abstract": "Clustering through organizing large text corpora has a key role in an easy navigation and browsing of massive amounts of text data and in particular in search engines. The documents comparison using the conventional clustering techniques is based on the surface similarities of words or extracted morphemes. This leads to non-semantic clusters usually. In this paper, Farsi, also known as Persian, has been taken into account with regards to the fact that the amount of electronic Farsi texts are growing rapidly. The documents are enriched by using semantic relationships - synonymy, hypernymy and hyponymy-extracted from FarsNet lexical ontology. A WSD procedure is proposed to decrease uncertainty. After preprocessing routines, three clustering algorithms including Bisecting K-means, LSI and PLSI based clustering is applied on the pre-categorized Persian Hamshahri corpus. Experimental results show the improvement of clustering quality when text data is enriched by the semantic relations especially using PLSI based approach.  2015, Journal of Information Science and Engineering.",
      "title": "222937 A new experience in Persian text clustering using FarsNet ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84920732935&partnerID=40&md5=1febdfeb26c12b5e4f8b78739829c65d"
    },
    {
      "abstract": "The paper describes approach to use statistically-based tools incorporated into Sketch Engine system for electronic text corpora processing to mining big textual data for search and extract word semantic properties. It presents and compares series of word search experiments using different statistical approaches and evaluates results for Bulgarian language EUROPARL 7 Corpus search to extract word semantic properties. Finally, the methodology is extended for multilingual application using Slovak language EUROPARL 7 Corpus.  2015 SPIE.",
      "title": "222946 Evaluating word semantic properties using Sketch Engine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84924405430&partnerID=40&md5=ed78737ace48e49ed5d04dd717f4c217"
    },
    {
      "abstract": "Time is an essential component for the analysis of medical data, andthe sentiment beneath the temporal information is intrinsically connected withthe medical reasoning tasks. The present paper introduces the problem of identifyingtemporal information as well as tracking of the sentiments/emotionsaccording to the temporal situations from the interviews of cancer patients.A supervised method has been used to identify the medical events using a list oftemporal words along with various syntactic and semantic features. We alsoanalyzed the sentiments of the patients with respect to the time-bins with thehelp of dependency based sentiment analysis techniques and several Sentimentlexicons. We have achieved the maximum accuracy of 75.38% and 65.06% inidentifying the temporal and sentiment information, respectively.  Springer International Publishing Switzerland 2015.",
      "title": "222952 Identifying temporal information and tracking sentiment in cancer patients' interviews",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942540507&partnerID=40&md5=cbbdfab77814329dccc7ed9c6a3c6440"
    },
    {
      "abstract": "This paper proposes a Wikipedia-based semantic similarity measurement method that is intended for real-world noisy short texts. Our method is a kind of explicit semantic analysis (ESA), which adds a bag of Wikipedia entities (Wikipedia pages) to a text as its semantic representation and uses the vector of entities for computing the semantic similarity. Adding related entities to a text, not a single word or phrase, is a challenging practical problem because it usually consists of several subproblems, e.g., key term extraction from texts, related entity finding for each key term, and weight aggregation of related entities. Our proposed method solves this aggregation problem using extended naive Bayes, a probabilistic weighting mechanism based on the Bayes theorem. Our method is effective especially when the short text is semantically noisy, i.e., they contain some meaningless or misleading terms for estimating their main topic. Experimental results on Twitter message and Web snippet clustering revealed that our method outperformed ESA for noisy short texts. We also found that reducing the dimension of the vector to representative Wikipedia entities scarcely affected the performance while decreasing the vector size and hence the storage space and the processing time of computing the cosine similarity.  2013 IEEE.",
      "title": "222959 Wikipedia-Based Semantic Similarity Measurements for Noisy Short Texts Using Extended Naive Bayes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84933048217&partnerID=40&md5=11657b593c7cdab0c549fae4b1bb1ad7"
    },
    {
      "abstract": "We present a neural network method for review rating prediction in this paper. Existing neural network methods for sentiment prediction typically only capture the semantics of texts, but ignore the user who expresses the sentiment. This is not desirable for review rating prediction as each user has an influence on how to interpret the textual content of a review. For example, the same word (e.g. good) might indicate different sentiment strengths when written by different users. We address this issue by developing a new neural network that takes user information into account. The intuition is to factor in user-specific modification to the meaning of a certain word. Specifically, we extend the lexical semantic composition models and introduce a userword composition vector model (UWCVM), which effectively captures how user acts as a function affecting the continuous word representation. We integrate UWCVM into a supervised learning framework for review rating prediction, and conduct experiments on two benchmark review datasets. Experimental results demonstrate the effectiveness of our method. It shows superior performances over several strong baseline methods.",
      "title": "222960 User modeling with neural network for review rating prediction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949741752&partnerID=40&md5=246002e807860c24db91aa0972926a81"
    },
    {
      "abstract": "The availability of highly-informative semantic descriptions of scholarly publishing contents enables an easier sharing and reuse of research findings as well as a better assessment of the quality of scientific productions. In the context of the ESWC2015 Semantic Publishing Challenge, we present a system that automatically generates rich RDF datasets from CEUR-WS workshop proceedings and exposes them as Linked Data. Web pages of proceedings and textual contents of papers are analyzed through proper text processing pipelines. Semantic annotations are added by a set of SVM classifiers and refined by heuristics, gazetteers and rule-based grammars. Web services are exploited to link annotations to external datasets like DBpedia, CrossRef, FundRef and Bibsonomy. Finally, the data is modelled and published as an RDF graph.  Springer International Publishing Switzerland 2015.",
      "title": "222964 On the automated generation of scholarly publishing linked datasets: The case of CEUR-WS proceedings",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951271843&partnerID=40&md5=16982f2ea1e9499f80f6dca41e9071ac"
    },
    {
      "abstract": "In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems",
      "title": "222965 Concepts extraction for medical documents using ontology",
      "url": ""
    },
    {
      "abstract": "Cross-domain learning is a very promising technique to improve classification in the target (testing) domain whose data distributions are very different from the source (training) domain. Many cross-domain text classification methods are built on topic modeling approaches. However, topic model methods are unsupervised in nature without fully utilizing the label information of the source domain. In addition, almost all cross-domain learning approaches utilize the knowledge of source domain in the later stage of the training process, and this limits the knowledge transfer. In this paper, we propose a model named Supervised Adaptive transfer Probabilistic Latent Semantic Analysis (SAtPLSA) for cross-domain text classification aiming to deal with the above two issues. The proposed model extends the original PLSA to a supervised learning paradigm. By defining the common labeled information from each term across domains, we transfer knowledge in source domain to assist classifying text in target domain. In addition, we adaptively modify the weight value controlling the proportion of the usage of knowledge from source domain in the model learning process. At last, we conducted experiments on nine benchmark datasets in cross domain text classification to compare the performance of our proposed algorithm with two classicalsupervised learning methods and five state-of-art transfer learning approaches. The experimental results have shown the effectiveness and efficiency of our proposed SAtPLSA algorithm.  2014 IEEE.",
      "title": "222970 Supervised adaptive-transfer PLSA for cross-domain text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84936857130&partnerID=40&md5=876deec443274a9b0efedb3836cd2e9b"
    },
    {
      "abstract": "The importance of semantic similarity measures between sentences is increasingly growing in text mining, text clustering, and question answering. Many studies have focused on finding exact term matching to predict sentence similarity. In this paper, we present a method for measuring sematic similarity of sentences based on constructed synonymy graph to avoid considering just exactly matching terms. When we construct graph which has terms as nodes and synonymy relation as edges, we use WordNet and part-of-speech to exploit synonyms. We assume synonym of a synonym is also similar",
      "title": "222972 Exploiting synonymy to measure semantic similarity of sentences",
      "url": ""
    },
    {
      "abstract": "The use of noun phrases as descriptors for text mining vectors has been proposed to overcome the poor semantic of the traditional bag-of-words (BOW). However, the solutions found in the literature are unsatisfactory, mainly due to the use of static definitions for noun phrases and the fact that noun phrases per se do not enable an adequate relevance representation since they are expressions that barely repeat. We present an approach to deal with these problems by (i) introducing a process that enables the definition of noun phrases interactively and (ii) considering similar noun phrases as a unique term. A case study compares both approaches, the one proposed in this paper and the other based on BOW. The main contribution of this paper is the improvement of the preprocessing phase of text mining, leading to better results in the overall process.  Springer International Publishing Switzerland 2015.",
      "title": "222976 An approach for text mining based on noun phrases",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944460007&partnerID=40&md5=8837a0986031ca7905f0486f516247b3"
    },
    {
      "abstract": "We present a work to evaluate the hypothesis that automatic evaluation metrics developed for Machine Translation (MT) systems have significant impact on predicting semantic similarity scores in Semantic Textual Similarity (STS) task for English, in light of their usage for paraphrase identification. We show that different metrics may have different behaviors and significance along the semantic scale [0-5] of the STS task. In addition, we compare several classification algorithms using a combination of different MT metrics to build an STS system",
      "title": "222979 Learning the impact of machine translation evaluation metrics for semantic textual similarity",
      "url": ""
    },
    {
      "abstract": "We present an automatic workflow that performs text segmentation and entity extraction from scientific literature to primarily address Task 2 of the Semantic Publishing Challenge 2015. The goal of Task 2 is to extract various information from full-text papers to represent the context in which a document is written, such as the affiliation of its authors and the corresponding funding bodies. Our proposed solution is composed of two subsystems: (i) A text mining pipeline, developed based on the GATE framework, which extracts structural and semantic entities, such as authors' information and references, and produces semantic (typed) annotations",
      "title": "222980 Automatic construction of a semantic knowledge base from CEUR workshop proceedings",
      "url": ""
    },
    {
      "abstract": "The overall objective of this paper is to acquire diagnostic knowledge about aircraft assembly in an automated manner, in order to minimize issues from occurring in new, similar situations. This research uses documents, prepared by experts, as a source of knowledge. The first step of the process of knowledge acquisition is segmentation of relevant sections of documents. From many methods that currently exist for such segmentation and classification, one method, namely - discourse analysis - is chosen for analyzing documents (with future knowledge considerations in mind). Using discourse analysis, entities from sentences are extracted to identify what is being discussed in a chunk of text. These entities are then compared to a domain knowledge base, such as an ontology, to see how (semantically) close the discussion is to the domain of interest. A method for such segmentation had been previously proposed, and is summarised here. This paper describes the efforts for partial implementation of this method. Computer-based tools are used for this implementation, such as Natural Language Toolkit, Boxer, and Ontologies. The Natural Language Toolkit is used for performing text processing, such as tokenization",
      "title": "222984 Implementation of an algorithm to classify discourse segments from documents for knowledge acquisition",
      "url": "Conference Paper"
    },
    {
      "abstract": "A lot of work has been done to give the individual words of a certain language adequate representations in vector space so that these representations capture semantic and syntactic properties of the language. In this paper, we compare different techniques to build vectorized space representations for Arabic, and test these models via intrinsic and extrinsic evaluations. Intrinsic evaluation assesses the quality of models using benchmark semantic and syntactic dataset, while extrinsic evaluation assesses the quality of models by their impact on two Natural Language Processing applications: Information retrieval and Short Answer Grading. Finally, we map the Arabic vector space to the English counterpart using Cosine error regression neural network and show that it outperforms standard mean square error regression neural networks in this task.  Springer International Publishing Switzerland 2015.",
      "title": "222985 Word representations in vector space and their applications for Arabic",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942636137&partnerID=40&md5=0429554548d8a6535b591aecea0ca3e6"
    },
    {
      "abstract": "Data warehouses and On-Line Analytical Processing tools, OLAP, together permit a multi-dimensional analysis of structured data information. However, as business systems are increasingly required to handle substantial quantities of unstructured textual information, the need arises for an effective and similar means of analysis. To manage unstructured text data stored in data warehouses, a new multi-dimensional analysis model is proposed that includes textual measures as well as a topic hierarchy. In this model, the textual measures that associate the topics with the text documents are generated by Probabilistic Latent Semantic Analysis, while the hierarchy is created automatically using a clustering algorithm. Documents are then able to be queried using OLAP tools. The model was evaluated from two viewpoints - query execution time and user satisfaction. Evaluation of execution time was carried out on scientific articles using two query types and user satisfaction (with query time and ease of use) using statistical frequency and multivariate analyses. Encouraging observations included that as the number of documents increases, query time increases as a lineal, rather than exponential tendency. In addition, the model gained an increasing acceptance with use, while the visualization of the model was also well received by users.  2015 Elsevier B.V. All rights reserved.",
      "title": "222987 Multidimensional analysis model for a document warehouse that includes textual measures",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925340790&partnerID=40&md5=1ba4de17cf7e82d94da522f662aef0ae"
    },
    {
      "abstract": "This paper presents word embedding-based approach to text classification. In this study, we introduce a new vector space model called Semantically-Augmented Statistical Vector Space Model (SASVSM) that is a statistical VSM with a semantic VSM for information access systems, especially for automatic text classification. In the SASVSM, we first implement a primary approach to concatenate continuousvalued semantic features with an existing statistical VSM. We, then, introduce the Centroid-Means-Embedding (CME) method that updates existing statistical feature vectors with semantic knowledge. Experimental results show that the proposed CME-based SAS-VSM approaches are promising over the different weighting approaches on the 20 Newsgroups and RCV1-v2/LYRL2004 datasets using Support Vector Machine (SVM) classifiers to enhance the classification tasks. Our approach outperformed other approaches in both micro-F1 and categorical performance.  Springer International Publishing Switzerland 2015.",
      "title": "222989 Centroid-means-embedding: An approach to infusing word embeddings into features for text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84945915717&partnerID=40&md5=e85e311c37af124591400a98953691f7"
    },
    {
      "abstract": "Introduction: The ambiguity of biomedical abbreviations is one of the challenges in biomedical text mining systems. In particular, the handling of term variants and abbreviations without nearby definitions is a critical issue. In this study, we adopt the concepts of topic of document and word link to disambiguate biomedical abbreviations. Methods: We newly suggest the link topic model inspired by the latent Dirichlet allocation model, in which each document is perceived as a random mixture of topics, where each topic is characterized by a distribution over words. Thus, the most probable expansions with respect to abbreviations of a given abstract are determined by word-topic, document-topic, and word-link distributions estimated from a document collection through the link topic model. The model allows two distinct modes of word generation to incorporate semantic dependencies among words, particularly long form words of abbreviations and their sentential co-occurring words",
      "title": "222990 Link-topic model for biomedical abbreviation disambiguation",
      "url": ""
    },
    {
      "abstract": "In this study, we have proposed a domain-independent unsupervised text segmentation method, which is applicable to even if unseen single document. This proposed method segments text documents by evaluating similarity between sentences. It is generally difficult to calculate semantic similarity between words that comprise sentences when the domain knowledge is insufficient. This problem influences segmentation accuracy. To address this problem, we use word 2 vec to calculate semantic similarity between words. Using word 2 vec, we embed semantic relationships between words in a vector space by training with large domain-independent corpora. Furthermore, we combine semantic and collocation similarities, i.e., The features between words within a document. The proposed method applies this combined similarity to affinity propagation clustering. Similarity between sentences is defined based on the earth movers distance between the frequencies of the obtained topical clusters. After calculating similarity between sentences, segmentation boundaries are automatically optimized using dynamic programming. The experimental results obtained using two datasets show that the proposed method clearly outperforms state-of-the-art domain-independent approaches and obtains equal performance with state-of-the-art domain-dependent approaches such as those that use topic modeling.  2014 IEEE.",
      "title": "222992 Domain-independent unsupervised text segmentation for data management",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84936858871&partnerID=40&md5=ea32b4ce07a30ac5811209f701ab630a"
    },
    {
      "abstract": "In this paper we present and evaluate a classification model to group product aspects from short user comments, found as pros and cons in consumer review websites. Because of the distinct vocabulary used by consumers to describe the same aspects of a product, it is necessary to group pros and cons to support consumers' decision making. For this purpose we propose a supervised classification model, consisting of an ensemble classifier that combines a main text classifier (e.g. Naive Bayes) and several string-based classifiers. Furthermore we make use of WordNet as a domain independent ontology to detect semantically related words. Experimental results using pros and cons from five heterogeneous product groups show, that the proposed method outperforms existing approaches to group pros and cons from short texts. We also found that the reusable short comments from our sample follow a power law distribution, that is usually present in social tagging systems.  Springer International Publishing Switzerland 2015.",
      "title": "222993 Grouping product aspects from short texts using multiple classifiers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84949986753&partnerID=40&md5=ea6b58ef5abf84815af2edcea3d348d5"
    },
    {
      "abstract": "Latent Dirichlet Allocation (LDA) probabilistic topic model is a very effective dimension-reduction tool which can automatically extract latent topics and dedicate to text representation in a lower-dimensional semantic topic space. But the original LDA and its most variants are unsupervised without reference to category label of the documents in the training corpus. And most of them view the terms in vocabulary as equally important, but the weight of each term is different, especially for a skewed corpus in which there are many more samples of some categories than others. As a result, we propose a supervised parameter estimation method based on category and document information which can estimate the parameters of LDA according to term weight. The comparative experiments show that the proposed method is superior for the skewed text classification, which can largely improve the recall and precision of the minority category.  Springer International Publishing Switzerland 2015.",
      "title": "222995 A supervised parameter estimation method of LDA",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84950299667&partnerID=40&md5=71b08025fe6aff6219715e3c20ee046f"
    },
    {
      "abstract": "MicroPomarketing makes microblog become an important advertisement platform. Some microblog posts usually describe some topics, and have a lot of comments. However, some comments related to the topics carry much market intelligence information, and the others may be useless. So, it is significant to extract these relevant comments. In this paper, we defined the Comment Relevant Score (CRS) for each comment, the CRS of a comment reected the correlation of the comment with the topic. Usually, if the contents of a comment had more semantic similarity with the topic, and the commenter had preference for the topic and higher credibility, this comment had higher CRS, so we proposed the Chinese Shorttext Semantic Similarity Algorithm (CSSSA) to calculate the semantic similarity of microblog comment and topic, and defined the User Topic Preference (UTP) and User Authority Value (UAV) for each commenter. Finally, we designed experiments on TENCENT which was one of the biggest Chinese microblog platforms in China, the results of experiments showed that the proposed way was effective. , 2015, Journal of Computational Information Systems. All right reserved.",
      "title": "223001 Extract chinese microblog comment related to topic",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84940756515&partnerID=40&md5=e4ee567bb719c11acb74259aa461dcdf"
    },
    {
      "abstract": "Mining opinions and analyzing sentiments from social network data help in various fields such as even prediction, analyzing overall mood of public on a particular social issue and so on. This paper involves analyzing the mood of the society on a particular news from Twitter posts. The key idea of the paper is to increase the accuracy of classification by including Natural Language Processing Techniques (NLP) especially semantics and Word Sense Disambiguation. The mined text information is subjected to Ensemble classification to analyze the sentiment. Ensemble classification involves combining the effect of various independent classifiers on a particular classification problem. Experiments conducted demonstrate that ensemble classifier outperforms traditional machine learning classifiers by 3-5%.  2015 IEEE.",
      "title": "223003 Performance analysis of Ensemble methods on Twitter sentiment analysis using NLP techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925666945&partnerID=40&md5=2c4e05128192c647be70816c11041c08"
    },
    {
      "abstract": "A number of metrics have been proposed in the literature to measure text re-use between pairs of sentences or short passages. These individual metrics fail to reliably detect paraphrasing or semantic equivalence between sentences, due to the subjectivity and complexity of the task, even for human beings. This paper analyzes a set of five simple but weak lexical metrics for measuring textual similarity and presents a novel paraphrase detector with improved accuracy based on abductive machine learning. The objective here is 2-fold. First, the performance of each individual metric is boosted through the abductive learning paradigm. Second, we investigate the use of decision-level and feature-level information fusion via abductive networks to obtain a more reliable composite metric for additional performance enhancement. Several experiments were conducted using two benchmark corpora and the optimal abductive models were compared with other approaches. Results demonstrate that applying abductive learning has significantly improved the results of individual metrics and further improvement was achieved through fusion. Moreover, building simple models of polynomial functional elements that identify and integrate the smallest subset of relevant metrics yielded better results than those obtained from the support vector machine classifiers utilizing the same datasets and considered metrics. The results were also comparable to the best result reported in the literature even with larger number of more powerful features and/or using more computationally intensive techniques.  2014 Elsevier B.V. All rights reserved.",
      "title": "223007 Boosting paraphrase detection through textual similarity metrics with abductive networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84912071686&partnerID=40&md5=995fd7aa8962de4f6076122a789dedae"
    },
    {
      "abstract": "Objective: Automatic detection of adverse drug reaction (ADR) mentions from text has recently received significant interest in pharmacovigilance research. Current research focuses on various sources of text-based information, including social media-where enormous amounts of user posted data is available, which have the potential for use in pharmacovigilance if collected and filtered accurately. The aims of this study are: (i) to explore natural language processing (NLP) approaches for generating useful features from text, and utilizing them in optimized machine learning algorithms for automatic classification of ADR assertive text segments",
      "title": "223009 Portable automatic text classification for adverse drug reaction detection via multi-corpus training",
      "url": "Article"
    },
    {
      "abstract": "Text analytics continue to proliferate as mass volumes of unstructured but highly useful data are generated at unbounded rates. Vector space models for text data-in which documents are represented by rows and words by columns-provide a translation of this unstructured data into a format that may be analyzed with statistical and machine learning techniques. This approach gives excellent results in revealing common themes, clustering documents, clustering words, and in translating unstructured text fields (such as an open-ended survey response) to usable input variables for predictive modeling. After discussing the collection and processing of text, we explore properties and transformations of the document-term matrix (DTM). We show how the singular value decomposition may be used to drastically reduce the size of the document space while also setting the stage for automatic topic extraction, courtesy of the varimax rotation. This latent semantic analysis (LSA) approach produces factors that are compatible with graphical exploration and advanced analytics. We also explore Latent Dirichlet Allocation for topic analysis. We reference published R packages to implement the methods and conclude with a summary of other popular open-source and commercial software packages.  2015 Wiley Periodicals, Inc.",
      "title": "223014 A practical guide to text mining with topic extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84938959565&partnerID=40&md5=c9d600cf77288ddea84e9fe4a55b8a0b"
    },
    {
      "abstract": "Farm management optimization models have been developed to support decision making for farming activities including planting, harvesting, handling, drying and storage. However, key decisions for maximizing farming output, timing of planting and other culture tasks, are affected by spatiotemporal changes of weather- related and other spatiotemporal, conditions. Using near real-time information describing actual farming decisions related to the timing of farming events across different regions can make the outputs of farm management optimization models more accurate. This study aims to develop analytical tools to extract near real-time information for identifying actual timing of crop planting and harvesting schedules from publicly available databases. Text mining is applied to identify planting and harvesting schedules for different crops in various regions. The spatiotemporal patterns of farming activities are also analyzed by comparing annual farming schedules across different regions. In order to support the reasoning behind scheduling of culture tasks, semantic networks representing relationships between concepts are constructed for gaining a qualitative understanding of factors contributing towards key agricultural decisions. Network analysis and visualization help characterize semantic networks assisting to highlight important information regarding farming activities. Preliminary results show the planting date of corn was severely delayed in 2013 but ahead of average in 2014 in Illinois. These patterns and schedules are in accordance with National Agricultural Statistics Service (NASS) database. Analyses of semantic networks also indicate rain in April was a main factor for planting delays in 2013. Given data describing actual spatiotemporal farming schedules, as well as information of weather-related conditions, we can gain insights determining changes in the timing of farming events that can potentially be improved inputs and thus constraints for farm management models.",
      "title": "223015 Improving farm management optimization: Application of text data analysis and semantic networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84951968718&partnerID=40&md5=439df224acb6a73bc03d772165ef0222"
    },
    {
      "abstract": "It is a challenge task to discover major topics from text, which provide a better understanding of the whole corpus and can be regarded as a text categorization problem. The goal of this paper is to apply latent semantic analysis (LSA) approach to extract common factors that representing concepts hidden in a large group of text. LSA involves three steps: the first step is to set up a term-document matrix",
      "title": "223016 An application of latent semantic analysis for text categorization",
      "url": "Article"
    },
    {
      "abstract": "Abstract Text categorization is an important and critical task in the current era of high volume data storage and handling. Feature selection is obviously one of the most important steps in text categorization. Traditional feature selection methods tend to only consider the correlation between features and categories, and have in the main ignored the semantic similarity between features and documents. To further explore this issue, this paper proposes a novel feature selection method that first selects features in documents with discriminative power and then computes the semantic similarity between features and documents. The proposed feature selection method is tested using a support vector machine (SVM) classifier upon two published datasets, viz. Reuters-21578 and 20-Newsgroups. The experimental results show that the proposed feature selection method generally outperforms the traditional feature selection methods for text categorization for both published datasets.  2015 Elsevier B.V.",
      "title": "223018 A discriminative and semantic feature selection method for text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84929519537&partnerID=40&md5=846550f485c37db2bedf9bfadee934fc"
    },
    {
      "abstract": "In the domain of IT benchmarking collected data are often stored in natural language text and therefore intrinsically unstructured. To ease data analysis and data evaluations across different types of IT benchmarking approaches a semantic representation of this information is crucial. Thus, the identification of conceptual (semantical) similarities is the first step in the development of an integrative data management in this domain. As an ontology is a specification of such a conceptualization an association of terms, relations between terms and related instances must be developed. Building on previous research we present an approach for an automated term extraction by the use of natural language processing (NLP) techniques. Terms are automatically extracted out of existing IT benchmarking documents leading to a domain specific dictionary. These extracted terms are representative for each document and describe the purpose and content of each file and server as a basis for the ontology development process in the domain of IT benchmarking.",
      "title": "223021 Natural language processing techniques for document classification in IT benchmarking: Automated identification of domain specific terms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84939516755&partnerID=40&md5=c3f688d361abb3ec7836bfe758b575ed"
    },
    {
      "abstract": "In this paper, we combine several tools used in text-mining in order to study both the lexicon and the semantic structure of a set of medieval texts. On the one hand, the study of occurrences (Principal Component Analysis, Topic Models, Self-Organizing Maps, Hierarchical Cluster Analysis) allows a wide scope of tools to extract and display information from big data. On the other hand, the study of co-occurrences (words belonging to a sentence, a paragraph) allows to keep track of the structure of each text, but is more tedious to handle and often leads to messy visualizations. Here we use the SOM algorithm to reduce the size of the data (clustering, removal of fickle information) while preserving the semantic structure",
      "title": "223022 Search for meaning through the study of co-occurrences in texts",
      "url": ""
    },
    {
      "abstract": "Motivated by a continually increasing demand for applications that depend on machine comprehension of text-based content, researchers in both academia and industry have developed innovative solutions for automated information extraction from text. In this article, the authors focus on a subset of such tools - semantic taggers - that not only extract and disambiguate entities mentioned in the text but also identify topics that unambiguously describe the texts main themes. The authors offer insight into the process of semantic tagging, the capabilities and specificities of todays semantic taggers, and also indicate some of the criteria to be considered when choosing a tagger.  1999-2012 IEEE.",
      "title": "223026 Automated semantic tagging of textual content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84913596758&partnerID=40&md5=fcae40a63c149284ce63355c81dffa15"
    },
    {
      "abstract": "Text documents are often high dimensional and sparse, it is a great challenge to discover the clusters among the unlabelled text data, because there are no obvious clusters by common distance measure. In this paper we present a latent subspace clustering method to find text clusters. In our algorithm, we use latent factors extracted by probability latent semantic analysis (PLSA) to generate latent clustering subspaces, and then use the distance between sample and each latent clustering subspace as similarity for text clustering. On some text document datasets our method shows effective implementation for text clustering.  2014 IEEE.",
      "title": "223028 Text document latent subspace clustering by PLSA factors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84912529485&partnerID=40&md5=65989aaacf96a9a95b3f00030c7873bd"
    },
    {
      "abstract": "Document understanding goal requires discovery of meaningful patterns in text, which in turn requires analyzing documents and extracting information useful for a purpose. The documents to be analyzed are expected to be represented in some way. It is true that different representations of the same piece of text might have different information extraction outcomes. Therefore, it is very important to propose a reliable text representation schema that may incorporate as many features as possible, and at the same time provides use of efficient document understanding algorithms. In this paper, we propose a graph-based representation of textual documents that employs different levels of formal representation of natural language. This schema takes into account different linguistic levels, such as lexical, morphological, syntactical and semantics. The representation schema proposed is accompanied with a proposal for a technique which allows to extract useful text patterns based on the idea of minimum paths in the graph. The efficiency of the representation schema proposed has been tested in one case of study (Question-Answering for machine Reading Evaluation - QA4MRE), and the results of experiments carried in it, are described. The results obtained show that the proposed graph-based multi-level linguistic representation schema may be successfully used in the broader framework of document understanding.  2013 Elsevier B.V. All rights reserved.",
      "title": "223035 A graph-based multi-level linguistic representation for document understanding",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84897632262&partnerID=40&md5=183a6a435332f136b00fd25e6c790dc6"
    },
    {
      "abstract": "Text classification which is an integral part of text mining has caught much attention in various industries and fields recently. The ability is in assigning text documents to one or more pre-defined categories based on content similarity. While most of application of text classification focuses on document level, question classification works at much granular level such as sentence and phrase. There have been numerous studies on question classification in accordance to Bloom taxonomy in assessments to measure cognitive level of learners in higher learning institutions. But it has not been effective yet to resolve overlapping issue of Bloom taxonomy verb keywords being assigned to more than one category of Bloom taxonomy. The presence of this poses a problem in respect of classifying a particular question into a right category of Bloom taxonomy. And feature extraction plays an important role in improving the accuracy of classifier such as Support Vector Machine in question classification. Much of the past related research work focus on feature extraction methods such as bag of word (BOW) and syntactic analysis to classify questions and to address the issue, an improvement in feature extraction is needed. In view of this, this study proposes an integrated approach in feature extraction involving semantic aspect in classifying questions in accordance to Bloom taxonomy. Support Vector Machine classifier is used as it is well known for its high accuracy in text classification. With all this in place, an improved accuracy in classifying questions in accordance to Bloom taxonomy can be expected.  2014 IEEE.",
      "title": "223041 A review in feature extraction approach in question classification using Support Vector Machine",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946692239&partnerID=40&md5=6d523085052232b51fe04d685f18afd0"
    },
    {
      "abstract": "According to relational database theory, solid foundation, and mature technology, various products have made it the first consideration for studying and storing massive data. But how to build an effective knowledge base which is based on the powerful storage and processing capability of relational database is not only one of the key issue for the research and application relating to knowledge engineering and intelligent system, but also is the technological bottleneck of how to turn knowledge management into application. In developing the query text data and content management application, there are some issues discovered with using the traditional method to retrieve content based on the contents of the title, author, and time to search. For instance, concerning a large amount of construction work on resources preparing, low query efficiency, and being unable to search out interrelated content and so on. In this paper, a relational model of semantic networks is given and a two-dimensional storage structure of relational data table based on the semantic network knowledge with practical examples is discussed. At the same time, this paper designs the stored procedure based on SQL language description",
      "title": "223043 A relational model based semantic network knowledge representation technology and its application",
      "url": ""
    },
    {
      "abstract": "Classification is a technique in data mining for categorizing objects. Text Classification is re-challenged for classifying very short documents or text as shown in social media collection. This paper proposes a method to improve the performance of classification on short documents. In this work, we expand words in every document before the documents are classified We use TFIDF model, Hidden Markov Model k-means clustering, and Latent Semantic Indexing (LSI) for expanding documents. The results show that extending document term by just 1 word will increase its accuracy, while extending by 2,4, and 8 words tend to give stable results.  2014 IEEE.",
      "title": "223044 Improving classification performance by extending documents terms",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946686368&partnerID=40&md5=ffaae9ab02e2112ebdd38cb2fe4eccb1"
    },
    {
      "abstract": "Text clustering is important in many application of information retrieval. This paper presents a study of clustering short texts in Bahasa Indonesia using semantic similarity approach where dictionary of synonyms and hyponyms is used to get information on word relatedness. We compare sentence similarity calculations based on lexical matching and word similarity. More than 250 sentences are involved. Our experiment shows that clustering using sentence similarity based on lexical matching performs better in terms of precision and F-measure than clustering using sentence similarity based on semantic approach.  2014 IEEE.",
      "title": "223045 Using dictionary in a knowledge based algorithm for clustering short texts in Bahasa Indonesia",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946691728&partnerID=40&md5=6bd722f8a3acca3d3890743659da6e36"
    },
    {
      "abstract": "Supervised learning methods rely on large sets of labeled training examples. However, large training sets are rare and making them is expensive. In this research, Latent Semantic Indexing Subspace Signature Model (LSISSM) is applied to labeling for active learning of unstructured text. Based on Singular Value Decomposition (SVD), LSISSM represents terms and documents as semantic signatures by the distribution of their local statistical contribution across the top-ranking LSI latent dimensions after dimension reduction. When utilized to an unlabeled text corpus, LSISSM finds the most important samples and terms according to their global statistical contribution ranking in the corresponding LSI subspaces without prior knowledge of labels or dependency to model-loss functions of the classifiers. These sample subsets also effectively maintain the sampling distribution of the whole corpus. Furthermore, tests demonstrate that the sample subsets with the optimized term subsets substantially improve the learning accuracy across three standard classifiers.  2014 IEEE.",
      "title": "223047 Active learning for text classification: Using the LSI subspace signature model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946687802&partnerID=40&md5=6500f1f915844206ab4c183ba622e6ea"
    },
    {
      "abstract": "In the last two decades, unstructured information has become a major challenge in information management. Such challenge is caused by the massive and increasing amount of information resulting from the conversion of almost all daily tasks into digital format. Tools and applications are necessary in organizing unstructured information, which can be found in structured data, such as in relational database management systems (RDBMS). RDBMS has robust and powerful structures for managing, organizing, and retrieving data. However, structured data still contains unstructured information. In this paper, the methods used for managing unstructured data in RDBMS are investigated. In addition, an incremental and dynamic repository for managing unstructured data in relational databases are introduced. The proposed technique organizes unstructured information through linkages among textual data based on semantics. Furthermore, it provides users with a good picture of the unstructured information. The proposed technique can rapidly and easily obtain useful data, and thus, it can be applied in numerous domains, particularly those who deal with textual data, such as news articles.  Springer Science+Business Media Singapore 2014.",
      "title": "223051 Model for automatic textual data clustering in relational databases schema",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893748493&partnerID=40&md5=3df82b53707b79e81a3b1f00b40e1e8d"
    },
    {
      "abstract": "Semantic relatedness measures are widely used in text mining and information retrieval applications. Considering these automated measures, in this research paper we attempt to improve Gloss Vector relatedness measure for more accurate estimation of relatedness between two given concepts. Generally, this measure, by constructing concepts definitions (Glosses) from a thesaurus, tries to find the angle between the concepts gloss vectors for the calculation of relatedness. Nonetheless, this definition construction task is challenging as thesauruses do not provide full coverage of expressive definitions for the particularly specialized concepts. By employing Wikipedia articles and other external resources, we aim at augmenting these concepts definitions. Applying both definition types to the biomedical domain, using MEDLINE as corpus, UMLS as the default thesaurus, and a reference standard of 68 concept pairs manually rated for relatedness, we show exploiting available resources on the Web would have positive impact on final measurement of semantic relatedness.  Springer Science+Business Media Singapore 2014.",
      "title": "223052 Augmenting concept definition in gloss vector semantic relatedness measure using wikipedia articles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893765191&partnerID=40&md5=1319663bebe7d15152258620c947862c"
    },
    {
      "abstract": "Online content has been recently subjected to ethical filtering in conformance to regional set of norms. Normative judgments are often difficult to determine the limits to dispersion and free-flow of information by state and society. Filtering unethical website content requires keyword analysis, a challenge in current manual IP filtering technologies. Due to sparse representation of objectionable words in online content, the seed sets need to focus on positive norms in order to segregate norm violations. In this research work, we exploit these polarities in norm semantics to solve the problem of ethical filtering. We scan text for belief-conforming ethical norms, and filter out words that fall below the norm threshold. Using the Noahide Norms mapped to seven colors and conforming scores, we visualize normative tag cloud. The objective of our positively oriented normative tag cloud is to conform and filter raw text according to the ethical dimensions. Our method has a high classification accuracy compared to manual gold standard. Hence, it can accurately summarize and illustrate visual comparison and contrast of cultural norms out of plain English text.  2014 IEEE.",
      "title": "223053 Visualizing norm polarities for ethical filtering: Using color-encoded normative tag cloud",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946689824&partnerID=40&md5=14c1a77aa516ed6adfcb42b651f42042"
    },
    {
      "abstract": "The efficient linking of endless bits of information has not led to truly mature communication, which connects the right to the right semantic place. This deficiency is further complicated by the continuous adaptability of information to each users changing context, such as environment and objective. This phenomenon has caused greater complication within Islam, because Islamic discourses are often thousands of years old, and so have produced abundant unconnected information that lacks ontological connection. Each day the strain to link the scriptural text with everyday life is getting more difficult. Moreover, the pluralistic feature of Shariah makes it more convoluted to the lay people in the West. The framework proposed in this paper provides an automated hierarchical compilation of laws and commandments, both historical and contemporary, to develop an integral knowledge base for efficient representation and easy reasoning. Such representation and reasoning will remove the controversial shroud of Shariah by bringing transparency into the existing Alternative Dispute Resolution (ADR). The novelty and efficiency of this knowledge base lies in a series of back end processing namely semantic text mining. This approach sort concepts, enriches the existing texts and creates greater relevancy of texts. The paper attempts to abridge three different areas. First it offers a brief overview of the Shariah life cycle. This foundation set the stage for the second area of identifying the macro scope of the integrated IT framework for Shariah. Finally, it dives deep into the logical architecture of the framework.  2014 IEEE.",
      "title": "223056 An ontology based framework for Shariah sustenance in the west",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946687715&partnerID=40&md5=d37299a00dbb278afbc21e68dab47d40"
    },
    {
      "abstract": "With the development of the Internet, people share their emotion statuses or attitudes on online social websites, leading to an explosive rise on the scale of data. Mining sentiment information behind these data helps people know about public opinions and social trends. In this paper a sentiment analysis algorithm adapting to Weibo (Microblog) data is proposed. Given that a Weibo post is usually short, LDA model is used to generate text features based on semantic information instead of text structure. To decide the sentiment polar and degree, SVR model is used here. Experiment shows the algorithm performs well on Weibo data.  2014 IEEE.",
      "title": "223057 Sentiment analysis on Weibo data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84946688649&partnerID=40&md5=01cb543b10bd9c10e670967fe08d1ec0"
    },
    {
      "abstract": "While many studies on big data analytics describe the data deluge and potential applications for such analytics, the required skill set for dealing with big data has not yet been studied empirically. The difference between big data (BD) and traditional business intelligence (BI) is also heavily discussed among practitioners and scholars. We conduct a latent semantic analysis (LSA) on job advertisements harvested from the online employment platform monster.com to extract information about the knowledge and skill requirements for BD and BI professionals. By analyzing and interpreting the statistical results of the LSA, we develop a competency taxonomy for big data and business intelligence. Our major findings are that (1) business knowledge is as important as technical skills for working successfully on BI and BD initiatives",
      "title": "223061 Comparing business intelligence and big data skills: A text mining study using job advertisements",
      "url": ""
    },
    {
      "abstract": "This paper presents an intelligent corporate governance analysis and rating system, called AAE System, capable of retrieving SEC required documents of public companies and performing analysis and rating in terms of recommended corporate governance practices. With Machine Learning, local knowledge bases, databases, and semantic networks, the AAE system is able to automatically evaluate the strengths, deficiencies, and risks of a companys corporate governance practices and board of directors based on the documents stored in the SEC EDGAR database[1]. The produced score reduces a complex corporate governance process and related policies into a single number which enables concerned government agencies, investors and legislators to assess the governance characteristics of individual companies.  2014 IEEE.",
      "title": "223063 Automated analysis and evaluation of SEC documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84936774174&partnerID=40&md5=a91be29329f4f466d1e324ef8052c3c5"
    },
    {
      "abstract": "The use of text similarity plays an important role in many applications in Computational Linguistics, such as Text Classification and Information Extraction and Retrieval. Besides, there are several tasks that require computing the similarity between two short segments of text. In this work, we propose a sentence similarity computing approach that takes account of the semantic and the syntactic information contained in the sentences. The proposed method can be applied in a variety of applications to mention, text knowledge representation and discovery. Experiments on a set of sentence pairs show that our approach presents a similarity measure that illustrates a considerable correlation to human judgment.  Springer International Publishing Switzerland 2014.",
      "title": "223067 An approach to semantic text similarity computing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84923782908&partnerID=40&md5=7c3c84921a3421a97150304ab6806a8e"
    },
    {
      "abstract": "Adoption of SPLE techniques is challenging and expensive. Hence, automation in the adoption process is desirable, especially with respect to variability management. Different methods have been suggested for (semi-)automatically generating feature models from requirements or textual descriptions of products. However, while there are different ways to represent the same SPL in feature models, addressing different stakeholders needs and preferences, existing methods usually follow fixed, predefined ways to generate feature models. As a result, the generated feature models may represent perspectives less relevant to the given tasks. In this paper we suggest an ontological approach that measures the semantic similarity, extracts variability, and automatically generates feature models that represent structural (objects-related) or functional (actions-related) perspectives. The stakeholders are able to control the perspective of the generated feature models, considering their needs and preferences for given tasks. Copyright 2014 ACM.",
      "title": "223072 Generating Feature Models from Requirements: Structural vs. Functional Perspectives",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907842523&partnerID=40&md5=f7fb46f393db8fe5daace22983f22dc0"
    },
    {
      "abstract": "Text annotation is the procedure of identifying the semantically dominant words of a text segment and attaching them with conceptual content information in their context. In this paper, we propose novel methods for automatic annotation of text fragments with entities of Wikipedia, the largest knowledge base online, a process commonly known as Wikification aiming at resolving the semantics of synonymous and polysemous terms accurately. The cornerstone of our contribution is a novel iterative Wikification approach, converging at optimal annotations while balancing high accuracy with performance. Our first two methods can be fine-tuned through a machine-learning technique over large homogenous data sets. Our experimental evaluation resulted in remarkable improvement over state-of-the-art Wikification approaches.  IFIP International Federation for Information Processing 2014.",
      "title": "223073 Novel techniques for text annotation with wikipedia entities",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921490250&partnerID=40&md5=b788a70bd08a8120c75ba017b06231ff"
    },
    {
      "abstract": "The bag-of-concepts model can represent semantics associated with natural language text much better than bags-of-words. In the bagof- words model, in fact, a concept such as cloud computing would be split into two separate words, disrupting the semantics of the input sentence. Working at concept-level is important for tasks such as opinion mining, especially in the case of microblogging analysis. In this work, we present Sentic API, a common-sense based application programming interface for concept-level sentiment analysis, which provides semantics and sentics (that is, denotative and connotative information) associated with 15,000 natural language concepts.",
      "title": "223074 Sentic API: A common-sense based API for concept-level sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922017789&partnerID=40&md5=2ccd5126d260589a407b67d848897852"
    },
    {
      "abstract": "Emotional tendency refers to peoples attitude towards people or things. It is a kind of subjective judgments and it can be divided into several parts, such as praise or criticize, positive or negative, good or bad. The judgment of emotional words emotional tendency and the problem of how to give emotional words a weight are the base of text tendency analysis. The study of semantic weight has been widely used in text tendency analysis, public sentiment, as well as text classification. This essay extracts words from glossary concept library (refer to glossary.dat) of HowNet and polish the library. In order to make the calculation study of the emotional words weight more accurately, the paper studies synonyms and antonyms, as well seed words selection manually. The experiment proved the method attains the results expected in sentiment judgment, weight calculation and application in text analysis.  2014 IEEE.",
      "title": "223077 Research on building Chinese semantic lexicon based on the concept definition of HowNet",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84941044151&partnerID=40&md5=ffebb9481711a7002794d22a1458c6cd"
    },
    {
      "abstract": "In this paper, we describe a method for producing Bangla word clusters based on semantic and contextual similarity. Word clustering is important for parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. Computerization of Bangla language processing has been started a long ago, but still it is in neophyte stage and suffers from resource scarcity. We propose anunsupervised machine learning technique to develop Bangla word clusters based on their semantic and contextual similarity using N-gram language model. According to N-gram model, a word can be predictedbased on its previous and next words sequence. N-gram model is applied successfully for word clustering in English and some other languages. As word clustering in Bangla is a new dimension in Bangla language processing research, so we think this process is good way to start and our assumption is true as our result is quite decent. We produced 456 clusters using a locally available large Bangla corpus. Subjective score derived from the clusters reveal strong similarity of the words in the same cluster.  2014 IEEE.",
      "title": "223080 Bangla word clustering based on N-gram language model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84910001802&partnerID=40&md5=10bb7ce30c3486425581f5d18bd80764"
    },
    {
      "abstract": "Sentences with different structures may convey the same meaning. Identification of sentences with paraphrases plays an important role in text related research and applications. This work focus on the statistical measures and semantic analysis of malayalam sentences to detect the paraphrases. The statistical similarity measures between sentences, based on symbolic characteristics and structural information, could measure the similarity between sentences without any prior knowledge but only on the statistical information of sentences. The semantic representation of Universal Networking Language(UNL), represents only the inherent meaning in a sentence without any syntactic details. Thus, comparing the UNL graphs of two sentences can give an insight into how semantically similar the two sentences are. Combination of statistical similarity and semantic similarity score results the overall similarity score. This is the first attempt towards paraphrases of malayalam sentences.  2013 IEEE.",
      "title": "223083 Paraphrase identification of Malayalam sentences - An experience",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84911164315&partnerID=40&md5=51932abf9de0ce263c9e23de9748b918"
    },
    {
      "abstract": "Relation extraction refers to a method of efficiently detecting and identifying predefined semantic relationships within a set of entities in text documents. Numerous relation extractionfc techniques have been developed thus far, owing to their innate importance in the domain of information extraction and text mining. The majority of the relation extraction methods proposed to date is based on a supervised learning method requiring the use of learning collections",
      "title": "223087 An intensive case study on kernel-based relation extraction",
      "url": ""
    },
    {
      "abstract": "Automatic methods of ontology alignment are essential for establishing interoperability across web services. These methods are needed to measure semantic similarity between two ontologies entities to discover reliable correspondences. While existing similarity measures suffer from some difficulties, semantic relatedness measures tend to yield better results",
      "title": "223088 Adapting Gloss Vector semantic relatedness measure for semantic similarity estimation: An evaluation in the biomedical domain",
      "url": ""
    },
    {
      "abstract": "Today, with the rapid development of the Internet, textual information is growing rapidly. So document retrieval which aims to find and organize relevant information in text collections is needed. With the availability of large scale inexpensive storage the amount of information stored by organizations will increase. Searching for information and deriving useful facts will become more cumbersome. How to extract a lot of information quickly and effectively has become the focus of current research and hot topics. The state of the art for traditional IR techniques is to find relevant documents depending on matching words in users query with individual words in text collections. The problem with Content-based retrieval systems is that documents relevant to a users query are not retrieved, and many unrelated or irrelevant materials are retrieved. In this paper information retrieval method is proposed based on LSI approach. Latent Semantic Indexing (LSI) model is a concept based retrieval method that exploits the idea of vector space model and singular value decomposition. The goal of this research is to evaluate the applicability of LSI technique for textual document search and retrieval.",
      "title": "223093 Framework for document retrieval using latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928137397&partnerID=40&md5=e4c55b7c363a05c2e39cba1409cc3ac7"
    },
    {
      "abstract": "In this study we propose an improved learning algorithm based on resource allocating network (RAN) for text categorization. RAN is a promising neural network of single hidden layer structure based on radial basis function. We firstly use the means clustering-based method to determine the initial centers in the hidden layer. Such method can effectively overcome the limitation of local-optimal of clustering algorithms. Subsequently, in order to improve the novelty criteria of RAN, we propose a root mean square (RMS) sliding window method which can reduce the underlying influence of undesirable noise data. Through the further research on the learning process of RAN, we divide the learning process of RAN into a preliminary study phase and a subsequent study phase. The former phase initializes the preliminary structure of RAN and decreases the complexity of network, while the latter phase refines its learning ability and improves the classification accuracy. Such a compact network structure decreases the computational complexity and maintains the higher convergence rate. Moreover, a latent semantic feature selection method is utilized to organize documents. This method reduces the input scale of network, and reveals the latent semantics between features. Extensive experiments are conducted on two benchmark datasets, and the results demonstrate the superiority of our algorithm in comparison with state of the art text categorization algorithms.  2014 Elsevier B.V.",
      "title": "223097 Taking advantage of improved resource allocating network and latent semantic feature selection approach for automated text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84898995782&partnerID=40&md5=a92eb1193e30f13d90738152c74151fe"
    },
    {
      "abstract": "The main issue that currently faces research in the information society is the flood of information",
      "title": "223104 Challenges in information retrieval from unstructured arabic data",
      "url": ""
    },
    {
      "abstract": "NER in microposts is a key and challenging task of mining semantics from social media. Our evaluation of a number of popular NE recognizers over a micropost dataset has shown a significant drop-off in results quality. Current state-of-theart NER methods perform much better on formal text than on microposts. However, the experiment provided us with an interesting observation - although individual NER tools did not perform very well on micropost data, we have received recall over 90% when we merged all the results of the examined tools. This means that if we would be able to combine different NE recognizers in a meaningful way, we might be able to get NER in microposts of an acceptable quality. In this paper, we propose a method for NER in microposts, which is designed to combine annotations yielded by existing NER tools in order to produce more precise results than input tools alone. We combine NE recognizers utilizing ML techniques, namely decision tree and random forest using the C4.5 algorithm. The main advantage of the proposed method lies in the possibility of combining arbitrary NER methods and in its application on short, informal texts. The evaluation on a standard dataset shows that the proposed approach outperforms underlying NER methods as well as a baseline recognizer, which is a simple combination of the best underlying recognizers for each target NE class. To the best of our knowledge, up-to-date, the proposed approach achieves the highest F1 score on the #MSM2013 dataset.",
      "title": "223106 Combining named entity recognition methods for concept extraction in microposts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922041977&partnerID=40&md5=ca162e68840e0fe3139cd316c48b5c5d"
    },
    {
      "abstract": "The paper presents the intermediate results of the research, the final goal of which is to develop the universal algorithm for process diagrams automatic visualization by text description of these processes. The purpose of this study is to check the use of verbs as markers for the semantic labeling of long fragments in scientific texts.",
      "title": "223109 Automatic selection of verbs-markers for segmentation task of process descriptions in natural language texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84925235697&partnerID=40&md5=0d42f39e2cde3022d586e1966c0287a7"
    },
    {
      "abstract": "We propose a novel approach to recognise textual entailment (RTE) following a two-stage architecture - alignment and decision - where both stages are based on semantic representations. In the alignment stage the entailment candidate pairs are represented and aligned using predicate-argument structures. In the decision stage, a Markov Logic Network (MLN) is learnt using rich relational information from the alignment stage to predict an entailment decision. We evaluate this approach using the RTE Challenge datasets. It achieves the best results for the RTE-3 dataset and shows comparable performance against the state of the art approaches for other datasets.  2014 Springer-Verlag Berlin Heidelberg.",
      "title": "223110 Statistical relational learning to recognise textual entailment",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899960363&partnerID=40&md5=f84182f36acc94224a580191b0017ce3"
    },
    {
      "abstract": "Text is a very important type of data within the biomedical domain. For example, patient records contain large amounts of text which has been entered in a non-standardized format, consequently posing a lot of challenges to processing of such data. For the clinical doctor the written text in the medical findings is still the basis for decision making-neither images nor multimedia data. However, the steadily increasing volumes of unstructured information need machine learning approaches for data mining, i.e. text mining. This paper provides a short, concise overview of some selected text mining methods, focusing on statistical methods, i.e. Latent Semantic Analysis, Probabilistic Latent Semantic Analysis, Latent Dirichlet Allocation, Hierarchical Latent Dirichlet Allocation, Principal Component Analysis, and Support Vector Machines, along with some examples from the biomedical domain. Finally, we provide some open problems and future challenges, particularly from the clinical domain, that we expect to stimulate future research.  Springer-Verlag Berlin Heidelberg 2014.",
      "title": "223114 Biomedical text mining: State-of-the-art, open problems and future challenges",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905218519&partnerID=40&md5=77ec4b7a05fe80a160549e17d38c962a"
    },
    {
      "abstract": "A weakly-supervised method acquires fine-grained class labels that do not occur verbatim in the input data or under-lying text collection. The method generates more specific class labels (gold mining companies listed on the toronto stock exchange) that capture the semantics of the under-lying classes, out of pairs of input class labels (companies listed on the toronto stock exchange, gold mining companies) available for an instance (Golden Star Resources). When applied to Wikipedia articles and their categories, the method generates new categories for existing articles, and expands existing categories with additional articles. Copyright is held by the International World Wide Web Conference Committee (IW3C2).",
      "title": "223128 Acquisition of open-domain classes via intersective semantics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84909619994&partnerID=40&md5=c6adca7fe215bfd37cd83d7d7fd55e9e"
    },
    {
      "abstract": "An original method of developing the algorithms of semantic-syntactic analysis of texts in natural language (NL) is set forth. It expands the method proposed by V.A. Fomichov in the monograph published by Springer in 2010. For building semantic representations, the class of SK-languages is used. The input texts may be at least from broad and practically interesting sublanguages of English, French, German, and Russian languages. The final part of the paper describes an application of the elaborated method to the design of a NL-interface to an action-based software system. The developed NL-interface NLC-1 (Natural Language Commander -Version 1) is implemented with the help of the functional programming language Haskell.  Springer International Publishing Switzerland 2014.",
      "title": "223133 A new method of extracting structured meanings from natural language texts and its application",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84903639415&partnerID=40&md5=e2b63c4c8d6bd815eecaa65bf9888bfc"
    },
    {
      "abstract": "The introduction of semantics in Sentiment Analysis research has proved to bring several benefits for what performances are concerned and has allowed to identify new challenging tasks to be accomplished. Semantics helps structuring the plain natural language text with formal representation. The current system we are developing performs sentiment analysis by hybridizing natural language processing techniques with Semantic Web technologies. Our system, called Sentilo, is able to recognize the holder of an opinion, to detect the topics and sub-topics in its scope, and to measure the sentiment expressed by them. This information is formally represented by means of RDF graphs according to an OWL opinion ontology, while holders and topics identity is resolved on the Linked Open Data cloud.",
      "title": "223136 Semantic web-based sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84926391179&partnerID=40&md5=2348848a14158c7f8c3ec2bdd1146d5a"
    },
    {
      "abstract": "The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques.  2014 Association for Computational Linguistics.",
      "title": "223143 Improving vector space word representations using multilingual correlation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905694258&partnerID=40&md5=f67c07beeb97b528f56f110fd6d6c033"
    },
    {
      "abstract": "The word-to-vector (W2V) technique represents words as low-dimensional continuous vectors in such a way that semantic related words are close to each other. This produces a semantic space where a word or a word collection (e.g., a document) can be well represented, and thus lends itself to a multitude of applications including document classification. Our previous study demonstrated that representations derived from word vectors are highly promising in document classification and can deliver better performance than the conventional LDA model. This paper extends the previous research and proposes to model distributions of word vectors in documents or document classes. This extends the naive approach to deriving document representations by average pooling and explores the possibility of modeling documents in the semantic space. Experiments on the sohu text database confirmed that the new approach may produce better performance on document classification.  2014 Asia-Pacific Signal and Information Processing Ass.",
      "title": "223147 Document classification with distributions of word vectors",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928922922&partnerID=40&md5=95f86642f6dfa682f586336c3b874f93"
    },
    {
      "abstract": "In this paper we propose a new approach based on text mining technique to predict students performance using LSA (latent semantic analysis) and K-means clustering method. The present study uses free style comments written by students after each lesson. Since the potentials of these comments can reflect students learning attitudes, understanding and difficulties to the lessons, they enable teachers to grasp the tendencies of students learning activities.To improve this basic approach, overlap method and similarity measuring technique are proposed. We conducted experiments to validate our proposed methods. The experimental results illustrated that prediction accuracy was 73.6% after applying the overlap method and that was 78.5% by adding the similarity measuring.  2014 Springer International Publishing.",
      "title": "223149 Prediction of students grades based on free-style comments data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905851384&partnerID=40&md5=55742f3ad7c5d06a50ecd17f91256f19"
    },
    {
      "abstract": "The bag of words (BOW) representation of documents is very common in text classification systems. However, the BOW approach ignores the position of the words in the document and more importantly, the semantic relations between the words. In this study, we present a simple semantic kernel for Support Vector Machines (SVM) algorithm. This kernel uses higher-order relations between terms in order to incorporate semantic information into the SVM. This is an easy to implement algorithm which forms a basis for future improvements. We perform a serious of experiments on different well known textual datasets. Experiment results show that classification performance improves over the traditional kernels used in SVM such as linear kernel which is commonly used in text classification.  2014 IEEE.",
      "title": "223151 A simple semantic kernel approach for SVM using higher-order paths",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906709302&partnerID=40&md5=187cb3624a1085853b19d994fe98f436"
    },
    {
      "abstract": "This article discusses a possible approach combining existing technologies for Natural Language Processing (NLP), Knowledge Representation and Reasoning (KRR) and Data Visualization in a coherent Decision Support System (DSS) aimed to formulate and verify specific interpretative hypotheses about a certain text. The approach chosen for developing this system implies (i.) the customization and integration of existing tools for automatic text annotation (first of all, linguistic, lexicographic and semantic) and (ii.) the construction of a user-friendly and highly expressive GUI. The interface is conceived to allow users to (i.) upload the text/s they are studying, (ii) run the desired annotation tools, and (iii.) visually interact with the resulting multi-layered network to explore the network, proof-read the results of the automatic annotations and add missing elements and/or relations between elements.  2014 IEEE.",
      "title": "223152 Graphic visualization in literary text interpretation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84912074766&partnerID=40&md5=5aea7ca375ca0fc5c7ef288d48697f20"
    },
    {
      "abstract": "We present OCMiner, a high-performance text processing system for large document collections of scientific publications. Several linguistic options allow adjusting the quality of annotation results which can be specialized and fine-tuned for the recognition of Life Science terms. Recognized terms are mapped to semantic concepts which are ontologically located within their respective domain taxonomies. Relying on a correct identification and semantic interpretation of mentions of domain concepts, relations between entities are extracted. The annotated text, as well as extracted knowledge triples, can be visualized on a web-based front-end at http://www.ocminer.com/, permitting an explorative information retrieval.",
      "title": "223153 OCMiner: Text processing, annotation and relation extraction for the life sciences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84920019928&partnerID=40&md5=672dc55139f3652bd2d2ae1cf89afe61"
    },
    {
      "abstract": "Semantic networks inspired by semantic information processing by the brain frequently do not improve the results of text classification. This counterintuitive fact is explained here by the multiple inheritance problem, which corrupts real-world knowledge representation attempts. After a review of early work on the use of semantic networks in text classification, our own heuristic solution to the problem is presented. Significance testing is used to contrast results obtained with pruned and entire semantic networks applied to medical text classification problems. The algorithm has been motivated by the process of spreading neural activation in the brain. The semantic network activation is propagated throughout the network until no more changes to the text representation are detected. Solving the multiple inheritance problem for the purpose of text classification is similar to embedding inhibition in the spreading activation process - a crucial mechanism for a healthy brain.  2014 Springer International Publishing.",
      "title": "223158 Multiple inheritance problem in semantic spreading activation networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905255709&partnerID=40&md5=0158e4b3278412c4f50103ed63da58eb"
    },
    {
      "abstract": "Nowadays, software developers often discuss the usage of various APIs in online forums. Automatically assigning pre-defined semantic categorizes to API discussions in these forums could help manage the data in online forums, and assist developers to search for useful information. We refer to this process as content categorization of API discussions. To solve this problem, Hou and Mo proposed the usage of naive Bayes multinomial, which is an effective classification algorithm. In this paper, we propose a Cache-bAsed compoSitE algorithm, short formed as CASE, to automatically categorize API discussions. Considering that the content of an API discussion contains both textual description and source code, CASE has 3 components that analyze an API discussion in 3 different ways: Text, code, and original. In the text component, CASE only considers the textual description",
      "title": "223161 Towards more accurate content categorization of API discussions",
      "url": "Conference Paper"
    },
    {
      "abstract": "The World Wide Web and related technologies are playing an increasing role in the field of Integrated Environmental Modelling (IEM). Model integration software frameworks are more and more becoming web-enabled. The technologies and standards of the Web are used to access and run simulation models remotely (known as the Web of models) and are considered for enabling interoperability across model integration frameworks. Furthermore there is a growing number of local and global initiatives to provide open access to environmental data (Web of data) that can potentially be used as input for the scientific models. The availability of descriptive information of sufficient quality about the scientific models and datasets that is semantically aligned is a necessity for efforts like the Web of models and the Web of data, and for their integration. The EU FP7 project LIAISE - Linking Impact Assessment Instruments to Sustainability Expertise - is collecting and providing meta-descriptions on good practices, experts, scientific models and guidelines in its shared Impact Assessment toolbox called the LIAISE KIT. An international community of experts has been formed that provides and maintains these meta-descriptions. This knowledge base however is currently primarily targeted at a human audience and only accessible from a website. To ensure future usage initial steps are being taken in two small sub-projects of LIAISE to publish the content as part of the semantic web, and to examine the use of ontology and Natural Language Processing for metadata extraction. This has as research goal the (semi-)automated creation of relevant metadata from existing unstructured text sources such as scientific publications and websites. Preliminary results of these two sub-projects are described in this paper. Although showing promise some further research is still required, including a proof of concept implementation.",
      "title": "223162 Metadata extraction using semantic and Natural Language Processing techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84911869703&partnerID=40&md5=f07d7d0c64f290e15fe4cd278be5296d"
    },
    {
      "abstract": "The task of textual entailment recognition is to determine whether a text entails a hypothesis. This paper proposes a hybrid technique to identify the entailment relation between texts and hypothesis. This technique includes an approach based on lexical similarities and an approach based on the classifier of support vector machine. The approach based on lexical similarities is to use the similarities between a set of words within a text and a set of words within a hypothesis. The approach based on the classifier means to treat this task as a classification problem. We propose two kinds of classification features which include features based on semantic roles, and ones based on dependency relations and WordNet. We use our hybrid technique to integrate the two sets of experimental results by the lexical similarities-based approach and the SVM classifier-based approach. The experimental results demonstrate that our technique is effective to solve the problem of textual entailment recognition.  2014. The authors - Published by Atlantis Press.",
      "title": "223163 A hybrid approach to textual entailment recognition",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928019200&partnerID=40&md5=846bb67bebd6e1061cde6182fe6f6e8a"
    },
    {
      "abstract": "Twitter messages, also known as tweets, are increasingly used by marketers worldwide to determine consumer sentiments towards brands, products or events. Currently, most existing approaches used for social networks sentiment analysis only extract simple feedbacks in terms of positive and negative perception. In this paper, TweetOntoSense is proposed - a semantic based approach that uses ontologies in order to infer the actual users' emotions. The extracted sentiments are described using a WordNet enriched emotional categories ontology. Thus, feelings such as happiness, affection, surprise, anger, sadness, etc. are put forth. Moreover, compared to existing approaches, TweetOntoSense also takes into consideration the fact that a single tweet message might express several, rather than a single emotion. A case study on Twitter is performed, also showing this approach's practical applicability.  Springer International Publishing Switzerland 2014.",
      "title": "223164 Understanding online social networks' users - twitter approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921463681&partnerID=40&md5=e2bc7302a0af80432558e64b6e4c1ad2"
    },
    {
      "abstract": "This paper presents a method to apply opinion mining on unstructured text for polarity extraction and classification at sentence level within a document. The generation of massive unstructured information about individuals makes the task of progress tracking and monitoring almost impracticable which results in the quest to find some way for automated text analysis and tagging. The proposed solution in this work is the development of a System (Sentiment Miner). It will provide features to process and classify text files (reviews and appraisals) for opinion mining at sentence level using Natural language Processing techniques and Opinion Mining algorithms. The prototype of a final product",
      "title": "223169 Sentiment miner: A prototype for sentiment analysis of unstructured data and text",
      "url": ""
    },
    {
      "abstract": "Topic modeling is a powerful tool to uncover hidden thematic structures of documents. Many conventional topic models represent documents as a bag-of-words, where the important linguistic structures of documents are neglected. In this paper, we propose a novel topic model that enriches text documents with collapsed typed dependency relations to effectively acquire syntactic and semantic dependencies between consecutive and nonconsecutive words of text documents. In addition, we propose to enforce coherent topic assignments for conceptually similar words by generalizing words with their synonyms. Our experimental studies show that the proposed model and strategy outperform the original LDA model and the Bigram Topic Model in terms of perplexity",
      "title": "223170 Topic modeling using collapsed typed dependency relations",
      "url": ""
    },
    {
      "abstract": "With the boom of web and social network, the amount of generated text data has increased enormously. On one hand, although text clustering methods are applicable to classify text data and facilitate data mining work such as information retrieval and recommendation, inadequate aspects are still evident. Especially, most existing text clustering methods provide either a hard partitioned or a hierarchical result, which cannot describe the data from various perspectives. On the other hand, multiple clustering approaches, which are proposed to classify data with various perspectives, meet several challenges such as high time complexity and incomprehensible results while applied to text documents. In this paper, we propose a frequent term-based multiple clustering approach for text documents. Our approach classifies text documents with various perspectives and provides a semantic explanation for each cluster. Through a series of experiments, we prove that our method is more scalable and provides more comprehensible results than traditional multiple clustering methods such as OSCLU and ASCLU while applied to text documents. In addition, we also found that our approach achieves a better clustering quality than existing text clustering approaches like FTC.  2014 Springer International Publishing Switzerland.",
      "title": "223171 A frequent term-based multiple clustering approach for text documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906492531&partnerID=40&md5=4c881824cf0cdd079cd6d4b37809b485"
    },
    {
      "abstract": "This paper shows a fuzzy ontology based approach to automatically build user profiles from a collection of user interest documents. The ontological representation of the user profile enhances the performance in tasks such as filtering, categorization and information retrieval. The proposed technique takes advantage of relevance measures to generate semantic representations of user context. The proposed work also presents a strategy for automatic generation of fuzzy ontologies to support user profile modeling. The experiments performed confirm that the automatically obtained fuzzy ontologies are good representation of the users preferences. In order to test the applicability of the obtained ontologies, a text categorization experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research.  2014 Springer International Publishing.",
      "title": "223173 Fuzzy ontology-based approach for automatic construction of user profiles",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84903761148&partnerID=40&md5=e9fb018de5320c49751be73f8c473a28"
    },
    {
      "abstract": "Sarcasm is a figure of speech used to express a strong opinion in a mild manner. It is often used to convey the opposite sense of what is expressed. Automatic recognition of sarcasm is a complex task. Sarcasm detection is of importance in effective opinion mining. Most sarcasm detectors use lexical and pragmatic features for this purpose. We incorporate statistical as well as linguistic features. Our approach considers the semantic and flipping of sentiment as main features. We use machine learning techniques for classification of sarcastic statements. We conduct experiments on different types of data sets, and compare our results with an existing approach in the literature. We also present human evaluation results. We propose to augment the present encouraging results by a new approach of integrating linguistic and cognitive aspects of text processing. Copyright  2014 SCITEPRESS - Science and Technology Publications All rights reserved.",
      "title": "223175 Sarcasm detection using sentiment and semantic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84909969590&partnerID=40&md5=f365fc79557c74ddb419da1e5b464890"
    },
    {
      "abstract": "Modern Code Review (MCR) is an informal practice whereby reviewers virtually discuss proposed changes by adding comments through a code review tool or mailing list. It has received much research attention due to its perceived cost-effectiveness and popularity with industrial and OSS projects. Recent studies indicate there is a positive relationship between the number of review comments and code quality. However, little research exists investigating how such discussion impacts software quality. The concern is that the informality of MCR encourages a focus on trivial, tangential, or unrelated issues. Indeed, we have observed that such comments are quite frequent and may even constitute the majority. We conjecture that an effective MCR actually depends on having a substantive quantity of comments that directly impact a proposed change (or are useful). To investigate this, a necessary first step requires distinguishing review comments that are useful to a proposed change from those that are not. For a large OSS projects such as our Qt case study, manual assessment of the over 72,000 comments is a daunting task. We propose to utilize semantic similarity as a practical, cost efficient, and empirically assurable approach for assisting with the manual usefulness assessment of MCR comments. Our case study results indicate that our approach can classify comments with an average F-measure score of 0.73 and reduce comment usefulness assessment effort by about 77%.  2014 IEEE.",
      "title": "223188 Assessing MCR discussion usefulness using semantic similarity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84920513703&partnerID=40&md5=fead3e001304d9167a289b3a93e26f24"
    },
    {
      "abstract": "Concept-level text analysis is superior to word-level analysis as it preserves the semantics associated with multi-word expressions. It offers a better understanding of text and helps to significantly increase the accuracy of many text mining tasks. Concept extraction from text is a key step in concept-level text analysis. In this paper, we propose a ConceptNet-based semantic parser that deconstructs natural language text into concepts based on the dependency relation between clauses. Our approach is domain-independent and is able to extract concepts from heterogeneous text. Through this parsing technique, 92.21% accuracy was obtained on a dataset of 3,204 concepts. We also show experimental results on three different text analysis tasks, on which the proposed framework outperformed state-of-the-art parsing techniques.  2014 Springer-Verlag Berlin Heidelberg.",
      "title": "223189 Dependency-based semantic parsing for concept-level text analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899925088&partnerID=40&md5=973d246c7ac06ea0b780e15a00bcbf50"
    },
    {
      "abstract": "Knowing about peoples opinions and viewpoints plays an essential role in decision-making processes involving regular customers to executive managers. Therefore, in the past decade, with the advent of Web 2.0, a new orientation of natural language processing science called opinion mining has been emerged. The main problem of exploring feature-level opinions is the complexity of feature extraction and its relations with the words containing the sentiment within unstructured texts, which reduces the accuracy of opinion mining. The purpose of the structured opinion summarization is to demonstrate the mentioned features in the reviews and express the sentiment value of users for each feature, quantitatively. The main idea of this research is to consider the semantic (knowledge) to analyze the sentiment in the review by developing the opinion ontology. Therefore, a semantic framework as an integrated method is proposed in all stages of feature-based opinion summarization.  2014 Springer International Publishing.",
      "title": "223190 Designing an integrated semantic framework for structured opinion summarization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902578901&partnerID=40&md5=987da4ee4e9c5afe888fe55abf79a217"
    },
    {
      "abstract": "We first analyzes the deviation when current similarity calculation methods for texts are applied to short texts, and proposes a similarity calculation method for short texts based on language network and word semantic information. Firstly, models the short texts as language network according to the complex-network characteristic of human beings language. Then analyzes the comprehensive eigenvalue of the words in the language network and the word similarity between different texts to obtain the word semantic. Calculate the similarity between short texts combining language network and word semantic. Finally the effectiveness of proposed algorithm is verified through clustering algorithm experiments.  Springer-Verlag Berlin Heidelberg 2014.",
      "title": "223191 Semantic Similarity Calculation of Short Texts Based on Language Network and Word Semantic Information",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905028222&partnerID=40&md5=5b1168fec5f957c3d25d0d6bfee207d0"
    },
    {
      "abstract": "This study considers issues in semantic representation of written texts, especially in the context of entropy-based approach to natural language processing in biomedical applications. These issues lie at the intersection of Web search methodologies, ontology studies, lexicon studies, and natural language processing. The presented in the article entropy-based methodology is aimed at enhancing search techniques and diagnostics by capturing semantic properties of written texts. The range of possible applications ranges from forensic linguistics to psychological diagnostics and evaluation. The presented case study assumes that for texts written under atypical mental conditions, the level of relative text entropy may fall below a certain threshold and the distribution of entropy across the text may show unusual patterns, thus contributing to the semantic assessment of a subjects mental state. Further processing methods potentially contributing to psychological evaluation diagnosis and ontology-based search are discussed.  2014 IEEE.",
      "title": "223193 Semantic search and NLP-based diagnostics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907408662&partnerID=40&md5=dd548295d59b3bd4e6ba7621b4ac9537"
    },
    {
      "abstract": "Short text messages a.k.a Microposts (e.g. Tweets) have proven to be an effective channel for revealing information about trends and events, ranging from those related to Disaster (e.g. hurricane Sandy) to those related to Violence (e.g. Egyptian revolution). Being informed about such events as they occur could be extremely important to authorities and emergency professionals by allowing such parties to immediately respond. In this work we study the problem of topic classification (TC) of Microposts, which aims to automatically classify short messages based on the subject(s) discussed in them. The accurate TC of Microposts however is a challenging task since the limited number of tokens in a post often implies a lack of sufficient contextual information. In order to provide contextual information to Microposts, we present and evaluate several graph structures surrounding concepts present in linked knowledge sources (KSs). Traditional TC techniques enrich the content of Microposts with features extracted only from the Microposts content. In contrast our approach relies on the generation of different weighted semantic meta-graphs extracted from linked KSs. We introduce a new semantic graph, called category meta-graph. This novel meta-graph provides a more fine grained categorisation of concepts providing a set of novel semantic features. Our findings show that such category meta-graph features effectively improve the performance of a topic classifier of Microposts. Furthermore our goal is also to understand which semantic feature contributes to the performance of a topic classifier. For this reason we propose an approach for automatic estimation of accuracy loss of a topic classifier on new, unseen Microposts. We introduce and evaluate novel topic similarity measures, which capture the similarity between the KS documents and Microposts at a conceptual level, considering the enriched representation of these documents. Extensive evaluation in the context of Emergency Response (ER) and Violence Detection (VD) revealed that our approach outperforms previous approaches using single KS without linked data and Twitter data only up to 31.4% in terms of F1 measure. Our main findings indicate that the new category graph contains useful information for TC and achieves comparable results to previously used semantic graphs. Furthermore our results also indicate that the accuracy of a topic classifier can be accurately predicted using the enhanced text representation, outperforming previous approaches considering content-based similarity measures.  2014 Elsevier B.V. All rights reserved.",
      "title": "223197 Linked knowledge sources for topic classification of microposts: A semantic graph-based approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901791291&partnerID=40&md5=8a8acaefca970a9e0c91b708b7b2634c"
    },
    {
      "abstract": "Natural language processing systems, even when given proper syntactic and semantic interpretations, still lack the common sense inference capabilities required for genuinely understanding a sentence. Recently, there have been several studies developing a semantic classification of verbs and their sentential complements, aiming at determining which inferences people draw from them. Such constructions may give rise to implied commitments that the author normally cannot disavow without being incoherent or without contradicting herself, as described for instance in the work of Kartunnen. In this paper, we model such knowledge at the semantic level by attempting to associate such inferences with specific word senses, drawing on WordNet and VerbNet. This allows us to investigate to what extent the inferences apply to semantically equivalent words within and across languages.  2014 Springer-Verlag Berlin Heidelberg.",
      "title": "223199 Sense-specific implicative commitments",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899913122&partnerID=40&md5=bbe4b262a2bbb88e9cddd2aaa218712a"
    },
    {
      "abstract": "Indexing of textual cases is commonly affected by the problem of variation in vocabulary. Semantic indexing is commonly used to address this problem by discovering semantic or conceptual relatedness between individual terms and using this to improve textual case representation. However, representations produced using this approach are not optimal for supervised tasks because standard semantic indexing approaches do not take into account class membership of these textual cases. Supervised semantic indexing approaches e.g. sprinkled Latent Semantic Indexing (SPLSI) and supervised Latent Dirichlet Allocation (SLDA) have been proposed for addressing this limitation. However, both SPLSI and SLDA are computationally expensive and require parameter tuning. In this work, we present an approach called Supervised Sub-Spacing (S3) for supervised semantic indexing of documents. S3 works by creating a separate sub-space for each class within which class-specific term relations and term weights are extracted. The power of S3 lies in its ability to modify document representations such that documents that belong to the same class are made more similar to one another while, at the same time, reducing their similarity to documents of other classes. In addition, S3 is flexible enough to work with a variety of semantic relatedness metrics and yet, powerful enough that it leads to significant improvements in text classification accuracy. We evaluate our approach on a number of supervised datasets and results show classification performance on S3-based representations to significantly outperform both a supervised version of Latent Semantic Indexing (LSI) called Sprinkled LSI, and supervised LDA.  Springer International Publishing Switzerland 2014.",
      "title": "223201 Supervised semantic indexing using sub-spacing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921635933&partnerID=40&md5=e541bf634927fd771614c2883e14176d"
    },
    {
      "abstract": "Semantic entailment is a fundamental problem in natural language understanding field which has a large number of applications. Knowledge acquisition and knowledge representation are crucial parts in semantic inference strategies. This paper presents a principled approach to semantic entailment problem that builds on a concept-based knowledge representation model (CKR). This model formally defines the concept as a triple (attribute, relation and behavior) and the knowledge of a concept can be illustrated by the triple. We propose a semantic inference strategy that against identify text segments which with dissimilar surface form but share a common meaning. The inference strategy avoids syntactic analysis steps. A preliminary evaluation on the PASCAL text collection is presented. Experimental results show that our concept-based inference strategy is effective and has strong development potential.  2014 TCCT, CAA.",
      "title": "223214 A concept-based knowledge representation model for semantic entailment inference",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907920627&partnerID=40&md5=fb2225e589ee235adc58e702d5d5f228"
    },
    {
      "abstract": "The velocity, volume and variety with which Twitter generates text is increasing exponentially. It is critical to determine latent sub-topics from such tweet data at any given point of time for providing better topic-wise search results relevant to users informational needs. The two main challenges in mining sub-topics from tweets in real-time are (1) understanding the semantic and the conceptual representation of the tweets, and (2) the ability to determine when a new sub-topic (or cluster) appears in the tweet stream. We address these challenges by proposing two unsupervised clustering approaches. In the first approach, we generate a semantic space representation for each tweet by keyword expansion and keyphrase identification. In the second approach, we transform each tweet into a conceptual space that represents the latent concepts of the tweet. We empirically show that the proposed methods outperform the state-of-the-art methods.  2014 Springer International Publishing Switzerland.",
      "title": "223216 Entity tracking in real-time using sub-topic detection on twitter",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899992315&partnerID=40&md5=5709e0b56ec7cfa4bf5bbf488f4d3c1e"
    },
    {
      "abstract": "In the present work we consider the problem of narrowdomain clustering of short texts, such as academic abstracts. Our main objective is to check whether it is possible to improve the quality of kmeans algorithm expanding the feature space by adding a dictionary of word groups that were selected from texts on the basis of a fixed set of patterns. Also, we check the possibility to increase the quality of clustering by mapping the feature spaces to a semantic space with a lower dimensionality using Latent Semantic Indexing (LSI). The results allow us to assume that the aforementioned modifications are feasible in practical terms as compared to the use of k-means in the feature space defined only by the main dictionary of the corpus.  Springer International Publishing Switzerland 2014.",
      "title": "223219 Clustering narrow-domain short texts using k-means, linguistic patterns and LSI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84916200231&partnerID=40&md5=61388acb03e656c9bf16747159504514"
    },
    {
      "abstract": "Extracting semantic associations from text corpora is an important problem with several applications. It is well understood that semantic associations from text can be discerned by observing patterns of co-occurrences of terms. However, much of the work in this direction has been piecemeal, addressing specific kinds of semantic associations. In this work, we propose a generic framework, using which several kinds of semantic associations can be mined. The framework comprises a co-occurrence graph of terms, along with a set of graph operators. A methodology for using this framework is also proposed, where the properties of a given semantic association can be hypothesized and tested over the framework. To show the generic nature of the proposed model, four different semantic associations are mined over a corpus comprising of Wikipedia articles. The design of the proposed framework is inspired from cognitive science - specifically the interplay between semantic and episodic memory in humans.  2014 Elsevier B.V. All rights reserved.",
      "title": "223224 A generic framework and methodology for extracting semantics from co-occurrences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905740029&partnerID=40&md5=1a4c764a1b1084ca9651d731fdeced46"
    },
    {
      "abstract": "PubMed is a search engine used to access the MEDLINE database, which comprises the massive amounts of biomedical literature. This an make more difficult for accessing to find the relevant medical literature. Therefore, this problem has been challenging in this work. We present a solution to retrieve the most relevant biomedical literature relating to Cholangiocarcinoma in clinical trials from PubMed. The proposed methodology is called ontology-based text classification (On-TC). We provide an ontology used as a semantic tool. It is called Cancer Technical Term Net (CCT-Net). This ontology is intergrated to the methodology to support automatic semantic interpretation during text processing, especially in the case of synonyms or term variations.  2014 Springer International Publishing.",
      "title": "223225 Ontology-based text classification for filtering cholangiocarcinoma documents from PubMed",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905270808&partnerID=40&md5=067bfd2edcc4b6411bbfbeac0b0f539a"
    },
    {
      "abstract": "This article presents results of structuring text documents using the classification process. The proposed system based on classification process which used to extract information about the semantics (meaning) segments (sentences) that build text documents. The analysis was made on the reports coming from the National Fire Service (Polish Fire Service) event evidence system. The article describes the results of classification using the proposed classifiers and presents some future directions of research.  2014 NSP Natural Sciences Publishing Cor.",
      "title": "223230 Detecting and extracting semantic topics from polish fire service raports",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904573888&partnerID=40&md5=b07098c249ae811b742dedc0d3e78098"
    },
    {
      "abstract": "This paper investigates Literature Based Discovery (LBD) approach to reveal linkages between technology and social issue to elucidate plausible contribution of science and technology for solving social issues. Robotics and gerontology were selected as an example in our analysis. The result shows various technological options of robotics contributing to healthcare and well-being of elderly people, mainly in surgery, rehabilitation, and companionship. In addition, we comparatively evaluated effectiveness of semantic similarity measures to extract these linkages from bibliographic database. Our methodology can be utilized as a decision support tool for managers and policy makers to extract and design promising research targets.  2013 Elsevier B.V.",
      "title": "223231 Finding linkage between technology and social issue: A Literature Based Discovery approach",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902240165&partnerID=40&md5=a412cd98c5510a7818930b2e34e428aa"
    },
    {
      "abstract": "Plagiarism is an important task since its number is increasing and the plagiarism technique is getting difficult. It means that there is not only literal plagiarism but also intelligence plagiarism. In order to handle the intelligence plagiarism, we employed latent semantic analysis (LSA) as the term-document representation. The LSA was used in the Heuristic Retrieval (HR) component and Detailed Analysis (DA) component. We conducted several experiments to compare the token type, the text segmentation and the threshold value. The test data were prepared manually from the available Indonesian paper corpus. Experimental results showed that the LSA outperformed the VSM (Vector Space Model), especially in test cases with intelligence plagiarism.  2014 IEEE.",
      "title": "223232 Experiments on the Indonesian plagiarism detection using latent semantic analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84909950568&partnerID=40&md5=ce69731697721ab848da7f2fd1933dc9"
    },
    {
      "abstract": "Literature information is an important content implicit in network. Focus on the lack of necessary ability of traditional search methods in literature information retrieval on the semantic comprehension, this paper combined with the existing web text mining, proposed a new information retrieval method using ontology concept for vector space model to integrate literature documents semantic information, and an appropriate evaluation methods is given. Application demonstrates the feasibility and effectiveness of this algorithm.  2014 IEEE.",
      "title": "223235 Research of literature information retrieval method based on ontology",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921302508&partnerID=40&md5=25a3af1e65c0e097437456e78126ad56"
    },
    {
      "abstract": "On line shopping, today, has become the call of the day. People are showing more inclination towards on line shops, due to large variety of options at fingertips, ease of access, access to global products. Further the buyer also benefits from information regarding user review of products, comparison of similar products etc. Therefore the importance of product review is also escalating exponentially. Most of the existing sentiment analysis systems require large training datasets and complex tools for implementation. This paper presents a Two-Parse algorithm with a training dataset of approximately 7,000 keywords, for automatic product review analysis. The proposed algorithm is more efficient as compared to some of the popular review analysis systems with enormous datasets. This algorithm is a solution to a very common problem of high polarity of datasets. This paper proposes a Weighted k-Nearest Neighbor (Weighted k-NN) Classifier which achieves a better efficiency than the classical k-Nearest Neighbor Classifier. The proposed Classifier is capable of successfully classifying weakly and mildly polar reviews along with the highly polar ones. The Classifier provides an option of modifying the parameters according to need of the system and thus overcomes the problem of static parameters in classical machine learning algorithms.  2014 IEEE.",
      "title": "223237 Supervised semantic analysis of product reviews using weighted k-NN classifier",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84903483865&partnerID=40&md5=cd2c4b2c5fd3e63160a69eaa657e5aca"
    },
    {
      "abstract": "We describe a state-of-the-art sentiment analysis system that detects (a) the sentiment of short informal textual messages such as tweets and SMS (message-level task) and (b) the sentiment of a word or a phrase within a message (term-level task). The system is based on a supervised statistical text classification approach leveraging a variety of surface-form, semantic, and sentiment features. The sentiment features are primarily derived from novel high-coverage tweet-specific sentiment lexicons. These lexicons are automatically generated from tweets with sentiment-word hashtags and from tweets with emoticons. To adequately capture the sentiment of words in negated contexts, a separate sentiment lexicon is generated for negated words. The system ranked first in the SemEval-2013 shared task Sentiment Analysis in Twitter (Task 2), obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. Post-competition improvements boost the performance to an F-score of 70.45 (message-level task) and 89.50 (term-level task). The system also obtains state-of-the-art performance on two additional datasets: the SemEval-2013 SMS test set and a corpus of movie review excerpts. The ablation experiments demonstrate that the use of the automatically generated lexicons results in performance gains of up to 6.5 absolute percentage points.  2014 National Research Council Canada. All rights reserved.",
      "title": "223241 Sentiment analysis of short informal texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907337360&partnerID=40&md5=df6cd8312745bddb2c6a0a1ae018fb4f"
    },
    {
      "abstract": "Ongoing research on novel methods and tools that can be applied in Natural Language Processing tasks has resulted in the design of a semantic compression mechanism. Semantic compression is a technique that allows for correct generalization of terms in some given context. Thanks to this generalization a common thought can be detected. The rules governing the generalization process are based on a data structure which is referred to as a domain frequency dictionary. Having established the domain for a given text fragment the disambiguation of possibly many hypernyms becomes a feasible task. Semantic compression, thus an informed generalization, is possible through the use of semantic networks as a knowledge representation structure. In the given overview, it is worth noting that the semantic compression allows for a number of improvements in comparison to already established Natural Language Processing techniques. These improvements, along with a detailed discussion of the various elements of algorithms and data structures that are necessary to make semantic compression a viable solution, are the core of this work. Semantic compression can be applied in a variety of scenarios, e.g. in detection of plagiarism. With increasing effort being spent on developing semantic compression, new domains of application have been discovered. What is more, semantic compression itself has evolved and has been refined by the introduction of new solutions that boost the level of disambiguation efficiency. Thanks to the remodeling of already existing data sources to suit algorithms enabling semantic compression, it has become possible to use semantic compression as a base for automata that, thanks to the exploration of hypernym-hyponym and synonym relations, new concepts that may be included in the knowledge representation structures can now be discovered.  Springer-Verlag Berlin Heidelberg 2014",
      "title": "223245 Semantic compression for text document processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921409845&partnerID=40&md5=3e276ad27b728892831fc5c6369c68c0"
    },
    {
      "abstract": "Entity search is becoming a popular alternative for full text search. Recently Google released its entity search based on confirmed, human-generated data such as Wikipedia. In spite of these developments, the task of entity discovery, search, or relation search in unstructured text remains a major challenge in the fields of information retrieval and information extraction. This paper tries to address that challenge, focusing specifically on entity relation discovery. This is achieved by processing unstructured text using simple information extraction methods, building lightweight semantic graphs and reusing them for entity relation discovery by applying algorithms from graph theory. An important part is also user interaction with semantic graphs, which can significantly improve information extraction results and entity relation search. Entity relations can be discovered by various text mining methods, but the advantage of the presented method lies in the similarity between the lightweight semantics extracted from a text and the information networks available as structured data. Both graph structures have similar properties and similar relation discovery algorithms can be applied. In addition, we can benefit from the integration of such graph data. We provide both a relevance and performance evaluations of the approach and showcase it in several use case applications.",
      "title": "223246 Discovering relations by entity search in lightweight semantic text graphs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921835914&partnerID=40&md5=c6cf4a1256a1d10eb337d75a005446d3"
    },
    {
      "abstract": "This paper addresses the problem of automatic acquisition of semantic relations between events. While previous works on semantic relation automatic acquisition relied on annotated text corpus, it is still unclear how to develop more generic methods to meet the needs of identifying related event pairs and extracting event-arguments (especially the predicate, subject and object). Motivated by this limitation, we develop a three-phased approach that acquires causality from the Web text. First, we use explicit connective markers (such as because) as linguistic cues to discover causal related events. Next, we extract the event-arguments based on local dependency parse trees of event expressions. At the last step, we propose a statistical model to measure the potential causal relations. The results of our empirical evaluations on a large-scale Web text corpus show that (a) the use of local dependency tree extensively improves both the accuracy and recall of event-arguments extraction task, and (b) our measure improves the traditional PMI method.  The Authors. Published by Elsevier B.V.",
      "title": "223247 Mining large-scale event knowledge from Web text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902793509&partnerID=40&md5=08e84154d77421d1cb5be80847d79358"
    },
    {
      "abstract": "With the exponential growth of texts on the Internet, text search is considered a crucial problem in many fields. Most of the traditional text search approaches are based on bag of words text representation based on frequency statics. However, these approaches ignore the semantic correlation of words in the text. So this may lead to inaccurate ranking of the search results. In this paper, we propose a new Wikipedia-based similar text search approach that the words in the texts and query text could be semantic correlated in Wikipedia. We propose a new text representation model and a new text similarity metric. Finally, the experiments on the real dataset demonstrate the high precision, recall and efficiency of our approach.  2014 Springer International Publishing Switzerland.",
      "title": "223248 A correlation-based semantic model for text search",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84903709774&partnerID=40&md5=e686abb6b30382e22220ca47d1acdcbe"
    },
    {
      "abstract": "This paper describes an information extraction system designed for obtaining CV-style structured information about publicly mentioned persons, organizations and their relations by analyzing newswire archives in the Latvian language. The described text analysis pipeline consists of morphosyntactic analysis, NER and coreference resolution, and a semantic role labeling system based on FrameNet principles. We also implement an entity linking process, matching the entity mentions in each document to an entity knowledge base that is initially seeded with authoritative information on relevant people and organizations. The accuracy of automated frame extraction varies depending on specifics of each frame type, but the average accuracy currently is 53% F-score for frame target identification, and 61% for frame element role classification. The currently targeted volume of text is the total archives of Latvian newspapers, magazines and news portals, consisting of about 3.5 million articles.  2014 The Authors and IOS Press.",
      "title": "223251 Latvian Newswire Information Extraction System and Entity Knowledge Base",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84948687631&partnerID=40&md5=d154116b5e3ab6e763fc593593c5532c"
    },
    {
      "abstract": "Automatic event extraction from natural language text in order to create a structure knowledge base is still one of challenging problems. Typical methods involve various forms of pattern matching and syntactic analysis. However, the success is limited when applying these methods to extract multiple events from a simple compound or complex sentence. This study classifies multiple events occurred in a sentence into two types of situations: multiple events with common actors or actions and multiple events as a series of related events. The proposed approach employs both syntactic and semantic processes to identify event types and event details. When demonstrated with a football domain the proposed approach achieved about 90% extraction rate comparing to 61% when a pattern matching approach was applied  2014 IEEE.",
      "title": "223254 Multiple event extraction from a sentence: Case study m a football domain",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942891505&partnerID=40&md5=bb7ff5fc2ca2736977b42ba1152b4dd6"
    },
    {
      "abstract": "Ontology is widely used in semantic computing and reasoning, and various biomedicine ontologies have become institutionalized to make the heterogeneous knowledge computationally amenable. Relation words, especially verbs, play an important role when describing the interaction between biological entities in molecular function, biological process, and cellular component",
      "title": "223255 Interaction relation ontology learning",
      "url": "Article"
    },
    {
      "abstract": "Large amount of data is ubiquitous in the internet giving the challenges to identify the relevant information from the documents. Marketing or sales people do analysis on their products reach using surveys, feedback and queries etc. using online forms. All these unstructured text and voluminous data have to be analyzed manually or semi-automatically to understand the context and make decisions on the business. Similarly to buy any products online, people usually search and get opinion on the products from friends before taking their decisions. Whenever a new product is launched in the market, the social networking sites are deluged with reviews and comments about the product. We can exploit this to the advantage of determining the reach of the product among the common mass by applying sentiment analysis over these reviews. The use of this is twofold-(i) The seller gets first-hand information about the products reach (ii) Buyer can buy the right product for his need. In this paper, we propose a model to store the product information as ontologies, thereby bringing strong relationships among the data stored. We add more semantics by embedding the opinions, determined using sentiment analysis, within the linked data thereby accounting for better reasoning and efficient querying. We propose a method to perform opinion mining on products by extracting the comments on products features from the reviews posted in a micro-blogging site such as twitter. The sentiment analysis is done on these reviews to rate the product features. These opinions are stored along with the product-feature descriptions in the domain ontology in OWL format. Once the opinions are embedded in the ontology, the model can serve for better reasoning and answering intelligent queries. This information can also be recommended to the people who are in need when they search for similar products which help them to make necessary decisions based on the others opinions.  2013 IEEE.",
      "title": "223256 An ontology based sentiment analysis for mobile products using tweets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84911091120&partnerID=40&md5=945e1e817506b3569575ee83ae8cabb9"
    },
    {
      "abstract": "In recent times, it has become easier to collect large quantities of customer reviews of products and services through the Internet. Thus, text-mining has become evermore important for various businesses. However, current techniques to visualize the correspondence relation between customer reviews and evaluation information are insufficient. The purpose of this paper is to propose a new method of visualizing information using a Self-organizing Map(SOM) that is robust for text data that is non-linear and multi-collinear. Our method involves, probabilistic Latent Semantic Indexing (pLSI) which does not require weighting for the dimension contraction of a word vector. Furthermore, we also propose a method to visualize the distribution of evaluation information on SOM. In order to assign a suitable value to dead nodes and nodes without evaluation values, we redefine the interpolation formula for the evaluation value. To confirm the effectiveness and accuracy of our proposal, we use our method to visualize customer review data on a major E-Commerce website.  2014 IEEE.",
      "title": "223257 Visualization of online customer reviews and evaluations based on self-organizing map",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84938063549&partnerID=40&md5=8b8e27afc1e298991a94dc8d786e1012"
    },
    {
      "abstract": "We report the first steps of a novel investigation into how a grammar induction algorithm can be modified and used to identify salient information structures in a corpus. The information structures are to be used as representations of semantic content for text mining purposes. We modify the learning regime of the ADIOS algorithm (Solan et al., 2005) so that text is presented as increasingly large snippets around key terms, and instances of selected structures are substituted with common identifiers in the input for subsequent iterations. The technique is applied to 1.4m blog posts about climate change which mention diverse topics and reflect multiple perspectives and different points of view. Observation of the resulting information structures suggests that they could be useful as representations of semantic content. Preliminary analysis shows that our modifications had a beneficial effect for inducing more useful structures.  2014 Association for Computational Linguistics.",
      "title": "223259 Applying grammar induction to text mining",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906929994&partnerID=40&md5=4fe21d83972ad96c7b0c409d7ff79770"
    },
    {
      "abstract": "Semantic Role Labeling (SRL) is a leading task of identifying arguments for a predicate and assigning semantically meaningful labels to them. SRL is crucial to information extraction, question answering, and machine translation. When applied to patent text, existing tools for SRL have unsatisfying performance because of long sentences. To improve performance in patent SRL systems, this study separates each sentence in patent abstracts into a simpler structure, and then labels semantic roles for the simplified sentence. At last, semantic information and semantic framework for frequently used words are used to extract patent knowledge. Our work demonstrates that the method used in this article can improve the performance in SRL system and obtain beneficial knowledge from patents. Copyright  2014 for the individual papers by the papers authors.",
      "title": "223260 Research of Semantic Role Labeling and application in patent knowledge extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84914693371&partnerID=40&md5=eda427de816daea6258241d584dee74b"
    },
    {
      "abstract": "Text classification using semantic information is the latest trend of research due to its greater potential to accurately represent text content compared with bag-of-words (BOW) approaches. On the other hand, representation of semantics through graphs has several advantages over the traditional representation of feature vector. Therefore, error tolerant graph matching techniques can be used for text classification. Nevertheless, very few methodologies exist in the literature which use semantic representation through graphs. In the present work, a methodology has been proposed to represent semantic information from a summarized text into a graph. The discourse representation structure of a text is utilized in order to represent its semantic content and, afterwards, it is transformed into a graph. Five different graph matching techniques based on Maximum Common Subgraphs (mcs) and Minimum Common Supergraphs (MCS) are evaluated on 20 classes from the Reuters dataset taking 10 docs of each class for both training and testing purposes using the k-NN classifier. From the results it can be observed that the technique has potential to perform text classification as well as the traditional BOW approaches. Moreover a majority voting based combination of the semantic representation and a traditional BOW approach provided an improved recognition accuracy on the same data set.  Springer International Publishing Switzerland 2014.",
      "title": "223262 Using graphs and semantic information to improve text classifiers",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921778532&partnerID=40&md5=fc4b5fd516491d9c46771c8b5eae9b55"
    },
    {
      "abstract": "The majority of existing knowledge is encoded in unstructured texts and is not linked to formalized knowledge, like ontologies and rules. The potential solution to this problem is to acquire this knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring more complex relationships from texts and codes them in the form of rules. Our approach starts with existing domain knowledge represented as OWL ontology and SWRL Semantic Web Rule Language rules by applying NLP tools and text matching techniques to deduce different atoms as classes, properties",
      "title": "223267 Automatic rules extraction from medical texts",
      "url": "Conference Paper"
    },
    {
      "abstract": "Automatic text categorisation systems is a type of software that every day it is receiving more interest, due not only to its use in documentaries environments but also to its possible application to tag properly documents on the Web. Many options have been proposed to face this subject using statistical approaches, natural language processing tools, ontologies and lexical databases. Nevertheless, there have been no too many empirical evaluations comparing the influence of the different tools used to solve these problems, particularly in a multilingual environment. In this paper we propose a multi-language rule-based pipeline system for automatic document categorisation and we compare empirically the results of applying techniques that rely on statistics and supervised learning with the results of applying the same techniques but with the support of smarter tools based on language semantics and ontologies, using for this purpose several corpora of documents. GENIE is being applied to real environments, which shows the potential of the proposal.",
      "title": "223268 The GENIE project: A semantic pipeline for automatic document categorisation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902363382&partnerID=40&md5=8e899a537f70939d8a7719ee4ad86277"
    },
    {
      "abstract": "We address the problem of the categorization of short texts, like those posted by users on social networks and microblogging platforms. We specifically focus on Twitter. Since short texts do not provide sufficient word occurrences, and they often contain abbreviations and acronyms, traditional classification methods such as quot",
      "title": "223269 Short text categorization exploiting contextual enrichment and external knowledge",
      "url": "Conference Paper"
    },
    {
      "abstract": "We present a method for the automatic classification of text documents into a dynamically defined set of topics of interest. The proposed approach requires only a domain ontology and a set of user-defined classification topics, specified as contexts in the ontology. Our method is based on measuring the semantic similarity of the thematic graph created from a text document and the ontology sub-graphs resulting from the projection of the defined contexts. The domain ontology effectively becomes the classifier, where classification topics are expressed using the defined ontological contexts. In contrast to the traditional supervised categorization methods, the proposed method does not require a training set of documents. More importantly, our approach allows dynamically changing the classification topics without retraining of the classifier. In our experiments, we used the English language Wikipedia converted to an RDF ontology to categorize a corpus of current Web news documents into selection of topics of interest. The high accuracy achieved in our tests demonstrates the effectiveness of the proposed method, as well as the applicability of Wikipedia for semantic text categorization purposes.  2014 IEEE.",
      "title": "223270 Ontology-based text classification into dynamically defined topics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84906969149&partnerID=40&md5=9aea9a765e947254eb099a0091cb02f5"
    },
    {
      "abstract": "Nowadays, information and knowledge are fundamental in our society. This has induced an information overload problem in the Internet. For this reason, we propose to create an automatic system to retrieve, select, and extract information from the Web whose methodology is based on fusion techniques. The system, called Diana, facilitates and improves the identification of interesting contents, and it allowsto extract the most relevant information about a certain topic from the Web as a summary. To do this, we have developed algorithms that use semantic tools, Natural Language Processing (NLP) techniques, statistics, a generic gazetteer, and fusion methods. The development of the system is undergoing, but the preliminary results that we have obtained so far are very promising and show the interest of our proposal.  2014 International Society of Information Fusion.",
      "title": "223276 Obtaining knowledge from the Web using fusion and summarization techniques",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84910601109&partnerID=40&md5=8fc56cba1648dcc910ffb71c9a1725a1"
    },
    {
      "abstract": "Semantic event extraction is helpful for video annotation and retrieval. For sports video, most previous works detect events by video content itself. Some useful external knowledge has been researched recently. In this paper, we proposed an unsupervised approach to extract semantic events from sports webcast text. First, unrelated words in the descriptions of webcast text are filtered out, and then the filtered descriptions are clustered into significant event categories. Finally, the keywords for each event category are extracted. According to our experimental results, the proposed approach actually extracts significant text events, which can be used for further video indexing and summarization. Furthermore, we also provide a hierarchical searching scheme for text event retrieval.  2012 Springer Science+Business Media New York.",
      "title": "223277 A novel approach for semantic event extraction from sports webcast text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904394182&partnerID=40&md5=2ba5b5cf6244e9b0638ff39e4d361f09"
    },
    {
      "abstract": "With the growth of the Internet and electronic commerce, there is more and more review data on the Internet. Quite a lot of Internet users refer to related comments of a product before they make a decision, which can teach them about the quality and reputation of the product and help them decide whether to buy it. A system that can automatically classify the polarity of a given text would be a great help to users. This paper is divided into two parts. The first part is about SVM classification method. We adopts a variety of feature extraction methods, such as TF-IDF (Term Frequency-Inverse Document Frequency), MI (Mutual Information), CHI. First, we calculate the weight of terms. And then, we adopt the Support Vector Machines (SVM) model for emotion classification. In the second part, we present a new method of semantic similarity computation for sentiment analysis. Our approach achieves the accuracy of 91% and 82% with the two methods.  2014 IEEE.",
      "title": "223281 Comparison of SVM classification method and semantic similarity method for sentiment classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84948465787&partnerID=40&md5=6ffff9a7bdcac6038e7f8eb394d528fb"
    },
    {
      "abstract": "This poster presents an early experimentation of applying topic modeling and visualization techniques to analyze on- line discourse. In particular, Latent Dirichlet Allocation was used to convert discourse into a high-dimensional semantic space. To explore meaningful visualizations of the space, Locally Linear Embedding was performed reducing it to two- dimensional. Further, Time Series Analysis was applied to track evolution of topics in the space. This work will lead to new analytic tools for collaborative learning. Copyright  2014 by the Association for Computing Machinery, Inc.",
      "title": "223287 Visualizing semantic space of online discourse: The Knowledge Forum case",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84898787682&partnerID=40&md5=03bbdaef6a4a77131953e6a120e4889f"
    },
    {
      "abstract": "This paper is devoted to a problem of partition documents from the news flow into groups, where each group contains documents that are similar to each other. We use thematic clustering to solve this problem. The existing clustering algorithms such as k-means, minimum spanning tree and etc. are considered and analyzed. It is shown which of these algorithms give the best results working with news texts. Clustering is a powerful tool for text processing, but it cant give a complete picture of news article semantics. This paper also presents a methodic of comprehensive news texts analysis based on a combination of statistical algorithms for keywords extracting and algorithms forming the semantic coherence of text blocks. Particular attention is paid to the structural features of the news texts.  Springer International Publishing Switzerland 2014.",
      "title": "223295 Thematic Clustering Methods Applied to News Texts Analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907351601&partnerID=40&md5=353cc075f63676ced6fbaab33140edb6"
    },
    {
      "abstract": "Kernel-based learning has been largely applied to semantic textual inference tasks. In particular, Tree Kernels (TKs) are crucial in the modeling of syntactic similarity between linguistic instances in Question Answering or Information Extraction tasks. At the same time, lexical semantic information has been studied through the adoption of the so-called Distributional Semantics (DS) paradigm, where lexical vectors are acquired automatically from large corpora. Notice how methods to account for compositional linguistic structures (e.g. grammatically typed bi-grams or complex verb or noun phrases) have been proposed recently by defining algebras on lexical vectors. The result is an extended paradigm called Distributional Compositional Semantics (DCS). Although lexical extensions have been already proposed to generalize TKs towards semantic phenomena (e.g. the predicate argument structures as for role labeling), currently studied TKs do not account for compositionality, in general. In this paper, a novel kernel called Compositionally Smoothed Partial Tree Kernel is proposed to integrate DCS operators into the tree kernel evaluation, by acting both over lexical leaves and non-terminal, i.e. complex compositional, nodes. The empirical results obtained on a Question Classification and Paraphrase Identification tasks show that state-of-the-art performances can be achieved, without resorting to manual feature engineering, thus suggesting that a large set of Web and text mining tasks can be handled successfully by the kernel proposed here. Copyright 2014 ACM.",
      "title": "223296 Semantic compositionality in tree kernels",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84937605047&partnerID=40&md5=a3b79ce3cc3ce0b293b1b265b17bbfdd"
    },
    {
      "abstract": "Most text clustering algorithms represent a corpus as a document-term matrix in the bag of words model. The feature values are computed based on term frequencies in documents and no semantic relatedness between terms is considered. Therefore, two semantically similar documents may sit in different clusters if they do not share any terms. One solution to this problem is to enrich the document representation using an external resource like Wikipedia. We propose a new way to integrate Wikipedia concepts in partitional text document clustering in this work. A text corpus is first represented as a document-term matrix and a documentconcept matrix. Terms that exist in the corpus are then clustered based on the document-term representation. Given the term clusters, we propose two methods, one based on the document-term representation and the other one based on the document-concept representation, to find two sets of seed documents. The two sets are then used in our text clustering algorithm in an ensemble approach to cluster documents. The experimental results show that even though the documentconcept representations do not result in good document clusters per se, integrating them in our ensemble approach improves the quality of document clusters significantly.  2014 ACM.",
      "title": "223297 An ensemble approach for text document clustering using wikipedia concepts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84908650220&partnerID=40&md5=a6fb5d308d3175c9e619f74638c60f8b"
    },
    {
      "abstract": "Search engines have always played an important role in helping web users to rapidly find information on the Web. However, their function is limited to returning a list of query relevant documents with reasonably good precision, but huge recall. The task of actually processing the returned documents to get the required information is the responsibility of the user. In recent years, Question-Answer systems are gaining popularity and have garnered much research interest in view of the proposed Semantic Web and future availability of fully structured data. The advantage of QA systems is that users have the luxury of asking queries in natural language and also get a precise answer instead of just displaying a list of links to documents that may or may not be relevant. This paper presents a question answer search engine prototype that uses natural language processing, natural language generation, question classification and query logs to find a precise answer to the submitted query. This is ongoing work and we focus on the methodology of query analysis in this paper. We describe our strategy of automatic query analysis by classifying it into nine categories and understanding the meaning of the query. We also discuss in detail how each of the question categories are automatically processed and how the proposed system determines the key word or key phrase to be searched.  2014 IEEE.",
      "title": "223299 Automated query analysis techniques for semantics based question answering system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84921047401&partnerID=40&md5=4d692f63ed1bf1d1c5ed9e0d7a6553ea"
    },
    {
      "abstract": "The ability to understand natural language text is far from being emulated in machines. One of the main hurdles to overcome is that computers lack both the common and common-sense knowledge that humans normally acquire during the formative years of their lives. To really understand natural language, a machine should be able to comprehend this type of knowledge, rather than merely relying on the valence of keywords and word co-occurrence frequencies. In this article, the largest existing taxonomy of common knowledge is blended with a natural-language-based semantic network of common-sense knowledge. Multidimensional scaling is applied on the resulting knowledge base for open-domain opinion mining and sentiment analysis.  2001-2011 IEEE.",
      "title": "223300 Semantic multidimensional scaling for open-domain sentiment analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84903161890&partnerID=40&md5=e00b56854de5e3042a9f85e020801a86"
    },
    {
      "abstract": "Proliferation of multilingual text on the Internet has increased the demand for efficient information retrieval independent of language. Among variety of languages, the Urdu language is one of the most commonly spoken and written language in South Asia. However, due to unstructured format the access of relevant information is still a big challenge. The semantic web technologies enable the advancement in information retrieval systems by assigning semantics to information. This paper presents a semantic annotation framework that can annotate documents written in Urdu language. The framework uses domain specific ontology and context keywords instead of NLP (Natural Language processing) techniques. The experiment has been conducted to evaluate the presented annotation framework. The set of corpora used in the experiment belong to the online classified ads posted on the online Urdu newspapers. The purpose of this research is to find the challenges involved in semantic annotation of Urdu language web documents.  2014 The Authors.",
      "title": "223302 Ontology based semantic annotation of Urdu language web documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84924203419&partnerID=40&md5=92c5186cf4c7bc2f2593cbb211c41ebb"
    },
    {
      "abstract": "Chinese grammar engineering has been a much debated task. Whilst semantic information has been reconed crucial for Chinese syntactic analysis and downstream applications, existing Chinese treebanks lack a consistent and strict sentential semantic formalism. In this paper, we introduce a semantics oriented grammar for Chinese, designed to provide basic supports for tasks such as automatic semantic parsing and sentence generation. It has a directed acyclic graph structure with a simple yet expressive label set, and leverages elementary predication to support logical form conversion. To our knowledge, it is the first Chinese grammar representation capable of direct transformation into logical forms.  2014 Springer-Verlag Berlin Heidelberg.",
      "title": "223303 A semantics oriented grammar for Chinese treebanking",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899909684&partnerID=40&md5=ac6b287672dbe72f8a33016528aa3a45"
    },
    {
      "abstract": "Computing the semantic similarity between words is one of the key tasks in many language-based applications. Recent work has focused on using contextual clues for semantic similarity computation. In this paper, we propose a method to the measure semantic similarity between words using plain text contents. It takes into account information attributes (local) and topic information (global) of words to disclose their semantic similarity scores. The method models the representation of a word as a high dimensional vector of word attributes and latent topics. Thus, the semantic similarity between two words is measured by the semantic distance between their respective vectors. We have tested the proposed method on WordSimilarity-353 dataset. The empirical results have shown the combination features contribute to improve the semantic similarity results the dataset in comparison with previous work on the same task using plain text contents.",
      "title": "223304 Combination features for semantic similarity measure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901494523&partnerID=40&md5=1cf7e7eacfbd81373127a7db940f1a0a"
    },
    {
      "abstract": "In a phishing attack, an unsuspecting victim is lured, typically via an email, to a web site designed to steal sensitive information such as bank/credit card account numbers, login information for accounts, etc. Each year Internet users lose billions of dollars to this scourge. In this paper, we present a general semantic feature selection method for text problems based on the statistical t-test and WordNet, and we show its effectiveness on phishing email detection by designing classifiers that combine semantics and statistics in analyzing the text in the email. Our feature selection method is general and useful for other applications involving text-based analysis as well. Our email body-text-only classifier classifier achieves more than 95% accuracy on detecting phishing emails with a false positive rate of 2.24 %. Due to its use of semantics, our feature selection method is robust against adaptive attacks and avoids the problem of frequent retraining needed by machine learning classifiers.  Springer International Publishing Switzerland 2014.",
      "title": "223307 Semantic feature selection for text with application to phishing email detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84911091723&partnerID=40&md5=f9329168bc079dfe1a838892192bb265"
    },
    {
      "abstract": "Protein-protein interactions (PPIs) are involved in the majority of biological processes. Identification of PPIs is therefore one of the key aims of biological research. Although there are many databases of PPIs, many other unidentified PPIs could be buried in the biomedical literature. Therefore, automated identification of PPIs from biomedical literature repositories could be used to discover otherwise hidden interactions. Search engines, such as Google, have been successfully applied to measure the relatedness among words. Inspired by such approaches, we propose a novel method to identify PPIs through semantic similarity measures among protein mentions. We define six semantic similarity measures as features based on the page counts retrieved from the MEDLINE database. A machine learning classifier, Random Forest, is trained using the above features. The proposed approach achieve an averaged micro-F of 71.28% and an averaged macro-F of 64.03% over five PPI corpora, an improvement over the results of using only the conventional co-occurrence feature (averaged micro-F of 68.79% and an averaged macro-F of 60.49%). A relation-word reinforcement further improves the averaged micro-F to 71.3% and averaged macro-F to 65.12%. Comparing the results of the current work with other studies on the AIMed corpus (ranging from 77.58% to 85.1% in micro-F, 62.18% to 76.27% in macro-F), we show that the proposed approach achieves micro-F of 81.88% and macro-F of 64.01% without the use of sophisticated feature extraction. Finally, we manually examine the newly discovered PPI pairs based on a literature review, and the results suggest that our approach could extract novel protein-protein interactions.  2014 Imperial College Press.",
      "title": "223308 Discovering novel protein-protein interactions by measuring the protein semantic similarity from the biomedical literature",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84930038930&partnerID=40&md5=2489da94bfd2f97e9955b8d9a0513f86"
    },
    {
      "abstract": "This paper presents the methodology and results of an exhaustive text-mining analysis performed on the subject Dematerialization and Environment. Three main fields have been investigated: international academic publications (documents extracted from the Web of Science database during the period 1994-2013), French blog publications (pages crawled on the web for the years 2011-2012-2013) and French newspaper publications (articles published between 1993 and 2013). Lexical extraction, word specific selection by experts and co-wording analysis were performed to produce thematic maps for each domain. Moreover, interviews were carried out with experts, researchers and practitioners to interpret the maps. The results presented in this article mainly show a semantic landscape focused on telework, telecommuting and coworking related activities. Semantic and historical evolutions highlight new working spaces such as third places impacted by digital uses and social practices. They reveal the main environmental issues practitioners have to tackle in a prospective way given the complexity of this emerging field.  2014. The authors.",
      "title": "223311 Dematerialization and the environment: A text-mining landscape on academic, blog and press publications",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84928009455&partnerID=40&md5=34a214e6c1f33a1ef54a33e62233480b"
    },
    {
      "abstract": "In this paper, we aim to define foundations and research questions for future large scale exploration of various types of semantic relationships in literature, namely Swedish prose fiction. More specifically, we are interested to get an in-depth understanding of storytelling in Swedish fiction by analyzing and mining the narrative discourse in a small sample of such data, focusing on interpersonal relationships and answering various questions such as how to recognize and assess gender patterns. Our intention is to apply our findings into a much larger scale in the near future in order to obtain useful insights about the social relations, structures, behavior and everyday life of characters found in literary works, thus enhancing the use of prose fiction as a source for research within the humanities and social sciences. Our work is inspired by the notions of distant reading and macroanalysis, a relatively new and often contested paradigm of literary research. In order to achieve our goal we strive for a combination of natural language processing techniques and simple visualizations that allow the user to rapidly focus on key areas of interest and provide the ability to discover latent semantic patterns and structures.",
      "title": "223316 Semantics in storytelling in Swedish fiction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901589766&partnerID=40&md5=59b2f2296a331dd77848622b578ce650"
    },
    {
      "abstract": "This paper discusses a few algorithms for updating the approximate singular value decomposition (SVD) in the context of information retrieval by latent semantic indexing (LSI) methods. A unifying framework is considered which is based on Rayleigh-Ritz projection methods. First, a Rayleigh-Ritz approach for the SVD is discussed and it is then used to interpret the Zha and Simon algorithms [SIAM J. Sci. Comput., 21 (1999),pp. 782-791]. This viewpoint leads to a few alternatives whose goal is to reduce computational cost and storage requirement by projection techniques that utilize subspaces of much smaller dimension. Numerical experiments show that the proposed algorithms yield accuracies comparable to those obtained from standard ones at a much lower computational cost.  2014 Society for Industrial and Applied Mathematics.",
      "title": "223318 Fast updating algorithms for latent semantic indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907821611&partnerID=40&md5=07d6ac75cbd827394776b7591b087c95"
    },
    {
      "abstract": "An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form. Text mining is an emerging technique for mining useful information from the web. We aim at discovering in particular tweets semantic patterns in consumers discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers positive opinions and negative opinions on social media.  2014 IEEE.",
      "title": "223320 Semantic properties of customer sentiment in tweets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904480031&partnerID=40&md5=624e72734da0dd684fcd1ac00f9e2f73"
    },
    {
      "abstract": "CroSer (Cross-language Semantic Retrieval) is an ir system able to discover links between e-gov services described in different languages. CroSeR supports public administrators to link their own source catalogs of e-gov services described in any language to a target catalog whose services are described in English and are available in the Linked Open Data (lod) cloud. Our system is based on a cross-language semantic matching method that i) translates service labels in English using a machine translation tool, ii) extracts a Wikipedia-based semantic representation from the translated service labels using Explicit Semantic Analysis (esa), iii) evaluates the similarity between two services using their Wikipedia-based representations. The user selects a service in a source catalog and exploits the ranked list of matches suggested by CroSeR to establish a relation (of type narrower, equivalent, or broader match) with other services in the English catalog. The method is independent from the language adopted in the source catalog and it does not assume the availability of information about the services other than very short text descriptions used as service labels. CroSeR is a web application accessible via http://siti-rack.siti.disco.unimib.it:8080/croser/.  2014 Springer International Publishing Switzerland.",
      "title": "223321 CroSeR: Cross-language semantic retrieval of open government data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899928218&partnerID=40&md5=ba74c42e7ea899ed68bf8f8c2b2c10dc"
    },
    {
      "abstract": "Measures of semantic relatedness are largely applicable in intelligent tasks of NLP and Bioinformatics. By taking these automated measures into account, this paper attempts to improve Second-order Co-occurrence Vector semantic relatedness measure for more effective estimation of relatedness between two given concepts. Typically, this measure, after constructing concepts definitions (Glosses) from a thesaurus, considers the cosine of the angle between the concepts gloss vectors as the degree of relatedness. Nonetheless, these computed gloss vectors of concepts are impure and rather large in size which would hinder the expected performance of the measure. By employing latent semantic analysis (LSA), we try to conduct some level of insignificant feature elimination to generate economic gloss vectors. Applying both approaches to the biomedical domain, using MEDLINE as corpus, UMLS as thesaurus, and reference standard of biomedical concept-pairs manually rated for relatedness, we show LSA implementation enforces positive impact in terms of performance and efficiency.  2013 Springer International Publishing.",
      "title": "223348 Applying latent semantic analysis to optimize second-order co-occurrence vectors for semantic relatedness measurement",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893390183&partnerID=40&md5=1676a38d9f0d7bcb569b3fc2090b1604"
    },
    {
      "abstract": "Sequential Pattern Mining which aims to discover all frequent sequences of itemsets (patterns) from a large data collection has been applied in the Text Mining domain such as Text Categorization and Pattern Identification. However, in the area of Document Summarization the effort is still considered as green and exploratory. In the real world, a sentence is more than just a collection of un-ordered sequence of words, where each sentence carries their own meaning. By discovering these textual patterns is essential since the patterns can describe the text, by preserving the sequential order of the words in the document. Thus, the motivation here is to investigate the feasibility to develop a Sequential Pattern-based Summarizer model near future in order to reduce redundancy information from multiple text resources",
      "title": "223350 Sequential pattern based multi document summarization - An exploratory approach",
      "url": ""
    },
    {
      "abstract": "Given the wide spread of social networks, research efforts to retrieve information using tagging from social networks communications have increased. In particular, in Twitter social network, hashtags are widely used to define a shared context for events or topics. While this is a common practice often the hashtags freely introduced by the user become easily biased. In this paper, we propose to deal with this bias defining semantic meta-hashtags by clustering similar messages to improve the classification. First, we use the user-defined hashtags as the Twitter message class labels. Then, we apply the meta-hashtag approach to boost the performance of the message classification. The meta-hashtag approach is tested in a Twitter-based dataset constructed by requesting public tweets to the Twitter API. The experimental results yielded by comparing a baseline model based on user-defined hashtags with the clustered meta-hashtag approach show that the overall classification is improved. It is concluded that by incorporating semantics in the meta-hashtag model can have impact in different applications, e.g. recommendation systems, event detection or crowdsourcing.  2013 Springer-Verlag Berlin Heidelberg.",
      "title": "223352 Defining semantic meta-hashtags for twitter classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893505918&partnerID=40&md5=5ae13a4ab7f4e46ca322765e890c87f0"
    },
    {
      "abstract": "The vast numbers of digitised documents containing historical data constitute a rich research data repository. However, computational methods and tools available to explore this data are still limited in functionality. Research on historical archives is still largely carried out manually. Text mining technologies offer novel methods to analyse digital content to identify various types of semantic information in these documents and to extract them as semantic metadata. Methods range from the automatic identification of named entities (e.g., people, places, organisations, etc.) to more sophisticated methods to extract information about events (e.g., births, deaths, arrests, etc.), allowing users to greatly increase the specificity of their search. We have created an extended model of event interpretation to allow searches to be refined based on various discourse facets, including isolating definite information about events from more speculative details, distinguishing positive and negative opinions and categorising events according to information source. We present ISHER as an example of a multi-faceted, semantically oriented system for searching news articles from the New York Times, dating back to 1987. We explain how our extended event interpretation model can enhance search capabilities in systems such as ISHER, including the identification of contrasting and contradictory information in news articles.  2013 IEEE.",
      "title": "223355 News search using discourse analytics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84896785098&partnerID=40&md5=d358cc3b5d93b04fa0e4444adf847701"
    },
    {
      "abstract": "Public administrations are aware of the advantages of sharing Open Government Data in terms of transparency, development of improved services, collaboration between stakeholders, and spurring new economic activities. Initiatives for the publication and interlinking of government service catalogs as Linked Open Data (lod) support the interoperability among European administrations and improve the capability of foreign citizens to access services across Europe. However, linking service catalogs to reference lod catalogs requires a significant effort from local administrations, preventing the uptake of interoperable solutions at a large scale. The web application presented in this paper is named CroSeR (Cross-language Service Retriever) and supports public bodies in the process of linking their own service catalogs to the lod cloud. CroSeR supports different European languages and adopts a semantic representation of e-gov services based on Wikipedia. CroSeR tries to overcome problems related to the short textual descriptions associated to a service by embodying a semantic annotation algorithm that enriches service labels with emerging Wikipedia concepts related to the service. An experimental evaluation carried-out on e-gov service catalogs in five different languages shows the effectiveness of our model.  2013 Springer-Verlag.",
      "title": "223356 Cross-language semantic retrieval and linking of e-gov services",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891943734&partnerID=40&md5=be71eaf761b7d5e816fbb648c4105e79"
    },
    {
      "abstract": "Mining the semantics of open relations is an important task in open information extraction (Open IE). For this task, the difficulty is that the expressions of a specific semantic in free texts are always not unique. Therefore, it needs us to deeply capture the semantics behind the various expressions. In this paper, we propose an open relation mapping method combining the instances and semantic expansion, which maps the open relation mentions in free texts to the attribute name in knowledge base to find the real semanics of each open relation mentions. Our method effectively mines semantic expansion beyond the text surface of relation mentions. Experimental results show that our method can achieve 74.4% average accuracy for open relation mapping.  2013 Springer-Verlag.",
      "title": "223357 Open relation mapping based on instances and semantics expansion",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893324779&partnerID=40&md5=b31521cb3bbbb72dc4531ad127598f61"
    },
    {
      "abstract": "Finding the appropriate information and understanding to human research is a delicate task when dealing with an outstanding number of unstructured texts created daily. Hence the objective of clustering algorithms which are part of the powerful text mining tools. In this paper, we propose a novel text document clustering based on a new hybrid feature selection method that we call HFSM. This technique extracts statistical and semantic relevant terms to pilot the clustering mechanism. The experiments conducted on Reuters corpus demonstrate the practical aspects of our algorithm and show that it generates more accurate clustering than the one obtained by other existing algorithms.  2013 ACM.",
      "title": "223360 Text document clustering with hybrid feature selection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84896879192&partnerID=40&md5=9741faa4ee816a4b185107ccb9e44667"
    },
    {
      "abstract": "An important contribution of the Semantic Web is a new format of data representation called Resource Description Framework (RDF). In RDF, every piece of information is represented as a triple: subject-property-object. In general, subjects and objects can be shared between multiple RDF triples, and all triples can constitute a densely interlinked network. RDF becomes a very popular format of representing data on the web. As of September 2012, the last available data, more than 31 billions of triples exist on the web. In the paper, we propose a system - called T2R - for automatic acquisition of syntactic and semantic relations among terms from a plain text. These relations are expressed in the form of RDF triples. The proposed method is independent of any prior knowledge and domain specific patterns, and is applicable to any textual resources. The system implementing the approach is capable of identifying grammatical structure of an input sentence and analysing its semantics to generate meaningful RDF triples. We evaluate this approach by proving the quality of our results through case studies.  2013 IEEE.",
      "title": "223361 T2R: System for converting textual documents into RDF triples",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893225770&partnerID=40&md5=47a948ff9bf483786d8d80186862dd6e"
    },
    {
      "abstract": "Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vast amounts of knowledge in a short time. Automatic discourse causality recognition can further reduce their workload by suggesting possible causal connections and aiding in the curation of pathway models. We describe here an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with and compare various parameter settings for three algorithms, i.e. Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). We also evaluate the impact of lexical, syntactic, and semantic features on each of the algorithms, showing that semantics improves the performance in all cases. We test our comprehensive feature set on two corpora containing gold standard annotations of causal relations, and demonstrate the need for more gold standard data. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types.  2013 Imperial College Press.",
      "title": "223362 Recognising discourse causality triggers in the biomedical domain",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891611392&partnerID=40&md5=46f2da9810e689ad47cc6ed8518ec576"
    },
    {
      "abstract": "Natural language dialogue is an important component of interaction between ordinary users and complex computer applications. Short Text Semantic Similarity algorithms have been developed to improve the efficiency of producing sophisticated dialogue systems. Such algorithms are currently unable to discriminate between different dialogue acts (assertions, questions, instructions etc.), requiring the addition of efficient dialogue act classifiers to enhance them. The Slim Function Word Classifier (SFWC) has proved promising, particularly in its computational simplicity. This study optimizes the SFWC by clustering function word features using grammatical principles. Experiments show a significant improvement in classification accuracy for a selection of sentence forms which were challenging for the unoptimized SFWC. Results are expected to be applicable to many intelligent text processing applications ranging from question answering to meeting summarization.  2013 IEEE.",
      "title": "223369 Optimizing features for dialogue act classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893542498&partnerID=40&md5=0450f1805f59b417a9c497420eda144b"
    },
    {
      "abstract": "This paper sets out to explore whether data about the usage of hashtags on Twitter contains information about their semantics. Towards that end, we perform initial statistical hypothesis tests to quantify the association between usage patterns and semantics of hashtags. To assess the utility of pragmatic features { which describe how a hashtag is used over time { for semantic analysis of hashtags, we conduct various hashtag stream classification experiments and compare their utility with the utility of lexical features. Our results indicate that pragmatic features indeed contain valuable information for classifying hashtags into semantic categories. Although pragmatic features do not outperform lexical features in our experiments, we argue that pragmatic features are important and relevant for settings in which textual information might be sparse or absent (e.g., in socialvideo streams).",
      "title": "223371 Meaning as collective use: Predicting semantic hashtag categories on twitter",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893101349&partnerID=40&md5=e9ca3c6a57feec85b727ef17bdb4f8a3"
    },
    {
      "abstract": "Microblogs are increasingly gaining attention as an important information source in emergency management. Nevertheless, it is still difficult to reuse this information source during emergency situations, because of the sheer amount of unstructured data. Especially for detecting small scale events like car crashes, there are only small bits of information, thus complicating the detection of relevant information. We present a solution for a real-time identification of small scale incidents using microblogs, thereby allowing to increase the situational awareness by harvesting additional information about incidents. Our approach is a machine learning algorithm combining text classification and semantic enrichment of microblogs. An evaluation based shows that our solution enables the identification of small scale incidents with an accuracy of 89% as well as the detection of all incidents published in real-time Linked Open Government Data.  Springer-Verlag 2013.",
      "title": "223374 I see a car crash: Real-time detection of small scale incidents in microblogs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893800494&partnerID=40&md5=2af179b38679e0a034a0ae142e9a24b2"
    },
    {
      "abstract": "Tolerance Rough Set Model (TRSM) has been introduced as a tool for approximation of hidden concepts in text databases. In recent years, numerous successful applications of TRSM in web intelligence including text classification, clustering, thesaurus generation, semantic indexing, and semantic search, etc., have been proposed. This paper will review the fundamental concepts of TRSM, some of its possible extensions and some typical applications of TRSM in text mining. Moreover, the architecture o a semantic information retrieval system, called SONCA, will be presented to demonstrate the main idea as well as stimulate the further research on TRSM.  2013 IEEE.",
      "title": "223375 Tolerance Rough Set Model and its applications in web intelligence",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893316395&partnerID=40&md5=c5adaece407755ac0c188426003cdd2c"
    },
    {
      "abstract": "By rapid growth of the Internet, finding desirable information would be a challenging and time consuming task. In order to tackle this issue, focused crawlers, as the ideal solution, through mining of the Web, help us to find web pages closely relevant to the desired information. For this purpose, a variety of methods are devised and implemented. Nonetheless, the majority of these methods do not favor more informative terms in a given multi-term topic. In this paper, we propose a new measure called Term Frequency-Information Content (TF-IC) to prioritize terms in a multi-term topic accordingly. Through conducted experiments, we compare our measure against both Term Frequency-Inverse Document Frequency (TF-IDF) and Latent Semantic Indexing (LSI) measures applied in focused crawlers. Experimental results indicate superiority of our measure over TF-IDF and LSI for collecting more relevant web pages of both general and specialized multi-term topics.  2013 IEEE.",
      "title": "223378 Improving multi-term topics focused crawling by introducing term Frequency-Information Content (TF-IC) measure",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84897855060&partnerID=40&md5=c7c7004bb252cf2a894bf9a83cf14a23"
    },
    {
      "abstract": "Analysis of Collocation is targeted for Natural Language Processing (NLP). From a linguistic perspective, collocation provides us with a way to place words close together in a natural manner. By this approach, we can examine deep structure of semantics through words and their situation. Although there have been some investigation based on co-occurrence, few discussion has been made about conditional collocation. In this investigation, we discuss a computational approach to extract conditional collocation by using data mining and statistical techniques. Copyright 2013 ACM.",
      "title": "223381 Conditional collocation in Japanese",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84892748378&partnerID=40&md5=0c8bcf31057e8a5fb794a0c7200488cd"
    },
    {
      "abstract": "FRED is an online tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences. In this paper we present a demonstration of such tool combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The tool is based on Boxer which implements a DRT-compliant deep parser. The logical output of Boxer enriched with semantic data from Verbnet or Framenet frames is transformed into RDF/OWL by means of a mapping model and a set of heuristics following ODP best-practice [5] of OWL ontologies and RDF data design.  Springer-Verlag 2013.",
      "title": "223385 FRED: From natural language text to RDF and OWL in one click",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893803453&partnerID=40&md5=2cd7d67e79cd159a935c9781ca4946af"
    },
    {
      "abstract": "Persistent homology is a mathematical tool from topological data analysis. It performs multi-scale analysis on a set of points and identifies clusters, holes, and voids therein. These latter topological structures complement standard feature representations, making persistent homology an attractive feature extractor for artificial intelligence. Research on persistent homology for AI is in its infancy, and is currently hindered by two issues: the lack of an accessible introduction to AI researchers, and the paucity of applications. In response, the first part of this paper presents a tutorial on persistent homology specifically aimed at a broader audience without sacrificing mathematical rigor. The second part contains one of the first applications of persistent homology to natural language processing. Specifically, our Similarity Filtration with Time Skeleton (SIFTS) algorithm identifies holes that can be interpreted as semantic tie-backs in a text document, providing a new document structure representation. We illustrate our algorithm on documents ranging from nursery rhymes to novels, and on a corpus with child and adolescent writings.",
      "title": "223386 Persistent homology: An introduction and a new text representation for natural language processing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84896062400&partnerID=40&md5=3e565d6f74437b5374e1e26b651eead0"
    },
    {
      "abstract": "Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting exible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.",
      "title": "223388 A graph-based approach to commonsense concept extraction and semantic similarity detection",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890636931&partnerID=40&md5=72912eae0083e5997ddaac061ac667c3"
    },
    {
      "abstract": "The paper models for identifying people in a 83-workers-company who are the most likely conspirators. The train of thought is that: (1) get a priority list for valuing the suspicious degree of the workers, (2) get a line separating conspirators from nonconspirators, (3) get the leader of the conspiracy. The paper first sets different values of suspicious degree for messages with various features in order to value the suspicious degree of everybody. Secondly, we optimizes the primary figure by using a formula based on weighted average method. Thirdly, we worked through each individual on the better priority list from both ends. Then, the paper used some methods of semantic analysis to better distinguish possible conspirators from the others and finally got the priority list. Next, the discriminate line is determined by using probability theory and clustering analysis theory. At last, get the leaders by the priority list and discriminate line.  2013 Springer-Verlag Berlin Heidelberg.",
      "title": "223390 Modeling for crime busting",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891530622&partnerID=40&md5=1ec9ca3608df0f77e3e95562eea65603"
    },
    {
      "abstract": "Clinical text, such as clinical trial eligibility criteria, is largely underused in state-of-the-art medical search engines due to difficulties of accurate parsing. This paper proposes a novel methodology to derive a semantic index for clinical eligibility documents based on a controlled vocabulary of frequent tags, which are automatically mined from the text. We applied this method to eligibility criteria on ClinicalTrials.gov and report that frequent tags (1) define an effective and efficient index of clinical trials and (2) are unlikely to grow radically when the repository increases. We proposed to apply the semantic index to filter clinical trial search results and we concluded that frequent tags reduce the result space more efficiently than an uncontrolled set of UMLS concepts. Overall, unsupervised mining of frequent tags from clinical text leads to an effective semantic index for the clinical eligibility documents and promotes their computational reuse.  2013 Elsevier Inc.",
      "title": "223393 Unsupervised mining of frequent tags for clinical eligibility text indexing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84888201364&partnerID=40&md5=481b43021aa26b737d9a46972a7e17b8"
    },
    {
      "abstract": "In this paper, we propose a novel constrained coclustering method to achieve two goals. First, we combine informationtheoretic coclustering and constrained clustering to improve clustering performance. Second, we adopt both supervised and unsupervised constraints to demonstrate the effectiveness of our algorithm. The unsupervised constraints are automatically derived from existing knowledge sources, thus saving the effort and cost of using manually labeled constraints. To achieve our first goal, we develop a two-sided hidden Markov random field (HMRF) model to represent both document and word constraints. We then use an alternating expectation maximization (EM) algorithm to optimize the model. We also propose two novel methods to automatically construct and incorporate document and word constraints to support unsupervised constrained clustering: 1) automatically construct document constraints based on overlapping named entities (NE) extracted by an NE extractor",
      "title": "223497 Constrained text coclustering with supervised and unsupervised constraints",
      "url": ""
    },
    {
      "abstract": "Automated annotation of scientific publications in real-world digital libraries requires dealing with challenges such as large number of concepts and training examples, multi-label training examples and hierarchical structure of concepts. BioASQ is a European project that contributes a large-scale biomedical publications corpus for working on these challenges. This paper documents the participation of our team to the large-scale biomedical semantic indexing task of BioASQ.",
      "title": "223562 Large-scale semantic indexing of biomedical publications at BioASQ",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84923116996&partnerID=40&md5=11af23d7fc40f225aae57255cbf4d055"
    },
    {
      "abstract": "Semantic Based Regularization (SBR) is a framework for injecting prior knowledge expressed as FOL clauses into a semi-supervised learning problem. The prior knowledge is converted into a set of continuous constraints, which are enforced during training. SBR employs the prior knowledge only at training time, hoping that the learning process is able to encode the knowledge via the training data into its parameters. This paper defines a collective classification approach employing the prior knowledge at test time, naturally reusing most of the mathematical apparatus developed for standard SBR. The experimental results show that the presented method outperforms state-of-the-art classification methods on multiple text categorization tasks.  2013 IEEE.",
      "title": "223568 Collective classification using semantic based regularization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84899462926&partnerID=40&md5=b23c3f805aaaf81edac9f5283ca7c79d"
    },
    {
      "abstract": "For hierarchical text classification problem, the existing prototype-based classifiers, such as k-NN, kNN Model and Centroid classifier, have achieved the aim of expected function and performance. However, due to high dimensionality and complex class structures of document data sets, they usually perform less effectively, we proposed a new method for text classification that extracts semantic labels and builds a tree structure for each level of the classification hierarchy. We compare the proposed method with KNN method, using several multi-hierarchical classification datasets. Our experimental analysis shows that our method fully considers the semantic information between the contact hierarchies of the category, as well as enhances the efficiency of text classification.  2013 Asian Network for Scientific Information.",
      "title": "223571 Study of documents multi-hierarchy categorization based on topic label and LSI",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84901799686&partnerID=40&md5=0355852bb0b496730a03f526ab1d2ce0"
    },
    {
      "abstract": "We study several techniques for representing, fusing and comparing content representations of news documents. As underlying models we consider the vector space model (both in a term setting and in a latent semantic analysis setting) and probabilistic topic models based on latent Dirichlet allocation. Content terms can be classified as topical terms or named entities, yielding several models for content fusion and comparison. All used methods are completely unsupervised. We find that simple methods can still outperform the current state-of-the-art techniques.  The Author(s) 2012.",
      "title": "223572 Representations for multi-document event clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902549834&partnerID=40&md5=ed9b440a21e3a8ac47d2493f6f0d5f3a"
    },
    {
      "abstract": "Ontologies, as representation of shared conceptualization for variety of specific domains, are the heart of the Semantic Web. In order to facilitate interoperability across multiple ontologies, we need an automatic mechanism to align ontologies. Therefore, many methods to measure similarity between concepts existing in two different ontologies are proposed. In this paper, we will enumerate these methods along with their shortcomings in each case. In information content (IC) based similarity measures, the process of IC computation for concepts is so challenging and in many cases with failing. We will propose our new approach that is based on concepts definitions. These definitions would help us to compute reliable and easy to calculate information contents for concepts. Applying these methods to the biomedical domain, using MEDLINE as corpus, International Classification of Diseases, Ninth Revision, Clinical Modification (ICD9CM) as thesaurus, and available reference standard, we will find our method outperforms other similarity measures.  Springer-Verlag Berlin Heidelberg 2013.",
      "title": "223575 Definition-based Information Content Vectors for Semantic Similarity Measurement",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904700899&partnerID=40&md5=99ce30882c1f11add96f28e343a3040d"
    },
    {
      "abstract": "Research in the Semantic Web, especially in modeling virtual communities, has provided models useful to represent the richness of these social network interactions. The SIOC (Semantically-Interlinked Online Communities) vocabulary provides concepts and properties that can be used to describe information from online communities (e.g., message boards, wikis, weblogs, etc.). However, the SIOC ontology does not consider social aspects nor the higher order semantics hidden in linkages between community members. This paper describes SIOC-SNA-DM, an extension of the SIOC vocabulary. SIOC-SNA-DMs model is tri-partite, consisting of People, Policies, and Purposes which are social aspects observable in most social communities. A challenge to using our model is how to populate these aspects, since higher order semantics from interactions need to be extracted. Thus, we explain how this population is done with advanced text mining using a latent semantic technique over a large virtual community called Plexilandia.cl with more than 2500 musicians working on the site. Our previous work, in this area, has shown how including these social aspects help to outperform results generated by state-of-the-art techniques. One of the novelties of this present work is the introduction and the elucidation of SIOC-SNA-DM, and how to populate the ontology in order to support the social aspects needed to enhance results of Social Network Mining techniques.  2013 - IOS Press and the authors. All rights reserved.",
      "title": "223588 Leveraging social network analysis with topic models and the Semantic Web (extended)",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84897997311&partnerID=40&md5=680468b6750b654ddbf015bc88a41b3b"
    },
    {
      "abstract": "The purpose of the present work is creating an intelligent system to retrieve desired documents in Marathi language. The system also focuses on providing the personalized documents in Marathi language to the end user based on their interests identified from the browsing history. This paper presents the automatic categorization of Marathi documents and the literature survey of the related work done in automatic categorization of text documents. Several supervised learning techniques are exists for the classification of text documents namely Decision trees, Support Vector machine (SVM), Neural Network, Ada Boost and Naive Bayes etc. Several clustering techniques are also available for text categorization namely K-means, Suffix Tree Clustering (STC), Semantic Online Hierarchical Clustering (SHOC), Label Induction Grouping Algorithm (LINGO) etc. In the literature survey it is found that vector space model (VSM) gives better result than probabilistic model. This paper presents categorization of the Marathi text documents using Lingo Clustering algorithm based on VSM. The data set consists of 107 Marathi documents of 3 different categories-Tourism, Health Programmes and Maharashtra festivals. The result shows that the performance of the LINGO clustering algorithm is good for categorizing the Marathi text documents. For the Marathi documents overall accuracy of the system is 91.10%.  2013 IEEE.",
      "title": "223598 Automatic text categorization of marathi documents using clustering technique",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893910265&partnerID=40&md5=3c4765a7c92476a0e45515fa8aca387d"
    },
    {
      "abstract": "The basic Bag of Words (BOW) representation, that is generally used in text documents clustering or categorization, loses important syntactic and semantic information contained in the documents. When the text document contains a lot of stop words or when they are of a short length this may be particularly problematic. In this paper, we study the contribution of incorporating syntactic features and semantic knowledge into the representation in clustering texts corpus. We investigate the quality of clusters produced when incorporating syntactic and semantic information into the representation of text documents by analyzing the internal structure of the cluster using the Davies- Bouldin (DBI) index. This paper studies and compares the quality of the clusters produced when four different sets of text representation used to cluster texts corpus. These text representations include the standard BOW representation, the standard BOW representation integrated with syntactic features, the standard BOW representation integrated with semantic background knowledge and finally the standard BOW representation integrated with both syntactic features and semantic background knowledge. Based on the experimental results, it is shown that the quality of clusters produced is improved by integrating the semantic and syntactic information into the standard bag of words representation of texts corpus.  Springer-Verlag Berlin Heidelberg 2013.",
      "title": "223599 Enrichment of BOW Representation with Syntactic and Semantic Background Knowledge",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904667610&partnerID=40&md5=ee99d183a5336c2be19a07eda5986e3b"
    },
    {
      "abstract": "To understand natural language accurately, we not only need to do natural language morphology and syntactic analysis, but also need to combine semantic knowledge and pragmatic information with a specific context. Due to short knowledge and lack in background information of conversation corpus which related to the pragmatic, there is a long way to go for computer fully understand natural language. In this paper, the pragmatic features were added to the text vector space model of language spoken conversation, and hierarchical clustering is executed. Our experimental results show that the clustering effect with pragmatic features outperforms than non-pragmatic features, and precision, recall rate and F values of the former were increased by 6.67%, 6.34% and 6.6%, respectively. It indicates that pragmatic information has played an important role in enhancing the effect of the text clustering.",
      "title": "223600 Auto-clustering of conversation corpus based on syntactic, semantic and pragmatic features",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84902158274&partnerID=40&md5=06540c9e67aa24f7a1f30f17cb538a30"
    },
    {
      "abstract": "Recent works on handling unstructured text employ multilevel filtering techniques for identifying the key terms in documents and then apply mining techniques to extract necessary information. Though these techniques are more efficient in information retrieval, they cannot be applied directly for information extraction, for documents that are more critical in context and also accuracy cannot be expected. Further, loss of hidden and significant information cannot be tolerated in data critical applications emerging based on unstructured documents. Hence, a novel idea of re-organizing the unstructured textual model into feature enriched structured graphical model by adding spatial, logical, lexical, syntactical and semantic features is proposed. The generated graph depicts relationships across the document at all levels from its micro level token to macro level document. Moreover, a structural pattern identification algorithm for generating an XML schema from the generated graph is also recommended. The experimental outcome for a real-time dataset is presented.  2013 IEEE.",
      "title": "223603 DAG based feature additive XML schema generation for unstructured text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893296348&partnerID=40&md5=5e1dbc7eeaf17c0dd92319d9c6fa7fbf"
    },
    {
      "abstract": "Most of the text retrieval and mining methods are still based on the exact word matching and they use term frequency (word or phrases) as basic measure. It captures the importance of the term in the document but may not capture the original semantics of the term, resulting in poor retrieval performance. To overcome the lack of semantic consideration, a new framework has been introduced which relies on the concept based mining model and semantic based approach. The core part of our model is concept extraction, which perform functions such as document cleaning, parts-of-speech tagging, parsing, term and phrase extraction, feasibility analysis and relation miner. Semantic net and synonym dictionary preserve the semantic relationship in the text document. The dataset used here is ACM abstract articles collected from ACM digital library. Large sets of experiments using the proposed model were conducted and the results demonstrate the accuracy of mining model using semantics preserved concepts, feasibility analysis using singular value decomposition and semantic net representation.  2013 IEEE.",
      "title": "223604 Semantics preserved concept based mining model",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84898418863&partnerID=40&md5=e85a51ffc5892bd40514f8225a0118d1"
    },
    {
      "abstract": "Recommending music that satisfies the users taste has been a challenging problem. Previous works on music recommendation system focused on the users purchase history or the content of the music. In this paper, we propose a music recommendation system purely based on analyzing textual input of the users. We first mine a large corpus of Korean radio episodes, which is written by the listener. Each episode is composed of a personal story and a song request which we assume to be somehow related to the story. We then performing probabilistic Latent Semantic Analysis (pLSA) to find similar documents and recommend music that are associated to those documents. We evaluate our system by computing the mean reciprocal rank and mean average precision, which are both conventional metrics in evaluating information retrieval systems. The result shows that music similarity and document similarity are closely correlated, and thus it is possible to recommend music purely based on text analysis.  2013 IEEE.",
      "title": "223610 Recommending music based on probabilistic latent semantic analysis on korean radio episodes",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84890122959&partnerID=40&md5=646c9edfa5a4af2c78e7dc9661188b72"
    },
    {
      "abstract": "We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof- the-Art semantic parsing baseline, yielding a 29% absolute improvement in accuracy.  2013 Association for Computational Linguistics.",
      "title": "223611 Using Semantic Unification to Generate Regular Expressions from Natural Language",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84926221440&partnerID=40&md5=cd63ca15558268ca280bf97f11dd41b1"
    },
    {
      "abstract": "Semantic frames are a rich linguistic resource. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Semantic frames help to generalize from specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task.  2013 Association for Computational Linguistics.",
      "title": "223618 Semantic frames to predict stock price movement",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84907335604&partnerID=40&md5=fc8a7d0c6379ac202f971e7cd6537663"
    },
    {
      "abstract": "The large diffusion of e-gov initiatives is increasing the attention of public administrations towards the Open Data initiative. The adoption of open data in the e-gov domain produces different advantages in terms of more transparent government, development of better public services, economic growth and social value. However, the process of data opening should adopt standards and open formats. Only in this way it is possible to share experiences with other service providers, to exploit best practices from other cities or countries, and to be easily connected to the Linked Open Data (LOD) cloud. In this paper we present CroSeR (Cross-language Service Retriever), a tool able to match and retrieve cross-language e-gov services stored in the LOD cloud. The main goal of this work is to help public administrations to connect their e-gov services to services, provided by other administrations, already connected to the LOD cloud. We adopted a Wikipedia-based semantic representation in order to overcome the problems related to match really short textual descriptions associated to the services. A preliminary evaluation on an open catalog of e-gov services showed that the adopted techniques are promising and are more effective than techniques based only on keyword representation.",
      "title": "223620 Cross-language semantic matching for discovering links to e-gov services in the LOD cloud",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922538421&partnerID=40&md5=623c690799697625acabade880c692e4"
    },
    {
      "abstract": "Positional MEDLINE (PosMed) is a web application that quickly prioritises biomedical entities such as genes and diseases based on statistical signicance of associations between these and a user-specied keyword by employing our original search engine named General and Rapid Association Study Engine (GRASE). GRASE search is modelled as an extension of SPARQL search with statistical analysis, which enables searching over semantic data including not only linked datasets but also signicant extracted semantic links over multiple biomedical documents. PosMed was originally implemented for in silico positional cloning stud- ies by prioritizing genes. Further applications include bioresource search with associated genetic functions or ontologies, and functional interpreta- Tion of gene variants found from exome sequencing of personal genomes. PosMed is available at http://database.riken.jp/PosMed/.",
      "title": "223627 PosMed: A biomedical entity prioritisation tool based on statistical inference over literature and the Semantic Web",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84908325801&partnerID=40&md5=e98034ca525bef01f19ac9f0a08b0783"
    },
    {
      "abstract": "We investigated the validity of applying topic modeling to unstructured student text data from online class discussion forums to predict students final grades. Using only student discussion data from introductory courses in biology and economics, both probabilistic latent semantic analysis (pLSA) and hierarchical latent Dirichlet allocation (hLDA) produced significantly better than chance predictions which improved with additional data collected over the duration of the course. Predictions were more accurate from hLDA than from pLSA, suggesting the feasibility and value of deriving conceptual hierarchies relevant to actual student data. Results indicate that topic modeling of studentgenerated text may provide a useful source of formative assessment to support learning and instruction.",
      "title": "223661 Predicting student outcomes from unstructured data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891750204&partnerID=40&md5=50c6a32055cb0d461d40ad9db6bcc627"
    },
    {
      "abstract": "This paper presents an ongoing work addressing the problem of opinion analysis. It takes part into a collaborative project with industrial partners, aiming at providing a professional with a help for strategic and technical intelligence. Thus, we focus on local semantic analysis rather than text or sentence classification. The purpose of our task-oriented approach is to characterize the properties of opinion statements in an applicative corpus. Inspired by linguistic models, the method we propose is a compositional one, consisting in detecting and analyzing valence shifters such as negation which contribute to the interpretation of the polarity and the intensity of opinion expressions. We describe our model and its first implementation before discussing the results of a proof-of-concept experiment focusing on adjectival expressions.",
      "title": "223679 Opinion analysis: The effect of negation on polarity and intensity",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893332407&partnerID=40&md5=87e854672b781d0f83bd3647c39b44bf"
    },
    {
      "abstract": "More than two decades have passed since the first design of the CONSTRUE system [2], a powerful rule-based model for the categorization of Reuters news. Nowadays, statistical approaches are well assessed and they allow for an easy design of text classification (TC) systems. Additionally, the Web has emphasized the need of approaches for digesting large amount of textual information and making it more easily accessible, e.g., thorough hierarchical taxonomies like Dmoz or Yahoo! categories. Surprisingly, automated approaches have not proved yet to be indispensable for such categorization processes. This suggests that the role of TC might be different from simply routing documents to different topical categories. In this paper, we provide evidence of the promising use of TC as a support for an interesting and high level human activity in the educational context. The latter refers to the selection and definition of educational programs tailored on specific needs of pupils, who sometime require particular attention and actions to solve their learning problems. TC in this context is exploited to automatically extract several aspects and properties from learning objects, i.e., didactic material, in terms of semantic labels. These can be used to organized the different pieces of material in specific didactic program, which can address specific deficiencies of pupils. The TC experiments, carried out with state-of-the-art algorithms and a small set of training data, show that automatic classifiers can easily derive labels like, didactic context, school matter, pupil difficulties and educative solution type.",
      "title": "223708 Hierarchical text classification for supporting educational programs",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84892164421&partnerID=40&md5=6b94c2ca07a4fa5ebefad9674dd1daf5"
    },
    {
      "abstract": "Sentiment analysis deals with the computational treatment of opinions expressed in written texts. The addition of the already mature semantic technologies to this field has proven to increase the results accuracy. In this work, a semantically-enhanced methodology for the annotation of sentiment polarity in financial news is presented. The proposed methodology is based on an algorithm that combines several gazetteer lists and leverages an existing financial ontology. The financial-related news are obtained from RSS feeds and then automatically annotated with positive or negative markers. The outcome of the process is a set of news organized by their degree of positivity and negativity.",
      "title": "223716 Semantic-based sentiment analysis in financial news",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84891937796&partnerID=40&md5=52d56ede4ee7692429329509b9264e1e"
    },
    {
      "abstract": "In this paper we present a methodology that makes it possible to mine a document collection from a domain without knowing the language in which the documents are written. We describe in detail a method, tools and results that can be used within a digital library context for Science Watch and Competitive Intelligence. We consider a collection associated with the aquaculture domain written in Chinese and extracted from a digital library. Based on the original coding (UNICODE) of the data and the tag marking the structure of the documents, we extract key elements (authors, phrases, etc.) from within the domain and analyse them. The results are displayed in the form of graphs and networks. We extract people networks and semantic networks before examining their evolution over a period of several years. The principles developed in this paper can be applied to any language.",
      "title": "224345 Content accessibility and semantic networks processed on foreign natural language analysis",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84905686105&partnerID=40&md5=a724da26977225fe185ad352bd5f4eb7"
    },
    {
      "abstract": "In this paper, we are concerned with the problem of auto-matic template creation for Information Extraction (IE) and we present a methodology for the creation of IE templates. Our approach proposes the semi-Automatic construction of a semantic representation of textual information based on recognition of multi-word and nested terms and Named Entities (NEs) and subsequent exploitation of term and NE con-text for the induction of Information Extraction template rules.  2000 Springer-Verlag Berlin Heidelberg.",
      "title": "224692 A term-based methodology for template creation in information extraction",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943185324&partnerID=40&md5=7ef333635d5985e27f58316c6a47d48b"
    },
    {
      "abstract": "The paper describes an experimental system for understand-ing short texts from a limited problem domain (weather forecast tele-grams written in Russian). A semantics-oriented and text type spe-cific approach to analysis is proposed which gives preference to lexical-semantic and topical coherence mechanisms in their relation to the do-main structure. The system is implemented with both classical means for knowledge representation and processing and methods of object-oriented and agent-based technique.  Springer-Verlag Berlin Heidelberg 2000.",
      "title": "224694 Approach to understanding weather forecast telegrams with agent-based technique",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943252641&partnerID=40&md5=7fd50d45858e64b2b8876f1ac539b6fa"
    },
    {
      "abstract": "This paper introduces a new type of Self-Organizing Map (SOM) for Text Categorization and Semantic Browsing. We propose a 'hyperbolic SOM' (HSOM) based on a regular tesselation of the hy-perbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. This approach is motivated by the observa-tion that hyperbolic spaces possess a geometry where the size of a neigh-borhood around a point increases exponentially and therefore provides more freedom to map a complex information space such as language into spatial relations. These theoretical findings are supported by our experi-ments, which show that hyperbolic SOMs can successfully be applied to text categorization and yield results comparable to other state-of-the-art methods. Furthermore we demonstrate that the HSOM is able to map large text collections in a semantically meaningful way and therefore allows a 'semantic browsing' of text databases.  Springer-Verlag Berlin Heidelberg 2001.",
      "title": "224698 Text categorization and semantic browsing with self-organizing maps on non-euclidean spaces",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943238553&partnerID=40&md5=29bcc7a422f5924074182b5329aa80e1"
    },
    {
      "abstract": "The need for sophisticated analysis of textual data is becoming very apparent. In the general context of knowledge discovery, text mining techniques aim to discover additional information from hidden patterns in unstructured large textual collections. Hence, we are interested especially in the extraction of the associations from unstructured databases. The objective is two fold. First, to propose a conceptual approach, based on the formal concept analysis (Ganter and Wille, 1999) and a semantic pruning, in order to discover implicit association rules, from large textual corpus. Second, to introduce an algorithm to derive additional and implicit association rules, using an associated taxonomy, from the already discovered association rules.  2001 IEEE.",
      "title": "224701 Generating implicit association rules from textual data",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84954162049&partnerID=40&md5=bd80d1f12ce24c4343b62c4c0ce27922"
    },
    {
      "abstract": "In this paper we propose a sentence analysis which relies on case-based reasoning principles. Our approach provides a semantic interpretation of a sentence in natural language, which can be used in a textual data mining process. This analysis is based on several types of knowledge: a thesaurus, a case base and a hierarchy of index. We adopt a case-based reasoning model founded on the classification principles and paths of similarity in order to guarantee the adaptability.  Springer-Verlag Berlin Heidelberg 2001.",
      "title": "224704 Sentence analysis by case-based reasoning",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84863164177&partnerID=40&md5=b46969723a185012e810ac8f2e54547d"
    },
    {
      "abstract": "In this paper we present a hybrid text categorization method based on Rough Sets theory. A central problem in good text classification for information filtering and retrieval (IF/IR) is the high dimensionality of the data. It may contain many unnecessary and irrelevant features. To cope with this problem, we propose a hybrid technique using Latent Semantic Indexing (LSI) and Rough Sets theory (RS) to alleviate this situation. Given corpora of documents and a training set of examples of classified documents, the technique locates a minimal set of co-ordinate keywords to distinguish between classes of documents, reducing the dimensionality of the keyword vectors. This simplifies the creation of knowledge-based IF/IR systems, speeds up their operation, and allows easy editing of the rule bases employed. Besides, we generate several knowledge base instead of one knowledge base for the classification of new object, hoping that the combination of answers of the multiple knowledge bases result in better performance. Multiple knowledge bases can be formulated precisely and in a unified way within the framework of RS. This paper describes the proposed technique, discusses the integration of a keyword acquisition algorithm, Latent Semantic indexing (LSI) with Rough Set-based rule generate algorithm, and provides experimental results. The test results show the hybrid method is better than the previous rough set-based approach.  2001 IEEE.",
      "title": "224708 A rough set-based hybrid method to text categorization",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-56749089936&partnerID=40&md5=b07a69d311ff0f8165910f4e838c7c5d"
    },
    {
      "abstract": "Multilayered Extended Semantic Networks (abbreviated: MultiNet) are one of the few knowledge representation paradigms along the line of Semantic Networks (abbreviated: SN) with a comprehensive, systematic, and publicly available documentation. In contrast to logically oriented meaning representation systems with their extensional interpretation, MultiNet is based on a use-theoretic operational semantics. MultiNet is distinguished from the afore-mentioned systems by fulfilling the criteria of homogeneity and cognitive adequacy. The paper describes the main features of MultiNet and the standard repertoire of representational means provided by this system. Besides of the structural information, which is manifested in the relational and functional connections between nodes of the semantic network, the conceptual representatives of MultiNet are characterized by embedding the nodes of the network into a multidimensional space of layer attributes. To warrant cognitive adequacy and universality of the knowledge representation system, every node of the SN uniquely represents a concept, while the relations between them have to be expressed by a predefined set of about 110 semantic primitive relations and functions. The knowledge representation language MultiNet has been used as an interface in several natural language processing systems. It is also suitable as an interlingua for machine translation systems.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224729 Multilayered extended semantic networks as a language for meaning representation in NLP systems",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84861081840&partnerID=40&md5=44ae0469c9eebb3ccaf4a11dc259221a"
    },
    {
      "abstract": "This paper describes two different approaches for incorporating background knowledge into nearest-neighbor text classification. Our first approach uses background text to assess the similarity between training and test documents rather than assessing their similarity directly. The second method redescribes examples using Latent Semantic Indexing on the background knowledge, assessing document similarities in this redescribed space. Our experimental results show that both approaches can improve the performance of nearest-neighbor text classification. These methods are especially useful when labeling text is a labor-intensive job and when there is a large amount of information available about a specific problem on the World Wide Web.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224730 Integrating background knowledge into nearest-neighbor text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84942780408&partnerID=40&md5=454519a932a32459b7efbee1905a64ec"
    },
    {
      "abstract": "We present a multilevel model of discussions in USENET newsgroups that includes the use of statistical and linguistic methods to obtain lexical, semantic and discourse characteristics of the text. We expose constraints that make information extraction and summarization more amenable to analysis at different levels. Our model makes use of posting structure, times of posting, time spans, and length and depth of a thread in order to extract higher-level information on subject matter, interest level, topicality, and discussion trends.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224735 A multilevel text processing model of newsgroup dynamics",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84860601172&partnerID=40&md5=340ea017d453b8a6107d1174659c8bc0"
    },
    {
      "abstract": "A large amount of information, stored in intranets and internet databases and accessed through the World-Wide Web, is organized in the form of full-text documents. Efficient retrieval of this information with regards to its meaning and content is an important problem in data mining systems for the creation, management and querying of very large such information bases. In this paper we deal with the main aspect of the problem of extracting meaning from documents, namely, with the problem of text categorization, outlining a novel and systematic approach to its solution. We present a text categorization system for non-domain specific full-text documents based on the learning and generalization capabilities of neural networks. The main contribution of this paper lies on the feature extraction methodology which, first, involves word semantic categories and not raw words as other rival approaches. As a consequence of coping with the problem of dimensionality reduction, the proposed approach introduces a novel second order approach for text categorization feature extraction by considering word semantic categories cooccurrence analysis. The suggested methodology compares favorably to widely accepted, raw word frequency based techniques in a collection of documents concerning the Dewey Decimal Classification (DDC) system. In these comparisons different Multilayer Perceptrons (MLP) algorithms as well as the Support Vector Machine (SVM), the LVQ and the conventional k-NN technique are involved.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224736 A robust meaning extraction methodology using supervised neural networks",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84943239373&partnerID=40&md5=d9464b1ab85507af07f2122d8dfbb23c"
    },
    {
      "abstract": "We describe a Classifier of email queries, which executes text categorization by topic. The specifics of our Classifier is that it allows accurate categorization of short messages containing only a few words. This advantage is achieved by executing morphological and semantic analyses of an incoming text. Specifically, the Classifier provides an efficient information extraction and takes the meaning of words into consideration. By using the hierarchically structured subject domain and classification rules, the Classifier's engine assigns an email query to the most relevant category or categories.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224738 Classification of email queries by topic: Approach based on hierarchically structured subject domain",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84947916280&partnerID=40&md5=ab4b4f8a47462324c60a77b9a4ba32ce"
    },
    {
      "abstract": "The availability of online text documents exposes readers to a vast amount of potentially valuable knowledge buried therein. The sheer scale of material has created the pressing need for automated methods of discovering relevant information without having to read it all. Hence the growing interest in recent years in Text Mining. A common approach to Text Mining is Information Extraction (IE), extracting specific types (or templates) of information from a document collection. Although many works on IE have been published, researchers have not paid much attention to evaluate the contribution of syntactic and semantic analysis using Natural Language Processing (NLP) techniques to the quality of IE results. In this work we try to quantify the contribution of NLP techniques, by comparing three strategies for IE: Naive co-occurrence, ordered co-occurrence, and the structure-driven method - a rule-based strategy that relies on syntactic analysis followed by the extraction of suitable semantic templates. We use the three strategies for the extraction of two templates from financial news stories. We show that the structure-driven strategy provides significantly better precision results than the two other strategies (80-90% for the structure-driven compared with about only 60% for the co-occurrence and ordered co-occurrence). These results indicate that a syntactical and semantic analysis is necessary if one wishes to obtain high accuracy.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224742 A comparative study of information extraction strategies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84957610779&partnerID=40&md5=c0996c8c4a08b0ef8f8ddcf5fffc7539"
    },
    {
      "abstract": "We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and TTof- rnarms aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.",
      "title": "224743 Latent dirichlet allocation",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84898995847&partnerID=40&md5=4c33c7615a7d5fbf288be7d0f2f9bafa"
    },
    {
      "abstract": "SEKE2 is a semantic expectation-based knowledge extraction system for extracting causation relations from natural language texts. It is inspired by capitalizing the human behavior of analyzing information with semantic expectations. The framework of SEKE2 consists of different kinds of generic templates organized in a hierarchical fashion. All kinds of templates are domain independent. They are robust and enable flexible changes for different domains and expected semantics. By associating a causation semantic template with a set of sentence templates, SEKE2 can extract causation knowledge from complex sentences without full-fledged syntactic parsing. To demonstrate the flexibility of SEKE2 for different domains, we study the application of causation semantic templates on two domain areas of news stories, namely, Hong Kong stock market movement and global warming.  Springer-Verlag Berlin Heidelberg 2002.",
      "title": "224745 Extracting causation knowledge from natural language texts",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35248870994&partnerID=40&md5=c0bef533d373c2e502a5dc6820773d4d"
    },
    {
      "abstract": "Document representation using the bag-of-words approach may require bringing the dimensionality of the representation down in order to be able to make effective use of various statistical classification methods. Latent Semantic Indexing (LSI) is one such method that is based on eigendecomposition of the covariance of the document-term matrix. Another often used approach is to select a small number of most important features out of the whole set according to some relevant criterion. This paper points out that LSI ignores discrimination while concentrating on representation. Furthermore, selection methods fail to produce a feature set that jointly optimizes class discrimination. As a remedy, we suggest supervised linear discriminative transforms, and report good classification results applying these to the Reuters-21578 database.  2002 IEEE.",
      "title": "224766 Discriminative features for document classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33751558390&partnerID=40&md5=b2e0cd6c6ecb2146d01bc94f8414b74e"
    },
    {
      "abstract": "Computational lexicons will undoubtedly form an essential component to make the vision of the Semantic Web a reality. In the Semantic Web scenario, ontologies are the key components to manage knowledge, whereas, in Human Language Technology (HLT), semantic description is committed to computational lexicons, which have to address the complexity of natural language. Language - and lexicons - are the gateway to knowledge: only through them can we tackle the twofold challenge of digital content availability and multilinguality. Aiming at making word content machine-understandable, computational lexicons intend to provide an explicit representation of word meaning, so that it can be directly accessed and used by computational agents, such as large-coverage parsers, modules for intelligent Information Retrieval or Information Extraction, etc In all these cases, semantic information is necessary to enhance the performance of Natural Language Processing tools, and to achieve a real understanding of text content. Moreover, in multilingual computational lexicons we find the linguistic (morpho-syntactic/seniantic) information necessary to establish links among words of different languages, information of great importance for systems performing multilingual text processing, such as Machine Translation, Cross-lingual Information Retrieval, etc. Semantic content processing lies at the heart of the Semantic Web vision. Semantic Web developers will therefore need very large repositories of words and terms - and knowledge about their relations. The cost of adding this structured and machine-understandable lexical information can be one of the factors that delays Semantic Web full deployment. A natural convergence thus exists between some of the core activities in the field of HLT and the Semantic Web long-term goals. In the last decade so-called language resources (LRs) (both lexicons and corpora, but also grammars and basic and robust software components) have been unanimously recognised as a necessary preliminary platfonn for developing an adequate HLT [3]. Large scale LRs are therefore considered as the infrastructure underlying language technology. If we broaden our perspective into the future, it is clear that the need of ever growing LRs makes it necessary to propose and promote a change in the paradigm. A radical shift in the lexical paradigm -whereby many participants add linguistic content descriptions in an open distributed and collaborative language framework - is required and proposed to make the Web usable. Existing experience in LR development proves that such a challenge can be tackled only by pursuing a truly interdisciplinary approach, and by establishing a highly advanced environment for the representation and acquisition of lexical information, open to the reuse and interchange of lexical data. As a prerequisite to enable this open language infrastructure two key issues are: (i) standards, which are critical to achieve interoperability and integration",
      "title": "224777 Language resources in the semantic web vision",
      "url": "LOW"
    },
    {
      "abstract": "In this paper, we consider ontologies as knowledge structures that specify terms, their properties and relations among them to enable knowledge extraction from texts. We represent ontologies using a graph-based model that reflect semantic relationship between concepts and apply them to text analysis and comparison. Instead of raw document comparison we compare document footprint enhanced with concepts from the ontology (using different enhancement algorithms). The result of this process may be that documents not similar prior to the enhancement become similar (semantically on some abstraction level) after the enhancement. This is because the enhancement process may introduce in the document footprint abstract concepts from the ontology. Using the ontology we can enhance the foot-prints by adding concepts that are not present in the original document. We may use synonyms for a horizontal expansion and broader terms/superclasses/types in a vertical expansion or both for that matter.  2003 IEEE.",
      "title": "224778 Ontology based semantic similarity comparison of documents",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-34547456335&partnerID=40&md5=86af1901c78652a1dad40adcce2824c5"
    },
    {
      "abstract": "The explosion in unsolicited mass electronic mail (junk e-mail) over the past decade has sparked interest in automatic filtering solutions. Traditional techniques tend to rely on header analysis, keyword/keyphrase matching and analogous rule-based predicates, and/or some probabilistic model of text generation. This paper aims instead at deciding whether or not the latent subject matter is consistent with the users interests. The underlying framework is latent semantic analysis: each e-maii is automatically classified against two semantic anchors, one for legitimate and one for junk messages. Experiments show that this approach is competitive with the state-of-the-art in e-mail classification, and potentially advantageous in real-world applications with high junk-to-legitimate ratios. The resulting technology has been successfully released in August 2002 as part of the e-mail client bundled with the MacOS 10.2 operating system.  2003 IEEE.",
      "title": "224783 Automatic junk e-mail filtering based on latent content",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84944452119&partnerID=40&md5=e88eb1ae4604722274a31cea5b21d9f4"
    },
    {
      "abstract": "A new methodology for sub-symbolic semantic encoding of words is presented. The methodology uses the WordNet lexical database and an ad hoc modified Sammon algorithm to associate a vector to each word in a semantic n-space. All words have been grouped according to the WordNet lexicographers files classification criteria: these groups have been called lexical sets. The word vector is composed by two parts: the first one, takes into account the belonging of the word to one of these lexical sets",
      "title": "224808 Sub-symbolic encoding of words",
      "url": ""
    },
    {
      "abstract": "In this paper, we present a noun phrase indexing and mining methodology for French Information Retrieval. Our assumption is that noun phrases constitute a better representation of text semantic content than single terms and can improve the effectiveness of an information retrieval system in particular when combined with a text mining process discovering associative relations with the aim of query expansion. Our experiments were conducted using two French test corpora and we compared different noun phrase indexing and mining strategies. We show that combining noun phrase indexing with associative relations can improve the information retrieval system performances, specially at low recall.  Springer-Verlag Berlin Heidelberg 2003.",
      "title": "224816 French noun phrase indexing and mining for an information retrieval system",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-0142218936&partnerID=40&md5=915a888f431ac1485f88724b55fa5bf6"
    },
    {
      "abstract": "The context of this paper is the application of unsupervised Machine Learning techniques to building ontology extraction tools for Natural Language Processing. Our method relies on exploiting large amounts of linguistically annotated text, and on linguistic concepts such as selectional restrictions and co-composition. We work with a corpus of medical texts in English. First we apply a shallow parser to the corpus to get subject-verb-object structures. We then extract verb-noun relations, and apply a clustering algorithm to them to build semantic classes of nouns. We have evaluated the adequacy of the clustering method when applied to a syntactically tagged corpus, and the relevance of the semantic content of the resulting clusters.  Springer-Verlag Berlin Heidelberg 2003.",
      "title": "224819 Is shallow parsing useful for unsupervised learning of semantic clusters?",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35248881991&partnerID=40&md5=af9a2d683a642145f13205e871d948ee"
    },
    {
      "abstract": "Link analysis methods have become popular for information access tasks, especially information retrieval, where the link information in a document collection is used to complement the traditionally used content information. However, there has been little firm evidence to confirm the utility of link information. We show that link information can be useful when the document collection has a sufficiently high link density and links are of sufficiently high quality. We report experiments on text classification of the Cora and WebKB data sets using Probabilistic Latent Semantic Analysis and Probabilistic Hypertext Induced Topic Selection. Comparison with manually assigned classes shows that link information enhances classification in data with sufficiently high link density, but is detrimental to performance at low link densities or if the quality of the links is degraded. We introduce a new frequency-based method for selecting the most useful citations from a document collection for use in the model.  Springer-Verlag Berlin Heidelberg 2003.",
      "title": "224822 When are links useful? Experiments in text classification",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35248878741&partnerID=40&md5=ec37841313e3ae3be865fe3a87b4ff06"
    },
    {
      "abstract": "Enabling navigation among a network of inter-related concepts associating conceptually relevant multilingual documents constitutes the fundamental support to global knowledge discovery. This requirement of organising multilingual document by concepts makes the goal of supporting global knowledge discovery a concept-based multilingual text categorization task. In this paper, intelligent methods for enabling concept-based multilingual text categorisation using fuzzy techniques are proposed. First, a universal concept space, encapsulating the semantic knowledge of the relationship between all multilingual terms and concepts, is generated using a fuzzy multilingual term clustering algorithm based on fuzzy c-means. Second, a fuzzy multilingual text classifier that applies the multilingual semantic knowledge for concept-based multilingual text categorization is developed using the fuzzy k-nearest neighbour classification technique. Referring to the multilingual text categorisation result as a browseable document directory, concept navigation among a multilingual document collection is facilitated.",
      "title": "224835 Fuzzy methods for knowledge discovery from multilingual text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-8344262336&partnerID=40&md5=bd1767559d911787ba5a01eaaeed760a"
    },
    {
      "abstract": "The growing size of electronically available text corpora like companies intranets or the WWW has made information access a hot topic within Computational Linguistics. Despite the success of statistical or keyword based methods, deeper Knowledge Representation (KR) techniques along with inference are often mentioned as mandatory, e.g. within the Semantic Web context, to enable e.g. better query answering based on semantical information. In this paper we try to contribute to the open question how to operationalize semantic information on a larger scale. As a basis we take the frame structures of the Berkeley FrameNet II project, which is a structured dictionary to explain the meaning of words from a lexicographic perspective. Our main contribution is a transformation of the FrameNet II frames into the answer set programming paradigm of logic programming. Because a number of different reasoning tasks are subsumed under inference in the context of natural language processing, we emphasize the flexibility of our transformation. Together with methods for automatic annotation of text documents with frame semantics which are currently developed at various sites, we arrive at an infrastructure that supports experimentation with semantic information access as is currently demanded for.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "224843 Logic programming infrastructure for inferences on FrameNet",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-22944438759&partnerID=40&md5=937a98f4462f00c809a299b4f44a59ba"
    },
    {
      "abstract": "Syntactic disambiguation frequently requires knowledge of the semantic categories of nouns, especially in languages with free word order. For example, in Spanish the phrases pinto un cuadro un pintor (lit. painted a picture a painter) and pinto un pintor un cuadro (lit. painted a painter a picture) mean the same: a painter painted a picture. The only way to tell the subject from the object is by knowing that pintor painter is a causal agent and cuadro is a thing. We present a method for extracting semantic information of this kind from existing machine-readable human-oriented explanatory dictionaries. Application of this procedure to two different human-oriented Spanish dictionaries gives additional information as compared with using solely Spanish EuroWordNet. In addition, we show the results of an experiment conducted to evaluate the similarity of word classifications using this method.  Springer-Verlag 2004.",
      "title": "224879 Extracting semantic categories of nouns for syntactic disambiguation from human-oriented explanatory dictionaries",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048815616&partnerID=40&md5=8c012a746bbde9ffdd7fb291272c453f"
    },
    {
      "abstract": "Analyzing the semantic representations of 10000 Chinese sentences and describing a new sentence analysis method that evaluates semantic preference knowledge, we create a model of semantic representation analysis based on the correspondence between lexical meanings and conceptual structures, and relations that underlie those lexical meanings. We also propose a semantical argument-head relation that combines basic conceptual structure and HeadDriven Principle. With this framework which is different from Fillmores case theory (1968) and HPSG among other, we can successfully disambiguate some troublesome sentences, and minimize the redundancy in language knowledge description for natural language processing.  Springer-Verlag 2004.",
      "title": "224894 An application of a semantic framework for the analysis of Chinese sentences",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048838156&partnerID=40&md5=1e4cabfa1ba0f82ba5949a5e1d8a30aa"
    },
    {
      "abstract": "A relation extraction system recognises pre-defined relation types between two identified entities from natural language documents. It is important for a task of automatically locating missing instances in knowledge base where the instance is represented as a triple (entity - relation - entity). A relation entry specifies a set of rules associated with the syntactic and semantic conditions under which appropriate relations would be extracted. Manually creating such rules requires knowledge from information experts and moreover, it is a time-consuming and error-prone task when the input sentences have little consistency in terms of structures and vocabularies. In this paper, we present an approach for applying a symbolic learning algorithm to sentences in order to automatically induce the extraction rules which then successfully classify a new sentence. The proposed approach takes into account semantic attributes (e.g., semantically close words and named-entities) in generalising common patterns among the sentences which enable the system to cope better with syntactically different but semantically similar sentences. Not only does this increase the number of relations extracted, but it also improves the accuracy in extracting relations by adding features which might not be discovered only with syntactic analysis. Experimental results show that this approach is effective on the sentences of the Web documents obtaining 17% higher precision and 34% higher recall values.  Springer-Verlag 2004.",
      "title": "224903 The impact of enriched linguistic annotation on the performance of extracting relation triples",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048822170&partnerID=40&md5=0bc4a742b02b8cd57a33d517f24435b6"
    },
    {
      "abstract": "In this paper, we explore the hypothesis that integrating symbolic top-down knowledge into text vector representations can improve neural exploratory bottom-up representations for text clustering. By extracting semantic rules from WordNet, terms with similar concepts are substituted with a more general term, the hypemym. This hypemym semantic relationship supplements the neural model in document clustering. The neural model is based on the extended significance vector representation approach into which predictive top-down knowledge is embedded. When we examine our hypothesis by six competitive neural models, the results are consistent and demonstrate that our robust hybrid neural approach is able to improve classification accuracy and reduce the average quantization error on 100,000 full-text articles.  Springer-Verlag 2004.",
      "title": "224904 Predictive top-down knowledge improves neural exploratory bottom-up clustering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048864024&partnerID=40&md5=f4a6c9ee980738d6edd3c19ecf9a3e7d"
    },
    {
      "abstract": "Currently, the main drawback for the development of the Semantic Web stems from the manual tagging of web pages according to a given ontology that conceptualizes its domain. This tasks is usually hard, even for experts, and it is prone to errors due to the different interpretations users can have about the same documents. In this paper we address the problem of automatically generating ontology instances starting from a collection of unstructured documents (e.g. plain texts, HTML pages, etc.). These instances will populate the Semantic Web that is described by the ontology. The proposed approach combines Information Extraction techniques, mainly entity recognition, information merging and Text Mining techniques. This approach has been successfully applied in the development of a Semantic Web for the Archaeology Research.  Springer-Verlag Berlin Heidelberg 2004.",
      "title": "224906 CRISOL: An approach for automatically populating semantic web from unstructured text collections",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-35048868515&partnerID=40&md5=b4f2f68d9179cfea04e8343899e2e18e"
    },
    {
      "abstract": "As in the previous QA@CLEF track, two separate groups at the University of Alicante participated this year using different approaches. This paper describes the work of Alicante 1 group. We have continued with the research line established in the past competition, where the main goal was to obtain a fully data-driven system based on machine learning techniques. Last year an XML framework was established in order to obtain a modular system where each component could be easily replaced or upgraded. In this framework, a question classification system based on Support Vector Machines (SVM) and surface text features was included, achieving remarkable performance in this stage. The main novelties introduced this year are focused on the information retrieval stage. First, we employed Indri as our search engine for passage retrieval. Secondly, we developed a module for passage re-ranking based on Latent Semantic Analysis (LSA). This technique provides a method for determining the similarity of meaning between words by analysis of large text corpora. In our experiments, every question was compared with every passage returned by the search engine by means of LSA in order to re-rank them. Looking at the results, this technique increased the retrieval accuracy for definition questions but it decreased accuracy on factoid ones. To take advantage of the flexibility and adaptability of our machine learning based proposal, this year we extended our participation to monolingual Spanish task and bilingual Spanish-English task. We reach a best overall accuracy of 29.47% in the first task and 20.00% in the second one.",
      "title": "225059 Experiments with LSA for passage re-ranking in question answering",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84922022249&partnerID=40&md5=44d3f343861204389fdf659dc38fe5eb"
    },
    {
      "abstract": "This paper focuses on the problem of choosing a representation of documents that can be suitable to induce more advanced semantic user profiles, in which concepts are used instead of keywords to represent user interests. We propose a method which integrates a word sense disambiguation algorithm based on the WordNet IS-A hierarchy, with two machine learning techniques to induce semantic user profiles, namely a relevance feedback method and a probabilistic one. The document representation proposed, that we called Bag-Of-Synsets improves the classic Bag-Of-Words approach, as shown by an extensive experimental session.  Springer-Verlag Berlin Heidelberg 2006.",
      "title": "225073 Learning semantic user profiles from text",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-33749396561&partnerID=40&md5=c326e04172a8a67777ceb6fe2bed9e4f"
    },
    {
      "abstract": "Lexical resources modelled after the original PrincetonWord-Net are being compiled for a considerable number of languages, however most have yet to reach a comparable level of coverage. In this paper, we show that automatically built WordNets, created from an existing WordNet in conjunction with translation dictionaries, are a suitable alternative for many applications, despite the errors introduced by the automatic building procedure. Apart from analysing the resources directly, we conducted tests on semantic relatedness assessment and cross-lingual text classification with very promising results.  University of Szeged, Department of Informatics, 2007. All rights are reserved.",
      "title": "225194 On the utility of automatically generated WordNets",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84904633732&partnerID=40&md5=8290d73fe4b77920cbadb78e842f73c0"
    },
    {
      "abstract": "This paper describes improving in Semantic Mapping [1], a feature extraction method useful to dimensionality reduction of vectors representing documents of large text collections. This method may be viewed as a specialization of the Random Mapping, method used in WEBSOM project [2]. Semantic Mapping, Random Mapping and Principal Component Analysis (PCA) are applied to categorization of document collections using Self-Organizing Maps (SOM) [3]. Semantic Mapping generated document representation as good as PCA and much better than Random Mapping.",
      "title": "225270 Dimensionality reduction of very large document collections by semantic mapping",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-84893438714&partnerID=40&md5=0d04a9c7b85d2dd797e1444c9faeea0c"
    },
    {
      "abstract": "Manual ontology building in the biomedical domain is a work-intensive task requiring the participation of both domain and knowledge representation experts. The representation of biomedical knowledge has been found of great use for biomedical text mining and integration of biomedical data. In this chapter we present an unsupervised method for learning arbitrary semantic relations between ontological concepts in the molecular biology domain. The method uses the GENIA corpus and ontology to learn relations between annotated named-entities by means of several standard natural language processing techniques. An in-depth analysis of the output evaluates the accuracy of the model and its potentials for text mining and ontology building applications. The proposed learning method does not require domain-specific optimization or tuning and can be straightforwardly applied to arbitrary domains, provided the basic processing components exist.  2008 The authors and IOS Press. All rights reserved.",
      "title": "225368 Unsupervised learning of semantic relations for molecular biology ontologies",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-77955273126&partnerID=40&md5=cb34b809882bbdfd48be5edcf28ad8ef"
    },
    {
      "abstract": "Many approaches have been proposed for providing feedback in academic writing, however, few of them are visually based. We describe a novel visualisation method for providing feedback to support formative essay assessment. The visualisation method makes use of text mining techniques to provide insight on the semantics of the topics in an essay. We propose that visualisation can be used to mitigate many of the problems associated with the subjectivity of formative essay assessment. The visualisation method involves a process of non-negative matrix factorisation (NMF), to uncover topics in an essay, followed by multidimensional scaling, to map the essay topics to a 2-dimensional representation. We evaluate our approach with a subset of the British Academic Written English corpus of 2761 assignments written by university students.  2009 The authors and IOS Press. All rights reserved.",
      "title": "225617 Analysing semantic flow in academic writing",
      "url": "http://www.scopus.com/inward/record.url?eid=2-s2.0-73149106811&partnerID=40&md5=22e00e2d3a4d13eab3ead8bd519031c1"
    },
    {
      "abstract": "",
      "title": "237423 Semantically-Guided Clustering of Text Documents via Frequent Subgraphs Discovery",
      "url": "http://link.springer.com/chapter/10.1007%2F978-3-642-21916-0 44 "
    },
    {
      "abstract": "",
      "title": "237440 Comparison of Different Graph Distance Metrics for Semantic Text Based Classification",
      "url": "http://www.scielo.org.mx/scielo.php?script=sci arttext&pid=S1870-90442014000100007"
    },
    {
      "abstract": "",
      "title": "237455 Using Graph-Kernels to Represent Semantic Information in Text Classification",
      "url": "http://link.springer.com/chapter/10.1007%2F978-3-642-03070-3 48  "
    },
    {
      "abstract": "",
      "title": "237548 A MODIFIED ANT-BASED TEXT CLUSTERING ALGORITHM WITH SEMANTIC SIMILARITY MEASURE",
      "url": "http://download.springer.com/static/pdf/334/art%253A10.1007%252Fs11518-006-5029-z.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11518-006-5029-z&token2=exp=1459728435~acl=%2Fstatic%2Fpdf%2F334%2Fart%25253A10.1007%25252Fs11518-006-5029-z.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs11518-006-5029-z*~hmac=489069f2aff5d979d25ccde862d37b20fa765733ac288ab94e3ecb25cf08a1bb "
    },
    {
      "abstract": "",
      "title": "237550 Semantic Enrichment of Text Representation with Wikipedia for Text Classification",
      "url": "http://ieeexplore.ieee.org/xpls/abs all.jsp?arnumber=5641812 "
    },
    {
      "abstract": "Abstract Semantic similarity measurement aims to determine the likeness between two text expressions that use different lexicographies for representing the same real object or idea. There are a lot of semantic similarity measures for addressing this problem. However, the best results have been achieved when aggregating a number of simple similarity measures. This means that after the various similarity values have been calculated, the overall similarity for a pair of text expressions is computed using an aggregation function of these individual semantic similarity values. This aggregation is often computed by means of statistical functions. In this work, we present CoTO (Consensus or Trade-Off) a solution based on fuzzy logic that is able to outperform these traditional approaches. ",
      "title": "261128 CoTO: A Novel Approach for Fuzzy Aggregation of Semantic Similarity Measures ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1389041716300158"
    },
    {
      "abstract": "Abstract This study aims to systematically review the cross disciplinary literature covering the time period from 1934 to January 2013 on behavioral operations in supply chain in order to identify and define the taxonomy of the research on power influences in supply chain. A list of noted journals and search results from Science Direct and Web of Knowledge, {IEEE} Xplore, and {INFORMS} (approximately 11,000 journal articles) is used to prepare content collection. Latent Semantic Analysis (LSA) is applied as the review and knowledge extraction methodology. Using the text analysis and mining method we can combine statistical methods and expert human judgment to extract knowledge in the form of key latent factors. The {LSA} based analysis gives the study a scientific grounding which helps to overcome the subjectivity of collective opinion about the trends. This approach allows proposing taxonomy of the research on power influences in supply chain. The adopted systems approach is used to find research gaps in each class of taxonomy. An emerging trend is noticed in the research of behavioral operations in supply chain. Understanding such a scholarly structure and future trends will assist researchers to assimilate the divergent developments of this multidisciplinary research in one place. This review will be beneficial for practitioners as they consider behavioral aspects in decision making. We have also studied articles related to supply chain published in Expert Systems with Applications (ESWA) journal. We have speculated what an ESWA-related community would like to see in future publications. This will encourage researchers to explore the recommended areas and publish to these outlets. ",
      "title": "261136 A journey from normative to behavioral operations in supply chain management: A review using Latent Semantic Analysis ",
      "url": "http://www.sciencedirect.com/science/article/pii/S095741741400517X"
    },
    {
      "abstract": "Abstract Cross impact analysis (CIA) consists of a set of related methodologies that predict the occurrence probability of a specific event and that also predict the conditional probability of a first event given a second event. The conditional probability can be interpreted as the impact of the second event on the first. Most of the {CIA} methodologies are qualitative that means the occurrence and conditional probabilities are calculated based on estimations of human experts. In recent years, an increased number of quantitative methodologies can be seen that use a large number of data from databases and the internet. Nearly 80% of all data available in the internet are textual information and thus, knowledge structure based approaches on textual information for calculating the conditional probabilities are proposed in literature. In contrast to related methodologies, this work proposes a new quantitative {CIA} methodology to predict the conditional probability based on the semantic structure of given textual information. Latent semantic indexing is used to identify the hidden semantic patterns standing behind an event and to calculate the impact of the patterns on other semantic textual patterns representing a different event. This enables to calculate the conditional probabilities semantically. A case study shows that this semantic approach can be used to predict the conditional probability of a technology on a different technology. ",
      "title": "261143 Quantitative cross impact analysis with latent semantic indexing ",
      "url": "http://www.sciencedirect.com/science/article/pii/S0957417413005538"
    },
    {
      "abstract": "Abstract To effectively manage the great amount of data on Arabic web pages and to enable the classification of relevant information are very important research problems. Studies on sentiment text mining have been very limited in the Arabic language because they need to involve deep semantic processing. Therefore, in this paper, we aim to retrieve machine-understandable data with the help of a Web content mining technique to detect covert knowledge within these data. We propose an approach to achieve clustering with semantic similarities. This approach comprises integrating k-means document clustering with semantic feature extraction and document vectorization to group Arabic web pages according to semantic similarities and then show the semantic annotation. The document vectorization helps to transform text documents into a semantic class probability distribution or semantic class density. To reach semantic similarities, the approach extracts the semantic class features and integrates them into the similarity weighting schema. The quality of the clustering result has evaluated the use of the purity and the mean intra-cluster distance (MICD) evaluation measures. We have evaluated the proposed approach on a set of common Arabic news web pages. We have acquired favorable clustering results that are effective in minimizing the MICD, expanding the purity and lowering the runtime. ",
      "title": "261144 Arabic web pages clustering and annotation using semantic class features ",
      "url": "http://www.sciencedirect.com/science/article/pii/S1319157814000263"
    },
    {
      "abstract": "Document classification or document categorization is one of the most studied areas in computer science due to its importance. The problem is to assign a document using its text to one or more classes or categories from a predefined set. We propose a new approach for fast text classification using randomized explicit semantic analysis (RS-ESA). It is based on a state of the art approach for word sense disambiguation based on Wikipedia, the largest encyclopedia in existence. Our method reduces Wikipedia repository using a random sample approach resulting in a throughput, which is an order of magnitude faster than the original explicit semantic analysis. RS-ESA approach has been implemented as part of the LITMUS project due to a need in classifying data from Social Media into relevant and irrelevant items with respect to landslide as a natural disaster. We demonstrate that our approach achieves 96% precision when classifying Social Media landslide data collected in December 2014. We also demonstrate the genericity of the proposed approach by using it for separating factual texts from fictional based on Wikipedia articles and fan fiction stories, where we achieve 97% in precision.",
      "title": "273069 Fast Text Classification Using Randomized Explicit Semantic Analysis",
      "url": "text analysis"
    },
    {
      "abstract": "Word-text matrix has been usually used as text representation model in text classification and text clustering. However its high dimension and sparsity reduce its expression ability. For improving its expression ability, authors mine word word relation and text-text relation, and integrate these semantic relationships into word-text matrix. The classification experiments show that these new representation models can improve the classification accuracy of text efficiently as well as represent the text information better.",
      "title": "273092 Research on Text Representation Model Integrated Semantic Relationship",
      "url": "text classification"
    },
    {
      "abstract": "With the rapid expansion of new available information presented to us online on a daily basis, text classification becomes imperative in order to classify and maintain it. Word2vec offers a unique perspective to the text mining community. By converting words and phrases into a vector representation, word2vec takes an entirely new approach on text classification. Based on the assumption that word2vec brings extra semantic features that helps in text classification, our work demonstrates the effectiveness of word2vec by showing that tf-idf and word2vec combined can outperform tf-idf because word2vec provides complementary features (e.g. semantics that tf-idf cant capture) to tf-idf. Our results show that the combination of word2vec weighted by tf-idf and tf-idf does not outperform tf-idf consistently. It is consistent enough to say the combination of the two can outperform either individually.",
      "title": "273108 Support vector machines and Word2vec for text classification with semantic features",
      "url": "support vector machines"
    },
    {
      "abstract": "In this paper we present a novel approach based on efficient text representation which employs semantic relations between words. We use singular value decomposition of the co-occurrence matrix to overcome its noise and sparseness. Thereby, we obtain a new refined co-occurrence matrix, which allows us to determine relations between words as distances in it. We use these distances as correction factors for the Bag-of-words text representation. In other words, we transform text representation vectors by inclusion relations between words. To validate our representation model, we apply it to binary classification task. We study how our model improves classification of documents, which are relevant to a given domain (topic). For this purpose, we implement Support Vector Machine and classify documents from Reuters-21578 collection. Results of our experiments demonstrate the superiority of our model.",
      "title": "273141 A new text representation model enriched with semantic relations",
      "url": "Reuters-21578 collection"
    },
    {
      "abstract": "Data from Social Networks and microblogs can provide useful information for prevention and investigation purposes, provided unstructured information is processed at both the lexical and the semantic level. The proposed methodology introduces a comprehensive Semantic Network (ConceptNet) in the interpretation chain of Twitter traffic. This additional interpretation level greatly enhances the effectiveness of semi-automated tools for monitoring purposes. In particular, the paper shows that the combined use of semantic and text-mining clustering tools also allows law-enforcement operators to early detect and track unscheduled events. Experimental results demonstrate the method effectiveness in real cases.",
      "title": "273145 Real-time monitoring of Twitter traffic by using semantic networks",
      "url": "Real-time systems"
    },
    {
      "abstract": "Researchers need to establish networks with colleagues that work similar topics, frequently, they are looking similar works by exploring free text in scientific publications in order to update them with the recent state of the art. They read the abstracts and decide whether or not it is a related and relevant work. Therefore, this paper presents an approach for linking researchers based on measuring the similarity between the abstracts of their scientific publications in English. Our approach discovers ontological relationships between free text scientific publications using statistical and semantic similarity measures. An evaluation of a gold standard data set is presented, it has shown an average of 0.6399 for Pearson product-moment correlation coefficient.",
      "title": "273152 Phrase-Based Semantic Textual Similarity for Linking Researchers",
      "url": "Pragmatics"
    },
    {
      "abstract": "Semantic Web offers many possibilities for future Web technologies. Therefore, it is a need to search for ways that can bring the huge amount of unstructured documents from current Web to Semantic Web automatically. One big challenge in searching for such ways is how to understand patterns by both humans and machine. To address this issue, we present an innovative model which interprets patterns to high level concepts. These concepts can explain the patterns meanings in a human understandable way while improving the information filtering performance. The model is evaluated by comparing it against one state-of-the-art benchmark model using standard Reuters dataset. The results show that the proposed model is successful. The significance of this model is three fold. It gives a way to interpret text mining output, provides a technique to find concepts relevant to the whole set of patterns which is an essential feature to understand the topic, and to some extent overcomes information mismatch and overload problems of existing models. This model will be very useful for knowledge based applications.",
      "title": "273220 Interpreting Discovered Patterns in Terms of Ontology Concepts",
      "url": "text analysis"
    },
    {
      "abstract": "The volume of digital information is growing considerably in the last two decades and there is currently a huge concern about obtaining this content quickly and effectively. In the health sector it is not different, to retrieve medical records that obtain relevant information about treatments and progresses of clinical conditions may speed up new patients diagnosis. In this work it is described a framework proposed for automatically indexing information based on semantics and on text mining techniques. This task should work in parallel with the insertion of data into electronic records. The original contributions come down to search engine in texts organized so as to potentiate the amount of results obtained, as evidenced by the experiments carried out. The stored information is automatically fragmented into words, which have a semantic dictionary based on a framework that enables the information retrieval through semantics.",
      "title": "273228 Automatic Knowledge Extraction Supported by Semantic Enrichment in Medical Records",
      "url": "knowledge acquisition"
    },
    {
      "abstract": "Text categorization is an important research in nature language process and content analysis. In this paper, we present latent factor SVM (LF-SVM) for text categorization which use latent factor vectors for category representation on text categorization. We prove that latent factors extracted by PLSA (probability latent semantic analysis) can span convex structure to express text category. Based on the category expression we adopt maximal margin hyper plane to divide the categories. The experiments on normal text datasets show that our motivation and algorithm are reasonable and effective.",
      "title": "273268 Latent Factor SVM for Text Categorization",
      "url": "text analysis"
    }
  ]
}
